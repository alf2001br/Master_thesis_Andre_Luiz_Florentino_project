{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3faa6a4b",
   "metadata": {},
   "source": [
    "### Faculdade de Engenharia Industrial - FEI\n",
    "\n",
    "### Centro Universitário da Fundação Educacional Inaciana \"Padre Sabóia de Medeiros\" (FEI)\n",
    "\n",
    "\n",
    "*FEI's Stricto Sensu Graduate Program in Electrical Engineering*\n",
    "\n",
    "Concentration area: ARTIFICIAL INTELLIGENCE APPLIED TO AUTOMATION AND ROBOTICS\n",
    "\n",
    "Master's thesis student Andre Luiz Florentino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd16632",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de63c0",
   "metadata": {},
   "source": [
    "# Chapter 01: Feature extraction exploration - BDLib2\n",
    "\n",
    "A few ideas were taken from: [Hackers Realm](https://www.youtube.com/watch?v=hX2sOvrWC1Q).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d9ae9",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4901fddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53326eea",
   "metadata": {},
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dfbdbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import random\n",
    "import mimetypes\n",
    "import random\n",
    "import sklearn\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import IPython.display   as ipd\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import matplotlib        as mp\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import numpy           as np\n",
    "\n",
    "from matplotlib  import pyplot as plt\n",
    "from tqdm        import tqdm\n",
    "from sklearn     import metrics\n",
    "from sklearn     import preprocessing\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.model_selection   import train_test_split, StratifiedKFold, StratifiedShuffleSplit, KFold\n",
    "from sklearn.model_selection   import GridSearchCV, learning_curve\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.neural_network  import MLPClassifier\n",
    "\n",
    "from matplotlib.patches      import Patch\n",
    "\n",
    "\n",
    "# Solution to play .ogg files in the IPython \n",
    "# https://stackoverflow.com/questions/39077987/ipython-display-audio-cannot-correctly-handle-ogg-file-type\n",
    "\n",
    "mimetypes.init()\n",
    "mimetypes.add_type('audio/ogg','.ogg')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 9)\n",
    "pd.set_option('display.width', 300)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "FRAME_SIZE = 1024\n",
    "HOP_LENGTH = 512\n",
    "SEED       = 100\n",
    "SR         = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64563cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Enter the complete path for the dataset BDLib2\n",
    "path        = \"C:\\\\Andre_Florentino\\\\03_particular\\\\04_mestrado-FEI\\\\98_dataset\\\\BDLib2\\\\\"\n",
    "\n",
    "path_pic    = \"BDLib2_results\"\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(path_pic):\n",
    "    os.makedirs(path_pic)\n",
    "\n",
    "subfolders  = next(os.walk(path))[1]\n",
    "dict_list   = []\n",
    "db_B        = pd.DataFrame(columns=['Class_categorical', 'Fold', 'File_name', 'Path'])\n",
    "nom_dataset = 'BDlib2' \n",
    "csv_file    = 'BDlib2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c539e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_files = []\n",
    "for folder in subfolders:\n",
    "    os.chdir(path + folder)\n",
    "    sounds = (glob.glob('*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f15234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to read the folders, sound files and create the dataframe\n",
    "\n",
    "def readDoc(path: str, subfolders: str):\n",
    "\n",
    "    for folder in subfolders:\n",
    "        os.chdir(path + folder)\n",
    "        sounds = (glob.glob('*.wav'))\n",
    "        for s in sounds:\n",
    "            row_dict = {'Fold': folder,\n",
    "                        'Class_categorical': s[:-6],\n",
    "                        'File_name': s,\n",
    "                        'Path': path + '\\\\' + folder + '\\\\' + s}\n",
    "            dict_list.append(row_dict)\n",
    "    return pd.DataFrame.from_dict(dict_list)\n",
    "\n",
    "\n",
    "# Export the dataframe as CSV file\n",
    "def exportCSV(path: str, db):\n",
    "\n",
    "    os.chdir(path)\n",
    "    db.to_csv(csv_file)\n",
    "    print(\"\\nCSV exported.\\nCheck the folder\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68debb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes:\n",
      "--------------------\n",
      "Class_categorical\n",
      "airplane       18\n",
      "alarms         18\n",
      "applause       18\n",
      "birds          18\n",
      "dogs           18\n",
      "motorcycles    18\n",
      "rain           18\n",
      "rivers         18\n",
      "seawaves       18\n",
      "thunders       18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of unique files..........:  180\n",
      "Total number of WAV files.............:  180\n",
      "\n",
      "CSV exported.\n",
      "Check the folder C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\n"
     ]
    }
   ],
   "source": [
    "db        = readDoc(path, subfolders)\n",
    "db_unique = np.unique(db[\"File_name\"])\n",
    "\n",
    "print(\"\\nClasses:\\n--------------------\")\n",
    "print(db[\"Class_categorical\"].value_counts())\n",
    "print(\"\\nTotal number of unique files..........: \", len(db_unique))\n",
    "print(\"Total number of WAV files.............: \", len(db))\n",
    "db\n",
    "\n",
    "exportCSV(path, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "138e44eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Andre_Florentino\\\\03_particular\\\\04_mestrado-FEI\\\\98_dataset\\\\BDLib2\\\\',\n",
       " ['fold-1', 'fold-2', 'fold-3'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path, subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0faa12e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 842E-4A17\n",
      "\n",
      " Directory of C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\n",
      "\n",
      "20/03/2024  09:02    <DIR>          .\n",
      "20/03/2024  09:02    <DIR>          ..\n",
      "23/04/2024  15:00            22.142 BDlib2.csv\n",
      "02/01/2024  08:48     2.170.871.734 BDLib2_features_augmented.pkl\n",
      "06/12/2023  21:06     1.114.646.594 BDLib2_features_augmented_no_windowing.pkl\n",
      "10/03/2024  08:01     1.301.714.062 BDLib2_features_aug_wind_CNN_2D.pkl\n",
      "20/03/2024  09:02       308.490.658 BDLib2_features_aug_wind_CNN_2D_agg.pkl\n",
      "03/03/2024  12:59        74.492.428 BDLib2_features_CNN_2D.pkl\n",
      "06/12/2023  20:45       159.340.177 BDLib2_features_original.pkl\n",
      "20/01/2019  14:06    <DIR>          fold-1\n",
      "20/01/2019  14:07    <DIR>          fold-2\n",
      "20/01/2019  14:07    <DIR>          fold-3\n",
      "               7 File(s)  5.129.577.795 bytes\n",
      "               5 Dir(s)  82.070.929.408 bytes free\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7045d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file created \n",
    "\n",
    "db = pd.read_csv(csv_file, dtype={'Class': str})\n",
    "db = db.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "924f548b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Class_categorical</th>\n",
       "      <th>File_name</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fold-1</td>\n",
       "      <td>airplane</td>\n",
       "      <td>airplane01.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fold-1</td>\n",
       "      <td>airplane</td>\n",
       "      <td>airplane02.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane02.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fold-1</td>\n",
       "      <td>airplane</td>\n",
       "      <td>airplane03.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane03.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fold-1</td>\n",
       "      <td>airplane</td>\n",
       "      <td>airplane04.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane04.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fold-1</td>\n",
       "      <td>airplane</td>\n",
       "      <td>airplane05.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane05.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>fold-3</td>\n",
       "      <td>thunders</td>\n",
       "      <td>thunders14.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders14.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>fold-3</td>\n",
       "      <td>thunders</td>\n",
       "      <td>thunders15.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders15.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>fold-3</td>\n",
       "      <td>thunders</td>\n",
       "      <td>thunders16.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders16.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>fold-3</td>\n",
       "      <td>thunders</td>\n",
       "      <td>thunders17.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders17.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>fold-3</td>\n",
       "      <td>thunders</td>\n",
       "      <td>thunders18.wav</td>\n",
       "      <td>C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders18.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Fold Class_categorical       File_name                                                                                        Path\n",
       "0    fold-1          airplane  airplane01.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane01.wav\n",
       "1    fold-1          airplane  airplane02.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane02.wav\n",
       "2    fold-1          airplane  airplane03.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane03.wav\n",
       "3    fold-1          airplane  airplane04.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane04.wav\n",
       "4    fold-1          airplane  airplane05.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-1\\airplane05.wav\n",
       "..      ...               ...             ...                                                                                         ...\n",
       "175  fold-3          thunders  thunders14.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders14.wav\n",
       "176  fold-3          thunders  thunders15.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders15.wav\n",
       "177  fold-3          thunders  thunders16.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders16.wav\n",
       "178  fold-3          thunders  thunders17.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders17.wav\n",
       "179  fold-3          thunders  thunders18.wav  C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\98_dataset\\BDLib2\\\\fold-3\\thunders18.wav\n",
       "\n",
       "[180 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24cd0713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = db['Class_categorical'].values\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "224da431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane' 'alarms' 'applause' 'birds' 'dogs' 'motorcycles' 'rain'\n",
      " 'rivers' 'seawaves' 'thunders']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y))\n",
    "print(len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create an one hot encoder vector (OHEV)\n",
    "\n",
    "def OHEV(df: list, cat_class: str):\n",
    "    \n",
    "    df_class  = df[cat_class]\n",
    "    \n",
    "    return np.array(pd.get_dummies(df_class, columns = [str], dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a OHEV for the audio classes\n",
    "\n",
    "class_enc = OHEV(db, 'Class_categorical')\n",
    "db.insert(loc = 2, column = 'Class_OHEV', value = class_enc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4098755",
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d471ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28674adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the class balancing\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "gTitle = 'BDLib2 - Number of classes = ' + str(len(pd.Series(db['Class_categorical']).unique()))\n",
    "g = sns.displot(db,x='Class_categorical', hue='Class_categorical',height = 5, aspect = 2).set(title=gTitle)\n",
    "g.set_xticklabels(rotation=45)\n",
    "g.set_titles('Number of classes')\n",
    "g._legend.remove()\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_pic + '01_Feature_extraction_01.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d16ef",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure to read all files in each subfolder\n",
    "\n",
    "def readFiles(path: str, subfolders: str):\n",
    "\n",
    "    dict_list  = []\n",
    "    for folder in subfolders:\n",
    "        os.chdir(path + folder)\n",
    "        sounds = (glob.glob('*.wav'))\n",
    "        row_dict = {'Folder': [folder],\n",
    "                    'File_name': sounds}\n",
    "        dict_list.append(row_dict)\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e914b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbFiles = readFiles(path, subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6bc675",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in dbFiles[0].items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6141dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dbFiles[0].items():\n",
    "    count = len(list(filter(bool, value)))\n",
    "    print(f\"{key} has {count} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dbFiles[0].get('Folder'),'\\n\\n',\n",
    "      len(dbFiles[0].get('Folder')), 'element(s)','\\n')\n",
    "print(dbFiles[0].get('File_name'),'\\n\\n',\n",
    "      len(dbFiles[0].get('File_name')), 'element(s)','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf491017",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path + subfolders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbFiles[0]['File_name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio('dogs06.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a724dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.loc[db['File_name'] == 'dogs06.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73212a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file as a floating point time series.\n",
    "# Data = Sound amplitude\n",
    "# Sampling rate = Number of samples per second (Hz)\n",
    "\n",
    "data, sampling_rate = librosa.load('dogs06.wav', sr = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffc871",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0291e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "librosa.display.waveshow(data, sr = sampling_rate, alpha = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.title('BDLib2 - Wave form of dog bark')\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_pic + '01_Feature_extraction_02.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb641b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio('dogs06.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc95cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde92471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure to get a folder, file name and its path ramdomly and show its wave form and content\n",
    "\n",
    "def random_data (df: list, root_path: str):\n",
    "    \n",
    "    dict_rd   = random.choice(list(df))\n",
    "    folder_rd = dict_rd['Folder'][0]\n",
    "    file_rd   = random.choice(list(dict_rd['File_name']))\n",
    "    path_rd   = os.path.join(root_path, folder_rd, file_rd)\n",
    "\n",
    "    print('Folder.......:',folder_rd, '\\n' 'File name....:', file_rd)\n",
    "    print('Path.........:', path_rd)\n",
    "    \n",
    "    data, sampling_rate = librosa.load(path_rd, sr = SR)\n",
    "    ipd.Audio(path_rd)\n",
    "\n",
    "    plt.figure(figsize = (12,4))\n",
    "    plt.title(f\"BDLib2 - Wave form of {file_rd}, ({folder_rd})\")\n",
    "    librosa.display.waveshow(data, sr = sampling_rate, alpha = 0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    return folder_rd, file_rd, path_rd, data, sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c402d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_folder, random_file, random_path, random_signal, random_sr = random_data(dbFiles, path)\n",
    "ipd.Audio(random_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8131ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = range(len(random_signal))\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047df07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61906a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = librosa.samples_to_time(samples, sr = random_sr)\n",
    "len(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531600da",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1/random_sr\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d900 = 900*d\n",
    "d900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "plt.plot(tx[10000:10900],random_signal[10000:10900])\n",
    "name        = '...\\\\' + folder + '\\\\' + random_file\n",
    "plt.title(f\"BDLib2 - Wave form of 20ms of the file {random_file}, ({random_folder})\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.savefig(path_pic + '01_Feature_extraction_03.png')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3faf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the magnitude spectrum of audio file\n",
    "\n",
    "def plot_magnitude_spectrum(path: str, folder: str, file_name, path_pic, fn_picture, t_ratio=1):\n",
    "    name        = '...\\\\' + folder + '\\\\' + file_name\n",
    "        \n",
    "    signal, sr  = librosa.load(path, sr=SR)\n",
    "    ft = np.fft.fft(signal) \n",
    "    magnitude_spectrum = np.abs(ft) \n",
    "\n",
    "    # plot magnitude spectrum \n",
    "    plt.figure(figsize = (18, 5)) \n",
    "\n",
    "    frequency = np.linspace(0, sr, len (magnitude_spectrum)) \n",
    "    num_frequency_bins = int(len(frequency) * t_ratio) \n",
    "\n",
    "    plt.plot(frequency[:num_frequency_bins], magnitude_spectrum[:num_frequency_bins])\n",
    "    plt.xlabel(\"Frequency (Hz) \" ) \n",
    "    plt.title(f\" BDLib2 - Magnitude spectrum for {name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_pic + fn_picture)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26177c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_magnitude_spectrum(random_path, random_folder, random_file, path_pic, '01_Feature_extraction_04.png', 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64471884",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_magnitude_spectrum(random_path, random_folder, random_file, path_pic, '01_Feature_extraction_05.png', 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot log-frequency power spectrogram with Short-Time Fourier Transform\n",
    "# In the example below the power was eliminated to emphasize all frequencies in the plot. Adding the power\n",
    "# after abs(X) --> abs(X)**2 would plot spectrograms with more emphasizes on the frequencies with higher power\n",
    "\n",
    "def plot_spectrogram(path: str, folder: str, file_name):\n",
    "    name        = '...\\\\' + folder + '\\\\' + file_name\n",
    "        \n",
    "    signal, sr  = librosa.load(path, sr=SR)\n",
    "    spectrogram = librosa.amplitude_to_db(np.abs(librosa.stft(signal)))\n",
    "    \n",
    "    plt.figure(figsize = (20, 8))\n",
    "    \n",
    "    librosa.display.specshow(spectrogram, sr = sr, y_axis = \"log\", x_axis='time')\n",
    "    plt.colorbar(format = \"%+2.0f dB\")\n",
    "    plt.title(f\" BDLib2 - Log-frequency spectrogram for {name}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_pic + '01_Feature_extraction_06.png')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1077014",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(random_path, random_folder, random_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c3d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_signal.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1530f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mel spectrogram and plot result\n",
    "\n",
    "def mel_spectrogram(path: str, folder: str, file_name):\n",
    "    name        = '...\\\\' + folder + '\\\\' + file_name\n",
    "    \n",
    "    signal, sr  = librosa.load(path, sr = SR)\n",
    "    X, _        = librosa.effects.trim(signal)\n",
    "    XS          = librosa.feature.melspectrogram(y=X, sr=SR, n_fft = FRAME_SIZE, hop_length = HOP_LENGTH, n_mels = 128)\n",
    "    Xdb         = librosa.power_to_db(XS, ref=np.max)\n",
    "   \n",
    "    plt.figure(figsize = (20, 8))\n",
    "    \n",
    "    librosa.display.specshow(Xdb, sr=SR, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format = \"%+2.0f dB\")\n",
    "    plt.title(f\" BDLib2 - Mel frequency spectrogram for {name}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_pic + '01_Feature_extraction_07.png')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6969b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram(random_path, random_folder, random_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e511f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration of one sample and the audio signal\n",
    "\n",
    "duration_sr     = 1 / random_sr\n",
    "duration_signal = duration_sr * len(random_signal)\n",
    "print(f\"Duration of one sample is....: {duration_sr: .6f} seconds\")\n",
    "print(f\"Duration of audio signal is..: {duration_signal: .2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dddd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the amplitude envelope\n",
    "\n",
    "def amplitude_envelope(signal, frame_size, hop_length):\n",
    "    ae = []\n",
    "\n",
    "    # Compute AE for each frame\n",
    "    \n",
    "    for i in range(0, len(signal), hop_length):\n",
    "        current_frame_ae = max(signal[i:i + frame_size])\n",
    "        ae.append(current_frame_ae)\n",
    "\n",
    "    return np.array(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff420b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ae = amplitude_envelope(random_signal, FRAME_SIZE, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the amplitude envelope with list comprehension\n",
    "\n",
    "def amplitude_envelope_ls(signal, frame_size, hop_length):\n",
    "    return np.array([max(signal[i:i + frame_size]) for i in range(0, signal.size, hop_length)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed494cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ae_ls = amplitude_envelope_ls(random_signal, FRAME_SIZE, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a842693",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c765122",
   "metadata": {},
   "outputs": [],
   "source": [
    "(random_ae == random_ae_ls).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames  = range(0, random_ae.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b470d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8224a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOP_LENGTH, FRAME_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd53f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = librosa.frames_to_time(frames, sr=random_sr, hop_length = HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10630931",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b955c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the amplitude envelope\n",
    "\n",
    "name_ae = '...\\\\' + random_folder + '\\\\' +  random_file \n",
    "frames  = range(0, random_ae.size)\n",
    "t       = librosa.frames_to_time(frames, sr = random_sr, hop_length = HOP_LENGTH)\n",
    "\n",
    "random_rmse = librosa.feature.rms(y=random_signal, frame_length = FRAME_SIZE, hop_length = HOP_LENGTH)[0]\n",
    "random_zcr  = librosa.feature.zero_crossing_rate(y=random_signal, frame_length = FRAME_SIZE, hop_length = HOP_LENGTH)[0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "librosa.display.waveshow(random_signal, sr = random_sr, alpha = 0.5)\n",
    "plt.plot(t, random_ae,   color=\"r\",      label = 'Amplitude envelope')\n",
    "plt.plot(t, random_rmse, color=\"yellow\", label = 'RMSE')\n",
    "\n",
    "# Normalized values of ZCR\n",
    "plt.plot(t, random_zcr,  color=\"magenta\", label = 'Normalized ZCR')\n",
    "\n",
    "# True values of ZCR\n",
    "#plt.plot(t, random_zcr * FRAME_SIZE,  color=\"magenta\", label = 'True ZCR')\n",
    "\n",
    "plt.title(f\" BDLib2 - Audio wave form, RMSE, ZCR and amplitude envelop for {name_ae}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_pic + '01_Feature_extraction_08.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_rmse.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31312dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36020a2b",
   "metadata": {},
   "source": [
    "## Remark on librosa.load()\n",
    "\n",
    "By default, librosa.load resampy’s high-quality mode (‘kaiser_best’).\n",
    "\n",
    "To use a faster method, set res_type=’kaiser_fast’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f036e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to parse each row and extract the defined audio feature\n",
    "\n",
    "def parser(row): \n",
    "     \n",
    "    # Load the audio file using \"kaiser_fast\" resampling method, otherwise use sr = 44100\n",
    "    x, sampling_rate = librosa.load(row.Path, sr = SR)\n",
    "\n",
    "    # Compute the mfccs (Mel Frequency Cepstral Coefficients)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y = x, sr = sampling_rate, n_mfcc = 40).T, axis = 0)\n",
    "    rmses = librosa.feature.rms(y=x, frame_length = FRAME_SIZE, hop_length = HOP_LENGTH)[0]\n",
    "    zcrs  = librosa.feature.zero_crossing_rate(y=x, frame_length = FRAME_SIZE, hop_length = HOP_LENGTH)[0]\n",
    "\n",
    "    return [mfccs, rmses, zcrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6cfcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MFCC for each audio file\n",
    "\n",
    "tqdm.pandas()\n",
    "db[['MFCC', 'RMSE', 'ZCR']] = db.progress_apply(lambda row: pd.Series(parser(row)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9015b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First attemp outputing a panda series and adding it to the dataframe column by column\n",
    "\n",
    "#tqdm.pandas()\n",
    "#feature_data = db.progress_apply(parser, axis = 1)\n",
    "\n",
    "# Adding the \"Path\" column\n",
    "#db['Path'] = feature_data.apply(lambda x: x[0])\n",
    "\n",
    "# Adding the \"MFCC\" column\n",
    "#db['MFCC'] = feature_data.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a946ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df400de",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['MFCC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['MFCC'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36608a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['ZCR'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a497cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['RMSE'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MFCC features..:\", len(db['MFCC'][2]))\n",
    "print(\"RMSE features..:\", db['RMSE'][2].size)\n",
    "print(\"ZCR features...:\", db['ZCR'][2].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd14ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MFFC is..:\", type(db['MFCC'][0]))\n",
    "print(\"RMSE is..:\", type(db['RMSE'][0]))\n",
    "print(\"ZCR is...:\", type(db['ZCR'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = db.groupby('Class_categorical')['Path'].apply(lambda s: s.sample(1))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5da5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dog = db[db['Class_categorical'] == 'dogs']\n",
    "sampled_dog = sampled_dog.groupby('Class_categorical')['Path'].apply(lambda s: s.sample(10))\n",
    "\n",
    "# Convert the resulting Series to a DataFrame and reset the index\n",
    "temp_dog_df = sampled_dog.reset_index()\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "dog_files   = temp_dog_df[['Class_categorical', 'Path']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30595751",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'category' and sample one value from each group\n",
    "sampled_data = db.groupby('Class_categorical')['Path'].apply(lambda s: s.sample(1))\n",
    "\n",
    "# Convert the resulting Series to a DataFrame and reset the index\n",
    "temp_df      = sampled_data.reset_index()\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "plot_files   = temp_df[['Class_categorical', 'Path']].values\n",
    "plot_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_files[0][0])\n",
    "print(plot_files[0][1])\n",
    "print(dog_files[0][0])\n",
    "print(dog_files[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(plot_files[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_audios = [librosa.load(plot_files[i][1],sr = SR) for i in range(10)]\n",
    "dog_audios  = [librosa.load(dog_files[i][1], sr = SR) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(plot_audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dog_audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(plot_files[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42808f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(17,7))\n",
    "plt.suptitle('Sound wave for 10 classes of BDLib2', fontsize=18)\n",
    "\n",
    "# Create the first subplot to set the shared y-axis\n",
    "plt.subplot(2, 5, 1)\n",
    "librosa.display.waveshow(plot_audios[0][0], sr=plot_audios[0][1], alpha=0.5)\n",
    "plt.title(\"Sound of \" + plot_files[0][0], fontsize=16)\n",
    "plt.xlabel('Time (s)', fontsize=16)\n",
    "plt.ylabel('Amplitude', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "xticks_locator = ticker.MultipleLocator(base=1.0)  # Set the xticks interval to 1.0 second\n",
    "plt.gca().xaxis.set_major_locator(xticks_locator)\n",
    "\n",
    "for i in range(1, 10):\n",
    "    plt.subplot(2, 5, i+1, sharey=plt.gca())\n",
    "    librosa.display.waveshow(plot_audios[i][0], sr=plot_audios[i][1], alpha=0.5)\n",
    "    plt.title(\"Sound of \" + plot_files[i][0], fontsize=16)\n",
    "    plt.xlabel('Time (s)', fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.gca().xaxis.set_major_locator(xticks_locator)  # Set the same xticks interval for all subplots\n",
    "    print(plot_files[i][1])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_09.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceca40d",
   "metadata": {},
   "source": [
    "## Fourier Transforms\n",
    "\n",
    "The Fourier transform is a mathematical technique widely used in audio signal processing to break down signals into their fundamental frequency components which produces a frequency-domain representation that reveals the relative amplitudes of different frequency elements within the signal, aiding in understanding auditory characteristics like pitch and timbre and enabling targeted manipulation.\n",
    "\n",
    "There are several types of Fourier transforms, with the discrete Fourier transform (DFT) being the most common. To enhance computational efficiency, the Fast Fourier transform (FFT) algorithm is frequently employed, but another variant, the short-time Fourier transform (STFT), it's also effective and it specializes in analyzing audio signals by segmenting them into smaller portions and applying the DFT to each segment. This approach provides a time-frequency representation, valuable for tasks such as pitch detection and audio data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902708c",
   "metadata": {},
   "source": [
    "## Windowing technique on feature extractions\n",
    "\n",
    "When shifting an audio signal from the time domain to the frequency domain using techniques like the Fast Fourier Transform (FFT), applying a windowing technique like the Hann window is essential. This process is often referred to as windowing or window function application. The main reasons to use this technique are:\n",
    "\n",
    "**Temporal Localization**: One of the primary reasons for using a window function like the Hann window is to provide temporal localization. In the time domain, audio signals are typically considered over finite time intervals or frames. If you were to directly apply the FFT to the entire frame without windowing, it assumes that the signal is periodic and extends infinitely in both directions. This assumption can lead to spectral leakage and inaccuracies in the frequency representation, as it effectively imposes abrupt boundaries at the beginning and end of the frame.\n",
    "\n",
    "**Minimizing Spectral Leakage**: The Hann window, along with other window functions like the Hamming or Blackman-Harris, tapers the signal within the frame. It smoothly reduces the signal amplitude towards the edges of the frame, which reduces spectral leakage. Spectral leakage occurs when the energy from a particular frequency component 'leaks' into adjacent frequency bins in the FFT output, making it challenging to accurately identify the frequency content of the original signal.\n",
    "\n",
    "**Improved Frequency Resolution**: By applying a Hann window or similar windowing functions, you effectively decrease the energy of frequencies near the edges of the frame, allowing you to have a better-defined frequency representation. This improved frequency resolution is particularly valuable when analyzing audio signals with distinct frequency components.\n",
    "\n",
    "**Mitigating Side Lobes**: The Hann window also helps reduce the amplitude of side lobes in the frequency domain. Side lobes are additional spectral components that appear alongside the primary frequency component you're interested in. Reducing these side lobes can make it easier to distinguish different frequency components.\n",
    "\n",
    "In summary, applying a windowing technique like the Hann window when shifting from the time domain to the frequency domain is crucial to ensure accurate and meaningful frequency representations of audio signals. It helps mitigate spectral leakage, improve frequency resolution, and minimize unwanted artifacts, making the analysis of audio signals more reliable and informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639022a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the Short Time Fourier Transform (STFT)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Magnitude plot for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    X = np.abs(librosa.stft(plot_audios[i][0], n_fft = FRAME_SIZE, hop_length = HOP_LENGTH))\n",
    "    plt.plot(X)\n",
    "    plt.xlabel(\"Frequency bins\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Short Time Fourier Transfor of \" + plot_files[i][0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_10.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594962c9",
   "metadata": {},
   "source": [
    "## Log-frequency power spectrogram\n",
    "\n",
    "A spectrogram serves as a vital time-frequency representation of signals, particularly in the context of audio signals, offering not only a graphical depiction of how a signal's frequency content evolves over time but also providing a valuable tool for the visualization and analysis of audio data. It's typically presented as a 2D image, and employs the x-axis for temporal information, the y-axis for frequency, and the color or grayscale intensity to denote the amplitude of each frequency component at a specific moment.\n",
    "\n",
    "The construction of a spectrogram entails the application of the Short-Time Fourier Transform (STFT) to the audio signal which dissects the audio into shorter segments and applies the Fourier transform to each segment, resulting in a collection of complex numbers representing the frequency characteristics of the audio within each segment. Spectrograms prove instrumental in tasks such as tracking the evolution of sound features over time, identifying attributes like pitch, timbre, and transient events, assessing spectral aspects, harmonic structures, and facilitating audio segmentation. They find widespread application in various audio-related domains, including but not limited to speech recognition, audio source separation, audio event detection, thus establishing themselves as indispensable tools within the realm of audio signal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6d147",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute and plot log-frequency power spectrogram with Short-Time Fourier Transform\n",
    "# In the example below the power was eliminated to emphasize all frequencies in the plot. Adding the power \n",
    "# after abs(X) --> abs(X)**2 would plot spectrograms with more emphasizes on the frequencies with higher power\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Log-frequency spectrogram for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    X   = librosa.stft(plot_audios[i][0], n_fft = FRAME_SIZE, hop_length = HOP_LENGTH)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X), ref=np.max)\n",
    "    librosa.display.specshow(Xdb, sr=SR, x_axis='time', y_axis='log')\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Log-frequency spectrogram of \" + plot_files[i][0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_11.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(plot_audios[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "((len(plot_audios[0][0])-FRAME_SIZE)/HOP_LENGTH) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa20681",
   "metadata": {},
   "outputs": [],
   "source": [
    "(FRAME_SIZE/2)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6af300",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac486e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency bins , frames\n",
    "\n",
    "# Frequency bins = ( Frame size / 2 ) + 1\n",
    "\n",
    "# Frames = ((samples - Frame size) / Hop lenght) + 1\n",
    "\n",
    "np.shape(Xdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = librosa.filters.mel(n_fft = 1024, sr = 44100, n_mels = 140)\n",
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_db = mel.dot(Xdb)\n",
    "mel_db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f725c",
   "metadata": {},
   "source": [
    "## Mel scale\n",
    "\n",
    "The Mel Scale represents a significant advancement in our understanding of how humans perceive frequencies. Research has revealed that our perception of frequencies is not linear; rather, we exhibit greater sensitivity to differences in lower frequencies compared to higher ones. To illustrate, distinguishing between 500 and 1000 Hz is relatively straightforward, while discerning a difference between 10,000 and 10,500 Hz proves considerably more challenging, despite the equal numerical interval between the two pairs.\n",
    "\n",
    "In 1937, the concept of the mel scale was introduced by Stevens, Volkmann, and Newmann as a unit of pitch. It was designed to ensure that equal pitch intervals sounded equally spaced to the human ear. To achieve this, a mathematical transformation is applied to frequencies, converting them into the mel scale, thereby aligning auditory perception more closely with human sensitivity to frequency differences, in other words, the Mel Scale is constructed such that sounds of equal distance from each other on the Mel Scale, also “sound” to humans as they are equal in distance from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e52955",
   "metadata": {},
   "source": [
    "## Mel spectrogram\n",
    "\n",
    "A mel spectrogram is a specialized form of spectrogram that transforms frequencies into the mel scale, which aligns more closely with human auditory perception. It achieves this by logarithmically adjusting frequencies above a specific threshold (the corner frequency). In contrast to linearly scaled spectrograms, where the vertical space between frequencies like 1,000 and 2,000Hz differs significantly from that between 2,000Hz and 4,000Hz, the mel spectrogram maintains a more uniform spacing across such ranges. This logarithmic scaling mirrors human hearing, which is better at distinguishing between similar low-frequency sounds than high-frequency ones. The computation of a mel spectrogram involves multiplying frequency-domain values by a filter bank, facilitating its representation of sound in a manner consistent with our perceptual acuity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df778883",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "& f=700\\left(10^{m / 2595}-1\\right)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbc980",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "& m=2595 \\cdot \\log \\left(1+\\frac{f}{500}\\right)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc748d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute and plot the mel frequency spectrogram\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Mel-frequency spectrogram for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "\n",
    "    # Trim the empty space in the audio file\n",
    "    #X, _ = librosa.effects.trim(plot_audios[i][0])\n",
    "    \n",
    "    XS   = librosa.feature.melspectrogram(y=plot_audios[i][0], sr=SR, n_fft = FRAME_SIZE, hop_length = HOP_LENGTH, n_mels = 140)\n",
    "    Xdb  = librosa.power_to_db(XS, ref=np.max)\n",
    "    librosa.display.specshow(Xdb, sr=SR, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Mel frequency spectrogram of \" + plot_files[i][0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_12.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3723d",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "& M = \\left(bands, \\frac{framesize}{2} +1\\right) \\cdot Y = \\left(\\frac{framesize}{2} +1, frames\\right)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(plot_audios[9][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44021698",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(XS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Xdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb662329",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade12b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_files[0][1][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f20b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Sound wave for 09 audio files of the class \"Dog bark\" of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    librosa.display.waveshow(dog_audios[i][0], sr = SR) # sr = SR or sr = dog_audios[i][1]\n",
    "    plt.title(\"Sound of dog bark \" + dog_files[i][1][-10:])\n",
    "    print(dog_files[i][1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_13.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196d8a2",
   "metadata": {},
   "source": [
    "## Mel filter banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbebb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_banks = librosa.filters.mel(n_fft = FRAME_SIZE, sr = SR, n_mels = 10)\n",
    "filter_banks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e97f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25, 10))\n",
    "librosa.display.specshow(filter_banks, sr = SR, x_axis = \"linear\")\n",
    "plt.colorbar(format = \"%2.f\")\n",
    "plt.ylabel('Mels')\n",
    "plt.title('Mel filter bank for 10 mels', fontsize = 18)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_14.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "T = np.abs(librosa.stft(plot_audios[0][0], n_fft = FRAME_SIZE, hop_length = HOP_LENGTH))\n",
    "plt.plot(T[0:,1])\n",
    "plt.xlabel(\"Frequency bins\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"BDLib2 - Short Time Fourier Transform of \" + plot_files[0][0])\n",
    "plt.savefig(path_pic + '01_Feature_extraction_15.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be428766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency bins x frames\n",
    "np.shape(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865732e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mels x Frequency bins\n",
    "filter_banks = librosa.filters.mel(n_fft = FRAME_SIZE, sr = SR, n_mels = 140)\n",
    "filter_banks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "T = np.abs(librosa.stft(plot_audios[0][0], n_fft = FRAME_SIZE, hop_length = HOP_LENGTH))\n",
    "plt.plot(T[0:,1])\n",
    "plt.plot(filter_banks.dot(T[0:,1]));\n",
    "plt.legend(labels=['Hz', 'Mel']);\n",
    "plt.xlabel(\"Frequency bins\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"BDLib2 - Mel and short Time Fourier Transform of \" + plot_files[0][0])\n",
    "plt.savefig(path_pic + '01_Feature_extraction_16.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filter_banks.dot(T[0:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ecedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_banks.dot(T[0:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d85016",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,8))\n",
    "\n",
    "#sns.histplot(df['Price'], ax=ax1)\n",
    "#sns.histplot(np.log(df['Price']),ax=ax2)\n",
    "\n",
    "T = np.abs(librosa.stft(plot_audios[0][0], n_fft = FRAME_SIZE, hop_length = HOP_LENGTH))\n",
    "ax1.plot(T)\n",
    "ax1.set_xlabel(\"Frequency bins\")\n",
    "ax1.set_ylabel(\"Amplitude\")\n",
    "\n",
    "ax2.plot(filter_banks.dot(T));\n",
    "ax2.set_xlabel(\"Mel filters\")\n",
    "ax2.set_ylabel(\"Amplitude\")\n",
    "\n",
    "plt.suptitle(\"BDLib2 - Short Time Fourier Transform of \" + plot_files[0][0] + \" and its related plot with mel filter banks\", fontsize = 18)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.90)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_17.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the mel frequency spectrogram for dog audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Mel frequency spectrogram for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    \n",
    "    # Trim the empty space in the audio file\n",
    "    # X, _ = librosa.effects.trim(dog_audios[i][0])\n",
    "    \n",
    "    XS   = librosa.feature.melspectrogram(y = plot_audios[i][0], sr=SR, n_fft = FRAME_SIZE, hop_length = HOP_LENGTH, n_mels = 140)\n",
    "    Xdb  = librosa.power_to_db(XS, ref=np.max)\n",
    "    librosa.display.specshow(Xdb, sr=SR, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Mel frequency spectrogram of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_18.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79aec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(XS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1197190",
   "metadata": {},
   "source": [
    "## Zero Crossing Rate (ZCR) \n",
    "\n",
    "The zero-crossing rate (ZCR) serves as a crucial metric in signal analysis, capturing the rate of transitions between positive and negative values within a signal. It finds widespread application in fields such as speech recognition, music information retrieval, and percussive sound classification. Additionally, ZCR proves valuable in tasks like basic pitch detection for monophonic tonal signals and voice activity detection (VAD) to ascertain the presence of human speech within audio segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(plot_files[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(plot_audios[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1/22050)*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fdb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Zero Crossings of a signal and plot the number of crossings within \"zoom\" samples\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Zero crossing rate for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "#G = []\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    zoom = 32\n",
    "    y    = plot_audios[i][0][0:zoom]\n",
    "    t    = np.linspace(0, 0.0014, num=zoom)\n",
    "    \n",
    "    librosa.display.waveshow(y)\n",
    "    \n",
    "    crossings = librosa.zero_crossings(y, pad=False)\n",
    "    plt.scatter(t[crossings],y[crossings]*0, color='r',linewidth=7.0)\n",
    "    plt.title(f\"{str(sum(crossings))} zeros crossed in {zoom} samples for \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    temp_zcr  = librosa.feature.zero_crossing_rate(y=plot_audios[i][0], frame_length = zoom*2)[0]\n",
    "    print(\"ZCR: \", str(temp_zcr[0]).ljust(15, ' '), \n",
    "          \"e \", str(sum(crossings)).ljust(4, ' '), \n",
    "          \"crossings, dividindo pelo no. de frames (\",zoom*2,\"), resulta em ZCR\", sum(crossings)/(zoom*2))\n",
    "    #G.append(temp_zcr)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_19.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb2def",
   "metadata": {},
   "source": [
    "## Harmonics and perceptrual\n",
    "\n",
    "Harmonic-Percussive Source Separation from Librosa separates an audio signal into two components: one containing the harmonic (tonal) content, typically representing pitched musical elements, and the other containing the percussive (transient) content, which often represents non-pitched or rhythmic elements.\n",
    "\n",
    "The librosa.effects.hpss function in librosa returns a tuple containing two NumPy arrays:\n",
    "\n",
    "- Harmonic Component (H): This is the harmonic part of the audio signal and typically contains pitched musical elements such as melodies and sustained notes. The harmonic component represents the tonal content of the audio.\n",
    "\n",
    "- Percussive Component (P): This is the percussive part of the audio signal and typically contains non-pitched or transient elements such as drums, percussion, and rhythmic noises. The percussive component represents the rhythmic content of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the harmonics and percussive components (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Harmonics and percussive for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    \n",
    "    y_harm, y_perc = librosa.effects.hpss(plot_audios[i][0])\n",
    "    plt.plot(y_harm, alpha=0.4);\n",
    "    plt.plot(y_perc, color = 'purple', alpha=0.8);\n",
    "    plt.title(\"Harmonics and percussive components of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_20.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6274eee",
   "metadata": {},
   "source": [
    "## Tempo\n",
    "\n",
    "It refers to the speed or pace of a musical piece and is typically measured in beats per minute (BPM). It is a fundamental attribute of music that describes how fast or slow a piece of music is played and  represents the rate at which musical beats or pulses occur within the music.\n",
    "\n",
    "- Beats: in music, a \"beat\" is a regular, recurring pulse that serves as the underlying rhythmic framework of a piece. These beats are often associated with foot-tapping or head-nodding in response to the music's rhythm. Beats can be perceived as the points in time where you would clap your hands or tap your foot in time with the music.\n",
    "\n",
    "- Beats per Minute (BPM): BPM is a measure of tempo and represents the number of beats that occur in one minute of music. For example, if a musical piece has a tempo of 120 BPM, it means that there are 120 beats in one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    \n",
    "    tempo, _ = librosa.beat.beat_track(y = plot_audios[i][0], sr = SR)\n",
    "    print(\"Sound of....: \" + plot_files[i][0])\n",
    "    print(\"File name...: \" + plot_files[i][1][-14:])\n",
    "    print('BMP.........:', \"%.2f\" %tempo )\n",
    "    print('----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e770429",
   "metadata": {},
   "source": [
    "## Spectral centroid\n",
    "\n",
    "Spectral centroid is a perceptual audio feature that characterizes the \"center of mass\" or \"center of gravity\" of the spectrum of a sound signal. It is a measure of where the \"center\" of the spectral content of an audio signal lies in terms of frequency and it is often used to describe the perceived \"brightness\" or \"timbre\" of a sound.\n",
    "\n",
    "Calculation: Spectral centroid is computed by finding the weighted mean of the frequencies in the spectrum of an audio signal. Each frequency component in the spectrum is multiplied by its magnitude, and the resulting values are then averaged to determine the centroid. The formula for calculating spectral centroid is as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "& Spectral Centroid =\\frac{\\sum_{n=0}^{N-1} f(n) \\cdot X(n)}{\\sum_{n=0}^N 1 X(n)}\n",
    "\\end{aligned}\n",
    "\n",
    "- $f(n)$ represents the frequency of the $n$-th spectral bin.\n",
    "- $X(n)$ represents the magnitude of the $n$-th spectral bin.\n",
    "- $N$ is the total number of spectral bins.\n",
    "\n",
    "\n",
    "\n",
    "Perceptual meaning: Spectral centroid is associated with the perceived \"brightness\" or \"tonal quality\" of a sound. Higher spectral centroid values indicate that the sound has a greater amount of high-frequency content, which tends to make it sound brighter or more \"trebly.\" Conversely, lower spectral centroid values indicate a greater emphasis on low-frequency content, resulting in a darker or \"bassier\" sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8273a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the spectral centroid of the audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Spectral centroid for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "# Normalising the spectral centroid for visualisation\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    librosa.display.waveshow(plot_audios[i][0], alpha=0.4, sr = SR)\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y = plot_audios[i][0], sr = SR)[0]\n",
    "    frames = range(len(spectral_centroids))\n",
    "    \n",
    "    t = librosa.frames_to_time(frames, sr = SR)\n",
    "    plt.plot(t, normalize(spectral_centroids), color='r')\n",
    "    \n",
    "    plt.title(\"Spectral centroid of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_21.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a36eb1",
   "metadata": {},
   "source": [
    "## Spectral bandwidth\n",
    "\n",
    "Spectral bandwidth is another important feature that characterizes the width or spread of the frequency content in an audio signal's spectrum. It provides information about how concentrated or dispersed the spectral energy is across different frequency components.\n",
    "\n",
    "Calculation: Spectral bandwidth is typically calculated by measuring the spread of spectral components around the spectral centroid. One common method is to compute the second central moment (variance) of the spectrum with respect to the spectral centroid. The formula for calculating spectral bandwidth is as follows:\n",
    "\n",
    "$$\n",
    "\\text { Spectral Bandwidth }=\\sqrt{\\frac{\\sum_{n=0}^{N-1}(f(n)-C)^2 \\cdot|X(n)|}{\\sum_{n=0}^N|X(n)|}}\n",
    "$$\n",
    "- $f(n)$ represents the frequency of the $n$-th spectral bin.\n",
    "- $X(n)$ represents the magnitude of the $n$-th spectral bin.\n",
    "- $N$ is the total number of spectral bins.\n",
    "- $C$ is the spectral centroid.\n",
    "\n",
    "Perceptual Meaning: Spectral bandwidth is associated with the perceived \"sharpness\" or \"clarity\" of a sound. A higher spectral bandwidth indicates that the spectral energy is spread out over a wider range of frequencies, which can result in a sound that is perceived as being more complex or noisy. In contrast, a lower spectral bandwidth suggests that the energy is concentrated around a narrow band of frequencies, leading to a cleaner or purer sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c591e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the spectral bandwidth of the audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Spectral bandwidth for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    librosa.display.waveshow(y = plot_audios[i][0], alpha=0.4, sr = SR)\n",
    "        \n",
    "    spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(y = plot_audios[i][0]+0.01, sr=SR)[0]\n",
    "    spectral_bandwidth_3 = librosa.feature.spectral_bandwidth(y = plot_audios[i][0]+0.01, sr=SR, p=3)[0]\n",
    "    spectral_bandwidth_4 = librosa.feature.spectral_bandwidth(y = plot_audios[i][0]+0.01, sr=SR, p=4)[0]\n",
    "    \n",
    "    frames = range(len(spectral_bandwidth_2))\n",
    "    t = librosa.frames_to_time(frames, sr=SR)\n",
    "    \n",
    "    plt.plot(t, normalize(spectral_bandwidth_2), color='r')\n",
    "    plt.plot(t, normalize(spectral_bandwidth_3), color='g')\n",
    "    plt.plot(t, normalize(spectral_bandwidth_4), color='y')\n",
    "    \n",
    "    plt.legend(('p = 2', 'p = 3', 'p = 4'))\n",
    "    \n",
    "    plt.title(\"Spectral bandwidth of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_22.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194bb5a",
   "metadata": {},
   "source": [
    "## Spectral rolloff\n",
    "\n",
    "Spectral rolloff is an important feature that characterizes the frequency below which a specified percentage (often a threshold like 85% or 95%) of the total spectral energy lies and it provides information about the lower boundary of the frequency range where most of the energy in an audio signal is concentrated.\n",
    "\n",
    "Calculation: Spectral rolloff is calculated by finding the frequency below which a certain percentage of the total spectral energy resides. The formula for calculating spectral rolloff typically involves finding the cumulative sum of the magnitudes of spectral components sorted in ascending order and then identifying the frequency bin below which the cumulative sum exceeds the specified percentage threshold.\n",
    "\n",
    "   Mathematically, it can be expressed as:\n",
    "$$\n",
    "Spectral\\ Rolloff = \\min \\left\\{f(n) \\mid \\frac{\\sum_{k=0}^n|X(k)|}{\\sum_{k=0}^N|X(k)|} \\geq\\ \\text{Threshold} \\right\\}\n",
    "$$\n",
    "\n",
    "- $f(n)$ represents the frequency of the $n$-th spectral bin.\n",
    "- $X(k)$ represents the magnitude of the $k$-th spectral bin.\n",
    "- $N$ is the total number of spectral bins.\n",
    "- \"Threshold\" is the specified percentage (e.g., $85 \\%$ or $95 \\%$ ).\n",
    "\n",
    "Perceptual Meaning: Spectral rolloff provides information about the point in the frequency spectrum below which most of the energy is concentrated. A higher spectral rolloff value indicates that the majority of the spectral energy is contained in lower frequencies, while a lower value suggests that energy extends into higher frequencies. This feature is often used to differentiate between sounds with different spectral characteristics, such as distinguishing between low-frequency sounds (e.g., bass instruments) and high-frequency sounds (e.g., cymbals or noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the spectral rolloff of the audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Spectral rolloff for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    librosa.display.waveshow(plot_audios[i][0],alpha=0.4, sr=SR)\n",
    "    \n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y = plot_audios[i][0]+0.01, sr=SR)[0]\n",
    "    \n",
    "    frames = range(len(spectral_rolloff))\n",
    "    t = librosa.frames_to_time(frames, sr=SR)\n",
    "    \n",
    "    plt.plot(t, normalize(spectral_rolloff), color='r')\n",
    "    \n",
    "    plt.title(\"Spectral rolloff of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_23.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608a839",
   "metadata": {},
   "source": [
    "## Spectral contrast\n",
    "\n",
    "Spectral contrast feature represents a way to describe the tonal characteristics or timbre of an audio signal. It is a feature that quantifies the difference in spectral energy between peaks and valleys in the frequency spectrum of an audio signal and it's very useful for tasks such as audio classification, speech recognition, and sound separation.\n",
    "\n",
    "Calculation: Spectral contrast features are typically computed by dividing the frequency range into multiple sub-bands and calculating the difference in energy between the maximum and minimum spectral values within each sub-band. To calculate spectral contrast features the audio signal is first transformed into the frequency domain using techniques like the Short-Time Fourier Transform (STFT). Then, for each short time frame of audio (e.g., 20-30 milliseconds), the frequency range is divided into sub-bands and measure the maximum and minimum energy within each sub-band. The result is a multi-dimensional vector representing the contrast in energy between different frequency regions.\n",
    "\n",
    "Perceptual Meaning: Spectral contrast features provide information about the distribution of spectral energy across different frequency regions in an audio signal. This means that spectral contrast features can capture the presence of distinct harmonics or timbral changes in the signal. They are particularly useful for analyzing the perception of different sound textures or distinguishing between different audio sources based on their spectral content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286780cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the spectral contrast of the audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.suptitle('Spectral contrast for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    \n",
    "    sc_temp           = np.abs(librosa.stft(plot_audios[i][0], n_fft = FRAME_SIZE, hop_length = HOP_LENGTH))**2\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(S          = sc_temp, \n",
    "                                                          sr         = SR,\n",
    "                                                          n_fft      = FRAME_SIZE,\n",
    "                                                          hop_length = HOP_LENGTH)   \n",
    "    librosa.display.specshow(spectral_contrast, x_axis='time', sr=SR)\n",
    "    plt.ylabel('Frequency bands')\n",
    "    plt.colorbar(format = \"%+2.0f dB\")\n",
    "    plt.title(\"Spectral contrast of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_24.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c1a2e",
   "metadata": {},
   "source": [
    "## Chroma feature\n",
    "\n",
    "Chroma feature represents a way to describe the harmonic content or pitch class of an audio signal. It is a feature that characterizes the distribution of musical pitches (notes) in an audio signal, ignoring the octave or absolute frequency information. It's very useful for tasks such as music analysis, chord recognition, and music genre classification.\n",
    "\n",
    "Calculation: Chroma features are typically computed using a chromagram, which is a representation of the 12 different pitch classes in Western music. These pitch classes correspond to the 12 semitones in an octave (e.g., C, C#, D, D#, E, F, F#, G, G#, A, A#, B). To calculate chroma features, you first transform the audio signal into the frequency domain using techniques like the Short-Time Fourier Transform (STFT). Then, for each short time frame of audio (e.g., 20-30 milliseconds), you measure the energy or magnitude of each of the 12 pitch classes. The result is a 12-dimensional vector representing the presence or strength of each pitch class in that time frame.\n",
    "\n",
    "Perceptual Meaning: Chroma features provide information about the pitch content of an audio signal, while disregarding the exact octave or frequency. This means that chroma features are invariant to changes in pitch that occur at higher or lower octaves. They are particularly useful for capturing the harmonic and tonal characteristics of music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the chroma feature of the audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Chroma feature for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    \n",
    "    chromagram = librosa.feature.chroma_stft(y = plot_audios[i][0], sr = SR, hop_length = HOP_LENGTH)\n",
    "    \n",
    "    librosa.display.specshow(chromagram, x_axis = 'time', y_axis = 'chroma', hop_length = HOP_LENGTH, cmap ='coolwarm', sr=SR)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.title(\"Chroma feature of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_25.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea0ff06",
   "metadata": {},
   "source": [
    "## Tonnetz\n",
    "\n",
    "This represents a way to describe the tonal relationships or harmonic structure of an audio signal that characterizes the tonal distance between musical pitches (notes) in an audio signal using the concept of tonnetz, which is a two-dimensional representation of pitch space. It's very useful for tasks such as music analysis, key estimation, and chord recognition.\n",
    "\n",
    "Calculation: Tonnetz features are typically computed by transforming the audio signal into the chroma domain using chroma feature extraction techniques. The chroma features are then converted into a two-dimensional representation called tonnetz using mathematical transformations. Tonnetz is a lattice-like structure where each node represents a pitch class and the edges represent tonal relationships between pitch classes. To calculate tonnetz features, first the chroma features is extracted using the Short-Time Fourier Transform (STFT). Then the chroma features are converted to tonnetz representation by using Euler's Tonnetz transformation. The result is a two-dimensional matrix or feature vector representing the tonal relationships between different pitch classes.\n",
    "\n",
    "Perceptual Meaning: Tonnetz features provide information about the harmonic relationships between different pitches in an audio signal. This means that tonnetz features can capture the overall tonal structure, key, and chord progressions present in the music. They are particularly useful for analyzing the harmonic content and tonal coherence of music, as well as for tasks like key identification and chord recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc48fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the tonnetz of the audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Tonnetz for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    \n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(plot_audios[i][0]), sr=SR)\n",
    "    \n",
    "    librosa.display.specshow(tonnetz, x_axis = 'time', y_axis = 'tonnetz', hop_length = HOP_LENGTH, sr=SR, cmap ='coolwarm')\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.title(\"Chroma feature of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_26.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(tonnetz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ba242",
   "metadata": {},
   "source": [
    "## MFCC (Mel Frequency Cepstral Coefficient)\n",
    "\n",
    "The concept of MFCC (Mef Frequency Cepstral Coefficient) was originally introduced in the 1980s for speech recognition tasks (DAVIS; MERMELSTEIN, 1980), aimed to address the limitations of simple spectral analysis techniques, which were found to be inadequate for representing the characteristics of human speech.\n",
    "\n",
    "The fundamental concept behind MFCC is to replicate aspects of human auditory perception, taking into account that the human ear is more attuned to variations in lower frequencies than higher ones. This understanding led to the creation of the Mel scale, which represents frequencies in a manner that aligns with human perceptual patterns. By transforming the linear frequency scale into the Mel scale, MFCC enables the capture of significant perceptual attributes of sound.\n",
    "\n",
    "The computation of MFCC involves several steps:\n",
    "1. The audio signal is divided into short overlapping frames, usually lasting around 20-40 milliseconds;\n",
    "2. A Fast Fourier Transform (FFT) is applied to each frame to obtain the magnitude spectrum;\n",
    "3. The spectrum is then converted from the linear frequency scale to the Mel-scale using a series of triangular filters;\n",
    "4. The logarithm of the filter bank energies is computed to approximate the nonlinear human perception of loudness;\n",
    "5. Finally, a discrete cosine transform (DCT) is applied to extract a compact representation of the logarithmic filter bank energies, resulting in the MFCC coefficients.\n",
    "\n",
    "MFCC has one significant limitation which is its sensitivity to noise and channel variations. If the speech signal is corrupted with noise or transmitted through a poor-quality channel, the extracted MFCC coefficients may not accurately represent the underlying speech information. Various noise-robust techniques, such as feature enhancement and model adaptation, have been proposed to mitigate this issue. Another limitation is the lack of temporal information in the MFCC representation. Since the computation of MFCC is frame-based, it does not capture the dynamics and temporal variations of the speech signal. To alleviate this limitation, dynamic features such as delta and acceleration coefficients are commonly computed by taking the derivatives of MFCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5890fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute and plot the MFCC (Mel Frequency Cepstral Coefficients) of the audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Mef Frequency Cepstral Coefficients (MFCC) for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y = plot_audios[i][0], sr = SR, n_mfcc = 40)\n",
    "    \n",
    "    librosa.display.specshow(mfccs, sr = plot_audios[i][1], x_axis = 'time', cmap ='Spectral')\n",
    "    \n",
    "    plt.title(\"MFCC (Mel Frequency Cepstral Coefficients) of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    plt.colorbar()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_27.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6c656",
   "metadata": {},
   "source": [
    "Compute and plot the $\\Delta$-MFCC (Delta Mel Frequency Cepstral Coefficients) of the audio files (9x samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the $\\Delta$-MFCC (Delta Mel Frequency Cepstral Coefficients) of the audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Delta Mef Frequency Cepstral Coefficients ($\\Delta$-MFCC) for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    \n",
    "    mfccs      = librosa.feature.mfcc(y = plot_audios[i][0], sr = SR, n_mfcc = 40)\n",
    "    mfccs_delta = librosa.feature.delta(mfccs)\n",
    "    \n",
    "    librosa.display.specshow(mfccs_delta, sr = plot_audios[i][1], x_axis = 'time', cmap ='Spectral')\n",
    "    \n",
    "    plt.title(\"$\\Delta$-MFCC (Delta Mel Frequency Cepstral Coefficients) of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_28.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4e536",
   "metadata": {},
   "source": [
    "Compute and plot the $\\Delta^2$-MFCC (Delta Mel Frequency Cepstral Coefficients) of the audio files (9x samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8603fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the $\\Delta^2$-MFCC (Delta^2 Mel Frequency Cepstral Coefficients) of the audio files (9x samples)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.suptitle('Delta^2 Mef Frequency Cepstral Coefficients ($\\Delta^2$-MFCC) for 09 classes of BDLib2', fontsize = 18)\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    \n",
    "    mfccs        = librosa.feature.mfcc(y = plot_audios[i][0], sr = SR, n_mfcc = 40)\n",
    "    mfccs_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "    \n",
    "    librosa.display.specshow(mfccs_delta2, sr = plot_audios[i][1], x_axis = 'time', cmap ='Spectral')\n",
    "    \n",
    "    plt.title(\"$\\Delta^2$-MFCC (Delta^2 Mel Frequency Cepstral Coefficients) of \" + plot_files[i][0] + '\\\\'+ plot_files[i][1][-14:])\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig(path_pic + '01_Feature_extraction_29.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75137566",
   "metadata": {},
   "source": [
    "## Windowing technique (normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[24:25]['Class_OHEV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[db['Class_categorical'] == 'dogs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = 'dogs06.wav'\n",
    "test_fn        = ['C:\\\\Andre_Florentino\\\\03_particular\\\\04_mestrado-FEI\\\\98_dataset\\\\BDLib2\\\\fold-1\\\\' + test_file_name]\n",
    "test_y_cod     = [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n",
    "teste_folder   = 'fold-1'\n",
    "SR = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, srate = librosa.load(test_fn[0], sr = SR)\n",
    "\n",
    "plt.figure(figsize = (12,4))\n",
    "librosa.display.waveshow(data, sr = srate, alpha = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Original audio file duration: ' , \"{:0.4f} s\".format(librosa.get_duration(y=data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73548204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data=data, rate=srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f221e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mel spectrogram and plot result\n",
    "\n",
    "def mel_spectrogram(path: str, folder: str, file_name:str):\n",
    "    name        = '...\\\\' + folder + '\\\\' + file_name\n",
    "    path        = path + folder + '\\\\' + file_name\n",
    "    \n",
    "    signal, srate = librosa.load(path, sr = SR)\n",
    "    XS            = librosa.feature.melspectrogram(y          = signal,\n",
    "                                                   sr         = srate, \n",
    "                                                   n_fft      = FRAME_SIZE,\n",
    "                                                   hop_length = HOP_LENGTH, \n",
    "                                                   n_mels     = 60)\n",
    "    print(XS.shape)\n",
    "    Xdb         = librosa.power_to_db(XS, ref=np.max)\n",
    "   \n",
    "    plt.figure(figsize = (20, 8))\n",
    "    \n",
    "    img = librosa.display.specshow(Xdb, sr=srate, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(img, format = \"%+2.0f dB\")\n",
    "    plt.title(f\" BDLib2 - Mel frequency spectrogram for {name}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram(path, teste_folder, test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6316bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an audio file wave form and its log mel spectrogram\n",
    "\n",
    "def plot_wave_spec(path: str, folder: str, fn: str):\n",
    "    \n",
    "    name  = '...\\\\' + folder + '\\\\' + fn\n",
    "    path  = path + folder + '\\\\' + fn\n",
    "    title = f\" BDLib2 - Audio wave form and Log mel frequency spectrogram for {name}\"\n",
    "    \n",
    "    signal, srate = librosa.load(path, sr = SR)\n",
    "    XS            = librosa.feature.melspectrogram(y          = signal, \n",
    "                                                   sr         = srate, \n",
    "                                                   n_fft      = FRAME_SIZE, \n",
    "                                                   hop_length = HOP_LENGTH, \n",
    "                                                   n_mels     = 60)\n",
    "    Xdb          = librosa.power_to_db(XS, ref=np.max)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, sharex=False, figsize=(12, 8))\n",
    "    plt.suptitle(title)\n",
    "    g1 = librosa.display.waveshow(signal, sr = srate, alpha = 0.5, ax=ax[0])\n",
    "    g2 = librosa.display.specshow(Xdb, sr = srate, x_axis = 'frames', y_axis = 'mel', ax=ax[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_pic + '01_Feature_extraction_30.png')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe67aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave_spec(path, teste_folder, test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fbf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing function \n",
    "\n",
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield int(start), int(start + window_size)\n",
    "        start += (window_size / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c41428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment the audio and extract the features\n",
    "        \n",
    "def window_audio(files: list, SR: int ):\n",
    "\n",
    "    frames         = 41\n",
    "    window_size    = 512 * (frames - 1)\n",
    "    audio_windowed = []\n",
    "\n",
    " \n",
    "    for f in tqdm(files):\n",
    "        sound_clip, srate  = librosa.load(f, sr = SR)\n",
    "        \n",
    "        for (start, end) in windows(sound_clip, window_size):\n",
    "            if(len(sound_clip[start:end]) == window_size):\n",
    "                \n",
    "                # Window the audio\n",
    "                signal  = sound_clip[start:end]\n",
    "\n",
    "                # Appends to array\n",
    "                audio_windowed.append(signal)\n",
    "    \n",
    "    return np.array(audio_windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd224915",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed = window_audio(test_fn, SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f698247",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3caa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data=windowed[1], rate=SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original, _ = librosa.load(test_fn[0], sr = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data=original, rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbe699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an windowed audio file wave form \n",
    "  \n",
    "def plot_wave_spec_windowed(data: list, SR: int, sharey=True):\n",
    "    fig, axes = plt.subplots(int(np.ceil(data.shape[0] / 3)), 3, \n",
    "                             figsize=(20, (np.ceil(data.shape[0] / 3))*6),\n",
    "                             layout='constrained', \n",
    "                             sharey=sharey)\n",
    "    fig.suptitle('BDlib2 - Wave form for the windowed audio file', fontsize = 12)\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < data.shape[0]:\n",
    "            img = librosa.display.waveshow(data[i], sr=SR, ax=ax, alpha=0.5)\n",
    "        else:\n",
    "            ax.axis('off')  # Turn off empty subplots\n",
    "\n",
    "    #fig.tight_layout()\n",
    "    fig.savefig(path_pic + '01_Feature_extraction_31.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5914e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave_spec_windowed(windowed, SR, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the windowed log mel spectrogram\n",
    "    \n",
    "def plot_mel_spec_windowed(data: list, sharey=True):\n",
    "    fig, axes = plt.subplots(int(np.ceil(np.shape(data)[0] / 3)), 3, \n",
    "                             figsize=(20, (np.ceil(data.shape[0] / 3))*6),\n",
    "                             layout='constrained', \n",
    "                             sharey=sharey)\n",
    "    plt.suptitle('BDlib2 - Log mel spectrogram for the windowed audio file', fontsize = 12)\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    \n",
    "    spectrograms = []\n",
    "    global_min  = 0\n",
    "    global_max  = 0\n",
    "\n",
    "    for audio in data:\n",
    "        XS = librosa.feature.melspectrogram(y          = audio,\n",
    "                                            sr         = SR, \n",
    "                                            n_fft      = FRAME_SIZE, \n",
    "                                            hop_length = HOP_LENGTH,\n",
    "                                            n_mels     = 60)\n",
    "        Xdb = librosa.power_to_db(XS, ref=np.median)\n",
    "        spectrograms.append(Xdb)\n",
    "        \n",
    "        segment_min = np.min(Xdb)\n",
    "        segment_max = np.max(Xdb)\n",
    "        global_min  = min(global_min, segment_min)\n",
    "        global_max  = max(global_max, segment_max)\n",
    "        \n",
    "        if global_min > global_max:\n",
    "            temp = global_min\n",
    "            global_min = global_max\n",
    "            global_max = temp\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < np.shape(data)[0]:\n",
    "            im  = librosa.display.specshow(spectrograms[i],  \n",
    "                                           sr = SR, \n",
    "                                           x_axis = 'frames', \n",
    "                                           y_axis = 'mel', \n",
    "                                           ax=ax,\n",
    "                                           vmin=global_min, \n",
    "                                           vmax=global_max,\n",
    "                                           cmap='magma')\n",
    "            plt.colorbar(im, format = \"%+2.0f dB\")\n",
    "        else:\n",
    "            ax.axis('off')  # Turn off empty subplots\n",
    "            \n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(path_pic + '01_Feature_extraction_32.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mel_spec_windowed(windowed, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 2x1 grid of subplots to show the wave form and the log mel spectrogram\n",
    "def plot_audio_overview(data, SR, ax, sharey):\n",
    "    \n",
    "    sub_axes = axes[i, j]\n",
    "    \n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax_waveform    = sub_axes.inset_axes([0, 0.7, 1, .3])\n",
    "    ax_spectrogram = sub_axes.inset_axes([0, 0.0, 1, .7])\n",
    "    \n",
    "    librosa.display.waveshow(data, sr = SR, alpha = 0.5, ax=ax_waveform)\n",
    "    ax_waveform.get_xaxis().set_visible(False)\n",
    "    ax_waveform.get_yaxis().set_visible(False)\n",
    "    ax_waveform.set_title(f'Frame {(i * 3 + j)+1}')\n",
    "    \n",
    "    if sharey:\n",
    "        ax_waveform.set_ylim(-1, 1)  \n",
    "\n",
    "    XS = librosa.feature.melspectrogram(y          = data, \n",
    "                                        sr         = SR,\n",
    "                                        n_fft      = FRAME_SIZE,\n",
    "                                        hop_length = HOP_LENGTH, \n",
    "                                        n_mels     = 60)\n",
    "    Xdb = librosa.power_to_db(XS, ref=np.max)\n",
    "\n",
    "    librosa.display.specshow(Xdb,  \n",
    "                             sr = SR, \n",
    "                             x_axis   = 'time', \n",
    "                             y_axis   = 'mel', \n",
    "                             ax       = ax_spectrogram,\n",
    "                             cmap     ='magma')\n",
    "    ax_spectrogram.get_xaxis().set_visible(False)\n",
    "    ax_spectrogram.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6aff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea142932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x3 grid of subplots\n",
    "\n",
    "fig, axes = plt.subplots(int(np.ceil(windowed.shape[0] / 3)), 3, figsize=(12, 16), layout='constrained')\n",
    "plt.suptitle('BDlib2 - Wave form and log mel spectrogram for the windowed audio file', fontsize = 12)\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# Loop through the 3x3 grid\n",
    "for i in range(0, int(np.ceil(windowed.shape[0] / 3))):\n",
    "    for j in range(3):\n",
    "        \n",
    "        # Extracting each row of data\n",
    "        if i * 3 + j < windowed.shape[0]:\n",
    "            row_data = windowed[i * 3 + j]  \n",
    "            plot_audio_overview(row_data, SR, axes[i, j], sharey=False)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "#plt.tight_layout()\n",
    "plt.savefig(path_pic + '01_Feature_extraction_33.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6926e5d",
   "metadata": {},
   "source": [
    "## Splitting an audio signal into non-silent intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a86464",
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_threshold = 40\n",
    "\n",
    "# Split the audio into non-silent intervals\n",
    "non_silent_intervals = librosa.effects.split(original, \n",
    "                                             top_db       = silence_threshold,\n",
    "                                             frame_length = FRAME_SIZE, \n",
    "                                             hop_length   = HOP_LENGTH)\n",
    "\n",
    "# Extract non-silent segments from the original audio data\n",
    "non_silent_audio = []\n",
    "\n",
    "for interval in non_silent_intervals:\n",
    "    start, end = interval\n",
    "    non_silent_audio.extend(original[start:end])\n",
    "\n",
    "# Convert the list back to a NumPy array\n",
    "non_silent_audio_array = np.array(non_silent_audio)\n",
    "\n",
    "# Desired window size in seconds\n",
    "desired_window_size = 10\n",
    "\n",
    "# Calculate the number of samples needed to fit the desired window size\n",
    "desired_samples = int(desired_window_size * SR)\n",
    "\n",
    "# Repeat the non-silent audio array to fit the desired window size\n",
    "extended_audio = np.tile(non_silent_audio_array, desired_samples // len(non_silent_audio_array) + 1)\n",
    "\n",
    "# Truncate the extended audio to match the desired duration\n",
    "extended_audio = extended_audio[:desired_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data=original, rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd502868",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data=non_silent_audio_array, rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a89166",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data=extended_audio, rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adeb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Audio file duration: ' , \"{:0.4f} s\".format(librosa.get_duration(y=original)))\n",
    "print('Audio file duration: ' , \"{:0.4f} s\".format(librosa.get_duration(y=non_silent_audio_array)))\n",
    "print('Audio file duration: ' , \"{:0.4f} s\".format(librosa.get_duration(y=extended_audio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wave(data, SR, picname):\n",
    "\n",
    "    title = 'BDLib2 - Audio file duration: ' + \"{:0.4f} s\".format(librosa.get_duration(y=data))\n",
    "    plt.figure(figsize = (12,5))\n",
    "    librosa.display.waveshow(data, sr = SR, alpha = 0.5)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_pic + picname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e059909",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(original,SR, '01_Feature_extraction_34.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(non_silent_audio_array, SR, '01_Feature_extraction_35.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(extended_audio, SR, '01_Feature_extraction_36.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment the audio and extract the features\n",
    "        \n",
    "def window_audio_file(data: list, SR: int ):\n",
    "\n",
    "    frames         = 41\n",
    "    window_size    = 512 * (frames - 1)\n",
    "    audio_windowed = []\n",
    "       \n",
    "    for (start, end) in windows(data, window_size):\n",
    "        if(len(data[start:end]) == window_size):\n",
    "\n",
    "            # Window the audio\n",
    "            signal  = data[start:end]\n",
    "\n",
    "            # Appends to array\n",
    "            audio_windowed.append(signal)\n",
    "    \n",
    "    return np.array(audio_windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286484a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_windowed = window_audio_file(extended_audio, SR)\n",
    "audio_file_windowed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f43601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x3 grid of subplots\n",
    "\n",
    "fig, axes = plt.subplots(int(np.ceil(audio_file_windowed.shape[0] / 3)), 3, figsize=(12, 35), layout='constrained')\n",
    "plt.suptitle('BDlib2 - Wave form and log mel spectrogram for the windowed audio file', fontsize = 12)\n",
    "\n",
    "plot_idxs = audio_file_windowed.shape[0]\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# Loop through the 3x3 grid\n",
    "for i in range(0, int(np.ceil(audio_file_windowed.shape[0] / 3))):\n",
    "    for j in range(3):\n",
    "        # Extracting each row of data\n",
    "        if i * 3 + j < audio_file_windowed.shape[0]:\n",
    "            row_data = audio_file_windowed[i * 3 + j]\n",
    "            plot_audio_overview(row_data, SR, axes[i, j], sharey=False)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "#plt.tight_layout()\n",
    "plt.savefig(path_pic + '01_Feature_extraction_37.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba4ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = db['Path'].values\n",
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cod = np.array(db.Class_OHEV.to_list())\n",
    "y_cat = np.array(db.Class_categorical.to_list())\n",
    "fold  = np.array(db.Fold.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdcf1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bcc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c977cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ac984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(fn: list):\n",
    "    \n",
    "    audio_array       = []\n",
    "    silence_threshold = 60 #dB (Threshold below the reference db, in this case np.max from amplitude to db resulted 80dB\n",
    "    time_length       = 10  #s\n",
    "    target_samples    = int(time_length * SR) # Calculate the number of samples needed to fit the target time length\n",
    "    \n",
    "    for i, f in tqdm(enumerate(fn, start=0)):\n",
    "        \n",
    "        non_silent_audio_array = []\n",
    "        extended_audio         = []\n",
    "        \n",
    "        # Load the audio files\n",
    "        rawdata, _ = librosa.load(f, sr = SR)\n",
    "\n",
    "        # Split the audio into non-silent intervals\n",
    "        non_silent_intervals = librosa.effects.split(rawdata, \n",
    "                                                     top_db       = silence_threshold,\n",
    "                                                     frame_length = FRAME_SIZE, \n",
    "                                                     hop_length   = HOP_LENGTH)\n",
    "\n",
    "        # Extract non-silent segments from the original audio data\n",
    "        non_silent_audio  = []\n",
    "        for interval in non_silent_intervals:\n",
    "            start, end = interval\n",
    "            non_silent_audio.extend(rawdata[start:end])\n",
    "\n",
    "        # Convert the list back to a NumPy array\n",
    "        non_silent_audio_array = np.array(non_silent_audio)\n",
    "\n",
    "        # Repeat the non-silent audio array to fit the target time length\n",
    "        extended_audio = np.tile(non_silent_audio_array, target_samples // len(non_silent_audio_array) + 1)\n",
    "\n",
    "        # Truncate the extended audio to match the desired duration\n",
    "        audio_array.append(extended_audio[:target_samples])\n",
    "        \n",
    "        #print('Raw audio file duration.....: ' , \"{:0.4f} s\".format(librosa.get_duration(y=rawdata)))\n",
    "        #print('Trimmed audio file duration.: ' , \"{:0.4f} s\".format(librosa.get_duration(y=non_silent_audio_array)))\n",
    "        #print('Extended audio file duration: ' , \"{:0.4f} s\".format(librosa.get_duration(y=audio_array[i])))\n",
    "        #print('---------------')\n",
    "\n",
    "    return audio_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = pre_processing(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c68f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(fn[15], rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2671f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd688cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_data[0], rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a348a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment the audio dataset considering 44 frames @22.050 Hz --> ~0,99s per window\n",
    "        \n",
    "def windowing(data: list, SR: int, y: list, fold_nr: list ):\n",
    "\n",
    "    frames         = 44\n",
    "    window_size    = 512 * (frames - 1)\n",
    "    audio_windowed = []\n",
    "    labels         = []\n",
    "    folds          = []\n",
    "    \n",
    "    for audio, label, fold in zip(tqdm(data), y, fold_nr):\n",
    "    \n",
    "        for (start, end) in windows(audio, window_size):\n",
    "            if(len(audio[start:end]) == window_size):\n",
    "\n",
    "                # Window the audio\n",
    "                signal  = audio[start:end]\n",
    "\n",
    "                # Appends to array\n",
    "                audio_windowed.append(signal)\n",
    "                labels.append(label)\n",
    "                folds.append(fold)\n",
    "\n",
    "    \n",
    "    return np.array(audio_windowed), np.array(labels), np.array(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, folds = windowing(audio_data, SR, y_cat, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aff0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb488139",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0924c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data = X[0], rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Raw audio file duration.....: ' , \"{:0.4f} s\".format(librosa.get_duration(y=X[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41baa253",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = (512 * (41 - 1)) * (1 / SR)\n",
    "print('Raw audio file duration.....: ' , \"{:0.4f} s\".format(check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1/22050)*20480"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e3f15",
   "metadata": {},
   "source": [
    "## Augmentation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_spectrogram_signal(signal:list):\n",
    "\n",
    "    XS = librosa.feature.melspectrogram(y          = signal,\n",
    "                                        sr         = srate,\n",
    "                                        n_fft      = FRAME_SIZE,\n",
    "                                        hop_length = HOP_LENGTH, \n",
    "                                        n_mels     = 60)\n",
    "\n",
    "    Xdb = librosa.power_to_db(XS, ref=np.max)\n",
    "   \n",
    "    plt.figure(figsize = (20, 8))\n",
    "    \n",
    "    img = librosa.display.specshow(Xdb, sr=SR, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(img, format = \"%+2.0f dB\")\n",
    "    plt.title(f\" BDLib2 - Mel frequency spectrogram \")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b8fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal_and_augmented_signal(signal, augmented_signal, sr, title, pic_name):\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 2, figsize = (12,7))\n",
    "    fig.suptitle('BDLib2 - Comparison of wave forms (original x augmented)')\n",
    "    librosa.display.waveshow(signal, sr = sr, ax = ax[0])\n",
    "    ax[0].set(title=f\"{title} - BDLib2 - Original signal\")\n",
    "    librosa.display.waveshow(augmented_signal, sr = sr, ax = ax[1])\n",
    "    ax[1].set(title=f\"{title} - BDLib2 - Augmented signal\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_pic + pic_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stretch(signal, rate=1):\n",
    "    \n",
    "    return librosa.effects.time_stretch(signal, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4befb005",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_signal_stretch = time_stretch(original, rate=0.9)\n",
    "plot_signal_and_augmented_signal(original, augmented_signal_stretch, SR, 'Time stretch', '01_Feature_extraction_38.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(original, rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b5c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(augmented_signal_stretch, rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38095f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(augmented_signal_stretch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28fdd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mel_spectrogram_signal(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93fface",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram_signal(augmented_signal_stretch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff32c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_scale(signal, sr, num_semitones=0):\n",
    "    \n",
    "    return librosa.effects.pitch_shift(signal, sr = sr, n_steps = num_semitones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b34a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_signal_pitch = pitch_scale(original, SR, num_semitones=-5 )\n",
    "plot_signal_and_augmented_signal(original, augmented_signal_pitch, SR, 'Pitch shifting', '01_Feature_extraction_39.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(original, rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(augmented_signal_pitch, rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b95282",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram_signal(augmented_signal_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962debf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shifting(signal):\n",
    "    \n",
    "    start_ = int(np.random.uniform(-4800,4800))\n",
    "    print('time shift: ',start_)\n",
    "    if start_ >= 0:\n",
    "        audio_time_shift = np.r_[signal[start_:], np.random.uniform(-0.001,0.001, start_)]\n",
    "    else:\n",
    "        audio_time_shift = np.r_[np.random.uniform(-0.001,0.001, -start_), signal[:start_]]\n",
    "    \n",
    "    return audio_time_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9866579",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_signal_time = time_shifting(original)\n",
    "plot_signal_and_augmented_signal(original, augmented_signal_time, SR, 'Time shifting', '01_Feature_extraction_40.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c24111",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(original, rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd18d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(augmented_signal_time, rate = SR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd581182",
   "metadata": {},
   "source": [
    "## Output of this notebook\n",
    "\n",
    "The output of this notebook was a class the pre-process a list of audio files and its labels into an array of audio data for feature extraction and its labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fe150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "\n",
    "ROOT        = \"C:\\\\Andre_Florentino\\\\03_particular\\\\04_mestrado-FEI\\\\98_dataset\\\\\"\n",
    "DATASET     = \"BDLib2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to load the dataset BDLib2\n",
    "\n",
    "Input : root path where the dataset is saved and its name\n",
    "Output: DataFrame\n",
    "\n",
    "DataFrame columns:\n",
    "- Class\n",
    "- Fold\n",
    "- File_name\n",
    "- Path\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class loadBDLib2:\n",
    "    \n",
    "    def __init__(self, root:str, dataset:str):\n",
    "        self.root        = root\n",
    "        self.dataset     = dataset\n",
    "        self.path        = self.root + self.dataset + \"\\\\\"\n",
    "        self.subfolders  = next(os.walk(self.path))[1]\n",
    "        \n",
    "        self.dict_list   = []\n",
    "        self.db_B        = pd.DataFrame(columns=['Class_categorical', 'Fold', 'File_name', 'Path'])\n",
    "        self.csv_file    = self.dataset + \".csv\"\n",
    "\n",
    "        self._readDoc()\n",
    "        self.OHEV()\n",
    "        self._exportCSV()\n",
    "        \n",
    "\n",
    "    # Procedure to read the folders, sound files and create the dataframe\n",
    "    def _readDoc(self):\n",
    "        for folder in self.subfolders:\n",
    "            os.chdir(self.path + folder)\n",
    "            sounds = (glob.glob('*.wav'))\n",
    "            for s in sounds:\n",
    "                row_dict = {'Fold': folder,\n",
    "                            'Class_categorical': s[:-6],\n",
    "                            'File_name': s,\n",
    "                            'Path': self.path + '\\\\' + folder + '\\\\' + s}\n",
    "                self.dict_list.append(row_dict)\n",
    "        self.db_B = pd.DataFrame.from_dict(self.dict_list)\n",
    "\n",
    "        \n",
    "    # Create the One Hot Encoder Vector (OHEV)\n",
    "    def OHEV(self):\n",
    "        df_class  = self.db_B['Class_categorical']\n",
    "        class_enc = np.array(pd.get_dummies(df_class, columns = [str], dtype=int))\n",
    "        self.db_B.insert(loc = 2, column = 'Class_OHEV', value = class_enc.tolist())\n",
    "    \n",
    "\n",
    "    # Export the dataframe as CSV file\n",
    "    def _exportCSV(self):\n",
    "        os.chdir(self.path)\n",
    "        self.db_B.to_csv(self.csv_file)\n",
    "        print(\"\\nCSV exported.\\nCheck the folder :\" ,self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc22a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    loadBDLib2 = loadBDLib2(ROOT, DATASET)\n",
    "    db      = loadBDLib2.db_B\n",
    "    \n",
    "    print(\"\\nClasses:\\n--------------------\")\n",
    "    print(db[\"Class_categorical\"].value_counts())\n",
    "    print(\"\\nTotal number of unique files..........: \", len(np.unique(db[\"File_name\"])))\n",
    "    print(\"Total number of OGG files.............: \", len(db))\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MT_audioPP import audioPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove silence samples, augment and normalize (windowed) the audio files\n",
    "\n",
    "audioPP = audioPP(db['Path'].values,\n",
    "                  db['Class_categorical'].values, \n",
    "                  db['Class_OHEV'].values, \n",
    "                  db['Fold'].values,\n",
    "                  time_length = 10,\n",
    "                  threshold = 120,\n",
    "                  aug = False,\n",
    "                  windowing = False,\n",
    "                  frames = 44)\n",
    "\n",
    "X, y_string, y_OHEV, folds  = audioPP.audio_windowed, audioPP.labels_cat_wind, audioPP.labels_cod_wind, audioPP.labels_fold_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c750d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_OHEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b16876",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37627d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = Counter(folds)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data = X[150], rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ede88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data = X[151], rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6131dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data = X[152], rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe26faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data = X[153], rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8841eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data = X[154], rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data = X[155], rate = SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal_and_augmented_signal(X[150], X[151], SR, 'Consecutive audios', '01_Feature_extraction_41.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_aug = pd.DataFrame({'Audio' : X, 'Class_categorical' : y_string, 'Class_OHEV' : y_OHEV, 'Fold' : folds})\n",
    "db_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b6b2d",
   "metadata": {},
   "source": [
    "# End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d14bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
