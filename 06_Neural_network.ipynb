{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3faa6a4b",
   "metadata": {},
   "source": [
    "### Faculdade de Engenharia Industrial - FEI\n",
    "\n",
    "### Centro Universitário da Fundação Educacional Inaciana \"Padre Sabóia de Medeiros\" (FEI)\n",
    "\n",
    "\n",
    "*FEI's Stricto Sensu Graduate Program in Electrical Engineering*\n",
    "\n",
    "Concentration area: ARTIFICIAL INTELLIGENCE APPLIED TO AUTOMATION AND ROBOTICS\n",
    "\n",
    "Master's thesis student Andre Luiz Florentino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd16632",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d17ec4",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b237808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "------------------------------------------------------------------------------------------\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "<function is_built_with_cuda at 0x00000265CB4030D0>\n",
      "/device:GPU:0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "pd = tf.config.experimental.list_physical_devices()\n",
    "for i in pd:\n",
    "    print(i)\n",
    "print('------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "\n",
    "print(tf.test.is_built_with_cuda)\n",
    "# <function is_built_with_cuda at 0x000001AA24AFEC10>\n",
    "\n",
    "print(tf.test.gpu_device_name())\n",
    "# /device:GPU:0\n",
    "\n",
    "#gvd = tf.config.get_visible_devices()\n",
    "for j in tf.config.get_visible_devices():\n",
    "    print(j)\n",
    "# PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
    "# PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices()\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de63c0",
   "metadata": {},
   "source": [
    "# Chapter 6: Neural networks\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4504379",
   "metadata": {},
   "source": [
    "## Importe modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f29048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import warnings\n",
    "import itertools\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import pandas     as pd\n",
    "import seaborn    as sns\n",
    "import numpy      as np\n",
    "\n",
    "from matplotlib  import pyplot  as plt\n",
    "from keras       import backend as K\n",
    "\n",
    "from tqdm                        import tqdm\n",
    "from collections                 import Counter\n",
    "\n",
    "from sklearn                     import metrics\n",
    "from sklearn.model_selection     import train_test_split\n",
    "from sklearn.metrics             import confusion_matrix, classification_report\n",
    "from sklearn.decomposition       import PCA\n",
    "\n",
    "from tensorflow                  import keras\n",
    "from tensorflow.keras.models     import Sequential, load_model\n",
    "from tensorflow.keras.layers     import Dense, Dropout, Conv1D, GlobalAveragePooling1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks             import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers          import l2\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "pd.set_option('display.max_columns', 9)\n",
    "pd.set_option('display.width', 300)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "cmap_cm   = plt.cm.Blues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0dbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# For the picture names\n",
    "pic_first_name = '06_Neural_network_'\n",
    "\n",
    "# For Librosa\n",
    "FRAME_SIZE  = 1024\n",
    "HOP_LENGTH  = 512\n",
    "SEED        = 1000\n",
    "SR          = 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53326eea",
   "metadata": {},
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097f8ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-) ESC-10\n",
      "2-) BDLib2\n",
      "3-) US8K\n",
      "4-) US8K_AV\n",
      "\n",
      "Select the dataset: 4\n"
     ]
    }
   ],
   "source": [
    "# Select the dataset\n",
    "\n",
    "opc = 0\n",
    "while str(opc) not in '1234':\n",
    "    print()\n",
    "    print(\"1-) ESC-10\")\n",
    "    print(\"2-) BDLib2\")\n",
    "    print(\"3-) US8K\")\n",
    "    print(\"4-) US8K_AV\")\n",
    "\n",
    "\n",
    "    opc = input(\"\\nSelect the dataset: \")\n",
    "    if opc.isdigit():\n",
    "        opc = int(opc)\n",
    "    else:\n",
    "        opc = 0\n",
    "\n",
    "if opc == 1:\n",
    "\n",
    "    path        = os.path.join(current_path, \"_dataset\", \"ESC-10\")\n",
    "    path_pic    = os.path.join(current_path, \"ESC-10_results\")\n",
    "    path_models = os.path.join(current_path, \"ESC-10_saved_models\")\n",
    "    \n",
    "   \n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'ESC-10' \n",
    "    csv_file    = 'ESC-10.csv'\n",
    "    fold        = '1'\n",
    "\n",
    "    pkl_features          = 'ESC-10_features_original.pkl'\n",
    "    pkl_aug_features      = 'ESC-10_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'ESC-10_features_augmented.pkl'\n",
    "\n",
    "    \n",
    "if opc == 2:\n",
    "    \n",
    "    path        = os.path.join(current_path, \"_dataset\", \"BDLib2\")\n",
    "    path_pic    = os.path.join(current_path, \"BDLib2_results\")\n",
    "    path_models = os.path.join(current_path, \"BDLib2_saved_models\")\n",
    "\n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'BDLib2' \n",
    "    csv_file    = 'BDLib2.csv'\n",
    "    fold        = 'fold-1'\n",
    "\n",
    "    pkl_features          = 'BDLib2_features_original.pkl'\n",
    "    pkl_aug_features      = 'BDLib2_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'BDLib2_features_augmented.pkl'\n",
    "\n",
    "    \n",
    "if opc == 3:\n",
    "    \n",
    "    path        = os.path.join(current_path, \"_dataset\", \"US8K\")\n",
    "    path_pic    = os.path.join(current_path, \"US8K_results\")\n",
    "    path_models = os.path.join(current_path, \"US8K_saved_models\")\n",
    "    \n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'US8K' \n",
    "    csv_file    = 'US8K.csv'\n",
    "    fold        = '1'\n",
    "    \n",
    "    pkl_features          = 'US8K_features_original.pkl'\n",
    "    pkl_aug_features      = 'US8K_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'US8K_features_windowed.pkl' # augmented and windowed makes no sense. Dataset is already quite large\n",
    "    \n",
    "\n",
    "if opc == 4:\n",
    "\n",
    "    path        = os.path.join(current_path, \"_dataset\", \"US8K_AV\")\n",
    "    path_pic    = os.path.join(current_path, \"US8K_AV_results\")\n",
    "    path_models = os.path.join(current_path, \"US8K_AV_saved_models\")\n",
    "\n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'US8K_AV' \n",
    "    csv_file    = 'US8K_AV.csv'\n",
    "    fold        = '1'\n",
    "\n",
    "    pkl_features          = 'US8K_AV_features_original.pkl'\n",
    "    pkl_aug_features      = 'US8K_AV_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'US8K_AV_features_windowed.pkl' # augmented and windowed makes no sense. Dataset is already quite large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c339e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_file_number(folder: str):\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) and f.startswith(pic_first_name)]\n",
    "    if not files:\n",
    "        return 1\n",
    "    else:\n",
    "        numbers = [int(f.split('.')[0].split('_')[-1]) for f in files]\n",
    "        return max(numbers) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9974f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MT_loadDataset import loadDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32aca6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes:\n",
      "--------------------\n",
      "Class_categorical\n",
      "dog_bark            1000\n",
      "children_playing    1000\n",
      "background          1000\n",
      "siren                929\n",
      "car_horn             429\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of unique files..........:  4358\n",
      "Total number of AUDIO files...........:  4358\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Folder_name</th>\n",
       "      <th>Class_OHEV</th>\n",
       "      <th>Class_categorical</th>\n",
       "      <th>...</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>...</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>...</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>...</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>...</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>...</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>159.522205</td>\n",
       "      <td>163.522205</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>181.142431</td>\n",
       "      <td>183.284976</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4355</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>242.691902</td>\n",
       "      <td>246.197885</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>253.209850</td>\n",
       "      <td>255.741948</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4357</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>332.289233</td>\n",
       "      <td>334.821332</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4358 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fold Folder_name       Class_OHEV Class_categorical  ...    fsID       start         end  salience\n",
       "0        5       fold5  [0, 0, 0, 1, 0]          dog_bark  ...  100032    0.000000    0.317551         1\n",
       "1        5       fold5  [0, 0, 1, 0, 0]  children_playing  ...  100263   58.500000   62.500000         1\n",
       "2        5       fold5  [0, 0, 1, 0, 0]  children_playing  ...  100263   60.500000   64.500000         1\n",
       "3        5       fold5  [0, 0, 1, 0, 0]  children_playing  ...  100263   63.000000   67.000000         1\n",
       "4        5       fold5  [0, 0, 1, 0, 0]  children_playing  ...  100263   68.500000   72.500000         1\n",
       "...    ...         ...              ...               ...  ...     ...         ...         ...       ...\n",
       "4353     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  159.522205  163.522205         2\n",
       "4354     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  181.142431  183.284976         2\n",
       "4355     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  242.691902  246.197885         2\n",
       "4356     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  253.209850  255.741948         2\n",
       "4357     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  332.289233  334.821332         2\n",
       "\n",
       "[4358 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadDataset = loadDataset(path)\n",
    "DB          = loadDataset.db_B\n",
    "\n",
    "print(\"\\nClasses:\\n--------------------\")\n",
    "print(DB[\"Class_categorical\"].value_counts())\n",
    "print(\"\\nTotal number of unique files..........: \", len(np.unique(DB[\"File_name\"])))\n",
    "print(\"Total number of AUDIO files...........: \", len(DB))\n",
    "DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7923e1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fold                   int64\n",
       "Folder_name           object\n",
       "Class_OHEV            object\n",
       "Class_categorical     object\n",
       "File_name             object\n",
       "Path                  object\n",
       "classID                int64\n",
       "fsID                   int64\n",
       "start                float64\n",
       "end                  float64\n",
       "salience               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72726b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1150.62x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analysis of the class balancing\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "gTitle = f'{nom_dataset} - Number of classes = ' + str(len(pd.Series(DB['Class_categorical']).unique()))\n",
    "g = sns.displot(DB,x='Class_categorical', hue='Class_categorical',height = 5, aspect = 2).set(title=gTitle)\n",
    "g.set_xticklabels(rotation=90)\n",
    "g.set_titles('Number of classes')\n",
    "\n",
    "# Retrieve the axes object from the plot\n",
    "axes = g.ax\n",
    "\n",
    "# Iterate over each bar in the plot\n",
    "for p in axes.patches:\n",
    "    # Get the coordinates of the bar\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    cord_x, cord_y = p.get_xy()\n",
    "    if height > 0:\n",
    "        axes.annotate(f'{height}', (cord_x + width/2, cord_y + height), ha='center')\n",
    "        \n",
    "g._legend.remove()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9727f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-) Features original\n",
      "2-) Features augmented\n",
      "3-) Features augmented and windowed (US8K is only windowed)\n",
      "\n",
      "Select the dataset: 3\n"
     ]
    }
   ],
   "source": [
    "# Read the pkl file with the augmented features extracted\n",
    "\n",
    "opc = 0\n",
    "while str(opc) not in '123':\n",
    "    print()\n",
    "    print(\"1-) Features original\")\n",
    "    print(\"2-) Features augmented\")\n",
    "    print(\"3-) Features augmented and windowed (US8K is only windowed)\")\n",
    "\n",
    "    opc = input(\"\\nSelect the dataset: \")\n",
    "    if opc.isdigit():\n",
    "        opc = int(opc)\n",
    "    else:\n",
    "        opc = 0\n",
    "\n",
    "if opc == 1:\n",
    "    DB_from_pkl   = pd.read_pickle(os.path.join(path_models, pkl_features))\n",
    "    model_surname = '_original'\n",
    "\n",
    "if opc == 2:\n",
    "    DB_from_pkl   = pd.read_pickle(os.path.join(path_models, pkl_aug_features))\n",
    "    model_surname = '_augmented'\n",
    "\n",
    "if opc == 3:\n",
    "    DB_from_pkl = pd.read_pickle(os.path.join(path_models, pkl_aug_wind_features))\n",
    "    model_surname = '_windowed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc2befe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Audio                  object\n",
       "Class_categorical      object\n",
       "Class_OHEV             object\n",
       "Fold                   object\n",
       "RMSE                  float64\n",
       "                       ...   \n",
       "TONNETZ_6             float64\n",
       "TONNETZ_std_6         float64\n",
       "TONNETZ_median_6      float64\n",
       "TONNETZ_skew_6        float64\n",
       "TONNETZ_kurtosis_6    float64\n",
       "Length: 379, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_from_pkl.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b9f36a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration of the dataset:  8.4608 h\n"
     ]
    }
   ],
   "source": [
    "total_duration = 0\n",
    "for audio in DB_from_pkl['Audio']:\n",
    "    total_duration = total_duration + librosa.get_duration(y=audio)\n",
    "print('Total duration of the dataset: ' , \"{:0.4f} h\".format(total_duration / 3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5a4f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Audio</th>\n",
       "      <th>Class_categorical</th>\n",
       "      <th>Class_OHEV</th>\n",
       "      <th>Fold</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.0034710653, -0.0050192624, -0.004654482, -0.0049833283, -0.0038681468, -0.0023575649, -0.00025486574, 0.00135406...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.046785</td>\n",
       "      <td>0.461191</td>\n",
       "      <td>-0.338882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.015428771, -0.0064468235, -0.002025701, -0.009768408, -0.020482529, -0.03246226, -0.046539657, -0.050950672, -0....</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026433</td>\n",
       "      <td>-0.030688</td>\n",
       "      <td>0.414979</td>\n",
       "      <td>-0.840459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.17165461, -0.1961453, -0.2095497, -0.116395764, 0.02499168, 0.15181583, 0.2456393, 0.24304995, 0.1697193, 0.0706...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023640</td>\n",
       "      <td>-0.045384</td>\n",
       "      <td>0.629947</td>\n",
       "      <td>0.096665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.004147315, -0.014180049, -0.016550057, -0.017083425, -0.010863152, -0.0018686495, 0.006234308, 0.00725661, 0.005...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>-0.025407</td>\n",
       "      <td>0.554764</td>\n",
       "      <td>-0.364067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.047271818, -0.08598116, -0.08329079, -0.15244874, -0.21367016, -0.26584676, -0.19677721, -0.17195633, -0.0841544...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>-0.036388</td>\n",
       "      <td>0.770805</td>\n",
       "      <td>0.232676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30501</th>\n",
       "      <td>[-0.024376377, -0.010054192, 0.0009410139, 0.005863713, -0.0005189497, -0.0021953415, 0.0013682423, -0.0043018945, -...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>-0.069738</td>\n",
       "      <td>0.175013</td>\n",
       "      <td>-0.974133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30502</th>\n",
       "      <td>[0.0066777063, 0.008175363, 0.006157635, 0.0026387093, 0.00059039844, 0.0030925209, 0.0033645788, 0.0056151543, 0.00...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>-0.018432</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.099409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30503</th>\n",
       "      <td>[-0.0020232266, 0.001032982, 0.0023606261, 0.0017964527, 0.0009908059, -0.0011728329, -0.003362489, -0.0054453667, -...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.028383</td>\n",
       "      <td>-1.071243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30504</th>\n",
       "      <td>[-0.0015619812, 0.0005301243, 0.0032789772, 0.002725502, 0.0023546133, -0.0019822496, -0.002666029, -0.004454969, -0...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027106</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>-0.121982</td>\n",
       "      <td>-0.606880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30505</th>\n",
       "      <td>[9.745802e-05, -0.003326854, -0.007318441, -0.0072634676, -0.0057516163, -0.0039380156, -0.0028245365, -0.000594039,...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.054895</td>\n",
       "      <td>-0.195825</td>\n",
       "      <td>-1.209213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30506 rows × 379 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         Audio Class_categorical       Class_OHEV Fold  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "0      [-0.0034710653, -0.0050192624, -0.004654482, -0.0049833283, -0.0038681468, -0.0023575649, -0.00025486574, 0.00135406...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.025644         -0.046785        0.461191           -0.338882\n",
       "1      [-0.015428771, -0.0064468235, -0.002025701, -0.009768408, -0.020482529, -0.03246226, -0.046539657, -0.050950672, -0....          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.026433         -0.030688        0.414979           -0.840459\n",
       "2      [-0.17165461, -0.1961453, -0.2095497, -0.116395764, 0.02499168, 0.15181583, 0.2456393, 0.24304995, 0.1697193, 0.0706...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.023640         -0.045384        0.629947            0.096665\n",
       "3      [-0.004147315, -0.014180049, -0.016550057, -0.017083425, -0.010863152, -0.0018686495, 0.006234308, 0.00725661, 0.005...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.024617         -0.025407        0.554764           -0.364067\n",
       "4      [-0.047271818, -0.08598116, -0.08329079, -0.15244874, -0.21367016, -0.26584676, -0.19677721, -0.17195633, -0.0841544...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.024934         -0.036388        0.770805            0.232676\n",
       "...                                                                                                                        ...               ...              ...  ...  ...            ...               ...             ...                 ...\n",
       "30501  [-0.024376377, -0.010054192, 0.0009410139, 0.005863713, -0.0005189497, -0.0021953415, 0.0013682423, -0.0043018945, -...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.033488         -0.069738        0.175013           -0.974133\n",
       "30502  [0.0066777063, 0.008175363, 0.006157635, 0.0026387093, 0.00059039844, 0.0030925209, 0.0033645788, 0.0056151543, 0.00...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.021007         -0.018432       -0.690702           -0.099409\n",
       "30503  [-0.0020232266, 0.001032982, 0.0023606261, 0.0017964527, 0.0009908059, -0.0011728329, -0.003362489, -0.0054453667, -...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.021124         -0.016391       -0.028383           -1.071243\n",
       "30504  [-0.0015619812, 0.0005301243, 0.0032789772, 0.002725502, 0.0023546133, -0.0019822496, -0.002666029, -0.004454969, -0...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.027106         -0.017513       -0.121982           -0.606880\n",
       "30505  [9.745802e-05, -0.003326854, -0.007318441, -0.0072634676, -0.0057516163, -0.0039380156, -0.0028245365, -0.000594039,...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.038524         -0.054895       -0.195825           -1.209213\n",
       "\n",
       "[30506 rows x 379 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_from_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d945e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'numpy.int32'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(DB_from_pkl['Fold'][0][0]))\n",
    "print(type(DB_from_pkl['Class_OHEV'][0][0]))\n",
    "print(type(DB_from_pkl['Class_OHEV'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad53881",
   "metadata": {},
   "source": [
    "## Input split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1b3e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate 1 fold for validation and create a DB for the training / testing according to the datasets specification\n",
    "\n",
    "DB_from_pkl_VAL = DB_from_pkl[DB_from_pkl['Fold'] == fold].copy()\n",
    "DB_from_pkl_TRN = DB_from_pkl[DB_from_pkl['Fold'] != fold].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93bc0a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3010\n",
      "27496\n",
      "Total:  30506 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(DB_from_pkl_VAL))\n",
    "print(len(DB_from_pkl_TRN))\n",
    "print('Total: ', len(DB_from_pkl_VAL) + len(DB_from_pkl_TRN),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28d4d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Audio</th>\n",
       "      <th>Class_categorical</th>\n",
       "      <th>Class_OHEV</th>\n",
       "      <th>Fold</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>[6.402111e-05, 8.269498e-05, 5.2123058e-05, 7.1509836e-05, 3.3138364e-05, -6.1828905e-07, -8.950657e-05, -9.0291964e...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.230194</td>\n",
       "      <td>-0.774566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>[0.0005136457, 0.00041881658, 0.00034897702, 0.00021603762, 0.0002278979, 0.00011100468, 0.00010083006, 0.0001630317...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>0.529034</td>\n",
       "      <td>0.970660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>[-5.3512427e-05, 2.2222208e-05, 2.7161423e-05, 0.00017825539, 0.00032240857, 0.00041231932, 0.0005614782, 0.00053010...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030452</td>\n",
       "      <td>-0.029692</td>\n",
       "      <td>0.291676</td>\n",
       "      <td>-0.927084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>[-9.721824e-05, -0.0002176053, -0.00031682133, -0.00042641407, -0.00044769727, -0.00042776082, -0.00044338158, -0.00...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>-0.005922</td>\n",
       "      <td>-0.290679</td>\n",
       "      <td>-1.082696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>[0.00023775743, 0.00023206181, 0.00023593163, 0.00017538742, 0.00011133426, 0.00021567091, 0.00011633049, 8.274122e-...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030058</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>1.003930</td>\n",
       "      <td>1.476461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30333</th>\n",
       "      <td>[-0.28165075, -0.39413118, -0.48126578, -0.54062337, -0.5627302, -0.5343282, -0.45225257, -0.33417547, -0.2137392, -...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>0.014569</td>\n",
       "      <td>0.348922</td>\n",
       "      <td>-0.837185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30334</th>\n",
       "      <td>[-0.74769396, -0.725761, -0.69069016, -0.64954436, -0.6015309, -0.53891814, -0.45484614, -0.35106456, -0.23903547, -...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>0.736680</td>\n",
       "      <td>0.175157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30335</th>\n",
       "      <td>[-0.0073982505, 0.00089106406, 0.0060087573, -0.0026300459, -0.028010733, -0.059386022, -0.07765661, -0.06947853, -0...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032426</td>\n",
       "      <td>0.030839</td>\n",
       "      <td>0.338930</td>\n",
       "      <td>-0.276007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30336</th>\n",
       "      <td>[-0.19841202, -0.19705483, -0.20790972, -0.22519125, -0.2374166, -0.23624307, -0.2194226, -0.18771836, -0.14198783, ...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053901</td>\n",
       "      <td>0.111005</td>\n",
       "      <td>-0.161630</td>\n",
       "      <td>-1.176320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30337</th>\n",
       "      <td>[0.028792929, -0.011599504, -0.03312391, -0.02478233, 0.00010168506, 0.017408311, 0.012290241, -0.011743451, -0.0403...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035027</td>\n",
       "      <td>0.117277</td>\n",
       "      <td>0.230642</td>\n",
       "      <td>-0.647329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3010 rows × 379 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         Audio Class_categorical       Class_OHEV Fold  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "231    [6.402111e-05, 8.269498e-05, 5.2123058e-05, 7.1509836e-05, 3.3138364e-05, -6.1828905e-07, -8.950657e-05, -9.0291964e...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.019867          0.005833        0.230194           -0.774566\n",
       "232    [0.0005136457, 0.00041881658, 0.00034897702, 0.00021603762, 0.0002278979, 0.00011100468, 0.00010083006, 0.0001630317...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.018672          0.009202        0.529034            0.970660\n",
       "233    [-5.3512427e-05, 2.2222208e-05, 2.7161423e-05, 0.00017825539, 0.00032240857, 0.00041231932, 0.0005614782, 0.00053010...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.030452         -0.029692        0.291676           -0.927084\n",
       "234    [-9.721824e-05, -0.0002176053, -0.00031682133, -0.00042641407, -0.00044769727, -0.00042776082, -0.00044338158, -0.00...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.031560         -0.005922       -0.290679           -1.082696\n",
       "235    [0.00023775743, 0.00023206181, 0.00023593163, 0.00017538742, 0.00011133426, 0.00021567091, 0.00011633049, 8.274122e-...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.030058          0.002691        1.003930            1.476461\n",
       "...                                                                                                                        ...               ...              ...  ...  ...            ...               ...             ...                 ...\n",
       "30333  [-0.28165075, -0.39413118, -0.48126578, -0.54062337, -0.5627302, -0.5343282, -0.45225257, -0.33417547, -0.2137392, -...        background  [1, 0, 0, 0, 0]    1  ...       0.029579          0.014569        0.348922           -0.837185\n",
       "30334  [-0.74769396, -0.725761, -0.69069016, -0.64954436, -0.6015309, -0.53891814, -0.45484614, -0.35106456, -0.23903547, -...        background  [1, 0, 0, 0, 0]    1  ...       0.023069          0.014551        0.736680            0.175157\n",
       "30335  [-0.0073982505, 0.00089106406, 0.0060087573, -0.0026300459, -0.028010733, -0.059386022, -0.07765661, -0.06947853, -0...        background  [1, 0, 0, 0, 0]    1  ...       0.032426          0.030839        0.338930           -0.276007\n",
       "30336  [-0.19841202, -0.19705483, -0.20790972, -0.22519125, -0.2374166, -0.23624307, -0.2194226, -0.18771836, -0.14198783, ...        background  [1, 0, 0, 0, 0]    1  ...       0.053901          0.111005       -0.161630           -1.176320\n",
       "30337  [0.028792929, -0.011599504, -0.03312391, -0.02478233, 0.00010168506, 0.017408311, 0.012290241, -0.011743451, -0.0403...        background  [1, 0, 0, 0, 0]    1  ...       0.035027          0.117277        0.230642           -0.647329\n",
       "\n",
       "[3010 rows x 379 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_from_pkl_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1a0a434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Audio</th>\n",
       "      <th>Class_categorical</th>\n",
       "      <th>Class_OHEV</th>\n",
       "      <th>Fold</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.0034710653, -0.0050192624, -0.004654482, -0.0049833283, -0.0038681468, -0.0023575649, -0.00025486574, 0.00135406...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.046785</td>\n",
       "      <td>0.461191</td>\n",
       "      <td>-0.338882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.015428771, -0.0064468235, -0.002025701, -0.009768408, -0.020482529, -0.03246226, -0.046539657, -0.050950672, -0....</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026433</td>\n",
       "      <td>-0.030688</td>\n",
       "      <td>0.414979</td>\n",
       "      <td>-0.840459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.17165461, -0.1961453, -0.2095497, -0.116395764, 0.02499168, 0.15181583, 0.2456393, 0.24304995, 0.1697193, 0.0706...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023640</td>\n",
       "      <td>-0.045384</td>\n",
       "      <td>0.629947</td>\n",
       "      <td>0.096665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.004147315, -0.014180049, -0.016550057, -0.017083425, -0.010863152, -0.0018686495, 0.006234308, 0.00725661, 0.005...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>-0.025407</td>\n",
       "      <td>0.554764</td>\n",
       "      <td>-0.364067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.047271818, -0.08598116, -0.08329079, -0.15244874, -0.21367016, -0.26584676, -0.19677721, -0.17195633, -0.0841544...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>-0.036388</td>\n",
       "      <td>0.770805</td>\n",
       "      <td>0.232676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30501</th>\n",
       "      <td>[-0.024376377, -0.010054192, 0.0009410139, 0.005863713, -0.0005189497, -0.0021953415, 0.0013682423, -0.0043018945, -...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>-0.069738</td>\n",
       "      <td>0.175013</td>\n",
       "      <td>-0.974133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30502</th>\n",
       "      <td>[0.0066777063, 0.008175363, 0.006157635, 0.0026387093, 0.00059039844, 0.0030925209, 0.0033645788, 0.0056151543, 0.00...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>-0.018432</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.099409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30503</th>\n",
       "      <td>[-0.0020232266, 0.001032982, 0.0023606261, 0.0017964527, 0.0009908059, -0.0011728329, -0.003362489, -0.0054453667, -...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.028383</td>\n",
       "      <td>-1.071243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30504</th>\n",
       "      <td>[-0.0015619812, 0.0005301243, 0.0032789772, 0.002725502, 0.0023546133, -0.0019822496, -0.002666029, -0.004454969, -0...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027106</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>-0.121982</td>\n",
       "      <td>-0.606880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30505</th>\n",
       "      <td>[9.745802e-05, -0.003326854, -0.007318441, -0.0072634676, -0.0057516163, -0.0039380156, -0.0028245365, -0.000594039,...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.054895</td>\n",
       "      <td>-0.195825</td>\n",
       "      <td>-1.209213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27496 rows × 379 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         Audio Class_categorical       Class_OHEV Fold  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "0      [-0.0034710653, -0.0050192624, -0.004654482, -0.0049833283, -0.0038681468, -0.0023575649, -0.00025486574, 0.00135406...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.025644         -0.046785        0.461191           -0.338882\n",
       "1      [-0.015428771, -0.0064468235, -0.002025701, -0.009768408, -0.020482529, -0.03246226, -0.046539657, -0.050950672, -0....          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.026433         -0.030688        0.414979           -0.840459\n",
       "2      [-0.17165461, -0.1961453, -0.2095497, -0.116395764, 0.02499168, 0.15181583, 0.2456393, 0.24304995, 0.1697193, 0.0706...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.023640         -0.045384        0.629947            0.096665\n",
       "3      [-0.004147315, -0.014180049, -0.016550057, -0.017083425, -0.010863152, -0.0018686495, 0.006234308, 0.00725661, 0.005...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.024617         -0.025407        0.554764           -0.364067\n",
       "4      [-0.047271818, -0.08598116, -0.08329079, -0.15244874, -0.21367016, -0.26584676, -0.19677721, -0.17195633, -0.0841544...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.024934         -0.036388        0.770805            0.232676\n",
       "...                                                                                                                        ...               ...              ...  ...  ...            ...               ...             ...                 ...\n",
       "30501  [-0.024376377, -0.010054192, 0.0009410139, 0.005863713, -0.0005189497, -0.0021953415, 0.0013682423, -0.0043018945, -...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.033488         -0.069738        0.175013           -0.974133\n",
       "30502  [0.0066777063, 0.008175363, 0.006157635, 0.0026387093, 0.00059039844, 0.0030925209, 0.0033645788, 0.0056151543, 0.00...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.021007         -0.018432       -0.690702           -0.099409\n",
       "30503  [-0.0020232266, 0.001032982, 0.0023606261, 0.0017964527, 0.0009908059, -0.0011728329, -0.003362489, -0.0054453667, -...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.021124         -0.016391       -0.028383           -1.071243\n",
       "30504  [-0.0015619812, 0.0005301243, 0.0032789772, 0.002725502, 0.0023546133, -0.0019822496, -0.002666029, -0.004454969, -0...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.027106         -0.017513       -0.121982           -0.606880\n",
       "30505  [9.745802e-05, -0.003326854, -0.007318441, -0.0072634676, -0.0057516163, -0.0039380156, -0.0028245365, -0.000594039,...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.038524         -0.054895       -0.195825           -1.209213\n",
       "\n",
       "[27496 rows x 379 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_from_pkl_TRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e0c05bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio\n",
      "Class_categorical\n",
      "Class_OHEV\n",
      "Fold\n",
      "RMSE\n",
      "ZCR\n",
      "CENTROIDS\n",
      "BANDWIDTH\n",
      "ROLLOFF\n",
      "MEL_1\n",
      "MEL_2\n",
      "MEL_3\n",
      "MEL_4\n",
      "MEL_5\n",
      "MEL_6\n",
      "MEL_7\n",
      "MEL_8\n",
      "MEL_9\n",
      "MEL_10\n",
      "MEL_11\n",
      "MEL_12\n",
      "MEL_13\n",
      "MEL_14\n",
      "MEL_15\n",
      "MEL_16\n",
      "MEL_17\n",
      "MEL_18\n",
      "MEL_19\n",
      "MEL_20\n",
      "MEL_21\n",
      "MEL_22\n",
      "MEL_23\n",
      "MEL_24\n",
      "MEL_25\n",
      "MEL_26\n",
      "MEL_27\n",
      "MEL_28\n",
      "MEL_29\n",
      "MEL_30\n",
      "MEL_31\n",
      "MEL_32\n",
      "MEL_33\n",
      "MEL_34\n",
      "MEL_35\n",
      "MEL_36\n",
      "MEL_37\n",
      "MEL_38\n",
      "MEL_39\n",
      "MEL_40\n",
      "MEL_41\n",
      "MEL_42\n",
      "MEL_43\n",
      "MEL_44\n",
      "MEL_45\n",
      "MEL_46\n",
      "MEL_47\n",
      "MEL_48\n",
      "MEL_49\n",
      "MEL_50\n",
      "MEL_51\n",
      "MEL_52\n",
      "MEL_53\n",
      "MEL_54\n",
      "MEL_55\n",
      "MEL_56\n",
      "MEL_57\n",
      "MEL_58\n",
      "MEL_59\n",
      "MEL_60\n",
      "MEL_61\n",
      "MEL_62\n",
      "MEL_63\n",
      "MEL_64\n",
      "MEL_65\n",
      "MEL_66\n",
      "MEL_67\n",
      "MEL_68\n",
      "MEL_69\n",
      "MEL_70\n",
      "MEL_71\n",
      "MEL_72\n",
      "MEL_73\n",
      "MEL_74\n",
      "MEL_75\n",
      "MEL_76\n",
      "MEL_77\n",
      "MEL_78\n",
      "MEL_79\n",
      "MEL_80\n",
      "MEL_81\n",
      "MEL_82\n",
      "MEL_83\n",
      "MEL_84\n",
      "MEL_85\n",
      "MEL_86\n",
      "MEL_87\n",
      "MEL_88\n",
      "MEL_89\n",
      "MEL_90\n",
      "MEL_91\n",
      "MEL_92\n",
      "MEL_93\n",
      "MEL_94\n",
      "MEL_95\n",
      "MEL_96\n",
      "MEL_97\n",
      "MEL_98\n",
      "MEL_99\n",
      "MEL_100\n",
      "MEL_101\n",
      "MEL_102\n",
      "MEL_103\n",
      "MEL_104\n",
      "MEL_105\n",
      "MEL_106\n",
      "MEL_107\n",
      "MEL_108\n",
      "MEL_109\n",
      "MEL_110\n",
      "MEL_111\n",
      "MEL_112\n",
      "MEL_113\n",
      "MEL_114\n",
      "MEL_115\n",
      "MEL_116\n",
      "MEL_117\n",
      "MEL_118\n",
      "MEL_119\n",
      "MEL_120\n",
      "MEL_121\n",
      "MEL_122\n",
      "MEL_123\n",
      "MEL_124\n",
      "MEL_125\n",
      "MEL_126\n",
      "MEL_127\n",
      "MEL_128\n",
      "MFCC_1\n",
      "MFCC_std_1\n",
      "MFCC_median_1\n",
      "MFCC_skew_1\n",
      "MFCC_kurtosis_1\n",
      "MFCC_delta1_mean_1\n",
      "MFCC_delta1_std_1\n",
      "MFCC_delta2_mean_1\n",
      "MFCC_delta2_std_1\n",
      "MFCC_2\n",
      "MFCC_std_2\n",
      "MFCC_median_2\n",
      "MFCC_skew_2\n",
      "MFCC_kurtosis_2\n",
      "MFCC_delta1_mean_2\n",
      "MFCC_delta1_std_2\n",
      "MFCC_delta2_mean_2\n",
      "MFCC_delta2_std_2\n",
      "MFCC_3\n",
      "MFCC_std_3\n",
      "MFCC_median_3\n",
      "MFCC_skew_3\n",
      "MFCC_kurtosis_3\n",
      "MFCC_delta1_mean_3\n",
      "MFCC_delta1_std_3\n",
      "MFCC_delta2_mean_3\n",
      "MFCC_delta2_std_3\n",
      "MFCC_4\n",
      "MFCC_std_4\n",
      "MFCC_median_4\n",
      "MFCC_skew_4\n",
      "MFCC_kurtosis_4\n",
      "MFCC_delta1_mean_4\n",
      "MFCC_delta1_std_4\n",
      "MFCC_delta2_mean_4\n",
      "MFCC_delta2_std_4\n",
      "MFCC_5\n",
      "MFCC_std_5\n",
      "MFCC_median_5\n",
      "MFCC_skew_5\n",
      "MFCC_kurtosis_5\n",
      "MFCC_delta1_mean_5\n",
      "MFCC_delta1_std_5\n",
      "MFCC_delta2_mean_5\n",
      "MFCC_delta2_std_5\n",
      "MFCC_6\n",
      "MFCC_std_6\n",
      "MFCC_median_6\n",
      "MFCC_skew_6\n",
      "MFCC_kurtosis_6\n",
      "MFCC_delta1_mean_6\n",
      "MFCC_delta1_std_6\n",
      "MFCC_delta2_mean_6\n",
      "MFCC_delta2_std_6\n",
      "MFCC_7\n",
      "MFCC_std_7\n",
      "MFCC_median_7\n",
      "MFCC_skew_7\n",
      "MFCC_kurtosis_7\n",
      "MFCC_delta1_mean_7\n",
      "MFCC_delta1_std_7\n",
      "MFCC_delta2_mean_7\n",
      "MFCC_delta2_std_7\n",
      "MFCC_8\n",
      "MFCC_std_8\n",
      "MFCC_median_8\n",
      "MFCC_skew_8\n",
      "MFCC_kurtosis_8\n",
      "MFCC_delta1_mean_8\n",
      "MFCC_delta1_std_8\n",
      "MFCC_delta2_mean_8\n",
      "MFCC_delta2_std_8\n",
      "MFCC_9\n",
      "MFCC_std_9\n",
      "MFCC_median_9\n",
      "MFCC_skew_9\n",
      "MFCC_kurtosis_9\n",
      "MFCC_delta1_mean_9\n",
      "MFCC_delta1_std_9\n",
      "MFCC_delta2_mean_9\n",
      "MFCC_delta2_std_9\n",
      "MFCC_10\n",
      "MFCC_std_10\n",
      "MFCC_median_10\n",
      "MFCC_skew_10\n",
      "MFCC_kurtosis_10\n",
      "MFCC_delta1_mean_10\n",
      "MFCC_delta1_std_10\n",
      "MFCC_delta2_mean_10\n",
      "MFCC_delta2_std_10\n",
      "MFCC_11\n",
      "MFCC_std_11\n",
      "MFCC_median_11\n",
      "MFCC_skew_11\n",
      "MFCC_kurtosis_11\n",
      "MFCC_delta1_mean_11\n",
      "MFCC_delta1_std_11\n",
      "MFCC_delta2_mean_11\n",
      "MFCC_delta2_std_11\n",
      "MFCC_12\n",
      "MFCC_std_12\n",
      "MFCC_median_12\n",
      "MFCC_skew_12\n",
      "MFCC_kurtosis_12\n",
      "MFCC_delta1_mean_12\n",
      "MFCC_delta1_std_12\n",
      "MFCC_delta2_mean_12\n",
      "MFCC_delta2_std_12\n",
      "MFCC_13\n",
      "MFCC_std_13\n",
      "MFCC_median_13\n",
      "MFCC_skew_13\n",
      "MFCC_kurtosis_13\n",
      "MFCC_delta1_mean_13\n",
      "MFCC_delta1_std_13\n",
      "MFCC_delta2_mean_13\n",
      "MFCC_delta2_std_13\n",
      "CONSTRAST_1\n",
      "CONSTRAST_std_1\n",
      "CONSTRAST_median_1\n",
      "CONSTRAST_skew_1\n",
      "CONSTRAST_kurtosis_1\n",
      "CONSTRAST_2\n",
      "CONSTRAST_std_2\n",
      "CONSTRAST_median_2\n",
      "CONSTRAST_skew_2\n",
      "CONSTRAST_kurtosis_2\n",
      "CONSTRAST_3\n",
      "CONSTRAST_std_3\n",
      "CONSTRAST_median_3\n",
      "CONSTRAST_skew_3\n",
      "CONSTRAST_kurtosis_3\n",
      "CONSTRAST_4\n",
      "CONSTRAST_std_4\n",
      "CONSTRAST_median_4\n",
      "CONSTRAST_skew_4\n",
      "CONSTRAST_kurtosis_4\n",
      "CONSTRAST_5\n",
      "CONSTRAST_std_5\n",
      "CONSTRAST_median_5\n",
      "CONSTRAST_skew_5\n",
      "CONSTRAST_kurtosis_5\n",
      "CONSTRAST_6\n",
      "CONSTRAST_std_6\n",
      "CONSTRAST_median_6\n",
      "CONSTRAST_skew_6\n",
      "CONSTRAST_kurtosis_6\n",
      "CONSTRAST_7\n",
      "CONSTRAST_std_7\n",
      "CONSTRAST_median_7\n",
      "CONSTRAST_skew_7\n",
      "CONSTRAST_kurtosis_7\n",
      "CHROMA_1\n",
      "CHROMA_std_1\n",
      "CHROMA_median_1\n",
      "CHROMA_skew_1\n",
      "CHROMA_kurtosis_1\n",
      "CHROMA_2\n",
      "CHROMA_std_2\n",
      "CHROMA_median_2\n",
      "CHROMA_skew_2\n",
      "CHROMA_kurtosis_2\n",
      "CHROMA_3\n",
      "CHROMA_std_3\n",
      "CHROMA_median_3\n",
      "CHROMA_skew_3\n",
      "CHROMA_kurtosis_3\n",
      "CHROMA_4\n",
      "CHROMA_std_4\n",
      "CHROMA_median_4\n",
      "CHROMA_skew_4\n",
      "CHROMA_kurtosis_4\n",
      "CHROMA_5\n",
      "CHROMA_std_5\n",
      "CHROMA_median_5\n",
      "CHROMA_skew_5\n",
      "CHROMA_kurtosis_5\n",
      "CHROMA_6\n",
      "CHROMA_std_6\n",
      "CHROMA_median_6\n",
      "CHROMA_skew_6\n",
      "CHROMA_kurtosis_6\n",
      "CHROMA_7\n",
      "CHROMA_std_7\n",
      "CHROMA_median_7\n",
      "CHROMA_skew_7\n",
      "CHROMA_kurtosis_7\n",
      "CHROMA_8\n",
      "CHROMA_std_8\n",
      "CHROMA_median_8\n",
      "CHROMA_skew_8\n",
      "CHROMA_kurtosis_8\n",
      "CHROMA_9\n",
      "CHROMA_std_9\n",
      "CHROMA_median_9\n",
      "CHROMA_skew_9\n",
      "CHROMA_kurtosis_9\n",
      "CHROMA_10\n",
      "CHROMA_std_10\n",
      "CHROMA_median_10\n",
      "CHROMA_skew_10\n",
      "CHROMA_kurtosis_10\n",
      "CHROMA_11\n",
      "CHROMA_std_11\n",
      "CHROMA_median_11\n",
      "CHROMA_skew_11\n",
      "CHROMA_kurtosis_11\n",
      "CHROMA_12\n",
      "CHROMA_std_12\n",
      "CHROMA_median_12\n",
      "CHROMA_skew_12\n",
      "CHROMA_kurtosis_12\n",
      "TONNETZ_1\n",
      "TONNETZ_std_1\n",
      "TONNETZ_median_1\n",
      "TONNETZ_skew_1\n",
      "TONNETZ_kurtosis_1\n",
      "TONNETZ_2\n",
      "TONNETZ_std_2\n",
      "TONNETZ_median_2\n",
      "TONNETZ_skew_2\n",
      "TONNETZ_kurtosis_2\n",
      "TONNETZ_3\n",
      "TONNETZ_std_3\n",
      "TONNETZ_median_3\n",
      "TONNETZ_skew_3\n",
      "TONNETZ_kurtosis_3\n",
      "TONNETZ_4\n",
      "TONNETZ_std_4\n",
      "TONNETZ_median_4\n",
      "TONNETZ_skew_4\n",
      "TONNETZ_kurtosis_4\n",
      "TONNETZ_5\n",
      "TONNETZ_std_5\n",
      "TONNETZ_median_5\n",
      "TONNETZ_skew_5\n",
      "TONNETZ_kurtosis_5\n",
      "TONNETZ_6\n",
      "TONNETZ_std_6\n",
      "TONNETZ_median_6\n",
      "TONNETZ_skew_6\n",
      "TONNETZ_kurtosis_6\n"
     ]
    }
   ],
   "source": [
    "for i in DB_from_pkl_TRN.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c4b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating data and labels\n",
    "\n",
    "X      = DB_from_pkl_TRN.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "y      = np.array(DB_from_pkl_TRN.Class_categorical.to_list())\n",
    "y_OHEV = np.array(DB_from_pkl_TRN.Class_OHEV.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1e3f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the augmented dataset (only validation set)\n",
    "\n",
    "X_val      = DB_from_pkl_VAL.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "y_val      = np.array(DB_from_pkl_VAL.Class_categorical.to_list())\n",
    "y_OHEV_val = np.array(DB_from_pkl_VAL.Class_OHEV.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15e67353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.117183</td>\n",
       "      <td>0.129483</td>\n",
       "      <td>2069.471399</td>\n",
       "      <td>1734.789901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.046785</td>\n",
       "      <td>0.461191</td>\n",
       "      <td>-0.338882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111724</td>\n",
       "      <td>0.131570</td>\n",
       "      <td>2098.418718</td>\n",
       "      <td>1743.608984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026433</td>\n",
       "      <td>-0.030688</td>\n",
       "      <td>0.414979</td>\n",
       "      <td>-0.840459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.124646</td>\n",
       "      <td>0.131459</td>\n",
       "      <td>2096.616414</td>\n",
       "      <td>1747.412264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023640</td>\n",
       "      <td>-0.045384</td>\n",
       "      <td>0.629947</td>\n",
       "      <td>0.096665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110741</td>\n",
       "      <td>0.131658</td>\n",
       "      <td>2093.378630</td>\n",
       "      <td>1736.615179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>-0.025407</td>\n",
       "      <td>0.554764</td>\n",
       "      <td>-0.364067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125728</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>2105.951159</td>\n",
       "      <td>1750.491381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>-0.036388</td>\n",
       "      <td>0.770805</td>\n",
       "      <td>0.232676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30501</th>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.165439</td>\n",
       "      <td>2435.099768</td>\n",
       "      <td>2199.072455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>-0.069738</td>\n",
       "      <td>0.175013</td>\n",
       "      <td>-0.974133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30502</th>\n",
       "      <td>0.007954</td>\n",
       "      <td>0.148016</td>\n",
       "      <td>2363.629594</td>\n",
       "      <td>2317.859881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>-0.018432</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.099409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30503</th>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.154519</td>\n",
       "      <td>2389.715351</td>\n",
       "      <td>2316.671469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.028383</td>\n",
       "      <td>-1.071243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30504</th>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>2467.091641</td>\n",
       "      <td>2211.234749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027106</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>-0.121982</td>\n",
       "      <td>-0.606880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30505</th>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.188943</td>\n",
       "      <td>2540.724327</td>\n",
       "      <td>2134.914700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.054895</td>\n",
       "      <td>-0.195825</td>\n",
       "      <td>-1.209213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27496 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           RMSE       ZCR    CENTROIDS    BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "0      0.117183  0.129483  2069.471399  1734.789901  ...       0.025644         -0.046785        0.461191           -0.338882\n",
       "1      0.111724  0.131570  2098.418718  1743.608984  ...       0.026433         -0.030688        0.414979           -0.840459\n",
       "2      0.124646  0.131459  2096.616414  1747.412264  ...       0.023640         -0.045384        0.629947            0.096665\n",
       "3      0.110741  0.131658  2093.378630  1736.615179  ...       0.024617         -0.025407        0.554764           -0.364067\n",
       "4      0.125728  0.131836  2105.951159  1750.491381  ...       0.024934         -0.036388        0.770805            0.232676\n",
       "...         ...       ...          ...          ...  ...            ...               ...             ...                 ...\n",
       "30501  0.012771  0.165439  2435.099768  2199.072455  ...       0.033488         -0.069738        0.175013           -0.974133\n",
       "30502  0.007954  0.148016  2363.629594  2317.859881  ...       0.021007         -0.018432       -0.690702           -0.099409\n",
       "30503  0.008383  0.154519  2389.715351  2316.671469  ...       0.021124         -0.016391       -0.028383           -1.071243\n",
       "30504  0.010459  0.173873  2467.091641  2211.234749  ...       0.027106         -0.017513       -0.121982           -0.606880\n",
       "30505  0.013639  0.188943  2540.724327  2134.914700  ...       0.038524         -0.054895       -0.195825           -1.209213\n",
       "\n",
       "[27496 rows x 375 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2d0edb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.061757</td>\n",
       "      <td>0.111211</td>\n",
       "      <td>1946.880604</td>\n",
       "      <td>1941.677599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028865</td>\n",
       "      <td>-0.001712</td>\n",
       "      <td>0.117679</td>\n",
       "      <td>-0.267797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.065669</td>\n",
       "      <td>0.075426</td>\n",
       "      <td>851.483462</td>\n",
       "      <td>537.300056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022403</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.575863</td>\n",
       "      <td>1.089295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>316.436258</td>\n",
       "      <td>478.380671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>-0.208415</td>\n",
       "      <td>-4.493470</td>\n",
       "      <td>-1.900563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1369.743048</td>\n",
       "      <td>1545.724198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016744</td>\n",
       "      <td>-0.018674</td>\n",
       "      <td>-0.229967</td>\n",
       "      <td>-0.896474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.039756</td>\n",
       "      <td>0.094150</td>\n",
       "      <td>1792.585279</td>\n",
       "      <td>1896.864013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022840</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>0.106490</td>\n",
       "      <td>-0.506396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.082771</td>\n",
       "      <td>0.130216</td>\n",
       "      <td>2276.412570</td>\n",
       "      <td>2283.346224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033357</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.450431</td>\n",
       "      <td>0.035052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.701893</td>\n",
       "      <td>0.640292</td>\n",
       "      <td>6674.189171</td>\n",
       "      <td>4340.169312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257286</td>\n",
       "      <td>0.314713</td>\n",
       "      <td>3.473303</td>\n",
       "      <td>21.586886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               RMSE           ZCR     CENTROIDS     BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "count  27496.000000  27496.000000  27496.000000  27496.000000  ...   27496.000000      27496.000000    27496.000000        27496.000000\n",
       "mean       0.061757      0.111211   1946.880604   1941.677599  ...       0.028865         -0.001712        0.117679           -0.267797\n",
       "std        0.065669      0.075426    851.483462    537.300056  ...       0.022403          0.035297        0.575863            1.089295\n",
       "min        0.000145      0.000000    316.436258    478.380671  ...       0.003497         -0.208415       -4.493470           -1.900563\n",
       "25%        0.018061      0.064742   1369.743048   1545.724198  ...       0.016744         -0.018674       -0.229967           -0.896474\n",
       "50%        0.039756      0.094150   1792.585279   1896.864013  ...       0.022840         -0.001946        0.106490           -0.506396\n",
       "75%        0.082771      0.130216   2276.412570   2283.346224  ...       0.033357          0.015029        0.450431            0.035052\n",
       "max        0.701893      0.640292   6674.189171   4340.169312  ...       0.257286          0.314713        3.473303           21.586886\n",
       "\n",
       "[8 rows x 375 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fd62f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dog_bark', 'dog_bark', 'dog_bark', ..., 'car_horn', 'car_horn',\n",
       "       'car_horn'], dtype='<U16')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e485221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_OHEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8fa7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_statistics = pd.DataFrame({'mean': X.mean(), 'std': X.std(), 'min': X.min(), 'max': X.max()})\n",
    "\n",
    "X_mean = X_statistics.values[:, 0]\n",
    "X_std  = X_statistics.values[:, 1]\n",
    "X_min  = X_statistics.values[:, 2]\n",
    "X_max  = X_statistics.values[:, 3]\n",
    "\n",
    "X_norm   =  (X.values - X_min) / (X_max - X_min)\n",
    "\n",
    "X_normDB = X.apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ec649f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08779795319423672\n"
     ]
    }
   ],
   "source": [
    "RMSE_lst = []\n",
    "for i in X_norm:\n",
    "    RMSE_lst.append([i][0][0])\n",
    "    \n",
    "print(np.mean(RMSE_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3d3be1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.087798</td>\n",
       "      <td>0.173688</td>\n",
       "      <td>0.256450</td>\n",
       "      <td>0.378917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099956</td>\n",
       "      <td>0.395129</td>\n",
       "      <td>0.578798</td>\n",
       "      <td>0.069517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.093580</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.133928</td>\n",
       "      <td>0.139132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088276</td>\n",
       "      <td>0.067473</td>\n",
       "      <td>0.072283</td>\n",
       "      <td>0.046378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.025530</td>\n",
       "      <td>0.101113</td>\n",
       "      <td>0.165673</td>\n",
       "      <td>0.276386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052194</td>\n",
       "      <td>0.362705</td>\n",
       "      <td>0.535161</td>\n",
       "      <td>0.042750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.056446</td>\n",
       "      <td>0.147041</td>\n",
       "      <td>0.232181</td>\n",
       "      <td>0.367313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076216</td>\n",
       "      <td>0.394680</td>\n",
       "      <td>0.577393</td>\n",
       "      <td>0.059358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.117743</td>\n",
       "      <td>0.203369</td>\n",
       "      <td>0.308281</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117657</td>\n",
       "      <td>0.427130</td>\n",
       "      <td>0.620565</td>\n",
       "      <td>0.082411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               RMSE           ZCR     CENTROIDS     BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "count  27496.000000  27496.000000  27496.000000  27496.000000  ...   27496.000000      27496.000000    27496.000000        27496.000000\n",
       "mean       0.087798      0.173688      0.256450      0.378917  ...       0.099956          0.395129        0.578798            0.069517\n",
       "std        0.093580      0.117800      0.133928      0.139132  ...       0.088276          0.067473        0.072283            0.046378\n",
       "min        0.000000      0.000000      0.000000      0.000000  ...       0.000000          0.000000        0.000000            0.000000\n",
       "25%        0.025530      0.101113      0.165673      0.276386  ...       0.052194          0.362705        0.535161            0.042750\n",
       "50%        0.056446      0.147041      0.232181      0.367313  ...       0.076216          0.394680        0.577393            0.059358\n",
       "75%        0.117743      0.203369      0.308281      0.467391  ...       0.117657          0.427130        0.620565            0.082411\n",
       "max        1.000000      1.000000      1.000000      1.000000  ...       1.000000          1.000000        1.000000            1.000000\n",
       "\n",
       "[8 rows x 375 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normDB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a069edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standard = (X.values - X_mean) / X_std\n",
    "\n",
    "X_standardDB = X.apply(lambda x: (x - x.mean()) / x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b42f5a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.269336465057902e-18\n",
      "0.9999818153714614\n"
     ]
    }
   ],
   "source": [
    "RMSE_lst    = []\n",
    "for i in X_standard:\n",
    "    RMSE_lst.append([i][0][0])\n",
    "    \n",
    "print(np.mean(RMSE_lst))\n",
    "print(np.std(RMSE_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "102f1c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>2.749600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-8.269336e-18</td>\n",
       "      <td>-5.788536e-17</td>\n",
       "      <td>1.488481e-16</td>\n",
       "      <td>-5.126989e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>5.168335e-17</td>\n",
       "      <td>-2.144859e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.033667e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.382175e-01</td>\n",
       "      <td>-1.474433e+00</td>\n",
       "      <td>-1.914828e+00</td>\n",
       "      <td>-2.723426e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.132315e+00</td>\n",
       "      <td>-5.856130e+00</td>\n",
       "      <td>-8.007378</td>\n",
       "      <td>-1.498921e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.654041e-01</td>\n",
       "      <td>-6.160890e-01</td>\n",
       "      <td>-6.778024e-01</td>\n",
       "      <td>-7.369316e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.410548e-01</td>\n",
       "      <td>-4.805513e-01</td>\n",
       "      <td>-0.603696</td>\n",
       "      <td>-5.771418e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.350271e-01</td>\n",
       "      <td>-2.262003e-01</td>\n",
       "      <td>-1.812077e-01</td>\n",
       "      <td>-8.340514e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.689247e-01</td>\n",
       "      <td>-6.645925e-03</td>\n",
       "      <td>-0.019429</td>\n",
       "      <td>-2.190398e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.199957e-01</td>\n",
       "      <td>2.519650e-01</td>\n",
       "      <td>3.870092e-01</td>\n",
       "      <td>6.358991e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.005251e-01</td>\n",
       "      <td>4.742897e-01</td>\n",
       "      <td>0.577832</td>\n",
       "      <td>2.780233e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.747881e+00</td>\n",
       "      <td>7.014547e+00</td>\n",
       "      <td>5.551850e+00</td>\n",
       "      <td>4.463971e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019584e+01</td>\n",
       "      <td>8.964689e+00</td>\n",
       "      <td>5.827128</td>\n",
       "      <td>2.006315e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               RMSE           ZCR     CENTROIDS     BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "count  2.749600e+04  2.749600e+04  2.749600e+04  2.749600e+04  ...   2.749600e+04      2.749600e+04    27496.000000        2.749600e+04\n",
       "mean  -8.269336e-18 -5.788536e-17  1.488481e-16 -5.126989e-16  ...   5.168335e-17     -2.144859e-17        0.000000       -1.033667e-17\n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  ...   1.000000e+00      1.000000e+00        1.000000        1.000000e+00\n",
       "min   -9.382175e-01 -1.474433e+00 -1.914828e+00 -2.723426e+00  ...  -1.132315e+00     -5.856130e+00       -8.007378       -1.498921e+00\n",
       "25%   -6.654041e-01 -6.160890e-01 -6.778024e-01 -7.369316e-01  ...  -5.410548e-01     -4.805513e-01       -0.603696       -5.771418e-01\n",
       "50%   -3.350271e-01 -2.262003e-01 -1.812077e-01 -8.340514e-02  ...  -2.689247e-01     -6.645925e-03       -0.019429       -2.190398e-01\n",
       "75%    3.199957e-01  2.519650e-01  3.870092e-01  6.358991e-01  ...   2.005251e-01      4.742897e-01        0.577832        2.780233e-01\n",
       "max    9.747881e+00  7.014547e+00  5.551850e+00  4.463971e+00  ...   1.019584e+01      8.964689e+00        5.827128        2.006315e+01\n",
       "\n",
       "[8 rows x 375 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_standardDB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae16f79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27496, 375)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81aa62e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27496, 375)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "832d29f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27496, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_OHEV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a638bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27496"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9017e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27496"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee2d7aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.031516</td>\n",
       "      <td>1152.318462</td>\n",
       "      <td>1898.286145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.230194</td>\n",
       "      <td>-0.774566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.035680</td>\n",
       "      <td>0.036821</td>\n",
       "      <td>1237.588550</td>\n",
       "      <td>1949.387843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>0.529034</td>\n",
       "      <td>0.970660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.036087</td>\n",
       "      <td>0.045854</td>\n",
       "      <td>1422.395537</td>\n",
       "      <td>2117.020765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030452</td>\n",
       "      <td>-0.029692</td>\n",
       "      <td>0.291676</td>\n",
       "      <td>-0.927084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.040949</td>\n",
       "      <td>1877.232022</td>\n",
       "      <td>2785.436746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>-0.005922</td>\n",
       "      <td>-0.290679</td>\n",
       "      <td>-1.082696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.076167</td>\n",
       "      <td>0.042880</td>\n",
       "      <td>1592.015997</td>\n",
       "      <td>2199.860601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030058</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>1.003930</td>\n",
       "      <td>1.476461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30333</th>\n",
       "      <td>0.197044</td>\n",
       "      <td>0.070557</td>\n",
       "      <td>1087.442478</td>\n",
       "      <td>886.308473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>0.014569</td>\n",
       "      <td>0.348922</td>\n",
       "      <td>-0.837185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30334</th>\n",
       "      <td>0.197267</td>\n",
       "      <td>0.073486</td>\n",
       "      <td>1114.525841</td>\n",
       "      <td>940.949276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>0.736680</td>\n",
       "      <td>0.175157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30335</th>\n",
       "      <td>0.196901</td>\n",
       "      <td>0.072865</td>\n",
       "      <td>1059.255494</td>\n",
       "      <td>857.858019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032426</td>\n",
       "      <td>0.030839</td>\n",
       "      <td>0.338930</td>\n",
       "      <td>-0.276007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30336</th>\n",
       "      <td>0.192756</td>\n",
       "      <td>0.066939</td>\n",
       "      <td>1058.530590</td>\n",
       "      <td>902.456226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053901</td>\n",
       "      <td>0.111005</td>\n",
       "      <td>-0.161630</td>\n",
       "      <td>-1.176320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30337</th>\n",
       "      <td>0.169005</td>\n",
       "      <td>0.068271</td>\n",
       "      <td>1053.554950</td>\n",
       "      <td>875.421447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035027</td>\n",
       "      <td>0.117277</td>\n",
       "      <td>0.230642</td>\n",
       "      <td>-0.647329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3010 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           RMSE       ZCR    CENTROIDS    BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "231    0.020142  0.031516  1152.318462  1898.286145  ...       0.019867          0.005833        0.230194           -0.774566\n",
       "232    0.035680  0.036821  1237.588550  1949.387843  ...       0.018672          0.009202        0.529034            0.970660\n",
       "233    0.036087  0.045854  1422.395537  2117.020765  ...       0.030452         -0.029692        0.291676           -0.927084\n",
       "234    0.000778  0.040949  1877.232022  2785.436746  ...       0.031560         -0.005922       -0.290679           -1.082696\n",
       "235    0.076167  0.042880  1592.015997  2199.860601  ...       0.030058          0.002691        1.003930            1.476461\n",
       "...         ...       ...          ...          ...  ...            ...               ...             ...                 ...\n",
       "30333  0.197044  0.070557  1087.442478   886.308473  ...       0.029579          0.014569        0.348922           -0.837185\n",
       "30334  0.197267  0.073486  1114.525841   940.949276  ...       0.023069          0.014551        0.736680            0.175157\n",
       "30335  0.196901  0.072865  1059.255494   857.858019  ...       0.032426          0.030839        0.338930           -0.276007\n",
       "30336  0.192756  0.066939  1058.530590   902.456226  ...       0.053901          0.111005       -0.161630           -1.176320\n",
       "30337  0.169005  0.068271  1053.554950   875.421447  ...       0.035027          0.117277        0.230642           -0.647329\n",
       "\n",
       "[3010 rows x 375 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e31ec6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.065873</td>\n",
       "      <td>0.113265</td>\n",
       "      <td>2027.375554</td>\n",
       "      <td>1988.820959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036852</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>0.118041</td>\n",
       "      <td>-0.305626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.066159</td>\n",
       "      <td>0.081072</td>\n",
       "      <td>972.540017</td>\n",
       "      <td>531.004250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041177</td>\n",
       "      <td>0.044567</td>\n",
       "      <td>0.595448</td>\n",
       "      <td>1.049706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>406.477247</td>\n",
       "      <td>695.196438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>-0.178653</td>\n",
       "      <td>-2.506518</td>\n",
       "      <td>-1.733951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.015080</td>\n",
       "      <td>0.060275</td>\n",
       "      <td>1318.434027</td>\n",
       "      <td>1621.312707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>-0.019668</td>\n",
       "      <td>-0.247503</td>\n",
       "      <td>-0.945905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.040219</td>\n",
       "      <td>0.101485</td>\n",
       "      <td>1894.516205</td>\n",
       "      <td>1917.492054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023036</td>\n",
       "      <td>-0.001336</td>\n",
       "      <td>0.113718</td>\n",
       "      <td>-0.550888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.137557</td>\n",
       "      <td>2361.173895</td>\n",
       "      <td>2314.618141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036021</td>\n",
       "      <td>0.016743</td>\n",
       "      <td>0.504786</td>\n",
       "      <td>0.015309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.469498</td>\n",
       "      <td>0.661577</td>\n",
       "      <td>6970.949555</td>\n",
       "      <td>3658.796731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227059</td>\n",
       "      <td>0.199019</td>\n",
       "      <td>2.530284</td>\n",
       "      <td>12.081950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              RMSE          ZCR    CENTROIDS    BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "count  3010.000000  3010.000000  3010.000000  3010.000000  ...    3010.000000       3010.000000     3010.000000         3010.000000\n",
       "mean      0.065873     0.113265  2027.375554  1988.820959  ...       0.036852         -0.001223        0.118041           -0.305626\n",
       "std       0.066159     0.081072   972.540017   531.004250  ...       0.041177          0.044567        0.595448            1.049706\n",
       "min       0.000083     0.005549   406.477247   695.196438  ...       0.003733         -0.178653       -2.506518           -1.733951\n",
       "25%       0.015080     0.060275  1318.434027  1621.312707  ...       0.016706         -0.019668       -0.247503           -0.945905\n",
       "50%       0.040219     0.101485  1894.516205  1917.492054  ...       0.023036         -0.001336        0.113718           -0.550888\n",
       "75%       0.097500     0.137557  2361.173895  2314.618141  ...       0.036021          0.016743        0.504786            0.015309\n",
       "max       0.469498     0.661577  6970.949555  3658.796731  ...       0.227059          0.199019        2.530284           12.081950\n",
       "\n",
       "[8 rows x 375 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94008013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_OHEV_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55830538",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_norm   =  (X_val.values - X_min) / (X_max - X_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e67b03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_standard = (X_val.values - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cb06ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3010, 375)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ebc21b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3010, 375)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d0bb619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3010, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_OHEV_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bd2a8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3010"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_OHEV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8087fae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(0, 0, 0, 1, 0): 700,\n",
       "         (1, 0, 0, 0, 0): 756,\n",
       "         (0, 0, 1, 0, 0): 700,\n",
       "         (0, 0, 0, 0, 1): 602,\n",
       "         (0, 1, 0, 0, 0): 252})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter_val = Counter(map(tuple, y_OHEV_val))\n",
    "Counter_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92b398cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_categorical       \n",
      "background         6079     [1, 0, 0, 0, 0]\n",
      "car_horn           26503    [0, 1, 0, 0, 0]\n",
      "children_playing   25242    [0, 0, 1, 0, 0]\n",
      "dog_bark           29124    [0, 0, 0, 1, 0]\n",
      "siren              2876     [0, 0, 0, 0, 1]\n",
      "Name: Class_OHEV, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'background': array([1, 0, 0, 0, 0]),\n",
       " 'car_horn': array([0, 1, 0, 0, 0]),\n",
       " 'children_playing': array([0, 0, 1, 0, 0]),\n",
       " 'dog_bark': array([0, 0, 0, 1, 0]),\n",
       " 'siren': array([0, 0, 0, 0, 1])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by the class and get one random sample of each class\n",
    "k = DB_from_pkl.groupby('Class_categorical')['Class_OHEV'].apply(lambda s: s.sample(1))\n",
    "print(k)\n",
    "\n",
    "# Convert the pandas series into a dataframe\n",
    "temp_k_df = k.reset_index()\n",
    "\n",
    "# Delete the index from the grouppby result\n",
    "del temp_k_df['level_1']\n",
    "\n",
    "# Set the \"Class\" as the dataframe index\n",
    "temp_k_df.set_index(\"Class_categorical\", inplace=True)\n",
    "\n",
    "# Convert the dataframe to a dictionary (Class: Class_encoder)\n",
    "encoder_dict = temp_k_df[\"Class_OHEV\"].to_dict()\n",
    "encoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28674adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['background', 'car_horn', 'children_playing', 'dog_bark', 'siren']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nom_classes = list(encoder_dict.keys())\n",
    "nom_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b53876a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of classes in the dataset\n",
    "\n",
    "num_classes = len(encoder_dict.keys())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0504cb8",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee250105",
   "metadata": {},
   "outputs": [],
   "source": [
    "del DB_from_pkl_VAL, DB_from_pkl_TRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e866b039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-) Normalization\n",
      "2-) Standardization\n",
      "\n",
      "Select the dataset: 2\n"
     ]
    }
   ],
   "source": [
    "# Separate 1 fold for validation and create a DB for the training / testing\n",
    "\n",
    "opc = 0\n",
    "while str(opc) not in '12':\n",
    "    print()\n",
    "    print(\"1-) Normalization\")\n",
    "    print(\"2-) Standardization\")\n",
    "\n",
    "    opc = input(\"\\nSelect the dataset: \")\n",
    "    if opc.isdigit():\n",
    "        opc = int(opc)\n",
    "    else:\n",
    "        opc = 0\n",
    "\n",
    "\n",
    "    DB_from_pkl_VAL = DB_from_pkl[DB_from_pkl['Fold'] == fold].copy()\n",
    "    DB_from_pkl_TRN = DB_from_pkl[DB_from_pkl['Fold'] != fold].copy()\n",
    "    \n",
    "    X      = DB_from_pkl_TRN.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "    y      = np.array(DB_from_pkl_TRN.Class_categorical.to_list())\n",
    "    y_OHEV = np.array(DB_from_pkl_TRN.Class_OHEV.to_list())\n",
    "\n",
    "    X_val      = DB_from_pkl_VAL.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "    y_val      = np.array(DB_from_pkl_VAL.Class_categorical.to_list())\n",
    "    y_OHEV_val = np.array(DB_from_pkl_VAL.Class_OHEV.to_list())\n",
    "\n",
    "    X_statistics = pd.DataFrame({'mean': X.mean(), 'std': X.std(), 'min': X.min(), 'max': X.max()})\n",
    "\n",
    "    X_mean   = X_statistics.values[:, 0]\n",
    "    X_std    = X_statistics.values[:, 1]\n",
    "    X_min    = X_statistics.values[:, 2]\n",
    "    X_max    = X_statistics.values[:, 3]\n",
    "    \n",
    "    # Normalization or standardization using values from the training set.\n",
    "    if opc == 1:\n",
    "        X_norm     = (X.values - X_min) / (X_max - X_min)\n",
    "        X_val_norm = (X_val.values - X_min) / (X_max - X_min)\n",
    "        norm_type  = '_norm'\n",
    "\n",
    "    if opc == 2:\n",
    "        X_norm     = (X.values - X_mean) / X_std\n",
    "        X_val_norm = (X_val.values - X_mean) / X_std\n",
    "        norm_type  = '_std'\n",
    "\n",
    "    # Retrieve the indexes used for training the classifiers\n",
    "    idx_trn = np.genfromtxt(os.path.join(path_models, '_idx_trn_' + nom_dataset + model_surname + '.csv'), delimiter=',', dtype = int)\n",
    "    idx_tst = np.genfromtxt(os.path.join(path_models, '_idx_tst_' + nom_dataset + model_surname + '.csv'), delimiter=',', dtype = int)\n",
    "\n",
    "    X_train      = X_norm[idx_trn]\n",
    "    X_test       = X_norm[idx_tst]\n",
    "    y_train      = y[idx_trn]\n",
    "    y_test       = y[idx_tst]\n",
    "    y_train_OHEV = y_OHEV[idx_trn]\n",
    "    y_test_OHEV  = y_OHEV[idx_tst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13d793b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Training set\n",
      "\n",
      "X_train.........: (24746, 375)\n",
      "y_train.........: (24746,)\n",
      "y_train_OHEV....: (24746, 5)\n",
      "\n",
      "==================================\n",
      "Testing set\n",
      "\n",
      "X_test..........: (2750, 375)\n",
      "y_test..........: (2750,)\n",
      "y_test_OHEV.....: (2750, 5)\n",
      "\n",
      "==================================\n",
      "Validation set\n",
      "\n",
      "X_val_norm......: (3010, 375)\n",
      "y_val...........: (3010,)\n",
      "y_OHEV_val......: (3010, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==================================\")\n",
    "print(\"Training set\\n\")\n",
    "\n",
    "print(f'X_train.........: {np.shape(X_train)}')\n",
    "print(f'y_train.........: {np.shape(y_train)}')\n",
    "print(f'y_train_OHEV....: {np.shape(y_train_OHEV)}')\n",
    "\n",
    "print(\"\\n==================================\")\n",
    "print(\"Testing set\\n\")\n",
    "\n",
    "print(f'X_test..........: {np.shape(X_test)}')\n",
    "print(f'y_test..........: {np.shape(y_test)}')\n",
    "print(f'y_test_OHEV.....: {np.shape(y_test_OHEV)}')\n",
    "\n",
    "print(\"\\n==================================\")\n",
    "print(\"Validation set\\n\")\n",
    "\n",
    "print(f'X_val_norm......: {np.shape(X_val_norm)}')\n",
    "print(f'y_val...........: {np.shape(y_val)}')\n",
    "print(f'y_OHEV_val......: {np.shape(y_OHEV_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73c4cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple confusion matrix\n",
    "\n",
    "def simple_conf_matrix(y_true, y_pred, nom_classes, clf, acc):\n",
    "    \n",
    "    picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "    conf_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    title = nom_dataset + model_surname + norm_type + ' - Classifier ' + clf + ' - Validation accuracy: '+ str(\"{:0.2f} %\".format(acc*100))\n",
    "\n",
    "    plt.figure(figsize = (10,10))\n",
    "    sns.heatmap(conf_matrix, \n",
    "                annot=True, \n",
    "                fmt='g', \n",
    "                cmap=cmap_cm, \n",
    "                annot_kws={\"size\": 8}, \n",
    "                xticklabels=nom_classes, \n",
    "                yticklabels=nom_classes)\n",
    "    plt.title(title, fontsize = 12)\n",
    "    plt.savefig(os.path.join(path_pic, picture_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "149e7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, labels, title, cmap, normalize):\n",
    "\n",
    "    if labels is not None:\n",
    "        tick_marks = np.arange(len(labels))\n",
    "        plt.xticks(tick_marks, labels, fontsize=10, rotation=45)\n",
    "        plt.yticks(tick_marks, labels, fontsize=10)\n",
    "   \n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 8)\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 8)\n",
    "\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title, fontsize=13)\n",
    "    plt.colorbar(shrink=1)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15571e04",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240cc9bf",
   "metadata": {},
   "source": [
    "- **Multilayer Perceptron** (MLP) is a type of Artificial Neural Network (ANN) used for supervised learning tasks, including classification, regression, and pattern recognition. It's a feedforward neural network that consists of multiple layers of nodes, including an input layer, one or more hidden layers, and an output layer. Each node, or neuron, in the network is connected to every node in the adjacent layers, and these connections have weights that are adjusted during training. MLP is capable of modeling complex relationships in data, making it suitable for tasks where the relationship between inputs and outputs is non-linear and intricate. It uses activation functions to introduce non-linearity into the network, allowing it to learn and approximate a wide variety of functions. One of the key advantages of MLP is its ability to learn from large and high-dimensional datasets. However, this advantage comes with the cost of increased complexity, making it more challenging to train and requiring careful tuning of hyperparameters like the number of hidden layers, the number of neurons in each layer, and the learning rate. Additionally, MLP is sensitive to feature scaling, and preprocessing techniques such as normalization are often applied to the input data to improve performance.\n",
    "***\n",
    "- **Convolutional Neural Networks** (CNNs) are a class of deep learning algorithms specifically designed for processing grid-like data, such as images and videos. CNNs are highly effective in tasks related to computer vision, including image recognition, object detection, and image segmentation. They are characterized by their ability to automatically and adaptively learn spatial hierarchies of features from input data. CNNs consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers apply convolution operations to the input data, enabling the network to automatically learn patterns and features from images, such as edges, textures, and more complex structures. The pooling layers downsample the spatial dimensions of the data, reducing computational complexity while retaining important features. Fully connected layers at the end of the network process the learned features and make predictions based on them. One of the significant advantages of CNNs is their ability to capture local patterns and spatial hierarchies of features. By using shared weights and biases in the convolutional layers, CNNs are capable of learning translation-invariant features, making them well-suited for tasks where the spatial arrangement of features in the input data is essential. Additionally, CNNs can automatically learn relevant features from raw pixel values, eliminating the need for manual feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72775254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of the dimensions of the input layer\n",
    "\n",
    "n_dim       = X_norm.shape[1]\n",
    "n_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "843264dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OHEV\n",
    "\n",
    "Counter_test = Counter(map(tuple, y_test_OHEV))\n",
    "Counter_train = Counter(map(tuple, y_train_OHEV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "020f71ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({(0, 0, 0, 0, 1): 5311,\n",
       "         (0, 0, 0, 1, 0): 5670,\n",
       "         (0, 1, 0, 0, 0): 2476,\n",
       "         (0, 0, 1, 0, 0): 5670,\n",
       "         (1, 0, 0, 0, 0): 5619})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training samples')\n",
    "Counter_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d8a4668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({(1, 0, 0, 0, 0): 625,\n",
       "         (0, 0, 0, 1, 0): 630,\n",
       "         (0, 1, 0, 0, 0): 275,\n",
       "         (0, 0, 1, 0, 0): 630,\n",
       "         (0, 0, 0, 0, 1): 590})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Testing samples')\n",
    "Counter_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4357c48",
   "metadata": {},
   "source": [
    "### ANN - Grid search for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3ffbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_model():\n",
    "    \n",
    "#    model = Sequential()\n",
    "#    model.add(Dense(n_dim, activation='relu', input_shape=(n_dim,)))\n",
    "#    model.add(Dropout(0.2))\n",
    "#    model.add(Dense(375, activation='relu'))\n",
    "#    model.add(Dropout(0.2))\n",
    "#    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#    model.compile(loss='MeanSquaredError', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b8805a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for the batch size and epochs\n",
    "\n",
    "#model = KerasClassifier(build_fn = create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "#batch_size  = [20, 40, 80, 160]\n",
    "#epochs      = [100, 250, 500]\n",
    "#param_grid  = dict(batch_size = batch_size, epochs = epochs)\n",
    "#grid        = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs=-1, cv=3)\n",
    "#grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#means  = grid_result.cv_results_['mean_test_score']\n",
    "#stds   = grid_result.cv_results_['std_test_score']\n",
    "#params = grid_result.cv_results_['params']\n",
    "\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7140571e",
   "metadata": {},
   "source": [
    "Results of the above GridSearch:\n",
    "\n",
    "Best: 0.857639 using {'batch_size': 80, 'epochs': 100}\n",
    "\n",
    "0.815972 (0.032200) with: {'batch_size': 20, 'epochs': 100}\n",
    "0.836806 (0.017705) with: {'batch_size': 20, 'epochs': 250}\n",
    "0.840278 (0.004910) with: {'batch_size': 20, 'epochs': 500}\n",
    "0.836806 (0.032200) with: {'batch_size': 40, 'epochs': 100}\n",
    "0.854167 (0.017010) with: {'batch_size': 40, 'epochs': 250}\n",
    "0.840278 (0.024552) with: {'batch_size': 40, 'epochs': 500}\n",
    "0.857639 (0.027340) with: {'batch_size': 80, 'epochs': 100}\n",
    "0.854167 (0.030666) with: {'batch_size': 80, 'epochs': 250}\n",
    "0.802083 (0.038976) with: {'batch_size': 80, 'epochs': 500}\n",
    "0.840278 (0.041955) with: {'batch_size': 160, 'epochs': 100}\n",
    "0.850694 (0.029869) with: {'batch_size': 160, 'epochs': 250}\n",
    "0.836806 (0.032200) with: {'batch_size': 160, 'epochs': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56f7bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "#K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68655d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, Adamax, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28d49a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hidden layers, neurons, activation, dropout_rate and optimizer\n",
    "\n",
    "#def tune_model(hidden_layers, neurons, activation, dropout_rate, optimizer='adam', learning_rate=0.001, momentum=0.9, nesterov=False, rho=0.9, epsilon=1e-07, centered=False, \n",
    "#                 initial_accumulator_value=0.1, amsgrad=False, beta_1=0.9, beta_2=0.999):\n",
    "    \n",
    "#    model = Sequential()\n",
    "#    model.add(Dense(units = neurons, activation = activation, input_shape = (n_dim,)))\n",
    "\n",
    "#    for i in range(hidden_layers):\n",
    "#        model.add(Dense(units = neurons, activation = activation))\n",
    "#        model.add(Dropout(dropout_rate))\n",
    "\n",
    "#    model.add(Dense(units = num_classes, activation = 'sigmoid'))\n",
    "    \n",
    "#    if optimizer == 'sgd':\n",
    "#        optimizer = SGD(lr=learning_rate, momentum=momentum, nesterov=nesterov)\n",
    "#    elif optimizer == 'rmsprop':\n",
    "#        optimizer = RMSprop(lr=learning_rate, rho=rho, epsilon=epsilon, centered=centered)\n",
    "#    elif optimizer == 'adam':\n",
    "#        optimizer = Adam(lr=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=amsgrad)\n",
    "#    elif optimizer == 'adagrad':\n",
    "#        optimizer = Adagrad(lr=learning_rate, initial_accumulator_value=initial_accumulator_value, epsilon=epsilon)\n",
    "#    elif optimizer == 'adadelta':\n",
    "#        optimizer = Adadelta(lr=learning_rate, rho=rho, epsilon=epsilon)\n",
    "#    elif optimizer == 'adamax':\n",
    "#        optimizer = Adamax(lr=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "#    elif optimizer == 'nadam':\n",
    "#        optimizer = Nadam(lr=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "        \n",
    "#    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "#    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3748cb9a",
   "metadata": {},
   "source": [
    "'learning_rate' represents the learning rate of the optimizer.\n",
    "'momentum' is the momentum factor for optimizers like SGD and RMSprop.\n",
    "'nesterov' is a boolean indicating whether to apply Nesterov momentum for SGD.\n",
    "'rho' is the decay factor for RMSprop.\n",
    "'epsilon' is a small constant for numerical stability.\n",
    "'centered' is a boolean indicating whether to compute centralized gradients for RMSprop.\n",
    "'initial_accumulator_value' is the starting value for accumulators in Adagrad.\n",
    "'amsgrad' is a boolean indicating whether to use the AMSGrad variant of Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a075bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_layers  = [1, 2, 3]\n",
    "#neurons        = [375, 500, 750, 1000]\n",
    "#activation     = ['relu', 'sigmoid']\n",
    "#dropout_rate   = [0.1, 0.2, 0.3]\n",
    "#optimizer      = ['sgd', 'rmsprop', 'adam', 'adagrad', 'adadelta', 'adamax', 'nadam']\n",
    "\n",
    "#learning_rate  = [0.001, 0.01, 0.1]\n",
    "\n",
    "#param_grid     = dict(hidden_layers = hidden_layers, \n",
    "#                      neurons       = neurons, \n",
    "#                      activation    = activation,\n",
    "#                     dropout_rate  = dropout_rate,\n",
    "#                      optimizer     = optimizer,\n",
    "#                      learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa178c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.autograph.set_verbosity(0)\n",
    "#tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "944f5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KerasClassifier(build_fn = tune_model, verbose=1, epochs = 100, batch_size = 80)\n",
    "#grid  = GridSearchCV(estimator = model, param_grid = param_grid, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d21c9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fc2f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize results\n",
    "#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#means  = grid_result.cv_results_['mean_test_score']\n",
    "#stds   = grid_result.cv_results_['std_test_score']\n",
    "#params = grid_result.cv_results_['params']\n",
    "\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "237bc9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f466e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN (Artificial Neural Network) or MLP (Multi layer Perceptron) using Tensorflow\n",
    "\n",
    "initializer = keras.initializers.Ones()\n",
    "\n",
    "def build_ANN_model(model_name: str, neurons: int):\n",
    "    \n",
    "    #optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=False)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001, \n",
    "                                      beta_1=0.5, \n",
    "                                      beta_2=0.999, \n",
    "                                      epsilon=1e-07, \n",
    "                                      amsgrad=True)\n",
    "    \n",
    "    \n",
    "    model = Sequential(name = model_name)\n",
    "    model.add(Dense(neurons, activation = 'relu', input_shape = (neurons,), name = 'Input'))\n",
    "\n",
    "    # First hiden layer with 375 neurons\n",
    "    model.add(Dense(neurons, activation ='relu', name = 'Hiden_1'))\n",
    "\n",
    "    # Dropout de 20%\n",
    "    model.add(Dropout(0.2, name = 'Dropout_1'))\n",
    "    \n",
    "    # Second hiden layer with 750 neurons (Kolmogorov's theorem)\n",
    "    model.add(Dense(n_dim * 2, activation ='relu', name = 'Hiden_2'))\n",
    "\n",
    "    # Dropout de 20%\n",
    "    model.add(Dropout(0.2, name = 'Dropout_2'))\n",
    "\n",
    "    # Final classification layer, with 1 neuron for each output class. Softmax divides the probability of each class.\n",
    "    model.add(Dense(num_classes, activation='softmax', name = 'Output'))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "383a342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=150, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "if not os.path.exists(path_models):\n",
    "    os.makedirs(path_models)\n",
    "\n",
    "filepath       = os.path.join(path_models, 'Model_ANN_weights_0_best' + norm_type + model_surname + '.hdf5')\n",
    "checkpoint     = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e40f2486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ANN_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 375)               141000    \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 375)               141000    \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               282000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 567,755\n",
      "Trainable params: 567,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ANN = build_ANN_model('ANN_1', neurons = n_dim)\n",
    "model_ANN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "649f2c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_ANN, to_file= os.path.join(path_models, 'Model_ANN' + norm_type + model_surname + '.png'), show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7006273",
   "metadata": {},
   "source": [
    "### Understanding the column \"Param\":\n",
    "\n",
    "- 141,000 parameters is the result of 375 neurons with 375 features + 375  bias values\n",
    "- 141,000 parameters is the result of 375 neurons with 375 features + 375  bias values\n",
    "- 282,000 parameters is the result of 750 neurons with 375 features + 750 bias values\n",
    "- 3,755   parameters is the result of 750 neurons with 5 features  + 5  bias values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2f2fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Training set\n",
      "\n",
      "X_train.........: (24746, 375)\n",
      "y_train.........: (24746,)\n",
      "y_train_OHEV....: (24746, 5)\n",
      "\n",
      "==================================\n",
      "Testing set\n",
      "\n",
      "X_test..........: (2750, 375)\n",
      "y_test..........: (2750,)\n",
      "y_test_OHEV.....: (2750, 5)\n",
      "\n",
      "==================================\n",
      "Validation set\n",
      "\n",
      "X_val_norm......: (3010, 375)\n",
      "y_val...........: (3010,)\n",
      "y_OHEV_val......: (3010, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==================================\")\n",
    "print(\"Training set\\n\")\n",
    "\n",
    "print(f'X_train.........: {np.shape(X_train)}')\n",
    "print(f'y_train.........: {np.shape(y_train)}')\n",
    "print(f'y_train_OHEV....: {np.shape(y_train_OHEV)}')\n",
    "\n",
    "print(\"\\n==================================\")\n",
    "print(\"Testing set\\n\")\n",
    "\n",
    "print(f'X_test..........: {np.shape(X_test)}')\n",
    "print(f'y_test..........: {np.shape(y_test)}')\n",
    "print(f'y_test_OHEV.....: {np.shape(y_test_OHEV)}')\n",
    "\n",
    "print(\"\\n==================================\")\n",
    "print(\"Validation set\\n\")\n",
    "\n",
    "print(f'X_val_norm......: {np.shape(X_val_norm)}')\n",
    "print(f'y_val...........: {np.shape(y_val)}')\n",
    "print(f'y_OHEV_val......: {np.shape(y_OHEV_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d16c3f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_OHEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41fe4f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.6720 - accuracy: 0.7555\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84327, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.6720 - accuracy: 0.7555 - val_loss: 0.4210 - val_accuracy: 0.8433\n",
      "Epoch 2/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.3677 - accuracy: 0.8681\n",
      "Epoch 00002: val_accuracy improved from 0.84327 to 0.88291, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.3662 - accuracy: 0.8688 - val_loss: 0.3344 - val_accuracy: 0.8829\n",
      "Epoch 3/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.2666 - accuracy: 0.9057\n",
      "Epoch 00003: val_accuracy improved from 0.88291 to 0.90473, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2673 - accuracy: 0.9056 - val_loss: 0.2736 - val_accuracy: 0.9047\n",
      "Epoch 4/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9310\n",
      "Epoch 00004: val_accuracy improved from 0.90473 to 0.91782, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2019 - accuracy: 0.9311 - val_loss: 0.2350 - val_accuracy: 0.9178\n",
      "Epoch 5/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.1560 - accuracy: 0.9478\n",
      "Epoch 00005: val_accuracy improved from 0.91782 to 0.92400, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1559 - accuracy: 0.9480 - val_loss: 0.2181 - val_accuracy: 0.9240\n",
      "Epoch 6/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9607\n",
      "Epoch 00006: val_accuracy improved from 0.92400 to 0.92836, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1198 - accuracy: 0.9608 - val_loss: 0.1989 - val_accuracy: 0.9284\n",
      "Epoch 7/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9688\n",
      "Epoch 00007: val_accuracy improved from 0.92836 to 0.93745, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0951 - accuracy: 0.9688 - val_loss: 0.1897 - val_accuracy: 0.9375\n",
      "Epoch 8/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9767\n",
      "Epoch 00008: val_accuracy did not improve from 0.93745\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0732 - accuracy: 0.9768 - val_loss: 0.1900 - val_accuracy: 0.9375\n",
      "Epoch 9/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9833\n",
      "Epoch 00009: val_accuracy improved from 0.93745 to 0.94073, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.9834 - val_loss: 0.1877 - val_accuracy: 0.9407\n",
      "Epoch 10/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0448 - accuracy: 0.9871\n",
      "Epoch 00010: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0451 - accuracy: 0.9869 - val_loss: 0.1978 - val_accuracy: 0.9404\n",
      "Epoch 11/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0344 - accuracy: 0.9901\n",
      "Epoch 00011: val_accuracy improved from 0.94073 to 0.94400, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.9901 - val_loss: 0.1843 - val_accuracy: 0.9440\n",
      "Epoch 12/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9932\n",
      "Epoch 00012: val_accuracy improved from 0.94400 to 0.94655, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9932 - val_loss: 0.1856 - val_accuracy: 0.9465\n",
      "Epoch 13/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9941\n",
      "Epoch 00013: val_accuracy improved from 0.94655 to 0.94764, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0225 - accuracy: 0.9943 - val_loss: 0.1810 - val_accuracy: 0.9476\n",
      "Epoch 14/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9957\n",
      "Epoch 00014: val_accuracy did not improve from 0.94764\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9957 - val_loss: 0.1876 - val_accuracy: 0.9451\n",
      "Epoch 15/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9962\n",
      "Epoch 00015: val_accuracy did not improve from 0.94764\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9961 - val_loss: 0.2029 - val_accuracy: 0.9473\n",
      "Epoch 16/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9975\n",
      "Epoch 00016: val_accuracy improved from 0.94764 to 0.95018, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9975 - val_loss: 0.1813 - val_accuracy: 0.9502\n",
      "Epoch 17/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 0.0108 - accuracy: 0.9975\n",
      "Epoch 00017: val_accuracy improved from 0.95018 to 0.95091, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0108 - accuracy: 0.9975 - val_loss: 0.1871 - val_accuracy: 0.9509\n",
      "Epoch 18/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9982\n",
      "Epoch 00018: val_accuracy did not improve from 0.95091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0096 - accuracy: 0.9981 - val_loss: 0.1910 - val_accuracy: 0.9509\n",
      "Epoch 19/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9988\n",
      "Epoch 00019: val_accuracy improved from 0.95091 to 0.95273, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0077 - accuracy: 0.9988 - val_loss: 0.1958 - val_accuracy: 0.9527\n",
      "Epoch 20/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0068 - accuracy: 0.9987\n",
      "Epoch 00020: val_accuracy did not improve from 0.95273\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.2041 - val_accuracy: 0.9469\n",
      "Epoch 21/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9991\n",
      "Epoch 00021: val_accuracy did not improve from 0.95273\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9991 - val_loss: 0.1962 - val_accuracy: 0.9495\n",
      "Epoch 22/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9988\n",
      "Epoch 00022: val_accuracy did not improve from 0.95273\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9988 - val_loss: 0.2019 - val_accuracy: 0.9520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9996\n",
      "Epoch 00023: val_accuracy did not improve from 0.95273\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 0.2110 - val_accuracy: 0.9509\n",
      "Epoch 24/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9993\n",
      "Epoch 00024: val_accuracy did not improve from 0.95273\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.1997 - val_accuracy: 0.9505\n",
      "Epoch 25/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9995\n",
      "Epoch 00025: val_accuracy improved from 0.95273 to 0.95382, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.2063 - val_accuracy: 0.9538\n",
      "Epoch 26/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9995\n",
      "Epoch 00026: val_accuracy did not improve from 0.95382\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2226 - val_accuracy: 0.9505\n",
      "Epoch 27/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9998\n",
      "Epoch 00027: val_accuracy did not improve from 0.95382\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.2087 - val_accuracy: 0.9538\n",
      "Epoch 28/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9995\n",
      "Epoch 00028: val_accuracy improved from 0.95382 to 0.95564, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.2029 - val_accuracy: 0.9556\n",
      "Epoch 29/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9997\n",
      "Epoch 00029: val_accuracy did not improve from 0.95564\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2290 - val_accuracy: 0.9469\n",
      "Epoch 30/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00030: val_accuracy did not improve from 0.95564\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2075 - val_accuracy: 0.9535\n",
      "Epoch 31/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00031: val_accuracy did not improve from 0.95564\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.2213 - val_accuracy: 0.9524\n",
      "Epoch 32/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0026 - accuracy: 0.9997\n",
      "Epoch 00032: val_accuracy did not improve from 0.95564\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.2196 - val_accuracy: 0.9531\n",
      "Epoch 33/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00033: val_accuracy did not improve from 0.95564\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.2143 - val_accuracy: 0.9520\n",
      "Epoch 34/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 00034: val_accuracy did not improve from 0.95564\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2117 - val_accuracy: 0.9524\n",
      "Epoch 35/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9999\n",
      "Epoch 00035: val_accuracy did not improve from 0.95564\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.2157 - val_accuracy: 0.9538\n",
      "Epoch 36/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00036: val_accuracy did not improve from 0.95564\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2211 - val_accuracy: 0.9524\n",
      "Epoch 37/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00037: val_accuracy improved from 0.95564 to 0.95600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2162 - val_accuracy: 0.9560\n",
      "Epoch 38/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00038: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2287 - val_accuracy: 0.9520\n",
      "Epoch 39/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00039: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2292 - val_accuracy: 0.9491\n",
      "Epoch 40/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 00040: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2314 - val_accuracy: 0.9505\n",
      "Epoch 41/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 00041: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.2292 - val_accuracy: 0.9520\n",
      "Epoch 42/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00042: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2318 - val_accuracy: 0.9545\n",
      "Epoch 43/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00043: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.2243 - val_accuracy: 0.9535\n",
      "Epoch 44/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00044: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2267 - val_accuracy: 0.9524\n",
      "Epoch 45/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00045: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2276 - val_accuracy: 0.9542\n",
      "Epoch 46/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00046: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2269 - val_accuracy: 0.9560\n",
      "Epoch 47/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 9.6551e-04 - accuracy: 0.9999\n",
      "Epoch 00047: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 9.5654e-04 - accuracy: 0.9999 - val_loss: 0.2345 - val_accuracy: 0.9513\n",
      "Epoch 48/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 9.7464e-04 - accuracy: 1.0000\n",
      "Epoch 00048: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 9.5630e-04 - accuracy: 1.0000 - val_loss: 0.2321 - val_accuracy: 0.9542\n",
      "Epoch 49/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00049: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2403 - val_accuracy: 0.9538\n",
      "Epoch 50/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00050: val_accuracy did not improve from 0.95600\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2333 - val_accuracy: 0.9549\n",
      "Epoch 51/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 7.4205e-04 - accuracy: 1.0000\n",
      "Epoch 00051: val_accuracy improved from 0.95600 to 0.95636, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.3982e-04 - accuracy: 1.0000 - val_loss: 0.2353 - val_accuracy: 0.9564\n",
      "Epoch 52/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 6.1019e-04 - accuracy: 1.0000\n",
      "Epoch 00052: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.1742e-04 - accuracy: 1.0000 - val_loss: 0.2427 - val_accuracy: 0.9531\n",
      "Epoch 53/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 6.9729e-04 - accuracy: 1.0000\n",
      "Epoch 00053: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8720e-04 - accuracy: 1.0000 - val_loss: 0.2333 - val_accuracy: 0.9527\n",
      "Epoch 54/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 6.9790e-04 - accuracy: 0.9999\n",
      "Epoch 00054: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.9790e-04 - accuracy: 0.9999 - val_loss: 0.2381 - val_accuracy: 0.9524\n",
      "Epoch 55/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 8.6538e-04 - accuracy: 0.9999\n",
      "Epoch 00055: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.5890e-04 - accuracy: 0.9999 - val_loss: 0.2376 - val_accuracy: 0.9538\n",
      "Epoch 56/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 7.5414e-04 - accuracy: 0.9999\n",
      "Epoch 00056: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.5198e-04 - accuracy: 0.9999 - val_loss: 0.2488 - val_accuracy: 0.9505\n",
      "Epoch 57/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 7.1300e-04 - accuracy: 0.9999\n",
      "Epoch 00057: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.1098e-04 - accuracy: 0.9999 - val_loss: 0.2514 - val_accuracy: 0.9516\n",
      "Epoch 58/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 7.4485e-04 - accuracy: 1.0000\n",
      "Epoch 00058: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 7.5129e-04 - accuracy: 1.0000 - val_loss: 0.2395 - val_accuracy: 0.9545\n",
      "Epoch 59/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 7.4015e-04 - accuracy: 0.9999\n",
      "Epoch 00059: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 7.3625e-04 - accuracy: 0.9999 - val_loss: 0.2407 - val_accuracy: 0.9542\n",
      "Epoch 60/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998  \n",
      "Epoch 00060: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2379 - val_accuracy: 0.9545\n",
      "Epoch 61/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00061: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 9.9302e-04 - accuracy: 0.9998 - val_loss: 0.2411 - val_accuracy: 0.9542\n",
      "Epoch 62/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00062: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.2357 - val_accuracy: 0.9538\n",
      "Epoch 63/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 7.4902e-04 - accuracy: 0.9999\n",
      "Epoch 00063: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.5349e-04 - accuracy: 0.9999 - val_loss: 0.2364 - val_accuracy: 0.9542\n",
      "Epoch 64/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 6.4065e-04 - accuracy: 1.0000\n",
      "Epoch 00064: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.3442e-04 - accuracy: 1.0000 - val_loss: 0.2364 - val_accuracy: 0.9553\n",
      "Epoch 65/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 6.4922e-04 - accuracy: 0.9999\n",
      "Epoch 00065: val_accuracy improved from 0.95636 to 0.95673, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.4617e-04 - accuracy: 0.9999 - val_loss: 0.2354 - val_accuracy: 0.9567\n",
      "Epoch 66/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 4.5946e-04 - accuracy: 1.0000\n",
      "Epoch 00066: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.5673e-04 - accuracy: 1.0000 - val_loss: 0.2375 - val_accuracy: 0.9560\n",
      "Epoch 67/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 5.8327e-04 - accuracy: 0.9999\n",
      "Epoch 00067: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.8566e-04 - accuracy: 0.9999 - val_loss: 0.2392 - val_accuracy: 0.9556\n",
      "Epoch 68/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 4.0823e-04 - accuracy: 1.0000\n",
      "Epoch 00068: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.0077e-04 - accuracy: 1.0000 - val_loss: 0.2367 - val_accuracy: 0.9553\n",
      "Epoch 69/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 6.1543e-04 - accuracy: 1.0000\n",
      "Epoch 00069: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.1318e-04 - accuracy: 1.0000 - val_loss: 0.2375 - val_accuracy: 0.9564\n",
      "Epoch 70/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 4.4733e-04 - accuracy: 1.0000\n",
      "Epoch 00070: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.4351e-04 - accuracy: 1.0000 - val_loss: 0.2373 - val_accuracy: 0.9560\n",
      "Epoch 71/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 3.8265e-04 - accuracy: 1.0000\n",
      "Epoch 00071: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.7943e-04 - accuracy: 1.0000 - val_loss: 0.2360 - val_accuracy: 0.9564\n",
      "Epoch 72/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 8.9098e-04 - accuracy: 0.9998\n",
      "Epoch 00072: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.8965e-04 - accuracy: 0.9998 - val_loss: 0.2438 - val_accuracy: 0.9531\n",
      "Epoch 73/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 4.4308e-04 - accuracy: 1.0000\n",
      "Epoch 00073: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.4957e-04 - accuracy: 1.0000 - val_loss: 0.2450 - val_accuracy: 0.9545\n",
      "Epoch 74/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 4.2401e-04 - accuracy: 1.0000\n",
      "Epoch 00074: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.1646e-04 - accuracy: 1.0000 - val_loss: 0.2471 - val_accuracy: 0.9549\n",
      "Epoch 75/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 4.9154e-04 - accuracy: 0.9999\n",
      "Epoch 00075: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.9104e-04 - accuracy: 0.9999 - val_loss: 0.2407 - val_accuracy: 0.9567\n",
      "Epoch 76/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 3.9255e-04 - accuracy: 1.0000\n",
      "Epoch 00076: val_accuracy did not improve from 0.95673\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8427e-04 - accuracy: 1.0000 - val_loss: 0.2407 - val_accuracy: 0.9556\n",
      "Epoch 77/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755/774 [============================>.] - ETA: 0s - loss: 5.9713e-04 - accuracy: 0.9999\n",
      "Epoch 00077: val_accuracy improved from 0.95673 to 0.95709, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.9353e-04 - accuracy: 0.9999 - val_loss: 0.2410 - val_accuracy: 0.9571\n",
      "Epoch 78/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 6.4664e-04 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.95709\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.4721e-04 - accuracy: 0.9999 - val_loss: 0.2464 - val_accuracy: 0.9549\n",
      "Epoch 79/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 6.2685e-04 - accuracy: 0.9999\n",
      "Epoch 00079: val_accuracy did not improve from 0.95709\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.1522e-04 - accuracy: 0.9999 - val_loss: 0.2437 - val_accuracy: 0.9564\n",
      "Epoch 80/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 4.7181e-04 - accuracy: 1.0000\n",
      "Epoch 00080: val_accuracy improved from 0.95709 to 0.95891, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7310e-04 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9589\n",
      "Epoch 81/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 3.5200e-04 - accuracy: 1.0000\n",
      "Epoch 00081: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.9783e-04 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9578\n",
      "Epoch 82/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 5.3237e-04 - accuracy: 1.0000\n",
      "Epoch 00082: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.2372e-04 - accuracy: 1.0000 - val_loss: 0.2469 - val_accuracy: 0.9545\n",
      "Epoch 83/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00083: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2631 - val_accuracy: 0.9487\n",
      "Epoch 84/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 8.2646e-04 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.3037e-04 - accuracy: 0.9998 - val_loss: 0.2594 - val_accuracy: 0.9513\n",
      "Epoch 85/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 00085: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.2482 - val_accuracy: 0.9535\n",
      "Epoch 86/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 9.0646e-04 - accuracy: 0.9999\n",
      "Epoch 00086: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.0108e-04 - accuracy: 0.9999 - val_loss: 0.2425 - val_accuracy: 0.9527\n",
      "Epoch 87/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 4.8242e-04 - accuracy: 1.0000\n",
      "Epoch 00087: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7815e-04 - accuracy: 1.0000 - val_loss: 0.2435 - val_accuracy: 0.9556\n",
      "Epoch 88/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 7.4511e-04 - accuracy: 0.9999\n",
      "Epoch 00088: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.7519e-04 - accuracy: 0.9998 - val_loss: 0.2443 - val_accuracy: 0.9549\n",
      "Epoch 89/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 5.9771e-04 - accuracy: 0.9999\n",
      "Epoch 00089: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.0416e-04 - accuracy: 0.9999 - val_loss: 0.2424 - val_accuracy: 0.9527\n",
      "Epoch 90/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 4.1109e-04 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.0343e-04 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 0.9513\n",
      "Epoch 91/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 4.7954e-04 - accuracy: 1.0000\n",
      "Epoch 00091: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7403e-04 - accuracy: 1.0000 - val_loss: 0.2550 - val_accuracy: 0.9535\n",
      "Epoch 92/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 3.8385e-04 - accuracy: 1.0000\n",
      "Epoch 00092: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8208e-04 - accuracy: 1.0000 - val_loss: 0.2469 - val_accuracy: 0.9524\n",
      "Epoch 93/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 4.8644e-04 - accuracy: 0.9999\n",
      "Epoch 00093: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.8238e-04 - accuracy: 0.9999 - val_loss: 0.2523 - val_accuracy: 0.9538\n",
      "Epoch 94/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 2.8394e-04 - accuracy: 1.0000\n",
      "Epoch 00094: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.8418e-04 - accuracy: 1.0000 - val_loss: 0.2468 - val_accuracy: 0.9538\n",
      "Epoch 95/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 4.0285e-04 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.2461e-04 - accuracy: 1.0000 - val_loss: 0.2506 - val_accuracy: 0.9545\n",
      "Epoch 96/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 3.1121e-04 - accuracy: 1.0000\n",
      "Epoch 00096: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 3.1119e-04 - accuracy: 1.0000 - val_loss: 0.2496 - val_accuracy: 0.9542\n",
      "Epoch 97/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 3.2906e-04 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 3.2886e-04 - accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9538\n",
      "Epoch 98/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 4.5011e-04 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 4.4581e-04 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 0.9545\n",
      "Epoch 99/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 2.6804e-04 - accuracy: 1.0000\n",
      "Epoch 00099: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.7019e-04 - accuracy: 1.0000 - val_loss: 0.2532 - val_accuracy: 0.9509\n",
      "Epoch 100/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 3.2918e-04 - accuracy: 1.0000\n",
      "Epoch 00100: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.2866e-04 - accuracy: 1.0000 - val_loss: 0.2518 - val_accuracy: 0.9556\n",
      "Epoch 101/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 5.9936e-04 - accuracy: 0.9999\n",
      "Epoch 00101: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.9420e-04 - accuracy: 0.9999 - val_loss: 0.2468 - val_accuracy: 0.9571\n",
      "Epoch 102/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 3.7393e-04 - accuracy: 1.0000\n",
      "Epoch 00102: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.7025e-04 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.9535\n",
      "Epoch 103/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 2.9800e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 2.9654e-04 - accuracy: 1.0000 - val_loss: 0.2504 - val_accuracy: 0.9571\n",
      "Epoch 104/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754/774 [============================>.] - ETA: 0s - loss: 2.9196e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 2.8934e-04 - accuracy: 1.0000 - val_loss: 0.2532 - val_accuracy: 0.9549\n",
      "Epoch 105/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 3.4828e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 3.4770e-04 - accuracy: 1.0000 - val_loss: 0.2520 - val_accuracy: 0.9560\n",
      "Epoch 106/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 3.5866e-04 - accuracy: 1.0000\n",
      "Epoch 00106: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 3.5793e-04 - accuracy: 1.0000 - val_loss: 0.2549 - val_accuracy: 0.9535\n",
      "Epoch 107/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 2.9030e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.9025e-04 - accuracy: 1.0000 - val_loss: 0.2547 - val_accuracy: 0.9538\n",
      "Epoch 108/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 2.6231e-04 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.6158e-04 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 0.9549\n",
      "Epoch 109/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 3.8949e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8311e-04 - accuracy: 0.9999 - val_loss: 0.2506 - val_accuracy: 0.9542\n",
      "Epoch 110/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 2.1911e-04 - accuracy: 1.0000\n",
      "Epoch 00110: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.1513e-04 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.9549\n",
      "Epoch 111/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 2.7252e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.7164e-04 - accuracy: 1.0000 - val_loss: 0.2466 - val_accuracy: 0.9549\n",
      "Epoch 112/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 4.8017e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 4.7022e-04 - accuracy: 0.9998 - val_loss: 0.2573 - val_accuracy: 0.9542\n",
      "Epoch 113/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 2.6278e-04 - accuracy: 1.0000\n",
      "Epoch 00113: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 2.5984e-04 - accuracy: 1.0000 - val_loss: 0.2515 - val_accuracy: 0.9560\n",
      "Epoch 114/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 2.2220e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 2.2135e-04 - accuracy: 1.0000 - val_loss: 0.2551 - val_accuracy: 0.9549\n",
      "Epoch 115/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 1.9570e-04 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 1.9917e-04 - accuracy: 1.0000 - val_loss: 0.2502 - val_accuracy: 0.9564\n",
      "Epoch 116/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 2.0026e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.0011e-04 - accuracy: 1.0000 - val_loss: 0.2488 - val_accuracy: 0.9553\n",
      "Epoch 117/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 2.2490e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.2237e-04 - accuracy: 1.0000 - val_loss: 0.2553 - val_accuracy: 0.9556\n",
      "Epoch 118/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 2.3117e-04 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.3012e-04 - accuracy: 1.0000 - val_loss: 0.2569 - val_accuracy: 0.9549\n",
      "Epoch 119/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 3.9263e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.9265e-04 - accuracy: 0.9999 - val_loss: 0.2569 - val_accuracy: 0.9545\n",
      "Epoch 120/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 2.2416e-04 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 2.2416e-04 - accuracy: 1.0000 - val_loss: 0.2569 - val_accuracy: 0.9553\n",
      "Epoch 121/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 3.1757e-04 - accuracy: 1.0000\n",
      "Epoch 00121: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 3.2151e-04 - accuracy: 1.0000 - val_loss: 0.2522 - val_accuracy: 0.9545\n",
      "Epoch 122/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 1.8389e-04 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 1.8352e-04 - accuracy: 1.0000 - val_loss: 0.2514 - val_accuracy: 0.9538\n",
      "Epoch 123/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00123: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2543 - val_accuracy: 0.9549\n",
      "Epoch 124/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 2.8466e-04 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 2.8558e-04 - accuracy: 1.0000 - val_loss: 0.2586 - val_accuracy: 0.9524\n",
      "Epoch 125/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 4.1941e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 4.1330e-04 - accuracy: 0.9999 - val_loss: 0.2542 - val_accuracy: 0.9535\n",
      "Epoch 126/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 2.0046e-04 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.0221e-04 - accuracy: 1.0000 - val_loss: 0.2529 - val_accuracy: 0.9535\n",
      "Epoch 127/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 1.9551e-04 - accuracy: 1.0000\n",
      "Epoch 00127: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.9451e-04 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 0.9549\n",
      "Epoch 128/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 1.8369e-04 - accuracy: 1.0000\n",
      "Epoch 00128: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.9595e-04 - accuracy: 1.0000 - val_loss: 0.2511 - val_accuracy: 0.9564\n",
      "Epoch 129/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 2.5439e-04 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.8592e-04 - accuracy: 0.9999 - val_loss: 0.2611 - val_accuracy: 0.9524\n",
      "Epoch 130/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 2.1970e-04 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.1685e-04 - accuracy: 1.0000 - val_loss: 0.2569 - val_accuracy: 0.9560\n",
      "Epoch 131/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 2.0358e-04 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.0256e-04 - accuracy: 1.0000 - val_loss: 0.2554 - val_accuracy: 0.9531\n",
      "Epoch 132/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/774 [============================>.] - ETA: 0s - loss: 2.0883e-04 - accuracy: 1.0000\n",
      "Epoch 00132: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.1133e-04 - accuracy: 1.0000 - val_loss: 0.2583 - val_accuracy: 0.9542\n",
      "Epoch 133/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 1.9420e-04 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.9405e-04 - accuracy: 1.0000 - val_loss: 0.2579 - val_accuracy: 0.9549\n",
      "Epoch 134/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 1.3220e-04 - accuracy: 1.0000\n",
      "Epoch 00134: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.3170e-04 - accuracy: 1.0000 - val_loss: 0.2544 - val_accuracy: 0.9545\n",
      "Epoch 135/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 2.0207e-04 - accuracy: 1.0000\n",
      "Epoch 00135: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.9879e-04 - accuracy: 1.0000 - val_loss: 0.2569 - val_accuracy: 0.9556\n",
      "Epoch 136/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 2.2027e-04 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.1736e-04 - accuracy: 1.0000 - val_loss: 0.2560 - val_accuracy: 0.9538\n",
      "Epoch 137/350\n",
      "742/774 [===========================>..] - ETA: 0s - loss: 3.1584e-04 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.0824e-04 - accuracy: 1.0000 - val_loss: 0.2543 - val_accuracy: 0.9549\n",
      "Epoch 138/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 4.2860e-04 - accuracy: 0.9998\n",
      "Epoch 00138: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.2860e-04 - accuracy: 0.9998 - val_loss: 0.2654 - val_accuracy: 0.9516\n",
      "Epoch 139/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 2.8611e-04 - accuracy: 0.9999\n",
      "Epoch 00139: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.8427e-04 - accuracy: 0.9999 - val_loss: 0.2617 - val_accuracy: 0.9545\n",
      "Epoch 140/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 1.7291e-04 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.7983e-04 - accuracy: 1.0000 - val_loss: 0.2635 - val_accuracy: 0.9535\n",
      "Epoch 141/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 2.4185e-04 - accuracy: 1.0000\n",
      "Epoch 00141: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.3715e-04 - accuracy: 1.0000 - val_loss: 0.2607 - val_accuracy: 0.9564\n",
      "Epoch 142/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 1.7430e-04 - accuracy: 1.0000\n",
      "Epoch 00142: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.7276e-04 - accuracy: 1.0000 - val_loss: 0.2589 - val_accuracy: 0.9567\n",
      "Epoch 143/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 2.2438e-04 - accuracy: 1.0000\n",
      "Epoch 00143: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.2212e-04 - accuracy: 1.0000 - val_loss: 0.2639 - val_accuracy: 0.9567\n",
      "Epoch 144/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 2.2356e-04 - accuracy: 1.0000\n",
      "Epoch 00144: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.2514e-04 - accuracy: 1.0000 - val_loss: 0.2800 - val_accuracy: 0.9520\n",
      "Epoch 145/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 1.7640e-04 - accuracy: 1.0000\n",
      "Epoch 00145: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.7532e-04 - accuracy: 1.0000 - val_loss: 0.2562 - val_accuracy: 0.9564\n",
      "Epoch 146/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 1.4330e-04 - accuracy: 1.0000\n",
      "Epoch 00146: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.4337e-04 - accuracy: 1.0000 - val_loss: 0.2636 - val_accuracy: 0.9549\n",
      "Epoch 147/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 2.3449e-04 - accuracy: 1.0000\n",
      "Epoch 00147: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.3283e-04 - accuracy: 1.0000 - val_loss: 0.2648 - val_accuracy: 0.9556\n",
      "Epoch 148/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 1.7574e-04 - accuracy: 1.0000\n",
      "Epoch 00148: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.7255e-04 - accuracy: 1.0000 - val_loss: 0.2689 - val_accuracy: 0.9564\n",
      "Epoch 149/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 2.0236e-04 - accuracy: 1.0000\n",
      "Epoch 00149: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.9940e-04 - accuracy: 1.0000 - val_loss: 0.2675 - val_accuracy: 0.9564\n",
      "Epoch 150/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 1.8205e-04 - accuracy: 1.0000\n",
      "Epoch 00150: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.8198e-04 - accuracy: 1.0000 - val_loss: 0.2679 - val_accuracy: 0.9556\n",
      "Epoch 151/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 1.4491e-04 - accuracy: 1.0000\n",
      "Epoch 00151: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.4457e-04 - accuracy: 1.0000 - val_loss: 0.2618 - val_accuracy: 0.9553\n",
      "Epoch 152/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 2.8457e-04 - accuracy: 1.0000\n",
      "Epoch 00152: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.8271e-04 - accuracy: 1.0000 - val_loss: 0.2628 - val_accuracy: 0.9549\n",
      "Epoch 153/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 2.3715e-04 - accuracy: 1.0000\n",
      "Epoch 00153: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.3550e-04 - accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 0.9531\n",
      "Epoch 154/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 1.5892e-04 - accuracy: 1.0000\n",
      "Epoch 00154: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.5743e-04 - accuracy: 1.0000 - val_loss: 0.2632 - val_accuracy: 0.9538\n",
      "Epoch 155/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 1.4328e-04 - accuracy: 1.0000\n",
      "Epoch 00155: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.4625e-04 - accuracy: 1.0000 - val_loss: 0.2664 - val_accuracy: 0.9538\n",
      "Epoch 156/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 1.1213e-04 - accuracy: 1.0000\n",
      "Epoch 00156: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1194e-04 - accuracy: 1.0000 - val_loss: 0.2667 - val_accuracy: 0.9549\n",
      "Epoch 157/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 9.9273e-05 - accuracy: 1.0000\n",
      "Epoch 00157: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.9256e-05 - accuracy: 1.0000 - val_loss: 0.2644 - val_accuracy: 0.9545\n",
      "Epoch 158/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 1.1904e-04 - accuracy: 1.0000\n",
      "Epoch 00158: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1906e-04 - accuracy: 1.0000 - val_loss: 0.2605 - val_accuracy: 0.9567\n",
      "Epoch 159/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 2.4757e-04 - accuracy: 1.0000\n",
      "Epoch 00159: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.4708e-04 - accuracy: 1.0000 - val_loss: 0.2671 - val_accuracy: 0.9545\n",
      "Epoch 160/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743/774 [===========================>..] - ETA: 0s - loss: 1.2604e-04 - accuracy: 1.0000\n",
      "Epoch 00160: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.2605e-04 - accuracy: 1.0000 - val_loss: 0.2578 - val_accuracy: 0.9564\n",
      "Epoch 161/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 2.8658e-04 - accuracy: 0.9999\n",
      "Epoch 00161: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.8628e-04 - accuracy: 0.9999 - val_loss: 0.2507 - val_accuracy: 0.9578\n",
      "Epoch 162/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 1.4180e-04 - accuracy: 1.0000\n",
      "Epoch 00162: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.4092e-04 - accuracy: 1.0000 - val_loss: 0.2526 - val_accuracy: 0.9545\n",
      "Epoch 163/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 1.5729e-04 - accuracy: 1.0000\n",
      "Epoch 00163: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.5600e-04 - accuracy: 1.0000 - val_loss: 0.2502 - val_accuracy: 0.9578\n",
      "Epoch 164/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 1.5178e-04 - accuracy: 1.0000\n",
      "Epoch 00164: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.5339e-04 - accuracy: 1.0000 - val_loss: 0.2526 - val_accuracy: 0.9560\n",
      "Epoch 165/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 1.3289e-04 - accuracy: 1.0000\n",
      "Epoch 00165: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.3331e-04 - accuracy: 1.0000 - val_loss: 0.2527 - val_accuracy: 0.9578\n",
      "Epoch 166/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 1.3132e-04 - accuracy: 1.0000\n",
      "Epoch 00166: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.3844e-04 - accuracy: 1.0000 - val_loss: 0.2631 - val_accuracy: 0.9564\n",
      "Epoch 167/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 1.2455e-04 - accuracy: 1.0000\n",
      "Epoch 00167: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.2282e-04 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 0.9549\n",
      "Epoch 168/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 2.1474e-04 - accuracy: 1.0000\n",
      "Epoch 00168: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.0998e-04 - accuracy: 1.0000 - val_loss: 0.2599 - val_accuracy: 0.9538\n",
      "Epoch 169/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 1.7508e-04 - accuracy: 1.0000\n",
      "Epoch 00169: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.7502e-04 - accuracy: 1.0000 - val_loss: 0.2622 - val_accuracy: 0.9538\n",
      "Epoch 170/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 1.7041e-04 - accuracy: 1.0000\n",
      "Epoch 00170: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.7227e-04 - accuracy: 1.0000 - val_loss: 0.2609 - val_accuracy: 0.9553\n",
      "Epoch 171/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 1.5018e-04 - accuracy: 1.0000\n",
      "Epoch 00171: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.4971e-04 - accuracy: 1.0000 - val_loss: 0.2613 - val_accuracy: 0.9564\n",
      "Epoch 172/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 1.1495e-04 - accuracy: 1.0000\n",
      "Epoch 00172: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1433e-04 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 0.9564\n",
      "Epoch 173/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 1.1845e-04 - accuracy: 1.0000\n",
      "Epoch 00173: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1778e-04 - accuracy: 1.0000 - val_loss: 0.2664 - val_accuracy: 0.9549\n",
      "Epoch 174/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 1.0750e-04 - accuracy: 1.0000\n",
      "Epoch 00174: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.0570e-04 - accuracy: 1.0000 - val_loss: 0.2679 - val_accuracy: 0.9553\n",
      "Epoch 175/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 9.1387e-05 - accuracy: 1.0000\n",
      "Epoch 00175: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.4799e-05 - accuracy: 1.0000 - val_loss: 0.2686 - val_accuracy: 0.9549\n",
      "Epoch 176/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 4.2617e-04 - accuracy: 0.9999\n",
      "Epoch 00176: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.2404e-04 - accuracy: 0.9999 - val_loss: 0.2740 - val_accuracy: 0.9527\n",
      "Epoch 177/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 2.6553e-04 - accuracy: 0.9999\n",
      "Epoch 00177: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.6514e-04 - accuracy: 0.9999 - val_loss: 0.2740 - val_accuracy: 0.9542\n",
      "Epoch 178/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 1.6342e-04 - accuracy: 1.0000\n",
      "Epoch 00178: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.7154e-04 - accuracy: 1.0000 - val_loss: 0.2720 - val_accuracy: 0.9538\n",
      "Epoch 179/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 1.4702e-04 - accuracy: 1.0000\n",
      "Epoch 00179: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.4467e-04 - accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 0.9542\n",
      "Epoch 180/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 1.2736e-04 - accuracy: 1.0000\n",
      "Epoch 00180: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.2615e-04 - accuracy: 1.0000 - val_loss: 0.2615 - val_accuracy: 0.9545\n",
      "Epoch 181/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 9.5062e-05 - accuracy: 1.0000\n",
      "Epoch 00181: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.7848e-05 - accuracy: 1.0000 - val_loss: 0.2679 - val_accuracy: 0.9560\n",
      "Epoch 182/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 1.1826e-04 - accuracy: 1.0000\n",
      "Epoch 00182: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1791e-04 - accuracy: 1.0000 - val_loss: 0.2695 - val_accuracy: 0.9538\n",
      "Epoch 183/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 9.9576e-05 - accuracy: 1.0000\n",
      "Epoch 00183: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.9011e-05 - accuracy: 1.0000 - val_loss: 0.2703 - val_accuracy: 0.9542\n",
      "Epoch 184/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 1.0468e-04 - accuracy: 1.0000\n",
      "Epoch 00184: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.0507e-04 - accuracy: 1.0000 - val_loss: 0.2695 - val_accuracy: 0.9542\n",
      "Epoch 185/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 1.6540e-04 - accuracy: 0.9999\n",
      "Epoch 00185: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.6540e-04 - accuracy: 0.9999 - val_loss: 0.2731 - val_accuracy: 0.9535\n",
      "Epoch 186/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 1.1772e-04 - accuracy: 1.0000\n",
      "Epoch 00186: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1533e-04 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9549\n",
      "Epoch 187/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 1.1536e-04 - accuracy: 1.0000\n",
      "Epoch 00187: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1679e-04 - accuracy: 1.0000 - val_loss: 0.2720 - val_accuracy: 0.9545\n",
      "Epoch 188/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774/774 [==============================] - ETA: 0s - loss: 1.0858e-04 - accuracy: 1.0000\n",
      "Epoch 00188: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.0858e-04 - accuracy: 1.0000 - val_loss: 0.2763 - val_accuracy: 0.9556\n",
      "Epoch 189/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 8.8954e-05 - accuracy: 1.0000\n",
      "Epoch 00189: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.9985e-05 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 0.9545\n",
      "Epoch 190/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 9.0177e-05 - accuracy: 1.0000\n",
      "Epoch 00190: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.9283e-05 - accuracy: 1.0000 - val_loss: 0.2709 - val_accuracy: 0.9553\n",
      "Epoch 191/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 2.7526e-04 - accuracy: 0.9999\n",
      "Epoch 00191: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.7107e-04 - accuracy: 0.9999 - val_loss: 0.2806 - val_accuracy: 0.9535\n",
      "Epoch 192/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 1.6966e-04 - accuracy: 1.0000\n",
      "Epoch 00192: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.6850e-04 - accuracy: 1.0000 - val_loss: 0.2792 - val_accuracy: 0.9538\n",
      "Epoch 193/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 1.0383e-04 - accuracy: 1.0000\n",
      "Epoch 00193: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.0375e-04 - accuracy: 1.0000 - val_loss: 0.2757 - val_accuracy: 0.9556\n",
      "Epoch 194/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 1.1168e-04 - accuracy: 1.0000\n",
      "Epoch 00194: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1103e-04 - accuracy: 1.0000 - val_loss: 0.2749 - val_accuracy: 0.9545\n",
      "Epoch 195/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 8.2586e-05 - accuracy: 1.0000\n",
      "Epoch 00195: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.2054e-05 - accuracy: 1.0000 - val_loss: 0.2719 - val_accuracy: 0.9560\n",
      "Epoch 196/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 1.0230e-04 - accuracy: 1.0000\n",
      "Epoch 00196: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.0230e-04 - accuracy: 1.0000 - val_loss: 0.2733 - val_accuracy: 0.9560\n",
      "Epoch 197/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 8.4281e-05 - accuracy: 1.0000\n",
      "Epoch 00197: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.3454e-05 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 0.9553\n",
      "Epoch 198/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 9.0389e-05 - accuracy: 1.0000\n",
      "Epoch 00198: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.9842e-05 - accuracy: 1.0000 - val_loss: 0.2715 - val_accuracy: 0.9567\n",
      "Epoch 199/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 7.1848e-05 - accuracy: 1.0000\n",
      "Epoch 00199: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.0994e-05 - accuracy: 1.0000 - val_loss: 0.2761 - val_accuracy: 0.9553\n",
      "Epoch 200/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 1.0533e-04 - accuracy: 1.0000\n",
      "Epoch 00200: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 1.0480e-04 - accuracy: 1.0000 - val_loss: 0.2758 - val_accuracy: 0.9549\n",
      "Epoch 201/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 4.4115e-04 - accuracy: 0.9998\n",
      "Epoch 00201: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 4.3843e-04 - accuracy: 0.9998 - val_loss: 0.2728 - val_accuracy: 0.9549\n",
      "Epoch 202/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 2.4313e-04 - accuracy: 1.0000\n",
      "Epoch 00202: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 2.4015e-04 - accuracy: 1.0000 - val_loss: 0.2939 - val_accuracy: 0.9480\n",
      "Epoch 203/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 1.4793e-04 - accuracy: 1.0000\n",
      "Epoch 00203: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.4787e-04 - accuracy: 1.0000 - val_loss: 0.2753 - val_accuracy: 0.9553\n",
      "Epoch 204/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 1.3339e-04 - accuracy: 1.0000\n",
      "Epoch 00204: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 1.3245e-04 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9542\n",
      "Epoch 205/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 2.1145e-04 - accuracy: 0.9999\n",
      "Epoch 00205: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 2.1129e-04 - accuracy: 0.9999 - val_loss: 0.2738 - val_accuracy: 0.9545\n",
      "Epoch 206/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 1.1754e-04 - accuracy: 1.0000\n",
      "Epoch 00206: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 1.1750e-04 - accuracy: 1.0000 - val_loss: 0.2760 - val_accuracy: 0.9538\n",
      "Epoch 207/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 1.1447e-04 - accuracy: 1.0000\n",
      "Epoch 00207: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 1.1442e-04 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9549\n",
      "Epoch 208/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 8.9843e-05 - accuracy: 1.0000\n",
      "Epoch 00208: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 8.8953e-05 - accuracy: 1.0000 - val_loss: 0.2784 - val_accuracy: 0.9542\n",
      "Epoch 209/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 8.7259e-05 - accuracy: 1.0000\n",
      "Epoch 00209: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 8.6972e-05 - accuracy: 1.0000 - val_loss: 0.2749 - val_accuracy: 0.9564\n",
      "Epoch 210/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 1.6159e-04 - accuracy: 1.0000\n",
      "Epoch 00210: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 1.5948e-04 - accuracy: 1.0000 - val_loss: 0.2756 - val_accuracy: 0.9560\n",
      "Epoch 211/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 1.3469e-04 - accuracy: 1.0000\n",
      "Epoch 00211: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 1.3928e-04 - accuracy: 1.0000 - val_loss: 0.2726 - val_accuracy: 0.9556\n",
      "Epoch 212/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 8.7375e-05 - accuracy: 1.0000\n",
      "Epoch 00212: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.7258e-05 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9564\n",
      "Epoch 213/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 8.2590e-05 - accuracy: 1.0000\n",
      "Epoch 00213: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 8.1794e-05 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 0.9553\n",
      "Epoch 214/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 8.2722e-05 - accuracy: 1.0000\n",
      "Epoch 00214: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 8.2559e-05 - accuracy: 1.0000 - val_loss: 0.2709 - val_accuracy: 0.9567\n",
      "Epoch 215/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 1.0903e-04 - accuracy: 1.0000\n",
      "Epoch 00215: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 1.0816e-04 - accuracy: 1.0000 - val_loss: 0.2708 - val_accuracy: 0.9538\n",
      "Epoch 216/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759/774 [============================>.] - ETA: 0s - loss: 1.6191e-04 - accuracy: 1.0000\n",
      "Epoch 00216: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.6183e-04 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 0.9571\n",
      "Epoch 217/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 9.2253e-05 - accuracy: 1.0000\n",
      "Epoch 00217: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 9.1555e-05 - accuracy: 1.0000 - val_loss: 0.2716 - val_accuracy: 0.9567\n",
      "Epoch 218/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 8.9270e-05 - accuracy: 1.0000\n",
      "Epoch 00218: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 8.9296e-05 - accuracy: 1.0000 - val_loss: 0.2701 - val_accuracy: 0.9575\n",
      "Epoch 219/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 1.2011e-04 - accuracy: 1.0000\n",
      "Epoch 00219: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1829e-04 - accuracy: 1.0000 - val_loss: 0.2732 - val_accuracy: 0.9560\n",
      "Epoch 220/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 4.7649e-04 - accuracy: 0.9999\n",
      "Epoch 00220: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7632e-04 - accuracy: 0.9999 - val_loss: 0.2674 - val_accuracy: 0.9575\n",
      "Epoch 221/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 9.8227e-05 - accuracy: 1.0000\n",
      "Epoch 00221: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.7741e-05 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9564\n",
      "Epoch 222/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 9.4719e-05 - accuracy: 1.0000\n",
      "Epoch 00222: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.4212e-05 - accuracy: 1.0000 - val_loss: 0.2734 - val_accuracy: 0.9567\n",
      "Epoch 223/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 1.1997e-04 - accuracy: 1.0000\n",
      "Epoch 00223: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1766e-04 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 0.9571\n",
      "Epoch 224/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 1.1607e-04 - accuracy: 1.0000\n",
      "Epoch 00224: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.1374e-04 - accuracy: 1.0000 - val_loss: 0.2772 - val_accuracy: 0.9560\n",
      "Epoch 225/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 1.4190e-04 - accuracy: 1.0000\n",
      "Epoch 00225: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 1.3971e-04 - accuracy: 1.0000 - val_loss: 0.2806 - val_accuracy: 0.9571\n",
      "Epoch 226/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 9.8313e-05 - accuracy: 1.0000\n",
      "Epoch 00226: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.7664e-05 - accuracy: 1.0000 - val_loss: 0.2802 - val_accuracy: 0.9556\n",
      "Epoch 227/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 8.0584e-05 - accuracy: 1.0000\n",
      "Epoch 00227: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.9910e-05 - accuracy: 1.0000 - val_loss: 0.2761 - val_accuracy: 0.9560\n",
      "Epoch 228/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 7.3801e-05 - accuracy: 1.0000\n",
      "Epoch 00228: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.3224e-05 - accuracy: 1.0000 - val_loss: 0.2736 - val_accuracy: 0.9560\n",
      "Epoch 229/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 7.6021e-05 - accuracy: 1.0000\n",
      "Epoch 00229: val_accuracy did not improve from 0.95891\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 7.5127e-05 - accuracy: 1.0000 - val_loss: 0.2780 - val_accuracy: 0.9556\n",
      "Epoch 230/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 2.5783e-04 - accuracy: 0.9999\n",
      "Epoch 00230: val_accuracy did not improve from 0.95891\n",
      "Restoring model weights from the end of the best epoch.\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 2.5397e-04 - accuracy: 0.9999 - val_loss: 0.2706 - val_accuracy: 0.9564\n",
      "Epoch 00230: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size_ANN = 32\n",
    "epochs_ANN     = 350\n",
    "\n",
    "history_ANN    = model_ANN.fit(X_train, y_train_OHEV,\n",
    "                               batch_size      = batch_size_ANN,\n",
    "                               epochs          = epochs_ANN,\n",
    "                               verbose         = 1,\n",
    "                               validation_data = (X_test, y_test_OHEV),\n",
    "                               callbacks       = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "337ca30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2433234602212906\n",
      "Test accuracy: 0.9589090943336487\n"
     ]
    }
   ],
   "source": [
    "score_ANN = model_ANN.evaluate(X_test, y_test_OHEV, verbose=0, batch_size = 32)\n",
    "print('Test loss:', score_ANN[0])\n",
    "print('Test accuracy:', score_ANN[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84b5d2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9589090943336487"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ANN[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "864b45b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "fig.suptitle(nom_dataset + norm_type + model_surname + ' - ANN - Training / Testing loss and accuracy', fontsize = 18)\n",
    "ax[0].plot(history_ANN.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history_ANN.history['val_loss'], color='r', label=\"Testing loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True, fontsize = 14)\n",
    "ax[0].tick_params(axis='x', labelsize=14)\n",
    "ax[0].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "ax[1].plot(history_ANN.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history_ANN.history['val_accuracy'], color='r',label=\"Testing accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True, fontsize = 14)\n",
    "ax[1].tick_params(axis='x', labelsize=14)\n",
    "ax[1].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(os.path.join(path_pic, picture_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1723f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file (not the best model though)\n",
    "\n",
    "#model_ANN.save(path_models + \"Model_ANN.h5\")\n",
    "#print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4c8a5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ANN = np.argmax(model_ANN.predict(X_val_norm),axis=1)\n",
    "y_pred_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c71e5f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc = np.argmax(y_OHEV_val, axis=1)\n",
    "y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b650a54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.84      0.79      0.82       756\n",
      "        car_horn       0.82      0.88      0.85       252\n",
      "children_playing       0.76      0.81      0.79       700\n",
      "        dog_bark       0.74      0.86      0.80       700\n",
      "           siren       0.90      0.70      0.79       602\n",
      "\n",
      "        accuracy                           0.80      3010\n",
      "       macro avg       0.81      0.81      0.81      3010\n",
      "    weighted avg       0.81      0.80      0.80      3010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_set_ANN = classification_report(y_test_enc, y_pred_ANN, target_names=nom_classes)\n",
    "print(metrics_set_ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07704de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ANN_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 375)               141000    \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 375)               141000    \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               282000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 567,755\n",
      "Trainable params: 567,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the highest accuracy\n",
    "\n",
    "model_ANN_saved = load_model(os.path.join(path_models, 'Model_ANN_weights_0_best' + norm_type + model_surname + '.hdf5'))\n",
    "model_ANN_saved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "690dfa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 1ms/step - loss: 2.7111 - accuracy: 0.8027\n",
      "Test loss: 2.711075782775879\n",
      "Test accuracy: 0.8026577830314636\n"
     ]
    }
   ],
   "source": [
    "score_ANN_saved = model_ANN_saved.evaluate(X_val_norm, y_OHEV_val, verbose=1, batch_size = 32)\n",
    "print('Test loss:', score_ANN_saved[0])\n",
    "print('Test accuracy:', score_ANN_saved[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "62882b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ANN_saved = np.argmax(model_ANN_saved.predict(X_val_norm),axis=1)\n",
    "y_pred_ANN_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eceba4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.84      0.79      0.82       756\n",
      "        car_horn       0.82      0.88      0.85       252\n",
      "children_playing       0.76      0.81      0.79       700\n",
      "        dog_bark       0.74      0.86      0.80       700\n",
      "           siren       0.90      0.70      0.79       602\n",
      "\n",
      "        accuracy                           0.80      3010\n",
      "       macro avg       0.81      0.81      0.81      3010\n",
      "    weighted avg       0.81      0.80      0.80      3010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_set_ANN_saved = classification_report(y_test_enc, y_pred_ANN_saved, target_names=nom_classes)\n",
    "print(metrics_set_ANN_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ade4c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple confusion matrix\n",
    "\n",
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(y_test_enc, y_pred_ANN_saved)\n",
    "title = nom_dataset + norm_type + model_surname + ' - Classifier ANN (best model) - Highest accuracy test: '+ str(\"{:0.2f}%\".format(score_ANN_saved[1]*100))\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True, \n",
    "            fmt='g', \n",
    "            cmap=cmap_cm, \n",
    "            annot_kws={\"size\": 8}, \n",
    "            xticklabels=nom_classes, \n",
    "            yticklabels=nom_classes)\n",
    "plt.title(title, fontsize = 12)\n",
    "plt.savefig(os.path.join(path_pic, picture_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca7b9cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x2667af52fa0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2667ad0c8b0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x265c3f8d430>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x265cc999d00>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x2667ac07e80>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x265cc9998e0>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ANN_saved.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f108fca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00617652,  0.0074141 , -0.03259105, ..., -0.06934977,\n",
      "        -0.01211499,  0.01012983],\n",
      "       [-0.03593363, -0.02493946, -0.10158896, ...,  0.0765382 ,\n",
      "         0.1033669 ,  0.06684168],\n",
      "       [-0.06668829,  0.0580395 ,  0.06656573, ..., -0.08797868,\n",
      "        -0.02697253, -0.01941396],\n",
      "       ...,\n",
      "       [-0.0320451 , -0.00680774, -0.0255002 , ..., -0.02849849,\n",
      "         0.00094115,  0.10453606],\n",
      "       [-0.02970457, -0.00199844, -0.03100302, ..., -0.00411687,\n",
      "         0.03040912, -0.05984844],\n",
      "       [ 0.0093646 , -0.0233783 , -0.06762841, ..., -0.06590807,\n",
      "         0.07432549,  0.06687903]], dtype=float32), array([-2.28268933e-02, -5.85257914e-03,  2.55066925e-03,  5.00838272e-03,\n",
      "        9.50783119e-03,  1.35360453e-02, -2.30955202e-02, -1.17225489e-02,\n",
      "        1.52969155e-02,  1.35888197e-02,  9.24894819e-04, -1.77176762e-02,\n",
      "       -2.55466998e-02, -5.91116445e-03,  7.46766711e-03, -6.15090504e-03,\n",
      "       -6.68980600e-03, -2.92973183e-02, -7.04758521e-03, -1.94932881e-03,\n",
      "        1.71450339e-02,  2.23674793e-02,  1.75733697e-02, -8.64203647e-03,\n",
      "        6.16869750e-03, -2.03479710e-03, -1.45341596e-02, -4.48291935e-03,\n",
      "        1.81114133e-02, -1.81338505e-03, -1.21754035e-02,  9.60743800e-03,\n",
      "       -2.33934894e-02,  1.92698576e-02, -2.05864646e-02, -4.91585757e-04,\n",
      "       -1.32417437e-02,  7.51712592e-03, -1.57306213e-02, -7.98397418e-03,\n",
      "       -8.77936743e-03,  9.85896215e-03, -1.64980348e-02,  2.33837939e-03,\n",
      "       -8.86889640e-03,  2.16185432e-02, -1.38692223e-02,  8.44045077e-03,\n",
      "       -2.26987582e-02, -1.85198002e-02, -1.57864410e-02,  6.97769178e-03,\n",
      "        4.75226808e-03, -4.79091238e-03,  1.68738719e-02, -1.08679831e-02,\n",
      "       -7.71794468e-03,  8.64897203e-03,  1.39864972e-02, -3.78206815e-03,\n",
      "        2.16686055e-02,  2.03909259e-03,  2.18608038e-04, -2.56566238e-03,\n",
      "       -3.44142802e-02, -1.45007465e-02, -7.94594362e-03,  8.86696298e-03,\n",
      "       -2.03132234e-03,  1.23478901e-02, -1.04277069e-02,  9.46204411e-04,\n",
      "       -1.40569666e-02, -1.48063982e-02,  2.87772692e-03, -1.62974931e-02,\n",
      "        2.17258511e-03,  2.69429348e-02,  7.00229406e-03, -4.74966038e-03,\n",
      "        7.55302096e-03,  9.03267879e-03,  1.74777210e-02, -1.15096048e-02,\n",
      "       -3.90363438e-03, -1.62986405e-02, -1.87004115e-02,  2.59788241e-03,\n",
      "       -1.91522893e-02,  1.19230151e-02,  2.22035265e-03, -1.22226030e-02,\n",
      "       -1.54339857e-02, -2.53888182e-02,  9.95597616e-03,  1.14773994e-03,\n",
      "        7.63661694e-04, -7.68949324e-03, -1.76492508e-03,  2.25856807e-02,\n",
      "        2.34998734e-04,  1.31902780e-05, -4.67061810e-03,  1.40648317e-02,\n",
      "        5.60385128e-03, -1.55773191e-02, -8.90182145e-03, -1.43361129e-02,\n",
      "        3.50839971e-03, -2.48034056e-02, -2.29387134e-02, -9.29482747e-03,\n",
      "        1.58763528e-02, -1.31805837e-02,  1.96628403e-02, -1.07515361e-02,\n",
      "       -5.41983964e-03, -1.00434003e-02,  3.22584361e-02, -2.74030808e-02,\n",
      "        7.59096863e-03, -1.44436397e-03,  1.37800979e-03,  1.31462766e-02,\n",
      "       -1.13576222e-02,  7.01252883e-03, -1.21939892e-03,  1.66387502e-02,\n",
      "       -3.61562450e-03, -1.08133545e-02,  1.89183075e-02, -1.64619349e-02,\n",
      "        5.51668787e-03, -3.12011950e-02, -1.04935812e-02,  2.33904831e-02,\n",
      "        6.12662127e-03,  1.47076603e-02, -1.64868124e-02,  3.77162057e-03,\n",
      "       -3.40509266e-02, -4.38840687e-03, -2.24018488e-02,  8.60654004e-03,\n",
      "       -1.52099142e-02, -1.08309807e-02,  1.47877512e-02,  2.04226747e-03,\n",
      "        1.30829876e-02,  8.13384354e-03, -1.23880543e-02,  1.69094652e-02,\n",
      "        6.22540293e-03,  2.28799637e-02, -1.24318358e-02,  7.99957011e-03,\n",
      "        1.13858012e-02,  9.99668892e-03, -1.00898277e-02,  5.16233500e-03,\n",
      "        3.93441645e-03,  8.53633136e-03,  7.35582225e-03, -2.72924709e-03,\n",
      "       -1.42175341e-02,  2.81699526e-04, -1.15008838e-02, -8.08673538e-03,\n",
      "       -1.32646468e-02, -2.86102071e-02,  3.25148203e-03,  7.72477873e-03,\n",
      "       -7.16876751e-03, -1.38910841e-02, -2.33342294e-02,  5.45070041e-03,\n",
      "       -2.28976440e-02, -7.41127413e-03, -4.04866412e-03, -3.12161190e-03,\n",
      "        1.40594300e-02, -2.37291045e-02,  1.05981785e-03,  3.26745072e-03,\n",
      "       -4.29008901e-03,  2.80691590e-03, -8.63034744e-03, -1.41734409e-03,\n",
      "       -1.74058191e-02, -1.47618195e-02,  2.24441383e-02, -2.83185393e-02,\n",
      "       -5.28854365e-03, -3.76880094e-02,  3.35457958e-02,  7.95031432e-03,\n",
      "       -3.15991440e-03, -2.10074093e-02,  6.43641362e-03, -8.55795760e-03,\n",
      "        1.36946328e-02,  4.09393664e-03,  1.37949977e-02, -1.24741578e-02,\n",
      "       -1.07939839e-02,  9.22083948e-03, -4.43468429e-03,  9.46350116e-03,\n",
      "        3.22718220e-03,  2.88175400e-02,  2.11876798e-02, -1.23161520e-03,\n",
      "       -1.90569628e-02,  1.48947979e-03, -2.23671794e-02,  1.21662708e-03,\n",
      "        2.11749622e-03,  1.71383061e-02, -2.14133295e-03, -2.53730286e-02,\n",
      "        2.20515556e-03,  1.20407436e-02, -1.42226201e-02, -5.73080964e-03,\n",
      "        1.16196126e-02,  1.02740694e-02,  2.67860759e-02, -5.45964809e-03,\n",
      "        5.46512427e-03, -6.94378186e-03, -1.20427343e-04,  7.70627661e-03,\n",
      "        4.07283287e-03,  6.21364219e-03, -7.62895495e-03, -2.16190238e-02,\n",
      "        4.22170432e-03, -2.01694425e-02,  8.27047229e-03, -2.98842117e-02,\n",
      "        7.61758656e-06,  1.76219102e-02,  1.33732939e-02, -1.30704856e-02,\n",
      "        1.17555808e-03, -5.20797540e-03, -1.73522215e-02, -4.12088297e-02,\n",
      "       -8.98503605e-03, -1.53760221e-02,  3.57188378e-03, -1.60700676e-03,\n",
      "       -9.74354148e-03, -1.41052464e-02, -2.19850428e-03, -1.47672817e-02,\n",
      "       -8.51284340e-03, -3.31403613e-02, -1.34532461e-02,  1.18327122e-02,\n",
      "       -1.47016556e-03,  4.42652404e-03, -1.45993084e-02,  3.01889703e-02,\n",
      "        1.01549923e-02, -4.73004859e-03, -6.43518381e-03,  2.13673376e-02,\n",
      "       -3.56793124e-03,  1.74514037e-02, -3.22174653e-02,  3.35633568e-03,\n",
      "        2.02408358e-02, -3.22093116e-03, -3.00025921e-02,  6.72062254e-03,\n",
      "        1.40002994e-02,  1.24933142e-02,  2.07280088e-03, -2.58876551e-02,\n",
      "        2.98176724e-02,  6.52495259e-03, -2.17055138e-02, -1.98227819e-02,\n",
      "       -7.23863440e-03, -1.62258539e-02,  9.98727884e-03, -2.05513537e-02,\n",
      "       -3.86232627e-03,  5.43096708e-03, -1.10858222e-02, -2.80801561e-02,\n",
      "       -2.78397743e-02,  6.41353894e-03,  3.22338706e-03, -3.49337538e-03,\n",
      "        2.73336889e-03, -4.78291325e-03,  1.03021469e-02,  7.62612838e-03,\n",
      "        3.13204131e-03, -3.87810008e-03, -7.96043966e-03, -2.46676113e-02,\n",
      "       -8.69626366e-03,  4.08634823e-03, -2.27845330e-02,  1.07731381e-02,\n",
      "       -1.92330219e-02, -3.38848941e-02,  1.17693832e-02,  7.98902940e-03,\n",
      "       -1.50189335e-02,  9.70975868e-03,  1.21068247e-02,  6.57001568e-04,\n",
      "        4.09414200e-03, -3.88017995e-03, -3.32713104e-03, -1.41844274e-02,\n",
      "       -1.22299865e-02, -3.43145570e-03, -2.11272202e-03,  9.96117108e-03,\n",
      "       -8.19498301e-03, -1.11388741e-02,  1.17251892e-02, -3.04401815e-02,\n",
      "       -6.57775672e-03,  1.54855689e-02, -3.64130177e-03, -1.14101116e-02,\n",
      "        2.11344585e-02, -5.80165675e-03, -1.39395539e-02, -2.40783785e-02,\n",
      "        6.36191526e-03,  5.25757717e-03,  2.32667569e-02, -9.47212707e-03,\n",
      "       -2.06790492e-03, -3.14208455e-02,  1.09539544e-02, -1.70275997e-02,\n",
      "        3.96507094e-03,  8.08050297e-03,  5.04740328e-03,  2.71598483e-03,\n",
      "       -1.44701703e-02, -1.40110664e-02, -1.58759989e-02, -8.95635132e-03,\n",
      "       -1.48921283e-02, -4.67709405e-03,  1.16881328e-02, -1.00357579e-02,\n",
      "        5.53893903e-03,  1.23343337e-02, -2.15167496e-02,  1.94871146e-02,\n",
      "        1.45304147e-02,  3.38823535e-02,  1.63907446e-02,  1.26248477e-02,\n",
      "        6.64276676e-03, -9.06180590e-03, -1.62998736e-02,  1.24503728e-02,\n",
      "        1.40564665e-02, -9.13127139e-03, -1.34859039e-02, -1.85982380e-02,\n",
      "       -1.85761391e-03,  7.32009718e-03,  1.86118914e-03], dtype=float32)]\n",
      "[array([[ 0.021144  ,  0.0270822 , -0.03237258, ...,  0.03728148,\n",
      "        -0.00142959, -0.08435362],\n",
      "       [-0.0299642 ,  0.02903894, -0.03666255, ...,  0.07572623,\n",
      "         0.026222  ,  0.08280992],\n",
      "       [ 0.02628736,  0.07408725, -0.01260204, ...,  0.05316445,\n",
      "        -0.00094219, -0.02453404],\n",
      "       ...,\n",
      "       [ 0.04366888, -0.02644758,  0.11814125, ..., -0.04778031,\n",
      "        -0.01923196, -0.02676605],\n",
      "       [-0.07927885,  0.06618554, -0.05820944, ..., -0.09138871,\n",
      "        -0.06012042,  0.01990888],\n",
      "       [ 0.02383763, -0.03943063, -0.00036387, ...,  0.01450938,\n",
      "         0.06335565, -0.0940685 ]], dtype=float32), array([ 7.89309200e-03, -1.40022598e-02,  1.17403520e-02,  3.21331061e-03,\n",
      "       -1.13011571e-02,  5.68192778e-03,  1.65441521e-02,  3.75855668e-03,\n",
      "       -1.16423275e-02,  7.05443276e-03, -3.48308799e-03, -1.37785403e-02,\n",
      "       -1.42509360e-02, -1.48534188e-02, -1.86673203e-03,  1.64631419e-02,\n",
      "       -8.40573994e-05, -2.46666139e-03,  1.44814122e-02, -1.35431625e-03,\n",
      "        1.18327187e-02,  5.85902994e-03,  1.70960289e-03, -6.94701169e-03,\n",
      "        4.41516237e-03, -4.80433041e-03,  2.36206083e-03, -7.95563776e-03,\n",
      "        2.37006359e-02, -1.64471623e-02, -1.92889583e-03,  7.23135006e-03,\n",
      "        1.26525089e-02,  1.68092102e-02, -2.07124394e-03,  5.35735488e-03,\n",
      "       -5.46275172e-03, -5.24038309e-03,  7.60498596e-03, -1.19773094e-02,\n",
      "       -1.41607150e-02, -2.16783723e-03,  2.77907850e-04,  1.56846531e-02,\n",
      "        1.66901313e-02,  3.67426663e-03, -1.48228789e-02, -2.67858291e-03,\n",
      "        8.48066714e-03, -8.63469299e-03,  7.31010176e-03,  1.24918725e-02,\n",
      "       -6.35323441e-03,  9.30061098e-03, -2.43743270e-04, -9.46633052e-03,\n",
      "        1.63658205e-02, -1.04640927e-02,  2.00790912e-02, -7.75106344e-03,\n",
      "        1.56150302e-02,  1.35008460e-02,  2.38666423e-02, -2.97756959e-03,\n",
      "       -6.56994432e-03, -6.14331570e-03,  2.16032518e-03, -8.88533890e-03,\n",
      "       -7.72389490e-03, -4.73969383e-03,  2.33778916e-03, -9.38050263e-03,\n",
      "       -4.82393755e-03,  1.33166267e-02,  7.17735523e-03, -8.08468368e-03,\n",
      "       -8.67081340e-03,  2.00111177e-02,  8.43252894e-03, -5.00622811e-03,\n",
      "        1.06544578e-02,  1.00547271e-02, -4.19574324e-03,  6.03030762e-03,\n",
      "        6.06002379e-03, -1.15550095e-02,  8.01776815e-03,  9.52505227e-03,\n",
      "        4.16857144e-03,  7.49039138e-03,  1.83868024e-03, -2.83185067e-03,\n",
      "        1.47726210e-02,  5.87709108e-03, -9.41152778e-03,  2.11885553e-02,\n",
      "       -8.50009732e-03, -2.58753239e-03,  3.67930625e-03,  8.51850212e-03,\n",
      "        9.67371464e-03,  8.59086681e-03, -5.79833193e-03, -8.96675792e-03,\n",
      "        1.27376821e-02, -7.04163406e-03,  3.68882320e-04,  1.10469665e-02,\n",
      "        5.57410903e-03, -7.46402191e-03,  1.00057805e-02,  2.97109643e-03,\n",
      "        1.19388234e-02,  9.77329258e-03, -1.03570493e-02, -1.08843297e-03,\n",
      "        4.22967644e-03,  1.63318180e-02,  8.14221450e-04,  1.98352896e-02,\n",
      "       -2.35121566e-04, -5.67321759e-03,  4.98384936e-03,  2.06378242e-03,\n",
      "        1.67652089e-02,  1.12975175e-02,  2.68767960e-03,  9.42333695e-03,\n",
      "       -2.10912316e-03, -2.36537587e-03,  1.04839234e-02,  8.00974946e-03,\n",
      "        1.85751580e-02,  2.07507014e-02, -7.88532291e-03,  1.08645605e-02,\n",
      "       -4.72442945e-03,  6.19271444e-03, -4.68121096e-03, -6.38506003e-03,\n",
      "       -6.73994888e-03,  1.32794250e-02,  1.04423696e-02,  8.88399035e-03,\n",
      "       -1.73192937e-03,  5.75228885e-04, -2.88046547e-03, -4.82039154e-03,\n",
      "       -3.57688870e-03,  1.55141447e-02,  3.84304649e-03,  3.98443267e-03,\n",
      "       -1.28240662e-03, -1.39945233e-02,  1.02927741e-02, -1.71426013e-02,\n",
      "       -6.65408373e-03, -1.75015954e-03, -7.43700308e-04,  1.06238890e-02,\n",
      "        3.48064373e-03, -1.20891584e-02,  1.80728436e-02,  4.82550636e-03,\n",
      "        1.25022633e-02, -1.42066851e-02,  6.68810122e-03, -1.29292579e-02,\n",
      "        1.92711279e-02,  1.91173423e-02,  9.38301813e-03, -1.05811143e-03,\n",
      "        5.44928433e-03, -2.30163964e-03, -1.49481383e-03, -1.54952724e-02,\n",
      "       -5.71985962e-03,  2.45765485e-02,  1.44597711e-02, -2.91771314e-04,\n",
      "        1.54378153e-02, -6.35130284e-03,  1.03218695e-02,  1.48740003e-03,\n",
      "       -8.38134717e-03,  5.20578120e-03,  1.11642331e-02,  9.83380713e-03,\n",
      "        8.31695565e-04,  8.07384867e-03, -3.33379884e-03,  5.30926837e-03,\n",
      "        8.13123863e-03, -7.28566898e-04, -1.43632793e-03,  1.54142305e-02,\n",
      "       -8.87828041e-03, -4.20744065e-03,  1.04712462e-02,  1.47204334e-02,\n",
      "       -1.02196609e-04, -1.13398544e-02,  4.41482896e-03,  1.44963553e-02,\n",
      "        1.46189937e-03,  2.49284301e-02,  4.19212360e-04,  1.53707862e-02,\n",
      "        2.07142904e-02,  2.44727987e-03, -5.21352654e-03,  4.38417448e-03,\n",
      "       -5.06644696e-03,  4.92337998e-03,  1.92551985e-02,  1.23633463e-02,\n",
      "       -1.05750589e-02,  4.37867362e-03,  1.54794538e-02,  1.01454994e-02,\n",
      "       -1.09619349e-02, -7.11897854e-03,  9.92340781e-03, -1.41089933e-03,\n",
      "        7.04727089e-03, -1.15669332e-02,  3.00460192e-03,  2.03858223e-03,\n",
      "        6.43136725e-03,  1.06127849e-02, -1.57226771e-02,  7.92099908e-03,\n",
      "        7.45553942e-03,  1.87142137e-02, -1.11780604e-02, -4.66506882e-03,\n",
      "        1.33708864e-02,  2.32434482e-03, -1.24464761e-02, -2.54599145e-03,\n",
      "       -9.17050615e-03, -1.69484454e-04,  1.58894081e-02,  1.44680114e-02,\n",
      "        4.35373606e-03,  6.51151233e-04,  7.73173105e-03,  6.19007554e-03,\n",
      "        2.55814265e-03,  1.40557457e-02,  2.97387782e-03,  1.81882307e-02,\n",
      "        2.13193870e-03,  1.06999502e-02,  9.18102544e-03,  1.15261618e-02,\n",
      "       -1.35983806e-03,  1.78869453e-03,  1.18392031e-03,  7.30953412e-03,\n",
      "       -2.12591309e-02, -1.80892542e-03,  1.07310228e-02,  1.70600973e-02,\n",
      "        1.73648447e-02, -1.70884887e-03,  3.19551909e-03,  4.93190018e-03,\n",
      "        9.31285229e-03,  7.68507458e-03,  1.02293706e-02, -5.15451189e-03,\n",
      "        1.84458401e-02, -4.65948891e-04,  1.41237816e-02, -5.49185649e-03,\n",
      "        1.09694749e-02,  2.54045124e-03, -1.32330391e-03, -9.54024121e-03,\n",
      "        7.34565267e-03,  1.71113219e-02, -8.89308006e-03,  1.25604570e-02,\n",
      "        6.80330582e-03,  4.10257373e-03,  1.53143685e-02, -8.17944482e-03,\n",
      "        2.00860240e-02,  1.38868263e-03,  2.32904870e-02,  1.46188242e-02,\n",
      "        3.03739030e-03, -2.91061727e-03, -7.69352959e-03, -1.88308884e-03,\n",
      "        1.19770747e-02,  2.31894981e-02,  1.23117873e-02, -4.46435716e-03,\n",
      "        6.17648661e-03, -7.51737447e-04,  1.04076667e-02,  1.50430566e-02,\n",
      "       -5.10146469e-03,  1.21253571e-02,  9.24020598e-04,  3.62291606e-03,\n",
      "        4.38280264e-03, -2.93877930e-03, -7.93533027e-03,  1.15909185e-02,\n",
      "       -4.23541339e-03, -2.13190890e-03, -9.17355809e-03,  1.45661728e-02,\n",
      "        1.10636307e-02,  2.26117228e-03, -5.77834761e-03, -5.06884744e-03,\n",
      "       -1.04447724e-02,  2.91805179e-03,  3.78410029e-03, -2.32625240e-03,\n",
      "       -4.26576938e-03,  1.31077319e-03, -7.29736360e-03,  9.63173865e-04,\n",
      "       -5.58386184e-03,  9.68614686e-03, -2.77969381e-03,  1.35085648e-02,\n",
      "        7.62436946e-04, -1.05307214e-02, -1.72560997e-02, -1.32712023e-02,\n",
      "        1.12606054e-02,  1.79198105e-02, -1.19763715e-02,  1.74658373e-02,\n",
      "        6.54150080e-03, -9.54269152e-03, -8.19744915e-03,  5.86203858e-03,\n",
      "        1.27495162e-03,  5.17947739e-03,  1.15920408e-02,  1.32647520e-02,\n",
      "        1.25736173e-03,  1.02378996e-02,  5.30246599e-03, -4.06065537e-03,\n",
      "        1.27001256e-02,  4.90234094e-03,  9.39501263e-03,  2.01955065e-03,\n",
      "        4.51208238e-04,  1.98721723e-03,  1.50184156e-02, -1.47743579e-02,\n",
      "       -1.19836451e-02,  6.10336335e-03, -2.28808052e-03,  1.75215658e-02,\n",
      "       -3.90488142e-03, -7.64405355e-03, -1.39492918e-02,  2.59047607e-03,\n",
      "        1.06465220e-02, -2.39930372e-03, -6.96906121e-03, -1.06507877e-03,\n",
      "        1.27475299e-02, -8.86341091e-03,  3.19602521e-04], dtype=float32)]\n",
      "[]\n",
      "[array([[ 0.01829996, -0.00352219, -0.00555539, ...,  0.01426842,\n",
      "         0.09006909,  0.02076388],\n",
      "       [ 0.07783809, -0.00263364, -0.07539979, ...,  0.03192178,\n",
      "        -0.00695989,  0.01888257],\n",
      "       [-0.09425826,  0.05104906,  0.0446612 , ...,  0.02073849,\n",
      "         0.01288175, -0.01094793],\n",
      "       ...,\n",
      "       [-0.02612575,  0.0043136 , -0.02611316, ..., -0.03569854,\n",
      "         0.01294735,  0.03362192],\n",
      "       [ 0.06642744,  0.03078956, -0.03781603, ...,  0.05843351,\n",
      "         0.04952642,  0.03112462],\n",
      "       [ 0.06055261, -0.0329772 , -0.03511037, ..., -0.06415141,\n",
      "         0.01443851,  0.02527911]], dtype=float32), array([-1.18764758e-03,  2.01201029e-02, -9.31669772e-03,  3.74821899e-03,\n",
      "        6.56139292e-03,  1.26112690e-02,  1.70369409e-02,  3.81847203e-04,\n",
      "        3.67214298e-03,  2.11981181e-02,  1.77662689e-02, -6.06924482e-03,\n",
      "        1.50098447e-02,  5.24753658e-03,  8.50061513e-03,  1.24626830e-02,\n",
      "        2.35683136e-02,  1.27257053e-02,  3.15185473e-03, -9.48293041e-03,\n",
      "        7.73580093e-03,  1.99318342e-02,  8.30825791e-03,  1.30771855e-02,\n",
      "        2.37006471e-02,  7.03307241e-03,  1.97811611e-02,  5.39942970e-03,\n",
      "        2.05947403e-02,  6.81589078e-03, -1.46981352e-03,  1.03442771e-02,\n",
      "        1.61429420e-02,  2.87283066e-04,  2.61019934e-02, -5.44213504e-03,\n",
      "       -1.88369697e-04,  1.31627806e-02,  1.34740304e-02,  2.15058774e-02,\n",
      "        2.12772079e-02,  1.92164071e-02, -4.66342177e-03, -7.82154221e-03,\n",
      "        1.04338555e-02,  8.24140967e-04,  5.43264672e-03, -3.92977241e-03,\n",
      "       -6.26099715e-03,  1.41994040e-02, -2.51792558e-03,  1.68040954e-02,\n",
      "        9.88563709e-03,  1.11354450e-02,  7.98110943e-03,  6.32982003e-03,\n",
      "        5.32127777e-03,  3.37154255e-03,  1.31125310e-02,  1.71715822e-02,\n",
      "       -1.78316885e-04,  8.81582219e-03,  1.51619175e-02,  1.52891893e-02,\n",
      "        4.02465556e-03,  1.18794283e-02,  1.04094883e-02,  1.86382886e-02,\n",
      "        2.20194533e-02, -7.08513241e-03,  3.32919811e-03,  1.61618851e-02,\n",
      "        2.31141895e-02,  1.65557358e-02,  1.88679877e-03,  2.67198645e-02,\n",
      "        2.25237627e-02,  1.73790995e-02,  2.83505451e-02, -5.74923935e-04,\n",
      "        6.93356665e-03,  1.06453635e-02,  2.13254690e-02,  1.69106089e-02,\n",
      "        3.12743746e-02,  1.24231577e-02,  2.94455774e-02,  1.47855291e-02,\n",
      "        6.09025266e-03,  1.58552937e-02, -2.54009035e-03, -7.33037433e-03,\n",
      "        3.15674511e-03,  7.91361276e-03,  9.46199335e-03,  1.78751815e-02,\n",
      "        1.27166091e-02,  1.20587181e-02,  7.49096833e-03,  1.72635913e-02,\n",
      "        2.39531137e-02,  1.02165807e-02,  9.40359384e-03,  4.23378916e-03,\n",
      "       -1.28864334e-03,  1.98404975e-02,  1.73921194e-02,  4.48315963e-03,\n",
      "        2.10350063e-02,  5.71935624e-03,  1.52660832e-02,  1.61333438e-02,\n",
      "        1.28717264e-02,  2.80265417e-02,  4.96004196e-03,  2.58940738e-02,\n",
      "        1.23525942e-02,  2.53823120e-02,  4.98117320e-03, -7.06269452e-03,\n",
      "        2.08503678e-02,  1.34698264e-02,  1.76391192e-02,  3.02045289e-02,\n",
      "        1.85705759e-02,  2.35115625e-02, -3.64048476e-03,  1.96897499e-02,\n",
      "        4.89235297e-03,  1.88497566e-02, -4.29859385e-03,  2.56008357e-02,\n",
      "        2.39162128e-02,  8.51054210e-03,  2.20166240e-02, -1.63918932e-03,\n",
      "        8.08632374e-03,  3.95803479e-03,  5.53293293e-03, -9.72544402e-03,\n",
      "        1.06572108e-02,  1.48270959e-02,  1.56214433e-02,  2.30858400e-02,\n",
      "       -3.81261180e-03,  1.14853932e-02,  1.74107519e-03, -3.98262497e-03,\n",
      "       -1.64498307e-03,  1.93194728e-02,  1.21990107e-02,  1.23719648e-02,\n",
      "        2.30420809e-02,  1.63953546e-02, -1.42874592e-03, -3.53357755e-03,\n",
      "        1.16400365e-02,  1.28931105e-02, -7.94940349e-03,  1.45580154e-02,\n",
      "        2.11905688e-02,  2.22650394e-02,  1.46247735e-02,  2.81842258e-02,\n",
      "        3.66409984e-03,  2.17889324e-02,  2.75286613e-03,  4.26566927e-03,\n",
      "        5.32706734e-03,  3.87213100e-03,  2.02490017e-02,  2.25177547e-03,\n",
      "        2.51653828e-02, -3.11994180e-03,  1.34105096e-02,  2.50009466e-02,\n",
      "        6.64775912e-03,  1.66329741e-02,  1.23854810e-02,  2.36218832e-02,\n",
      "        8.07192549e-03,  7.98005797e-03,  2.42964234e-02,  1.66995786e-02,\n",
      "        1.57635976e-02,  1.38254249e-02,  1.82447396e-02,  1.76963601e-02,\n",
      "        8.11089482e-03,  7.57979811e-04, -5.66844875e-03,  1.43974479e-02,\n",
      "       -9.55720060e-03, -1.26922748e-03,  2.01750901e-02,  1.00511638e-02,\n",
      "        2.38205027e-02,  1.06307175e-02,  1.76120251e-02,  1.26231788e-03,\n",
      "        1.30329989e-02,  2.10126210e-02,  3.05980742e-02,  3.27216228e-03,\n",
      "        1.61290616e-02,  8.10394622e-03,  2.62498315e-02,  1.44470120e-02,\n",
      "        1.75902247e-02,  2.35428140e-02,  5.64812543e-03,  4.73982701e-03,\n",
      "        3.26799639e-02,  1.05903717e-02,  5.80258435e-03,  2.41914913e-02,\n",
      "        2.63425987e-02,  3.11653875e-02,  3.81506688e-04,  7.44728930e-03,\n",
      "        5.78831974e-03,  8.42719991e-03,  4.80008341e-04,  1.91225652e-02,\n",
      "        1.42916208e-02,  1.15907090e-02,  4.78171743e-03, -3.47583625e-03,\n",
      "        1.51380114e-02,  2.85106171e-02,  2.53322050e-02,  4.07999288e-03,\n",
      "        2.73714066e-02,  1.35565223e-02,  7.61411339e-03,  2.04974916e-02,\n",
      "        6.52449112e-03, -4.39646654e-03,  1.85016729e-02,  3.47984135e-02,\n",
      "        1.56466849e-02,  1.37274023e-02,  1.97412781e-02,  1.11967726e-02,\n",
      "       -3.15260841e-03,  1.74058564e-02,  1.94490096e-03,  1.66360531e-02,\n",
      "        2.55496055e-02,  9.52351652e-03,  1.49887586e-02,  2.48134024e-02,\n",
      "        2.26474535e-02,  1.62555221e-02,  2.87750401e-02,  1.00307250e-02,\n",
      "        1.11816293e-02,  3.52730253e-03,  1.71463657e-02,  1.08149024e-02,\n",
      "        2.25434210e-02,  5.78023540e-03,  1.40525037e-02,  1.22535750e-02,\n",
      "        4.68072109e-03,  2.37841718e-03,  1.90574955e-02,  1.67434067e-02,\n",
      "        1.59068648e-02,  4.71991394e-03,  2.41815802e-02,  1.08395582e-02,\n",
      "        1.50122622e-03,  1.09650688e-02,  1.41848987e-02,  2.24554259e-02,\n",
      "        1.43333497e-02,  7.08581787e-03,  2.80876420e-02,  1.88797880e-02,\n",
      "        1.90907996e-02,  1.34947775e-02,  4.48970124e-03,  1.66956484e-02,\n",
      "        1.74763761e-02,  3.45662721e-02,  2.37578731e-02,  2.29796786e-02,\n",
      "       -3.93765979e-03,  2.50505656e-02,  2.22837757e-02, -4.68133949e-03,\n",
      "        1.70438662e-02,  2.07587034e-02,  1.50805907e-02,  1.82517618e-02,\n",
      "        1.74386352e-02,  9.56386328e-03,  2.21007280e-02,  1.82567276e-02,\n",
      "        1.16545660e-02,  1.51076000e-02,  4.38230997e-03,  2.13329997e-02,\n",
      "        1.69646014e-02,  2.81735342e-02,  6.54041208e-03,  1.47546008e-02,\n",
      "       -1.37830561e-03,  2.67536584e-02,  1.50663368e-02, -1.20302651e-03,\n",
      "        1.51636191e-02,  5.38266264e-04,  4.25254507e-03,  2.07080860e-02,\n",
      "        1.86729785e-02,  7.41935614e-03,  3.68210953e-03,  2.97957119e-02,\n",
      "        2.73778345e-02,  2.70437449e-03,  7.35607650e-03, -7.26165576e-03,\n",
      "        2.08090749e-02,  6.90238317e-03,  9.91251040e-03,  2.61625685e-02,\n",
      "        1.22569054e-02, -4.03495133e-03,  1.18397456e-02,  2.43327841e-02,\n",
      "        5.28923748e-03, -4.04971419e-03,  4.17963834e-03,  2.03017704e-02,\n",
      "        1.33257611e-02,  1.10427830e-02,  1.30556822e-02,  1.47443134e-02,\n",
      "       -1.01726479e-03, -5.78693626e-03,  7.86889810e-03,  1.72415785e-02,\n",
      "        2.61558294e-02,  6.49295794e-03,  1.76622514e-02,  2.34393645e-02,\n",
      "        4.44218926e-02,  2.00898256e-02,  1.04824053e-02,  2.31316667e-02,\n",
      "        1.64935309e-02,  8.18449352e-03,  2.09093392e-02,  1.82948653e-02,\n",
      "        1.25396000e-02,  1.12371910e-02,  1.96730085e-02,  1.92409419e-02,\n",
      "       -3.37887369e-03,  1.10786362e-02,  1.13592707e-02,  2.26800609e-02,\n",
      "        1.45339519e-02,  1.97602659e-02,  8.56669527e-03,  3.00914068e-02,\n",
      "        6.96224067e-03,  1.60746425e-02,  1.16108749e-02,  1.61604267e-02,\n",
      "        2.02767905e-02,  1.97154582e-02,  1.17745204e-02,  5.91270719e-03,\n",
      "        1.42524531e-03,  1.96134560e-02, -6.19803555e-03,  1.54720778e-02,\n",
      "        4.22670087e-03,  2.63787843e-02,  1.95293929e-02,  1.10521913e-02,\n",
      "        1.70790665e-02,  9.01731197e-04,  1.83163639e-02,  1.69998687e-02,\n",
      "        2.09332015e-02,  3.14312652e-02,  1.65824238e-02,  2.56769732e-02,\n",
      "        2.36808974e-02,  1.74310934e-02,  3.08326874e-02,  2.15751585e-02,\n",
      "        3.88562772e-03,  2.89289206e-02,  2.81839464e-02,  1.42468056e-02,\n",
      "        1.33123454e-02,  3.36009101e-03,  1.83523446e-02,  1.90678816e-02,\n",
      "        1.57684414e-03,  2.53277961e-02,  2.22120471e-02,  2.53229532e-02,\n",
      "        2.20582057e-02,  2.09423378e-02,  2.29420848e-02,  6.79569505e-03,\n",
      "        1.62308626e-02,  3.75474524e-03,  9.55627300e-03, -3.54236993e-03,\n",
      "        6.24532485e-03,  1.40353097e-02,  1.04195706e-03,  1.27939302e-02,\n",
      "        1.44929131e-02,  1.82609037e-02,  1.88640449e-02,  2.01301612e-02,\n",
      "        7.79250683e-03,  1.24290737e-03,  2.58573648e-02,  4.23790328e-03,\n",
      "        1.39331082e-02,  7.24378368e-03,  9.12443362e-03,  1.48676708e-02,\n",
      "        6.14139764e-03,  1.73633720e-03,  2.35216990e-02,  7.73048028e-03,\n",
      "        9.46685765e-03,  2.86616888e-02, -9.46418149e-04,  4.53112228e-03,\n",
      "        3.01843528e-02,  2.23303605e-02,  1.83530431e-02,  1.64311547e-02,\n",
      "       -1.74200209e-03, -8.45739897e-03, -3.67076416e-03,  2.48888731e-02,\n",
      "        1.43014574e-02,  5.42331673e-03,  1.26564279e-02,  1.39450720e-02,\n",
      "        1.73015557e-02, -5.74378204e-03,  1.35171283e-02,  2.52402201e-02,\n",
      "        8.05765484e-03,  1.75500882e-03,  2.49436032e-03,  3.41590382e-02,\n",
      "        1.36065511e-02, -6.46882411e-03,  3.36382585e-03,  1.25794287e-03,\n",
      "        1.33646997e-02,  1.77969802e-02,  8.82529828e-04,  1.28342072e-02,\n",
      "        2.33996566e-02,  2.01380774e-02,  1.44121423e-02,  2.02488201e-03,\n",
      "        9.68243275e-03,  1.64381601e-02, -8.47269723e-04,  1.89592727e-02,\n",
      "        2.73755193e-03,  1.74058916e-03, -9.33983756e-05, -1.44366769e-03,\n",
      "        2.73044780e-02,  2.68871933e-02,  4.53630928e-03,  1.30586512e-02,\n",
      "        1.82093885e-02, -3.70580214e-03,  1.21944193e-02,  1.86235812e-02,\n",
      "        1.90278776e-02,  2.10419390e-03,  3.83110680e-02,  2.47317627e-02,\n",
      "        1.61365680e-02,  2.15491746e-02,  7.08336663e-03,  1.52411014e-02,\n",
      "        1.20878946e-02,  3.63138169e-02,  2.95275380e-03,  1.10416589e-02,\n",
      "        1.46681545e-02,  8.01603310e-03,  2.89593521e-03,  3.44317243e-03,\n",
      "        3.84997553e-03, -6.89852424e-03,  4.17512283e-03,  5.94559917e-03,\n",
      "        2.27501597e-02,  2.65809633e-02,  7.67007517e-03, -9.02090222e-03,\n",
      "        1.14292861e-03,  2.73110252e-03,  1.68622993e-02,  2.29429337e-04,\n",
      "        1.71757229e-02, -6.13210863e-03,  7.79184792e-03,  9.70395841e-03,\n",
      "        1.56224957e-02,  1.22342231e-02,  8.22104607e-03,  6.45118766e-03,\n",
      "        1.61897279e-02,  2.71096341e-02,  1.41909886e-02,  8.24029371e-03,\n",
      "        2.23714299e-02, -4.54792957e-04,  1.38669368e-02,  3.31148994e-03,\n",
      "       -8.64709169e-03,  2.09755432e-02,  1.98955517e-02,  2.36926209e-02,\n",
      "        2.32519349e-03, -1.03631592e-03,  7.57707609e-03,  5.17963106e-03,\n",
      "        2.16792934e-02,  2.29344750e-03,  2.24430598e-02,  1.01004615e-02,\n",
      "        7.81761669e-03,  1.15599493e-02,  1.08112290e-03, -4.48285695e-03,\n",
      "        1.69684738e-02,  1.45988697e-02,  1.42355729e-02,  1.79956108e-02,\n",
      "        1.88460127e-02,  1.56671721e-02,  2.20043119e-02,  9.63607430e-03,\n",
      "        8.18825327e-03,  9.87336412e-03,  8.73227604e-03, -1.25347963e-02,\n",
      "        6.16450189e-03, -2.58935324e-04,  1.39446901e-02,  2.27234587e-02,\n",
      "        5.56024956e-03,  1.94802657e-02,  1.14675499e-02,  8.20897054e-03,\n",
      "        1.70399658e-02,  2.22118292e-02,  9.68761928e-03,  9.27146990e-03,\n",
      "        1.61956344e-02,  7.68945878e-03,  8.47267359e-03,  2.27554198e-02,\n",
      "        1.95404552e-02,  2.92708050e-03,  2.27158610e-02,  1.09201036e-02,\n",
      "        1.86403822e-02, -8.95838998e-03,  7.66861485e-03,  1.84521135e-02,\n",
      "        1.47399371e-02,  2.28355713e-02,  1.61872935e-02,  2.19607335e-02,\n",
      "        4.97866189e-03,  2.51761042e-02,  2.12353878e-02,  2.05433592e-02,\n",
      "        1.88988708e-02,  7.06916163e-03,  6.47606980e-03,  2.90825162e-02,\n",
      "        1.69150867e-02,  1.39820185e-02,  1.47667639e-02,  2.46477015e-02,\n",
      "        2.39420333e-03,  1.91734191e-02,  3.16736624e-02,  1.79478712e-02,\n",
      "        3.04389969e-02, -1.76091015e-03,  2.44312156e-02,  2.55738143e-02,\n",
      "        1.93239842e-02, -2.65724608e-03,  1.19456882e-02,  4.44562081e-03,\n",
      "        1.61857321e-03,  1.75221637e-02,  1.40943816e-02,  1.39601445e-02,\n",
      "       -1.59214716e-03,  9.64421500e-03, -4.92330361e-03,  1.97985750e-02,\n",
      "       -2.44239927e-03,  7.10469298e-03,  1.88350631e-03,  1.27387913e-02,\n",
      "        4.59106034e-03, -4.74336836e-03,  2.43856516e-02,  2.46043969e-02,\n",
      "        1.68017298e-02,  1.47266779e-02,  1.02450391e-02,  2.58982163e-02,\n",
      "        2.29387060e-02,  3.73145845e-03,  1.49541972e-02,  8.58619250e-03,\n",
      "        1.04478579e-02,  2.23619118e-02,  1.96576137e-02,  2.90341508e-02,\n",
      "        2.62643788e-02,  8.95988196e-03,  7.43771251e-03,  2.50539668e-02,\n",
      "       -3.68010296e-05,  2.41094735e-02,  1.04611693e-02,  2.58641262e-02,\n",
      "        1.86960381e-02,  8.79034027e-03,  2.03861156e-03,  7.35323131e-03,\n",
      "        2.97073498e-02,  1.37865040e-02, -3.30719049e-04,  2.41604969e-02,\n",
      "        2.78679449e-02,  3.54393795e-02,  1.97142158e-02,  5.36191557e-03,\n",
      "        8.91472958e-03,  8.10846034e-03,  1.78322215e-02,  3.64251016e-03,\n",
      "        1.84568111e-02, -8.61724094e-03,  2.39043646e-02,  1.11378795e-02,\n",
      "        1.05569065e-02, -4.81725158e-03,  2.37899687e-04,  6.39770972e-03,\n",
      "        5.57356514e-03,  1.62999928e-02,  7.07575865e-03,  1.96489673e-02,\n",
      "        1.85046140e-02,  4.37245658e-03,  2.53706034e-02,  1.58724654e-02,\n",
      "        1.22075304e-02,  4.65694722e-03,  1.49284732e-02, -1.70078054e-02,\n",
      "        2.78641805e-02,  3.02344393e-02,  4.16123308e-03,  4.38513671e-04,\n",
      "        2.45290715e-02,  2.13190150e-02,  2.67345887e-02,  7.39493361e-03,\n",
      "        1.41082508e-02, -3.16927442e-03,  1.41387694e-02,  1.09835435e-03,\n",
      "       -4.42864932e-03,  9.70644236e-04,  6.33002492e-03,  1.40989227e-02,\n",
      "        1.05618378e-02,  1.02032516e-02,  1.36432853e-02,  1.99588556e-02,\n",
      "        1.61565877e-02,  2.58471947e-02,  1.91759448e-02,  1.60134304e-02,\n",
      "        1.56088602e-02,  1.55314533e-02,  1.94750894e-02,  5.24486415e-03,\n",
      "        8.23909324e-03,  6.51686778e-03,  1.47254225e-02,  1.02757132e-02,\n",
      "        2.23987773e-02,  5.88360662e-03,  8.78158305e-03, -8.84814747e-03,\n",
      "       -7.36045837e-03,  2.16339692e-03,  4.54540178e-03,  8.67243297e-03,\n",
      "        8.68941285e-03,  1.94512233e-02,  1.64292287e-02,  2.67616697e-02,\n",
      "        1.43896844e-02,  2.79943682e-02, -2.72688153e-03,  1.47656752e-02,\n",
      "        9.72132199e-03,  1.60072241e-02,  1.14462851e-02,  4.33157757e-03,\n",
      "        2.15409547e-02,  1.62655972e-02,  3.79037904e-03,  1.44971171e-02,\n",
      "       -5.55649493e-03,  1.07303206e-02,  1.80406887e-02,  1.52331367e-02,\n",
      "        1.49698779e-02,  2.21336447e-03,  2.27089692e-02,  2.03485098e-02,\n",
      "        2.97378027e-03,  3.39813740e-03], dtype=float32)]\n",
      "[]\n",
      "[array([[-0.07100847, -0.0506745 , -0.06806362,  0.05929948, -0.01883668],\n",
      "       [ 0.05627139, -0.02874283,  0.12244166,  0.01283117, -0.05049294],\n",
      "       [ 0.05047762,  0.02167685, -0.06092309, -0.0199836 , -0.02571777],\n",
      "       ...,\n",
      "       [ 0.06295196, -0.11193097,  0.06000063, -0.00230839,  0.04514375],\n",
      "       [-0.09497841, -0.04332707, -0.1279283 ,  0.10552206,  0.00430932],\n",
      "       [-0.10098404,  0.04395951, -0.0702332 ,  0.11660624, -0.04534686]],\n",
      "      dtype=float32), array([ 0.00611983, -0.00645709,  0.01358221, -0.01235961, -0.0048225 ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model_ANN_saved.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73c9686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00611983, -0.00645709,  0.01358221, -0.01235961, -0.0048225 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model_ANN_saved.get_layer('Output').get_weights()\n",
    "weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "76f7bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model_CNN_1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b430548",
   "metadata": {},
   "source": [
    "### CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4deabf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN (Convolutional Neural Network) using Tensorflow\n",
    "\n",
    "def build_CNN_1D_model(model_name: str, neurons: int):\n",
    "    \n",
    "    model = Sequential(name = model_name)\n",
    "\n",
    "    # 1st conv layer\n",
    "    model.add(Conv1D(28, 7, activation = 'relu', input_shape = (neurons, 1), name = 'Conv1D_1'))\n",
    "    #model.add(MaxPooling1D(3, name = 'MaxPool1D_1'))\n",
    "\n",
    "    # 2nd conv layer\n",
    "    model.add(Conv1D(34, 5, activation = 'relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.01), padding='same', name = 'Conv1D_2'))\n",
    "    #model.add(MaxPooling1D(2, name = 'MaxPool1D_2'))\n",
    "    \n",
    "    # 3nd conv layer \n",
    "    model.add(Conv1D(56, 3, activation = 'relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.01), padding='same', name = 'Conv1D_3'))\n",
    "    model.add(MaxPooling1D(2, name = 'MaxPool1D_3'))\n",
    "    model.add(Dropout(0.2, name = 'Dropout_1'))\n",
    "    \n",
    "    # 4nd conv layer + dropout 20%\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, name = 'Dense'))\n",
    "\n",
    "    # Final classification layer, with 1 neuron for each output class. Softmax divides the probability of each class.\n",
    "    model.add(Dense(num_classes, activation = 'softmax', name = 'Output'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adamax', metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2e20c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=50, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "if not os.path.exists(path_models):\n",
    "    os.makedirs(path_models)\n",
    "    \n",
    "filepath       = os.path.join(path_models, 'Model_CNN_1D_weights_0_best' + norm_type + model_surname + '.hdf5')\n",
    "checkpoint     = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, monitor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9fe8a297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_1D\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 369, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 369, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 369, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 184, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 184, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10304)             0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                515250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 526,291\n",
      "Trainable params: 526,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CNN_1D = build_CNN_1D_model('CNN_1D', neurons = n_dim)\n",
    "model_CNN_1D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "10a28354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_ANN, to_file= os.path.join(path_models, 'Model_CNN_1D' + norm_type + model_surname + '.png'), show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372210cd",
   "metadata": {},
   "source": [
    "### Understanding the column \"Param\":\n",
    "\n",
    "1. For `Conv1D` layer:\n",
    "   - The number of parameters for a `Conv1D` layer is calculated as `(kernel_size * input_channels + 1) * output_channels`, where `kernel_size` is the size of the convolutional kernel, `input_channels` is the number of input channels (1 in this case), and `output_channels` is the number of output channels.\n",
    "\n",
    "2. For `Dense` layer:\n",
    "   - The number of parameters for a `Dense` layer is calculated as `(input_units + 1) * output_units`, where `input_units` is the number of input units and `output_units` is the number of output units.\n",
    "   \n",
    "3. In the calculation of parameters for a convolutional layer, the term \"channels\" refers to the number of filters used in that layer.\n",
    "\n",
    "- 224   parameters is the result of 28 filters * (7 kernels * 1 filter + 1)\n",
    "- 4,794 parameters is the result of 34 filter * (5 kernels * 28 filters + 1)\n",
    "- 5,768  parameters is the result of 56 filters * (3 kernels * 34 filters + 1)\n",
    "- 515,250  parameters is the result of 50 neurons with 10,304 features + 50 bias values\n",
    "- 255    parameters is the result of 5 neurons with 50 features + 5 bias values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bcda9a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.8021\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85818, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.6227 - accuracy: 0.8021 - val_loss: 0.4612 - val_accuracy: 0.8582\n",
      "Epoch 2/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.4158 - accuracy: 0.8729\n",
      "Epoch 00002: val_accuracy improved from 0.85818 to 0.86691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.4158 - accuracy: 0.8728 - val_loss: 0.4431 - val_accuracy: 0.8669\n",
      "Epoch 3/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.3553 - accuracy: 0.8959\n",
      "Epoch 00003: val_accuracy improved from 0.86691 to 0.88436, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3566 - accuracy: 0.8957 - val_loss: 0.3814 - val_accuracy: 0.8844\n",
      "Epoch 4/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.9105\n",
      "Epoch 00004: val_accuracy improved from 0.88436 to 0.89745, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3164 - accuracy: 0.9107 - val_loss: 0.3668 - val_accuracy: 0.8975\n",
      "Epoch 5/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.9198\n",
      "Epoch 00005: val_accuracy did not improve from 0.89745\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2845 - accuracy: 0.9201 - val_loss: 0.3744 - val_accuracy: 0.8953\n",
      "Epoch 6/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.2632 - accuracy: 0.9281\n",
      "Epoch 00006: val_accuracy improved from 0.89745 to 0.89927, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2628 - accuracy: 0.9283 - val_loss: 0.3381 - val_accuracy: 0.8993\n",
      "Epoch 7/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.2428 - accuracy: 0.9350\n",
      "Epoch 00007: val_accuracy improved from 0.89927 to 0.90655, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2426 - accuracy: 0.9350 - val_loss: 0.3134 - val_accuracy: 0.9065\n",
      "Epoch 8/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9414\n",
      "Epoch 00008: val_accuracy did not improve from 0.90655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2267 - accuracy: 0.9416 - val_loss: 0.3134 - val_accuracy: 0.9058\n",
      "Epoch 9/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.2134 - accuracy: 0.9440\n",
      "Epoch 00009: val_accuracy improved from 0.90655 to 0.90945, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2131 - accuracy: 0.9441 - val_loss: 0.3095 - val_accuracy: 0.9095\n",
      "Epoch 10/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1978 - accuracy: 0.9492\n",
      "Epoch 00010: val_accuracy improved from 0.90945 to 0.91091, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1978 - accuracy: 0.9492 - val_loss: 0.3050 - val_accuracy: 0.9109\n",
      "Epoch 11/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9513\n",
      "Epoch 00011: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1904 - accuracy: 0.9515 - val_loss: 0.3116 - val_accuracy: 0.9105\n",
      "Epoch 12/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9526\n",
      "Epoch 00012: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1799 - accuracy: 0.9526 - val_loss: 0.3128 - val_accuracy: 0.9091\n",
      "Epoch 13/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1754 - accuracy: 0.9565\n",
      "Epoch 00013: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1757 - accuracy: 0.9564 - val_loss: 0.3098 - val_accuracy: 0.9105\n",
      "Epoch 14/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1677 - accuracy: 0.9592\n",
      "Epoch 00014: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1684 - accuracy: 0.9590 - val_loss: 0.3068 - val_accuracy: 0.9062\n",
      "Epoch 15/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1605 - accuracy: 0.9616\n",
      "Epoch 00015: val_accuracy improved from 0.91091 to 0.91345, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1601 - accuracy: 0.9617 - val_loss: 0.2913 - val_accuracy: 0.9135\n",
      "Epoch 16/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1534 - accuracy: 0.9629\n",
      "Epoch 00016: val_accuracy improved from 0.91345 to 0.91418, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1535 - accuracy: 0.9629 - val_loss: 0.2965 - val_accuracy: 0.9142\n",
      "Epoch 17/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9653\n",
      "Epoch 00017: val_accuracy did not improve from 0.91418\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1491 - accuracy: 0.9653 - val_loss: 0.2928 - val_accuracy: 0.9135\n",
      "Epoch 18/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1414 - accuracy: 0.9679\n",
      "Epoch 00018: val_accuracy improved from 0.91418 to 0.91709, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1415 - accuracy: 0.9679 - val_loss: 0.2972 - val_accuracy: 0.9171\n",
      "Epoch 19/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9673\n",
      "Epoch 00019: val_accuracy improved from 0.91709 to 0.92036, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1396 - accuracy: 0.9672 - val_loss: 0.2944 - val_accuracy: 0.9204\n",
      "Epoch 20/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9688\n",
      "Epoch 00020: val_accuracy did not improve from 0.92036\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1346 - accuracy: 0.9688 - val_loss: 0.3057 - val_accuracy: 0.9167\n",
      "Epoch 21/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1321 - accuracy: 0.9697\n",
      "Epoch 00021: val_accuracy improved from 0.92036 to 0.92655, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1320 - accuracy: 0.9697 - val_loss: 0.2821 - val_accuracy: 0.9265\n",
      "Epoch 22/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9730\n",
      "Epoch 00022: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1233 - accuracy: 0.9730 - val_loss: 0.2842 - val_accuracy: 0.9247\n",
      "Epoch 23/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772/774 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9715\n",
      "Epoch 00023: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1249 - accuracy: 0.9716 - val_loss: 0.3068 - val_accuracy: 0.9178\n",
      "Epoch 24/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9744\n",
      "Epoch 00024: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1189 - accuracy: 0.9745 - val_loss: 0.2747 - val_accuracy: 0.9265\n",
      "Epoch 25/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1145 - accuracy: 0.9751\n",
      "Epoch 00025: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1144 - accuracy: 0.9750 - val_loss: 0.3054 - val_accuracy: 0.9207\n",
      "Epoch 26/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9733\n",
      "Epoch 00026: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1157 - accuracy: 0.9732 - val_loss: 0.2916 - val_accuracy: 0.9255\n",
      "Epoch 27/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1116 - accuracy: 0.9759\n",
      "Epoch 00027: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1116 - accuracy: 0.9760 - val_loss: 0.2913 - val_accuracy: 0.9244\n",
      "Epoch 28/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9767\n",
      "Epoch 00028: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1091 - accuracy: 0.9768 - val_loss: 0.2882 - val_accuracy: 0.9247\n",
      "Epoch 29/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9789\n",
      "Epoch 00029: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1037 - accuracy: 0.9789 - val_loss: 0.3025 - val_accuracy: 0.9222\n",
      "Epoch 30/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1030 - accuracy: 0.9780\n",
      "Epoch 00030: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1029 - accuracy: 0.9781 - val_loss: 0.2847 - val_accuracy: 0.9240\n",
      "Epoch 31/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0993 - accuracy: 0.9795\n",
      "Epoch 00031: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0994 - accuracy: 0.9795 - val_loss: 0.2934 - val_accuracy: 0.9258\n",
      "Epoch 32/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9801\n",
      "Epoch 00032: val_accuracy improved from 0.92655 to 0.92909, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0996 - accuracy: 0.9801 - val_loss: 0.2816 - val_accuracy: 0.9291\n",
      "Epoch 33/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9791\n",
      "Epoch 00033: val_accuracy improved from 0.92909 to 0.93200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0997 - accuracy: 0.9790 - val_loss: 0.2687 - val_accuracy: 0.9320\n",
      "Epoch 34/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0967 - accuracy: 0.9795\n",
      "Epoch 00034: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 5s 7ms/step - loss: 0.0967 - accuracy: 0.9794 - val_loss: 0.2761 - val_accuracy: 0.9276\n",
      "Epoch 35/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0946 - accuracy: 0.9801\n",
      "Epoch 00035: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0946 - accuracy: 0.9802 - val_loss: 0.2877 - val_accuracy: 0.9240\n",
      "Epoch 36/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0932 - accuracy: 0.9811\n",
      "Epoch 00036: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0930 - accuracy: 0.9812 - val_loss: 0.2965 - val_accuracy: 0.9280\n",
      "Epoch 37/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9811\n",
      "Epoch 00037: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0912 - accuracy: 0.9810 - val_loss: 0.2703 - val_accuracy: 0.9302\n",
      "Epoch 38/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9818\n",
      "Epoch 00038: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0888 - accuracy: 0.9818 - val_loss: 0.2848 - val_accuracy: 0.9258\n",
      "Epoch 39/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9813\n",
      "Epoch 00039: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0901 - accuracy: 0.9810 - val_loss: 0.2773 - val_accuracy: 0.9298\n",
      "Epoch 40/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9821\n",
      "Epoch 00040: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0860 - accuracy: 0.9823 - val_loss: 0.2935 - val_accuracy: 0.9284\n",
      "Epoch 41/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0846 - accuracy: 0.9826\n",
      "Epoch 00041: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0850 - accuracy: 0.9824 - val_loss: 0.2829 - val_accuracy: 0.9302\n",
      "Epoch 42/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0837 - accuracy: 0.9832\n",
      "Epoch 00042: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0839 - accuracy: 0.9831 - val_loss: 0.2838 - val_accuracy: 0.9276\n",
      "Epoch 43/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9842\n",
      "Epoch 00043: val_accuracy did not improve from 0.93200\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0839 - accuracy: 0.9842 - val_loss: 0.2916 - val_accuracy: 0.9291\n",
      "Epoch 44/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0814 - accuracy: 0.9840\n",
      "Epoch 00044: val_accuracy improved from 0.93200 to 0.93527, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0812 - accuracy: 0.9842 - val_loss: 0.2714 - val_accuracy: 0.9353\n",
      "Epoch 45/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.9849\n",
      "Epoch 00045: val_accuracy did not improve from 0.93527\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0804 - accuracy: 0.9849 - val_loss: 0.2815 - val_accuracy: 0.9320\n",
      "Epoch 46/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9850\n",
      "Epoch 00046: val_accuracy improved from 0.93527 to 0.93600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0784 - accuracy: 0.9851 - val_loss: 0.2873 - val_accuracy: 0.9360\n",
      "Epoch 47/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0811 - accuracy: 0.9845\n",
      "Epoch 00047: val_accuracy did not improve from 0.93600\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0810 - accuracy: 0.9845 - val_loss: 0.2830 - val_accuracy: 0.9331\n",
      "Epoch 48/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9853\n",
      "Epoch 00048: val_accuracy did not improve from 0.93600\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0776 - accuracy: 0.9853 - val_loss: 0.2988 - val_accuracy: 0.9280\n",
      "Epoch 49/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9861\n",
      "Epoch 00049: val_accuracy did not improve from 0.93600\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0773 - accuracy: 0.9861 - val_loss: 0.2882 - val_accuracy: 0.9324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9856\n",
      "Epoch 00050: val_accuracy did not improve from 0.93600\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0769 - accuracy: 0.9856 - val_loss: 0.2853 - val_accuracy: 0.9305\n",
      "Epoch 51/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9854\n",
      "Epoch 00051: val_accuracy did not improve from 0.93600\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0768 - accuracy: 0.9854 - val_loss: 0.2872 - val_accuracy: 0.9331\n",
      "Epoch 52/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9869\n",
      "Epoch 00052: val_accuracy did not improve from 0.93600\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0723 - accuracy: 0.9869 - val_loss: 0.2805 - val_accuracy: 0.9349\n",
      "Epoch 53/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9870\n",
      "Epoch 00053: val_accuracy did not improve from 0.93600\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0731 - accuracy: 0.9870 - val_loss: 0.2839 - val_accuracy: 0.9305\n",
      "Epoch 54/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9863\n",
      "Epoch 00054: val_accuracy did not improve from 0.93600\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0727 - accuracy: 0.9863 - val_loss: 0.2796 - val_accuracy: 0.9320\n",
      "Epoch 55/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9872\n",
      "Epoch 00055: val_accuracy did not improve from 0.93600\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0709 - accuracy: 0.9871 - val_loss: 0.2903 - val_accuracy: 0.9335\n",
      "Epoch 56/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9860\n",
      "Epoch 00056: val_accuracy improved from 0.93600 to 0.93673, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0717 - accuracy: 0.9861 - val_loss: 0.2905 - val_accuracy: 0.9367\n",
      "Epoch 57/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.9874\n",
      "Epoch 00057: val_accuracy did not improve from 0.93673\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0695 - accuracy: 0.9871 - val_loss: 0.2988 - val_accuracy: 0.9287\n",
      "Epoch 58/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.9868\n",
      "Epoch 00058: val_accuracy did not improve from 0.93673\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0690 - accuracy: 0.9867 - val_loss: 0.2963 - val_accuracy: 0.9298\n",
      "Epoch 59/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9875\n",
      "Epoch 00059: val_accuracy did not improve from 0.93673\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0694 - accuracy: 0.9876 - val_loss: 0.2784 - val_accuracy: 0.9338\n",
      "Epoch 60/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9889\n",
      "Epoch 00060: val_accuracy did not improve from 0.93673\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0672 - accuracy: 0.9889 - val_loss: 0.2938 - val_accuracy: 0.9313\n",
      "Epoch 61/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9877\n",
      "Epoch 00061: val_accuracy did not improve from 0.93673\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0687 - accuracy: 0.9877 - val_loss: 0.2992 - val_accuracy: 0.9338\n",
      "Epoch 62/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9880\n",
      "Epoch 00062: val_accuracy did not improve from 0.93673\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0664 - accuracy: 0.9881 - val_loss: 0.2913 - val_accuracy: 0.9364\n",
      "Epoch 63/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9893\n",
      "Epoch 00063: val_accuracy did not improve from 0.93673\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0636 - accuracy: 0.9891 - val_loss: 0.3276 - val_accuracy: 0.9284\n",
      "Epoch 64/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9883\n",
      "Epoch 00064: val_accuracy did not improve from 0.93673\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0643 - accuracy: 0.9882 - val_loss: 0.3027 - val_accuracy: 0.9335\n",
      "Epoch 65/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9895\n",
      "Epoch 00065: val_accuracy improved from 0.93673 to 0.93709, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0615 - accuracy: 0.9895 - val_loss: 0.2906 - val_accuracy: 0.9371\n",
      "Epoch 66/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9879\n",
      "Epoch 00066: val_accuracy did not improve from 0.93709\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0651 - accuracy: 0.9879 - val_loss: 0.2849 - val_accuracy: 0.9327\n",
      "Epoch 67/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9887\n",
      "Epoch 00067: val_accuracy improved from 0.93709 to 0.93782, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0639 - accuracy: 0.9886 - val_loss: 0.2802 - val_accuracy: 0.9378\n",
      "Epoch 68/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9892\n",
      "Epoch 00068: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0642 - accuracy: 0.9892 - val_loss: 0.2925 - val_accuracy: 0.9324\n",
      "Epoch 69/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9903\n",
      "Epoch 00069: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0596 - accuracy: 0.9902 - val_loss: 0.2989 - val_accuracy: 0.9345\n",
      "Epoch 70/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9902\n",
      "Epoch 00070: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0600 - accuracy: 0.9902 - val_loss: 0.3260 - val_accuracy: 0.9335\n",
      "Epoch 71/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9897\n",
      "Epoch 00071: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0599 - accuracy: 0.9897 - val_loss: 0.3346 - val_accuracy: 0.9247\n",
      "Epoch 72/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0584 - accuracy: 0.9904\n",
      "Epoch 00072: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0584 - accuracy: 0.9904 - val_loss: 0.3079 - val_accuracy: 0.9345\n",
      "Epoch 73/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9878\n",
      "Epoch 00073: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0641 - accuracy: 0.9878 - val_loss: 0.3004 - val_accuracy: 0.9356\n",
      "Epoch 74/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0592 - accuracy: 0.9896\n",
      "Epoch 00074: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0592 - accuracy: 0.9896 - val_loss: 0.3121 - val_accuracy: 0.9324\n",
      "Epoch 75/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0567 - accuracy: 0.9911\n",
      "Epoch 00075: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0567 - accuracy: 0.9912 - val_loss: 0.2824 - val_accuracy: 0.9375\n",
      "Epoch 76/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9887\n",
      "Epoch 00076: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0617 - accuracy: 0.9888 - val_loss: 0.2937 - val_accuracy: 0.9305\n",
      "Epoch 77/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770/774 [============================>.] - ETA: 0s - loss: 0.0573 - accuracy: 0.9908\n",
      "Epoch 00077: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0573 - accuracy: 0.9907 - val_loss: 0.2977 - val_accuracy: 0.9356\n",
      "Epoch 78/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9903\n",
      "Epoch 00078: val_accuracy did not improve from 0.93782\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0577 - accuracy: 0.9903 - val_loss: 0.3038 - val_accuracy: 0.9356\n",
      "Epoch 79/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0564 - accuracy: 0.9906\n",
      "Epoch 00079: val_accuracy improved from 0.93782 to 0.93855, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0566 - accuracy: 0.9906 - val_loss: 0.2861 - val_accuracy: 0.9385\n",
      "Epoch 80/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0558 - accuracy: 0.9912\n",
      "Epoch 00080: val_accuracy improved from 0.93855 to 0.93927, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0556 - accuracy: 0.9912 - val_loss: 0.2888 - val_accuracy: 0.9393\n",
      "Epoch 81/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9896\n",
      "Epoch 00081: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0581 - accuracy: 0.9896 - val_loss: 0.2755 - val_accuracy: 0.9378\n",
      "Epoch 82/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9914\n",
      "Epoch 00082: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0537 - accuracy: 0.9914 - val_loss: 0.2892 - val_accuracy: 0.9382\n",
      "Epoch 83/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9900\n",
      "Epoch 00083: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0570 - accuracy: 0.9900 - val_loss: 0.2777 - val_accuracy: 0.9385\n",
      "Epoch 84/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0559 - accuracy: 0.9911\n",
      "Epoch 00084: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0559 - accuracy: 0.9911 - val_loss: 0.2949 - val_accuracy: 0.9342\n",
      "Epoch 85/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9917\n",
      "Epoch 00085: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0514 - accuracy: 0.9918 - val_loss: 0.3032 - val_accuracy: 0.9356\n",
      "Epoch 86/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 0.9912\n",
      "Epoch 00086: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0539 - accuracy: 0.9912 - val_loss: 0.3286 - val_accuracy: 0.9302\n",
      "Epoch 87/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0533 - accuracy: 0.9910\n",
      "Epoch 00087: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0532 - accuracy: 0.9910 - val_loss: 0.3075 - val_accuracy: 0.9367\n",
      "Epoch 88/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 0.9894\n",
      "Epoch 00088: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0556 - accuracy: 0.9894 - val_loss: 0.3004 - val_accuracy: 0.9345\n",
      "Epoch 89/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0559 - accuracy: 0.9894\n",
      "Epoch 00089: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0559 - accuracy: 0.9895 - val_loss: 0.2957 - val_accuracy: 0.9356\n",
      "Epoch 90/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0540 - accuracy: 0.9903\n",
      "Epoch 00090: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0541 - accuracy: 0.9902 - val_loss: 0.2878 - val_accuracy: 0.9378\n",
      "Epoch 91/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9890\n",
      "Epoch 00091: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0583 - accuracy: 0.9890 - val_loss: 0.2906 - val_accuracy: 0.9371\n",
      "Epoch 92/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 0.9906\n",
      "Epoch 00092: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0537 - accuracy: 0.9907 - val_loss: 0.2891 - val_accuracy: 0.9364\n",
      "Epoch 93/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0517 - accuracy: 0.9916\n",
      "Epoch 00093: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0516 - accuracy: 0.9916 - val_loss: 0.3033 - val_accuracy: 0.9305\n",
      "Epoch 94/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9908\n",
      "Epoch 00094: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0530 - accuracy: 0.9908 - val_loss: 0.3048 - val_accuracy: 0.9367\n",
      "Epoch 95/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 0.9918\n",
      "Epoch 00095: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0522 - accuracy: 0.9918 - val_loss: 0.2945 - val_accuracy: 0.9382\n",
      "Epoch 96/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9913\n",
      "Epoch 00096: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0500 - accuracy: 0.9914 - val_loss: 0.3006 - val_accuracy: 0.9382\n",
      "Epoch 97/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0524 - accuracy: 0.9911\n",
      "Epoch 00097: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0522 - accuracy: 0.9912 - val_loss: 0.3032 - val_accuracy: 0.9331\n",
      "Epoch 98/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0525 - accuracy: 0.9910\n",
      "Epoch 00098: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0524 - accuracy: 0.9911 - val_loss: 0.2853 - val_accuracy: 0.9335\n",
      "Epoch 99/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0494 - accuracy: 0.9920\n",
      "Epoch 00099: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0494 - accuracy: 0.9920 - val_loss: 0.3048 - val_accuracy: 0.9356\n",
      "Epoch 100/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0497 - accuracy: 0.9918\n",
      "Epoch 00100: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0496 - accuracy: 0.9918 - val_loss: 0.3030 - val_accuracy: 0.9345\n",
      "Epoch 101/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9913\n",
      "Epoch 00101: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0490 - accuracy: 0.9913 - val_loss: 0.3043 - val_accuracy: 0.9342\n",
      "Epoch 102/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9928\n",
      "Epoch 00102: val_accuracy did not improve from 0.93927\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0486 - accuracy: 0.9928 - val_loss: 0.2929 - val_accuracy: 0.9389\n",
      "Epoch 103/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9924\n",
      "Epoch 00103: val_accuracy improved from 0.93927 to 0.94073, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0481 - accuracy: 0.9924 - val_loss: 0.2905 - val_accuracy: 0.9407\n",
      "Epoch 104/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/774 [============================>.] - ETA: 0s - loss: 0.0516 - accuracy: 0.9906\n",
      "Epoch 00104: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0516 - accuracy: 0.9906 - val_loss: 0.3137 - val_accuracy: 0.9342\n",
      "Epoch 105/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0492 - accuracy: 0.9919\n",
      "Epoch 00105: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0492 - accuracy: 0.9919 - val_loss: 0.3199 - val_accuracy: 0.9345\n",
      "Epoch 106/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0488 - accuracy: 0.9914\n",
      "Epoch 00106: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0488 - accuracy: 0.9915 - val_loss: 0.3037 - val_accuracy: 0.9302\n",
      "Epoch 107/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0497 - accuracy: 0.9914\n",
      "Epoch 00107: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0498 - accuracy: 0.9914 - val_loss: 0.2891 - val_accuracy: 0.9400\n",
      "Epoch 108/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0475 - accuracy: 0.9921\n",
      "Epoch 00108: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0477 - accuracy: 0.9919 - val_loss: 0.3036 - val_accuracy: 0.9375\n",
      "Epoch 109/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0480 - accuracy: 0.9922\n",
      "Epoch 00109: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0480 - accuracy: 0.9922 - val_loss: 0.3047 - val_accuracy: 0.9393\n",
      "Epoch 110/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9928\n",
      "Epoch 00110: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0451 - accuracy: 0.9928 - val_loss: 0.2948 - val_accuracy: 0.9404\n",
      "Epoch 111/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0493 - accuracy: 0.9915\n",
      "Epoch 00111: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0494 - accuracy: 0.9914 - val_loss: 0.2866 - val_accuracy: 0.9367\n",
      "Epoch 112/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0486 - accuracy: 0.9925\n",
      "Epoch 00112: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0486 - accuracy: 0.9925 - val_loss: 0.3023 - val_accuracy: 0.9345\n",
      "Epoch 113/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0483 - accuracy: 0.9919\n",
      "Epoch 00113: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0483 - accuracy: 0.9919 - val_loss: 0.3028 - val_accuracy: 0.9335\n",
      "Epoch 114/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9917\n",
      "Epoch 00114: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0485 - accuracy: 0.9918 - val_loss: 0.3042 - val_accuracy: 0.9335\n",
      "Epoch 115/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0485 - accuracy: 0.9914\n",
      "Epoch 00115: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0483 - accuracy: 0.9915 - val_loss: 0.2937 - val_accuracy: 0.9385\n",
      "Epoch 116/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9923\n",
      "Epoch 00116: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0458 - accuracy: 0.9921 - val_loss: 0.2937 - val_accuracy: 0.9360\n",
      "Epoch 117/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9918\n",
      "Epoch 00117: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0469 - accuracy: 0.9918 - val_loss: 0.2863 - val_accuracy: 0.9378\n",
      "Epoch 118/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9924\n",
      "Epoch 00118: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0453 - accuracy: 0.9924 - val_loss: 0.2890 - val_accuracy: 0.9378\n",
      "Epoch 119/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0477 - accuracy: 0.9920\n",
      "Epoch 00119: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0475 - accuracy: 0.9920 - val_loss: 0.2949 - val_accuracy: 0.9364\n",
      "Epoch 120/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0475 - accuracy: 0.9912\n",
      "Epoch 00120: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0474 - accuracy: 0.9912 - val_loss: 0.3080 - val_accuracy: 0.9305\n",
      "Epoch 121/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0452 - accuracy: 0.9925\n",
      "Epoch 00121: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0451 - accuracy: 0.9926 - val_loss: 0.2901 - val_accuracy: 0.9389\n",
      "Epoch 122/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0466 - accuracy: 0.9917\n",
      "Epoch 00122: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0467 - accuracy: 0.9916 - val_loss: 0.2985 - val_accuracy: 0.9353\n",
      "Epoch 123/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0448 - accuracy: 0.9927\n",
      "Epoch 00123: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0448 - accuracy: 0.9926 - val_loss: 0.2904 - val_accuracy: 0.9367\n",
      "Epoch 124/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9924\n",
      "Epoch 00124: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0452 - accuracy: 0.9924 - val_loss: 0.3089 - val_accuracy: 0.9382\n",
      "Epoch 125/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0462 - accuracy: 0.9921\n",
      "Epoch 00125: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0462 - accuracy: 0.9922 - val_loss: 0.2866 - val_accuracy: 0.9338\n",
      "Epoch 126/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0466 - accuracy: 0.9919\n",
      "Epoch 00126: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0466 - accuracy: 0.9919 - val_loss: 0.2857 - val_accuracy: 0.9349\n",
      "Epoch 127/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0454 - accuracy: 0.9921\n",
      "Epoch 00127: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0454 - accuracy: 0.9921 - val_loss: 0.3077 - val_accuracy: 0.9349\n",
      "Epoch 128/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0436 - accuracy: 0.9935\n",
      "Epoch 00128: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0437 - accuracy: 0.9934 - val_loss: 0.2924 - val_accuracy: 0.9396\n",
      "Epoch 129/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0431 - accuracy: 0.9936\n",
      "Epoch 00129: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0431 - accuracy: 0.9936 - val_loss: 0.3004 - val_accuracy: 0.9393\n",
      "Epoch 130/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0442 - accuracy: 0.9929\n",
      "Epoch 00130: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0444 - accuracy: 0.9928 - val_loss: 0.2830 - val_accuracy: 0.9375\n",
      "Epoch 131/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9928\n",
      "Epoch 00131: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0425 - accuracy: 0.9928 - val_loss: 0.2978 - val_accuracy: 0.9404\n",
      "Epoch 132/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9922\n",
      "Epoch 00132: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0451 - accuracy: 0.9922 - val_loss: 0.3104 - val_accuracy: 0.9385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0435 - accuracy: 0.9935\n",
      "Epoch 00133: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0435 - accuracy: 0.9935 - val_loss: 0.2904 - val_accuracy: 0.9396\n",
      "Epoch 134/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0415 - accuracy: 0.9931\n",
      "Epoch 00134: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0415 - accuracy: 0.9932 - val_loss: 0.3024 - val_accuracy: 0.9371\n",
      "Epoch 135/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0419 - accuracy: 0.9932\n",
      "Epoch 00135: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0419 - accuracy: 0.9932 - val_loss: 0.3323 - val_accuracy: 0.9335\n",
      "Epoch 136/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0436 - accuracy: 0.9933\n",
      "Epoch 00136: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0435 - accuracy: 0.9934 - val_loss: 0.3243 - val_accuracy: 0.9335\n",
      "Epoch 137/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9929\n",
      "Epoch 00137: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0424 - accuracy: 0.9930 - val_loss: 0.2908 - val_accuracy: 0.9378\n",
      "Epoch 138/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0417 - accuracy: 0.9933\n",
      "Epoch 00138: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0418 - accuracy: 0.9933 - val_loss: 0.3146 - val_accuracy: 0.9360\n",
      "Epoch 139/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0417 - accuracy: 0.9938\n",
      "Epoch 00139: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0416 - accuracy: 0.9938 - val_loss: 0.2976 - val_accuracy: 0.9400\n",
      "Epoch 140/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0415 - accuracy: 0.9934\n",
      "Epoch 00140: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0414 - accuracy: 0.9935 - val_loss: 0.2868 - val_accuracy: 0.9385\n",
      "Epoch 141/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0421 - accuracy: 0.9938\n",
      "Epoch 00141: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0421 - accuracy: 0.9938 - val_loss: 0.3242 - val_accuracy: 0.9382\n",
      "Epoch 142/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0400 - accuracy: 0.9942\n",
      "Epoch 00142: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0400 - accuracy: 0.9942 - val_loss: 0.2991 - val_accuracy: 0.9364\n",
      "Epoch 143/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9929\n",
      "Epoch 00143: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0427 - accuracy: 0.9929 - val_loss: 0.3041 - val_accuracy: 0.9393\n",
      "Epoch 144/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0421 - accuracy: 0.9928\n",
      "Epoch 00144: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0422 - accuracy: 0.9927 - val_loss: 0.2891 - val_accuracy: 0.9389\n",
      "Epoch 145/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0420 - accuracy: 0.9931\n",
      "Epoch 00145: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.9931 - val_loss: 0.3066 - val_accuracy: 0.9371\n",
      "Epoch 146/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0433 - accuracy: 0.9929\n",
      "Epoch 00146: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0433 - accuracy: 0.9929 - val_loss: 0.3258 - val_accuracy: 0.9331\n",
      "Epoch 147/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0375 - accuracy: 0.9949\n",
      "Epoch 00147: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0374 - accuracy: 0.9950 - val_loss: 0.2933 - val_accuracy: 0.9396\n",
      "Epoch 148/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0388 - accuracy: 0.9936\n",
      "Epoch 00148: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0387 - accuracy: 0.9936 - val_loss: 0.3163 - val_accuracy: 0.9338\n",
      "Epoch 149/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0409 - accuracy: 0.9935\n",
      "Epoch 00149: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0410 - accuracy: 0.9934 - val_loss: 0.2988 - val_accuracy: 0.9378\n",
      "Epoch 150/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0390 - accuracy: 0.9939\n",
      "Epoch 00150: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0392 - accuracy: 0.9939 - val_loss: 0.3224 - val_accuracy: 0.9353\n"
     ]
    }
   ],
   "source": [
    "batch_size_CNN_1D = 32\n",
    "epochs_CNN_1D     = 150\n",
    "\n",
    "history_CNN_1D    = model_CNN_1D.fit(X_train[..., np.newaxis], y_train_OHEV,\n",
    "                                     batch_size      = batch_size_CNN_1D,\n",
    "                                     epochs          = epochs_CNN_1D,\n",
    "                                     verbose         = 1,\n",
    "                                     validation_data =(X_test[..., np.newaxis], y_test_OHEV),\n",
    "                                     callbacks       = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "331031ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3224424123764038\n",
      "Test accuracy: 0.935272753238678\n"
     ]
    }
   ],
   "source": [
    "score_CNN_1D = model_CNN_1D.evaluate(X_test[..., np.newaxis], y_test_OHEV, verbose=0, batch_size = 32)\n",
    "print('Test loss:', score_CNN_1D[0])\n",
    "print('Test accuracy:', score_CNN_1D[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "975bf891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935272753238678"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_CNN_1D[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "42b09bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "fig.suptitle(nom_dataset + norm_type + model_surname + ' - CNN 1D - Training / Testing loss and accuracy', fontsize = 18)\n",
    "ax[0].plot(history_CNN_1D.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history_CNN_1D.history['val_loss'], color='r', label=\"Testing loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True, fontsize = 14)\n",
    "ax[0].tick_params(axis='x', labelsize=14)\n",
    "ax[0].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "ax[1].plot(history_CNN_1D.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history_CNN_1D.history['val_accuracy'], color='r',label=\"Testing accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True, fontsize = 14)\n",
    "ax[1].tick_params(axis='x', labelsize=14)\n",
    "ax[1].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(os.path.join(path_pic, picture_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1768a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file (Not the best model though)\n",
    "\n",
    "#model_CNN_1D.save(path_models + \"Model_CNN_1D.h5\")\n",
    "#print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6bb21a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_CNN_1D = np.argmax(model_CNN_1D.predict(X_val_norm[..., np.newaxis]),axis=1)\n",
    "y_pred_CNN_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37047e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc = np.argmax(y_OHEV_val, axis=1)\n",
    "y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "04983276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.84      0.88      0.86       756\n",
      "        car_horn       0.88      0.91      0.90       252\n",
      "children_playing       0.74      0.77      0.76       700\n",
      "        dog_bark       0.83      0.84      0.84       700\n",
      "           siren       0.87      0.76      0.81       602\n",
      "\n",
      "        accuracy                           0.83      3010\n",
      "       macro avg       0.84      0.83      0.83      3010\n",
      "    weighted avg       0.83      0.83      0.83      3010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_set_CNN_1D = classification_report(y_test_enc, y_pred_CNN_1D, target_names=nom_classes)\n",
    "print(metrics_set_CNN_1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b8d8936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_1D\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 369, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 369, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 369, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 184, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 184, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10304)             0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                515250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 526,291\n",
      "Trainable params: 526,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the highest accuracy\n",
    "\n",
    "model_CNN_1D_saved = load_model(os.path.join(path_models, 'Model_CNN_1D_weights_0_best' + norm_type + model_surname + '.hdf5'))\n",
    "model_CNN_1D_saved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e7d38f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 2ms/step - loss: 1.8050 - accuracy: 0.8140\n",
      "Test loss: 1.8049901723861694\n",
      "Test accuracy: 0.8139534592628479\n"
     ]
    }
   ],
   "source": [
    "score_CNN_1D_saved = model_CNN_1D_saved.evaluate(X_val_norm[..., np.newaxis], y_OHEV_val, verbose=1, batch_size = 32)\n",
    "print('Test loss:', score_CNN_1D_saved[0])\n",
    "print('Test accuracy:', score_CNN_1D_saved[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8b118cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_CNN_1D_saved = np.argmax(model_CNN_1D_saved.predict(X_val_norm[..., np.newaxis]),axis=1)\n",
    "y_pred_CNN_1D_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a8731110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.86      0.83      0.84       756\n",
      "        car_horn       0.88      0.89      0.89       252\n",
      "children_playing       0.75      0.77      0.76       700\n",
      "        dog_bark       0.77      0.86      0.81       700\n",
      "           siren       0.89      0.77      0.82       602\n",
      "\n",
      "        accuracy                           0.81      3010\n",
      "       macro avg       0.83      0.82      0.82      3010\n",
      "    weighted avg       0.82      0.81      0.81      3010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_set_CNN_1D_saved = classification_report(y_test_enc, y_pred_CNN_1D_saved, target_names=nom_classes)\n",
    "print(metrics_set_CNN_1D_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f26e1dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple confusion matrix\n",
    "\n",
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(y_test_enc, y_pred_CNN_1D_saved)\n",
    "title = nom_dataset + norm_type + model_surname + ' - Classifier CNN 1D (best model) - Highest accuracy test: '+ str(\"{:0.2f}%\".format(score_CNN_1D_saved[1]*100))\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True, \n",
    "            fmt='g', \n",
    "            cmap=cmap_cm, \n",
    "            annot_kws={\"size\": 8}, \n",
    "            xticklabels=nom_classes, \n",
    "            yticklabels=nom_classes)\n",
    "plt.title(title, fontsize = 12)\n",
    "plt.savefig(os.path.join(path_pic, picture_name))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0322d5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv1D at 0x2667ac9d670>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x2667ac9d790>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x2667aca5970>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling1D at 0x2667ac9ddf0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x2667acb1d90>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x2667aca5e50>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x265c40abcd0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2667acb1d00>]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CNN_1D_saved.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0847ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.03484167, -0.0646465 ,  0.15503293, -0.37033963,\n",
      "         -0.26520488,  0.37492216,  0.5255017 , -0.38596538,\n",
      "          0.11027458, -0.33351308, -0.39755902,  0.39524055,\n",
      "          0.23325212, -0.2521273 , -0.50564605, -0.33885208,\n",
      "         -0.52452207,  0.5309633 ,  0.30194122,  0.4993035 ,\n",
      "         -0.4318981 , -0.03662192, -0.14966308,  0.02395315,\n",
      "         -0.0216932 , -0.73105675, -0.5725185 ,  0.30554822]],\n",
      "\n",
      "       [[ 0.21301146, -0.39881358, -0.23893122, -0.2643677 ,\n",
      "          0.35482034, -0.04820268,  0.00743094, -0.5103248 ,\n",
      "          0.3770277 ,  0.38623267, -0.01453623, -0.4359002 ,\n",
      "          0.20162028,  0.07738566,  0.43983793,  0.14073697,\n",
      "         -0.1781655 , -0.20447   , -0.62610626, -0.01248927,\n",
      "         -0.00802864,  0.24477404, -0.3040584 ,  0.1930342 ,\n",
      "         -0.07392575,  0.08446299, -0.06298292,  0.12115344]],\n",
      "\n",
      "       [[-0.41335714,  0.14019011,  0.15521294,  0.26508313,\n",
      "          0.34212312,  0.2632417 ,  0.03215424, -0.08613508,\n",
      "          0.07074787, -0.28332824, -0.42665595,  0.02238819,\n",
      "          0.21389647,  0.14711642, -0.07333391, -0.12642269,\n",
      "         -0.27483436, -0.38627148, -0.12139857,  0.09051877,\n",
      "          0.27847788, -0.28520173, -0.04030095,  0.21108598,\n",
      "         -0.68484485, -0.10473668, -0.1142671 , -0.21702069]],\n",
      "\n",
      "       [[ 0.28913745,  0.19506973,  0.24214157, -0.02913659,\n",
      "          0.15722352,  0.24881428, -0.19830647,  0.33531502,\n",
      "         -0.07481458, -0.11363187, -0.45449066, -0.0696554 ,\n",
      "         -0.36843196,  0.4016377 ,  0.4053134 , -0.16365159,\n",
      "         -0.0056198 , -0.49686986,  0.07780625, -0.13967086,\n",
      "         -0.55672264,  0.16027726, -0.42619354,  0.16010846,\n",
      "         -0.12728526, -0.02073128, -0.04542606, -0.15638746]],\n",
      "\n",
      "       [[-0.18609619,  0.2955907 ,  0.2285657 ,  0.06888311,\n",
      "          0.13452815,  0.35487944, -0.04422881,  0.07457989,\n",
      "          0.29232264, -0.44029185, -0.34590366, -0.4275558 ,\n",
      "          0.26093102,  0.27316532, -0.24987175, -0.31241956,\n",
      "         -0.14571218,  0.1917674 ,  0.33068407, -0.39354265,\n",
      "         -0.39352655,  0.32875338, -0.2238977 , -0.07512914,\n",
      "          0.14929935,  0.30282336, -0.3356435 ,  0.19250512]],\n",
      "\n",
      "       [[ 0.24393913,  0.0521502 , -0.6941177 ,  0.24666944,\n",
      "          0.32127386,  0.01965941, -0.17709345, -0.3387272 ,\n",
      "         -0.31271845, -0.5606352 , -0.20323925,  0.4496922 ,\n",
      "         -0.35059068,  0.38927615, -0.07779959,  0.32447907,\n",
      "          0.39837447, -0.02994275,  0.36291248, -0.24384898,\n",
      "         -0.0194306 , -0.02851421, -0.33108208,  0.4395854 ,\n",
      "         -0.13051768, -0.43988335, -0.25644782, -0.42073265]],\n",
      "\n",
      "       [[-0.26173818,  0.01575373, -0.01067098,  0.13418913,\n",
      "          0.42904714, -0.17309614,  0.43026292, -0.29397643,\n",
      "          0.39431852, -0.51545364,  0.4103898 , -0.38835907,\n",
      "         -0.4334889 , -0.22589158, -0.20018116,  0.33616415,\n",
      "          0.49272475,  0.01723633, -0.12492399, -0.5940554 ,\n",
      "          0.24954817,  0.08499379, -0.21647926,  0.36346936,\n",
      "         -0.01676502,  0.4002232 ,  0.37685904,  0.06940962]]],\n",
      "      dtype=float32), array([ 0.02300707,  0.26502863,  0.2224669 ,  0.1626615 ,  0.13801275,\n",
      "       -0.08337171,  0.0160464 ,  0.09268378,  0.15704091, -0.01770269,\n",
      "        0.04152162,  0.05313817,  0.05182691,  0.13877131,  0.01468819,\n",
      "        0.0264695 ,  0.08661755,  0.17494187,  0.1536072 ,  0.04281578,\n",
      "        0.12414946,  0.35017127,  0.06330155, -0.02585672,  0.03661656,\n",
      "        0.03832019, -0.01733587,  0.1279421 ], dtype=float32)]\n",
      "[array([[[-7.18868803e-03, -4.58655134e-02, -1.71200130e-02, ...,\n",
      "          3.37949954e-02,  7.70235285e-02,  1.29211415e-02],\n",
      "        [-1.57213248e-02,  7.51547366e-02, -3.40553303e-03, ...,\n",
      "          3.15659381e-02,  2.99888873e-03, -1.23556918e-02],\n",
      "        [ 4.24026363e-02,  1.73429877e-03,  8.44924618e-03, ...,\n",
      "         -1.35992980e-03, -6.62099123e-02,  7.66430469e-03],\n",
      "        ...,\n",
      "        [-1.33707859e-02, -2.91119982e-02,  6.88811392e-02, ...,\n",
      "          4.77654627e-03,  9.34586301e-02,  5.14080338e-02],\n",
      "        [-1.72683094e-02, -6.56929612e-03,  4.57512587e-03, ...,\n",
      "         -1.47484196e-02,  6.94017410e-02,  4.78937179e-02],\n",
      "        [ 1.69844478e-02,  6.24713488e-03,  3.56621593e-02, ...,\n",
      "          3.29230651e-02, -6.74004480e-02, -6.71563745e-02]],\n",
      "\n",
      "       [[ 2.76973913e-03,  3.18147242e-02, -4.25629430e-02, ...,\n",
      "         -5.99733517e-02, -2.47049462e-02, -1.01876520e-02],\n",
      "        [ 1.11738369e-02, -3.80898379e-02,  3.83556597e-02, ...,\n",
      "          3.47147025e-02, -6.76744506e-02,  3.29937413e-02],\n",
      "        [-2.85330713e-02, -6.40196055e-02, -1.82069782e-02, ...,\n",
      "         -1.63755193e-02,  2.34109852e-02, -2.03022379e-02],\n",
      "        ...,\n",
      "        [-1.05699964e-01,  2.98418067e-02,  1.66553643e-03, ...,\n",
      "          1.60637833e-02,  7.20099658e-02,  5.92304356e-02],\n",
      "        [-5.87262921e-02,  1.20155374e-02,  2.37844288e-02, ...,\n",
      "          2.62795761e-02, -4.43017372e-04,  2.11533532e-02],\n",
      "        [-2.28823125e-02,  9.50841792e-03, -2.01007742e-02, ...,\n",
      "         -1.59888007e-02,  1.36287753e-02,  3.67792621e-02]],\n",
      "\n",
      "       [[ 6.30501956e-02,  4.57125306e-02, -9.98754986e-03, ...,\n",
      "          7.59253800e-02, -2.97101960e-02,  5.09810448e-02],\n",
      "        [ 1.29318738e-03, -5.42832091e-02,  5.03880642e-02, ...,\n",
      "         -4.08754162e-02,  1.97925102e-02,  1.15857916e-02],\n",
      "        [ 1.53993648e-02, -6.44328026e-03, -7.19320700e-02, ...,\n",
      "         -1.04414061e-01, -4.01530080e-02, -1.32636148e-02],\n",
      "        ...,\n",
      "        [ 7.80668706e-02,  2.87146792e-02, -1.22824460e-02, ...,\n",
      "          8.54674801e-02, -4.11416665e-02,  7.25041609e-03],\n",
      "        [ 2.85001602e-02,  5.31311333e-03, -5.11960033e-03, ...,\n",
      "         -2.44427603e-02,  2.71813162e-02, -2.80496292e-02],\n",
      "        [ 2.69620381e-02, -2.44150124e-02, -4.80996743e-02, ...,\n",
      "         -6.00459566e-03, -1.62755754e-02,  1.79768242e-02]],\n",
      "\n",
      "       [[ 3.74434963e-02, -4.69802227e-03, -3.41432877e-02, ...,\n",
      "         -5.07205352e-02, -5.12796827e-02,  2.16722712e-02],\n",
      "        [ 4.11816910e-02, -4.77181971e-02,  3.78073603e-02, ...,\n",
      "          7.14732632e-02, -2.20404509e-02,  8.31897836e-03],\n",
      "        [ 6.80131465e-02,  8.60664845e-02,  5.68195023e-02, ...,\n",
      "          6.48334697e-02, -2.96381824e-02,  9.15700346e-02],\n",
      "        ...,\n",
      "        [-2.88756322e-02,  9.31872725e-02,  5.99942878e-02, ...,\n",
      "          1.03701437e-02,  3.18223387e-02, -2.04309137e-04],\n",
      "        [ 5.46055986e-03,  4.86685783e-02, -9.83318686e-03, ...,\n",
      "          9.88756213e-03,  1.40847331e-02,  5.63839860e-02],\n",
      "        [ 4.79566045e-02, -8.22458067e-04,  2.92472560e-02, ...,\n",
      "          7.43746981e-02,  7.94899184e-03,  1.57459192e-02]],\n",
      "\n",
      "       [[ 4.26524675e-05,  3.17631960e-02,  4.20901552e-03, ...,\n",
      "          5.43127432e-02,  1.69914868e-02,  1.21171221e-01],\n",
      "        [ 2.38756258e-02, -2.80052740e-02,  4.28112708e-02, ...,\n",
      "         -4.57350835e-02, -3.35668176e-02, -8.11055768e-03],\n",
      "        [ 3.00478619e-02, -8.85139406e-02, -9.33954865e-02, ...,\n",
      "         -6.04631903e-04, -2.13825535e-02, -5.86370155e-02],\n",
      "        ...,\n",
      "        [ 6.45594299e-02,  3.45392674e-02, -8.04877877e-02, ...,\n",
      "         -2.64776349e-02, -4.08947058e-02, -1.02645986e-01],\n",
      "        [ 5.94223849e-02,  5.29567190e-02, -4.70926007e-03, ...,\n",
      "         -1.65929217e-02, -3.90728265e-02, -7.97682479e-02],\n",
      "        [-1.65694337e-02, -3.76615487e-02, -4.47147712e-02, ...,\n",
      "         -6.07961603e-02, -1.99894011e-02, -2.49405596e-02]]],\n",
      "      dtype=float32), array([ 0.00407593, -0.01941384,  0.00929663, -0.00121747,  0.01817351,\n",
      "        0.00050009,  0.02508957, -0.01685397, -0.01957728,  0.02045117,\n",
      "       -0.00797814,  0.00240984,  0.01419561,  0.00080597,  0.00794686,\n",
      "        0.00979113, -0.00491701, -0.00837682,  0.02963843,  0.00825424,\n",
      "        0.02466203,  0.01650951, -0.00187223,  0.00165273, -0.00200481,\n",
      "        0.00220672,  0.01282347, -0.00188702, -0.02240902, -0.00171852,\n",
      "        0.00671722, -0.0122165 , -0.0251263 ,  0.00519668], dtype=float32)]\n",
      "[array([[[-0.02219962,  0.05776038,  0.0526727 , ..., -0.0558489 ,\n",
      "         -0.00369178, -0.00098903],\n",
      "        [-0.0455328 , -0.02909952, -0.00302399, ..., -0.05922531,\n",
      "         -0.06499074, -0.02397141],\n",
      "        [-0.041711  ,  0.02041841,  0.04548197, ..., -0.05861825,\n",
      "         -0.02664831, -0.03095246],\n",
      "        ...,\n",
      "        [-0.07949253, -0.00556001,  0.00958572, ..., -0.10338137,\n",
      "          0.02645107, -0.08570673],\n",
      "        [-0.00972935, -0.0492803 , -0.03108036, ..., -0.07932691,\n",
      "          0.0735117 ,  0.02429523],\n",
      "        [ 0.0666531 , -0.10601269,  0.03156633, ...,  0.03329718,\n",
      "          0.06487107, -0.01766958]],\n",
      "\n",
      "       [[-0.0236699 , -0.01520531, -0.01576643, ..., -0.08702757,\n",
      "         -0.04738574, -0.03712726],\n",
      "        [-0.01690902, -0.03089204, -0.05174148, ..., -0.03999373,\n",
      "         -0.01572827, -0.04520678],\n",
      "        [ 0.02587639, -0.04256088, -0.02688975, ..., -0.01082513,\n",
      "         -0.01106906, -0.01094007],\n",
      "        ...,\n",
      "        [-0.02645179, -0.03280089,  0.02764338, ..., -0.02402214,\n",
      "         -0.0757691 , -0.06479702],\n",
      "        [-0.0143069 , -0.07056911,  0.08959259, ...,  0.03972768,\n",
      "         -0.07993377, -0.02408661],\n",
      "        [-0.00537206,  0.10033135,  0.00445213, ..., -0.10915247,\n",
      "         -0.04701062, -0.0020032 ]],\n",
      "\n",
      "       [[ 0.01653586, -0.01556631,  0.03189986, ...,  0.08361981,\n",
      "         -0.11474241,  0.05480062],\n",
      "        [-0.06884892, -0.01351303,  0.06139135, ...,  0.12126999,\n",
      "         -0.03603142,  0.07950386],\n",
      "        [-0.01887073, -0.01266017,  0.00893831, ...,  0.06342746,\n",
      "          0.01939481, -0.05248115],\n",
      "        ...,\n",
      "        [ 0.00715843, -0.08935142,  0.03714214, ...,  0.02355368,\n",
      "         -0.01621557, -0.05162136],\n",
      "        [-0.02877932, -0.09617293,  0.06663922, ..., -0.05235534,\n",
      "          0.00963187, -0.05903425],\n",
      "        [-0.01807327, -0.03775378, -0.04662388, ...,  0.0238108 ,\n",
      "         -0.02639319, -0.02717605]]], dtype=float32), array([ 0.0021098 , -0.00232143, -0.03489888,  0.01754732, -0.02047488,\n",
      "        0.00416053, -0.01617186, -0.01036889, -0.00257654,  0.01905985,\n",
      "       -0.03310992, -0.01425372,  0.02152439, -0.01207309,  0.01866185,\n",
      "        0.02046383, -0.00803153, -0.00159816, -0.00078583,  0.01463913,\n",
      "       -0.01932969,  0.02164949,  0.00406471, -0.01819738, -0.01881322,\n",
      "        0.02000411, -0.02750342,  0.00906985, -0.01962848,  0.02817201,\n",
      "        0.01556091, -0.00389763,  0.00966218,  0.01439616,  0.02511424,\n",
      "        0.02414747,  0.00705329,  0.025972  , -0.01304821, -0.00389069,\n",
      "        0.01235149,  0.00490242,  0.00056035,  0.02086245,  0.00448536,\n",
      "       -0.01752109, -0.02533078,  0.02724   ,  0.01586171,  0.02677861,\n",
      "        0.02559018, -0.02645095,  0.0053057 ,  0.01059368, -0.01143229,\n",
      "       -0.03337066], dtype=float32)]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[array([[ 0.00159167,  0.07072011, -0.05719199, ...,  0.02637537,\n",
      "         0.04520917,  0.03408343],\n",
      "       [-0.02408248,  0.1772711 ,  0.07726727, ...,  0.09869962,\n",
      "        -0.03920475,  0.17320038],\n",
      "       [-0.13271618,  0.1889738 ,  0.04817507, ..., -0.03986232,\n",
      "        -0.0481084 ,  0.11075838],\n",
      "       ...,\n",
      "       [-0.05083464,  0.07879884, -0.04337205, ..., -0.01232059,\n",
      "         0.0809149 ,  0.08662368],\n",
      "       [ 0.02819232, -0.005187  ,  0.00923473, ...,  0.04808467,\n",
      "         0.00407059,  0.01180349],\n",
      "       [ 0.02726155,  0.00650227,  0.04712464, ...,  0.01846625,\n",
      "        -0.06031214,  0.02612677]], dtype=float32), array([-0.09387563,  0.00410826,  0.19002295, -0.04763485, -0.05299419,\n",
      "       -0.12189256,  0.08992479, -0.07859991,  0.09134495, -0.02987365,\n",
      "       -0.01355104, -0.18989998, -0.10279971, -0.14932743, -0.17488505,\n",
      "        0.07391405, -0.03355432,  0.09113048,  0.02609841,  0.10734599,\n",
      "        0.18138316, -0.08067666, -0.1219972 , -0.15186249,  0.04364232,\n",
      "        0.05041849,  0.092454  ,  0.18595292,  0.15707535,  0.17755039,\n",
      "       -0.19432326, -0.1852805 ,  0.1320098 , -0.17590037,  0.10623853,\n",
      "        0.13848875, -0.07695743,  0.01888974,  0.14965148, -0.04269683,\n",
      "       -0.1733761 , -0.07761455, -0.17653124, -0.00523663, -0.02738076,\n",
      "        0.17352365, -0.02873429, -0.02459335, -0.16176015, -0.11848814],\n",
      "      dtype=float32)]\n",
      "[array([[-0.19880727,  1.0702398 , -0.31171066, -0.12848587, -0.06392939],\n",
      "       [ 0.21415831, -1.0478932 ,  0.15619726,  0.23661882, -0.41314372],\n",
      "       [ 0.5702333 , -0.10894526,  0.6247407 , -0.36181632, -0.43062735],\n",
      "       [-0.593782  , -0.43146122,  0.24278888,  0.5907508 , -0.08044446],\n",
      "       [ 0.2688056 ,  1.0243732 , -0.30512792, -0.10656335, -0.5669266 ],\n",
      "       [-0.18145481, -0.31157273,  0.01715828,  0.62648726, -0.19563206],\n",
      "       [ 0.23310687,  1.1794591 , -0.14149268, -0.41351652, -0.16710424],\n",
      "       [ 0.56152594,  0.5170731 , -0.16763932,  0.15591712, -0.42652896],\n",
      "       [-0.34038195, -0.60127515,  0.4358922 ,  0.11772216,  0.45238233],\n",
      "       [-0.63512486, -0.71434337,  0.18708277,  0.29663268,  0.43361962],\n",
      "       [ 0.6970991 ,  0.6880061 , -0.19189449, -0.09977462, -0.519298  ],\n",
      "       [-0.36917785,  0.6746584 , -0.51829845,  0.36651203,  0.6090622 ],\n",
      "       [ 0.41013217, -0.44990507, -0.22585945,  0.32441103, -0.87049216],\n",
      "       [ 0.17239715, -0.45933506, -0.38406458,  0.2590068 , -0.58998215],\n",
      "       [ 0.28843936, -0.7942909 , -0.2953831 ,  0.40904397,  0.02616069],\n",
      "       [ 0.514909  , -0.9134999 , -0.12734433, -0.69495636,  0.42151618],\n",
      "       [ 0.18399955,  0.749709  , -0.4074428 , -0.46495378,  0.7337697 ],\n",
      "       [-0.49044558,  0.50839335,  0.34448692,  0.08680862, -0.3674934 ],\n",
      "       [-0.14154634, -0.35536006, -0.15408534, -0.42883673,  1.0674192 ],\n",
      "       [-0.39887667,  0.5060308 ,  0.3552844 , -0.16655368,  0.767842  ],\n",
      "       [ 0.33860782,  0.3210232 ,  0.33296445, -0.47436786, -0.610457  ],\n",
      "       [ 0.6718491 , -0.30819046, -0.23576199,  0.05225695, -0.14589405],\n",
      "       [-0.07766797,  0.76645756, -0.05826996,  0.5802404 , -0.43646675],\n",
      "       [-0.16289312, -0.8346904 , -0.19429913,  0.23008387, -0.15866417],\n",
      "       [-0.03117039,  1.5004541 , -0.03064281, -0.1342327 , -0.1038569 ],\n",
      "       [-0.28215578,  0.32096726,  0.5228136 ,  0.4771203 , -0.7174203 ],\n",
      "       [ 0.6590275 , -0.48902798,  0.02946277, -0.32004607, -0.64116216],\n",
      "       [ 0.13708313, -0.41094518,  0.69899684, -0.1453585 ,  0.2203065 ],\n",
      "       [ 0.40775785,  0.05143227,  0.36199844, -0.6944345 ,  0.43395612],\n",
      "       [-0.23117012,  0.5274693 ,  0.25982848, -0.47295687, -0.01397129],\n",
      "       [ 0.05397181,  0.2661366 , -1.067777  ,  0.22026011,  0.41415262],\n",
      "       [ 0.01399144, -0.38257295, -0.6560866 ,  0.21354753,  0.5920071 ],\n",
      "       [ 0.8246275 , -0.4101914 ,  0.21820545, -0.47002262, -0.41393363],\n",
      "       [ 0.29792386,  0.17945139, -0.78697395,  0.1359911 ,  0.53783697],\n",
      "       [-0.5902269 , -0.34119475,  0.5129791 ,  0.10490236, -0.02910821],\n",
      "       [-0.34473482, -0.4638865 ,  0.5249441 , -0.13826203,  0.52441424],\n",
      "       [ 0.8039696 , -0.23764426, -0.40281618, -0.06641859, -0.17352942],\n",
      "       [-0.13530302, -0.95294994,  0.0427961 ,  0.09779263, -0.8397332 ],\n",
      "       [ 0.38929632,  0.4116507 ,  0.08789527, -0.48649162, -0.6888003 ],\n",
      "       [ 0.20555893,  0.46528476,  0.07552153,  0.42590645, -1.1042478 ],\n",
      "       [-0.17378268, -0.2317318 , -0.2507481 ,  0.4211714 ,  0.8071822 ],\n",
      "       [ 0.45735827, -0.5612061 , -0.14426771,  0.35399154, -1.0643891 ],\n",
      "       [ 0.24118853, -0.10459527, -0.9300661 ,  0.08461309,  0.27221224],\n",
      "       [-0.14999539,  0.23665442,  0.1079185 ,  0.30063638, -1.2175266 ],\n",
      "       [ 0.5419222 ,  0.32131848, -0.35057285, -0.37458774,  0.36685222],\n",
      "       [-0.06970608,  0.3983602 ,  0.17777911, -0.3369568 , -0.6850853 ],\n",
      "       [-0.62523144, -0.49505478,  0.08244552,  0.30510557, -0.31895882],\n",
      "       [-0.574337  ,  0.59803754,  0.12520115,  0.41931736, -0.66289234],\n",
      "       [-0.46478143, -0.1376671 , -0.27277645,  0.5463848 ,  0.1209987 ],\n",
      "       [ 0.4909352 , -0.01133898, -0.23522869,  0.294143  , -0.49057382]],\n",
      "      dtype=float32), array([-0.00058239, -0.01852785,  0.1856057 , -0.16601506, -0.0439766 ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model_CNN_1D_saved.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9e2b7df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00058239, -0.01852785,  0.1856057 , -0.16601506, -0.0439766 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model_CNN_1D_saved.get_layer('Output').get_weights()\n",
    "weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73169e4d",
   "metadata": {},
   "source": [
    "## Metrics for the classifiers\n",
    "\n",
    "\n",
    "1. Accuracy: Accuracy is a measure of how many correct predictions a model makes overall, i.e., the ratio of correct predictions to the total number of predictions. It's a commonly used metric for evaluating models, but it may not be suitable in certain situations.\n",
    "\n",
    "2. Precision: Precision measures the ratio of true positives (correctly predicted positive instances) to all instances predicted as positive. It focuses on the accuracy of positive predictions.\n",
    "\n",
    "3. Recall: Recall, also known as sensitivity or true positive rate, measures the ratio of true positives to all actual positive instances. It focuses on how well a model captures all the positive instances.\n",
    "\n",
    "4. F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure that takes into account both false positives and false negatives. The F1 score is especially useful when you want to strike a balance between precision and recall.\n",
    "\n",
    "\n",
    "The F1 score is a metric that combines precision and recall, and it is particularly useful in situations where class imbalance or unequal misclassification costs are present. In such contexts, the F1 score can be more informative and meaningful than accuracy.\n",
    "\n",
    "A context where considering the F1 score makes more sense than accuracy:\n",
    "\n",
    "**Medical Diagnosis:**\n",
    "\n",
    "Imagine you're developing a model to diagnose a rare disease, and only 5% of the population has this disease. In this case, you have a significant class imbalance, where the majority of cases are negative (non-disease) and only a small fraction are positive (disease). If you were to use accuracy as the evaluation metric, the model could achieve a high accuracy by simply predicting \"negative\" for every case, because it would be correct 95% of the time due to the class imbalance. However, this would be entirely useless for detecting the actual disease.\n",
    "\n",
    "In this scenario, you'd be more interested in the F1 score. The F1 score considers both precision and recall, helping you find a balance between correctly identifying the disease (high recall) and not making too many false positive predictions (high precision). A high F1 score in this context indicates that your model is effective at correctly identifying the disease while minimizing false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "46413ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['ANN', 'CNN_1D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1a186ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to run the classifiers and their metrics\n",
    "\n",
    "def model_classifiers(classifiers:list, \n",
    "                      db: pd.DataFrame, \n",
    "                      scalerOpt: str, \n",
    "                      use_PCA = False):\n",
    "    \n",
    "    # Clear the session to start a new training\n",
    "    K.clear_session()\n",
    " \n",
    "    es = EarlyStopping(monitor='accuracy', min_delta=0.0001, patience=50, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    \n",
    "    count       = 1\n",
    "    batch_size  = 32\n",
    "    verbose     = True\n",
    "    models      = []\n",
    "    acc_set     = pd.DataFrame(index=None, columns=['Model',\n",
    "                                                    'Fold',\n",
    "                                                    'Accuracy(Train)',\n",
    "                                                    'Accuracy(Val)',\n",
    "                                                    'F1(Train)',\n",
    "                                                    'F1(Val)', \n",
    "                                                    'Precision(Train)',\n",
    "                                                    'Precision(Val)', \n",
    "                                                    'Recall(Train)',\n",
    "                                                    'Recall(Val)', \n",
    "                                                    'Conf_M',\n",
    "                                                    'Process_time',                                                     \n",
    "                                                    'Class_report(Val)'])\n",
    "    \n",
    "    for fold in np.unique(db['Fold']):\n",
    "        print(f\"Validation fold: {fold}\")\n",
    "\n",
    "        DB_VAL = db[db['Fold'] == fold]\n",
    "        DB_TRN = db[db['Fold'] != fold]\n",
    "\n",
    "        X      = DB_TRN.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "        y      = np.array(DB_TRN.Class_categorical.to_list())\n",
    "        y_OHEV = np.array(DB_TRN.Class_OHEV.to_list())\n",
    "\n",
    "        X_val      = DB_VAL.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "        y_val      = np.array(DB_VAL.Class_categorical.to_list())\n",
    "        y_OHEV_val = np.array(DB_VAL.Class_OHEV.to_list())\n",
    "        \n",
    "        neurons  = X.shape[1]\n",
    "        \n",
    "        X_statistics = pd.DataFrame({'mean': X.mean(), 'std': X.std(), 'min': X.min(), 'max': X.max()})\n",
    "\n",
    "        X_mean   = X_statistics.values[:, 0]\n",
    "        X_std    = X_statistics.values[:, 1]\n",
    "        X_min    = X_statistics.values[:, 2]\n",
    "        X_max    = X_statistics.values[:, 3]\n",
    "        \n",
    "        if scalerOpt == \"normalization\":\n",
    "            X_train_norm = (X.values - X_min) / (X_max - X_min)\n",
    "            X_val_norm   = (X_val.values - X_min) / (X_max - X_min)\n",
    "            batch_type    = '_norm'\n",
    "            print(f'X_train_norm shape...:{X_train_norm.shape}')\n",
    "            print(f'X_val_norm shape.....:{X_val_norm.shape}\\n')\n",
    "            \n",
    "        elif scalerOpt == \"standardization\":\n",
    "            X_train_norm = (X.values - X_mean) / X_std\n",
    "            X_val_norm   = (X_val.values - X_mean) / X_std\n",
    "            batch_type    = '_std'\n",
    "            print(f'X_train_norm shape...:{X_train_norm.shape}')\n",
    "            print(f'X_val_norm shape.....:{X_val_norm.shape}\\n')\n",
    "            \n",
    "        else:\n",
    "            sys.exit()\n",
    "            \n",
    "        if use_PCA:\n",
    "            pcaT = PCA()\n",
    "            pcaT.fit(X_train_norm)\n",
    "            ratio = pcaT.explained_variance_ratio_\n",
    "\n",
    "            batch_type = batch_type + '_PCA'\n",
    "\n",
    "            T           = 0.98\n",
    "            current_sum = 0\n",
    "            countComp   = 0\n",
    "\n",
    "            for element in ratio:\n",
    "                current_sum += element\n",
    "                countComp   += 1\n",
    "\n",
    "                if current_sum >= T:\n",
    "                    break\n",
    "\n",
    "            # Print the result\n",
    "            print(\"Sum of elements:\", current_sum)\n",
    "            print(\"Number of elements summed:\", countComp)           \n",
    "\n",
    "            pca          = PCA(n_components = countComp)\n",
    "            X_train_norm = pca.fit_transform(X_train_norm)\n",
    "            X_val_norm   = pca.transform(X_val_norm)\n",
    "            neurons      = countComp\n",
    "        \n",
    "        # The training dataset will be 10% reduced compared with the ML techniques, separating a test set to monitor\n",
    "        # the accuracy during training\n",
    "        X_train_norm, X_test_norm, y_train, y_test = train_test_split(X_train_norm, y_OHEV, test_size=0.1, random_state=42, stratify=y_OHEV)\n",
    "\n",
    "        for i in tqdm(range(len(classifiers))):\n",
    "            \n",
    "            name         = classifiers[i]\n",
    "            model_name   = ('Model_' + classifiers[i] + '_' + str(count))\n",
    "            count        = count + 1\n",
    "            \n",
    "            if classifiers[i] == 'ANN':\n",
    "                \n",
    "                filepath       = os.path.join(path_models, 'Model_ANN_weights_0_best' + norm_type + model_surname + '.hdf5')\n",
    "                checkpoint     = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "                callbacks_list = [checkpoint, es]\n",
    "               \n",
    "                model = build_ANN_model(model_name, neurons)\n",
    "                model.summary()\n",
    "                print(name)\n",
    "                print(np.shape(X_train_norm))    \n",
    "\n",
    "                model.fit(X_train_norm, \n",
    "                          y_train, \n",
    "                          batch_size      = batch_size, \n",
    "                          epochs          = 350, \n",
    "                          verbose         = verbose,                               \n",
    "                          validation_data = (X_test_norm, y_test),\n",
    "                          callbacks       = callbacks_list)\n",
    "                \n",
    "                model= load_model(os.path.join(path_models, 'Model_ANN_weights_0_best' + norm_type + model_surname + '.hdf5'))\n",
    "                print('Best model loaded')\n",
    "\n",
    "            else:\n",
    "\n",
    "                filepath       = os.path.join(path_models, 'Model_CNN_1D_weights_0_best' + norm_type + model_surname + '.hdf5')\n",
    "                checkpoint     = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "                callbacks_list = [checkpoint]                \n",
    "\n",
    "                X_train_norm = X_train_norm[..., np.newaxis]\n",
    "                X_val_norm   = X_val_norm[..., np.newaxis]\n",
    "                X_test_norm  = X_test_norm[..., np.newaxis]\n",
    "\n",
    "                model = build_CNN_1D_model(model_name, neurons)\n",
    "                model.summary()\n",
    "                print(name)\n",
    "                print(np.shape(X_train_norm))    \n",
    "                \n",
    "                model.fit(X_train_norm, \n",
    "                          y_train, \n",
    "                          batch_size = batch_size, \n",
    "                          epochs = 150, \n",
    "                          verbose = verbose,                          \n",
    "                          validation_data = (X_test_norm, y_test),\n",
    "                          callbacks       = callbacks_list)\n",
    "\n",
    "                model= load_model(os.path.join(path_models, 'Model_CNN_1D_weights_0_best' + norm_type + model_surname + '.hdf5'))\n",
    "                print('Best model loaded')\n",
    "\n",
    "            # Get the model predictions\n",
    "            y_train_enc = np.argmax(y_train, axis=1)\n",
    "            y_val_enc   = np.argmax(y_OHEV_val, axis=1)\n",
    "\n",
    "            y_train_predicted = np.argmax(model.predict(X_train_norm), axis=1)\n",
    "            \n",
    "            t_srt             = time.process_time_ns()\n",
    "            y_val_predicted   = np.argmax(model.predict(X_val_norm), axis=1)\n",
    "            t_end             = time.process_time_ns()\n",
    "            proc_time         = ((t_end - t_srt) / 1000000)         \n",
    "    \n",
    "            # Compute the classifier metrics\n",
    "            accuracy_train = metrics.accuracy_score(y_train_enc, y_train_predicted)\n",
    "            accuracy_val   = metrics.accuracy_score(y_val_enc,  y_val_predicted)\n",
    "\n",
    "            f1_Score_train = metrics.f1_score(y_train_enc, y_train_predicted, average = 'weighted')\n",
    "            f1_Score_val   = metrics.f1_score(y_val_enc,  y_val_predicted,  average = 'weighted')\n",
    "\n",
    "            precision_score_train = metrics.precision_score(y_train_enc, y_train_predicted, average = 'weighted')\n",
    "            precision_score_val   = metrics.precision_score(y_val_enc,  y_val_predicted,  average = 'weighted')\n",
    "\n",
    "            recall_score_train = metrics.recall_score(y_train_enc, y_train_predicted, average = 'weighted')\n",
    "            recall_score_val   = metrics.recall_score(y_val_enc,  y_val_predicted,  average = 'weighted')\n",
    "\n",
    "            class_report_val = classification_report(y_val_enc, y_val_predicted, target_names = nom_classes)\n",
    "            print(class_report_val)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            CM = metrics.confusion_matrix(y_val_enc, y_val_predicted)\n",
    "            y_val_enc       = []\n",
    "            y_val_predicted = []\n",
    "\n",
    "            # Store the name, test accuracy results and model\n",
    "            models.append((name, accuracy_val, model))\n",
    "            \n",
    "            K.clear_session()\n",
    "            del model\n",
    "                    \n",
    "            acc_set = pd.concat([acc_set, pd.DataFrame({'Model': [name],\n",
    "                                                        'Fold': [fold],\n",
    "                                                        'Accuracy(Train)': [accuracy_train],\n",
    "                                                        'Accuracy(Val)': [accuracy_val],\n",
    "                                                        'F1(Train)': [f1_Score_train],\n",
    "                                                        'F1(Val)': [f1_Score_val],\n",
    "                                                        'Precision(Train)': [precision_score_train],\n",
    "                                                        'Precision(Val)': [precision_score_val],\n",
    "                                                        'Recall(Train)': [recall_score_train],\n",
    "                                                        'Recall(Val)': [recall_score_val],\n",
    "                                                        'Conf_M': [CM],\n",
    "                                                        'Process_time': [proc_time],\n",
    "                                                        'Class_report(Val)': class_report_val})], ignore_index = True)\n",
    "                   \n",
    "    return acc_set, models, batch_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "358bab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation fold: 1\n",
      "X_train_norm shape...:(27496, 375)\n",
      "X_val_norm shape.....:(3010, 375)\n",
      "\n",
      "Sum of elements: 0.9801846635764112\n",
      "Number of elements summed: 231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 231)               53592     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 231)               53592     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 231)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               174000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 284,939\n",
      "Trainable params: 284,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24746, 231)\n",
      "Epoch 1/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.7861 - accuracy: 0.7111\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.81745, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.7816 - accuracy: 0.7131 - val_loss: 0.5138 - val_accuracy: 0.8175\n",
      "Epoch 2/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.4588 - accuracy: 0.8343\n",
      "Epoch 00002: val_accuracy improved from 0.81745 to 0.84800, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.4589 - accuracy: 0.8342 - val_loss: 0.4131 - val_accuracy: 0.8480\n",
      "Epoch 3/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.3533 - accuracy: 0.8731\n",
      "Epoch 00003: val_accuracy improved from 0.84800 to 0.87455, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.3541 - accuracy: 0.8726 - val_loss: 0.3526 - val_accuracy: 0.8745\n",
      "Epoch 4/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8944\n",
      "Epoch 00004: val_accuracy improved from 0.87455 to 0.88691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2910 - accuracy: 0.8946 - val_loss: 0.3129 - val_accuracy: 0.8869\n",
      "Epoch 5/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.9117\n",
      "Epoch 00005: val_accuracy improved from 0.88691 to 0.89564, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.2463 - accuracy: 0.9117 - val_loss: 0.2878 - val_accuracy: 0.8956\n",
      "Epoch 6/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9263\n",
      "Epoch 00006: val_accuracy improved from 0.89564 to 0.90909, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2073 - accuracy: 0.9266 - val_loss: 0.2548 - val_accuracy: 0.9091\n",
      "Epoch 7/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9406\n",
      "Epoch 00007: val_accuracy improved from 0.90909 to 0.91782, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1737 - accuracy: 0.9404 - val_loss: 0.2428 - val_accuracy: 0.9178\n",
      "Epoch 8/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9498\n",
      "Epoch 00008: val_accuracy improved from 0.91782 to 0.91964, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1477 - accuracy: 0.9496 - val_loss: 0.2305 - val_accuracy: 0.9196\n",
      "Epoch 9/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9573\n",
      "Epoch 00009: val_accuracy improved from 0.91964 to 0.92618, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1268 - accuracy: 0.9573 - val_loss: 0.2239 - val_accuracy: 0.9262\n",
      "Epoch 10/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9641\n",
      "Epoch 00010: val_accuracy improved from 0.92618 to 0.93273, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1075 - accuracy: 0.9641 - val_loss: 0.2130 - val_accuracy: 0.9327\n",
      "Epoch 11/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9702\n",
      "Epoch 00011: val_accuracy improved from 0.93273 to 0.93709, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0920 - accuracy: 0.9703 - val_loss: 0.2081 - val_accuracy: 0.9371\n",
      "Epoch 12/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9757\n",
      "Epoch 00012: val_accuracy did not improve from 0.93709\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0770 - accuracy: 0.9752 - val_loss: 0.2134 - val_accuracy: 0.9353\n",
      "Epoch 13/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9779\n",
      "Epoch 00013: val_accuracy improved from 0.93709 to 0.93891, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0682 - accuracy: 0.9779 - val_loss: 0.2085 - val_accuracy: 0.9389\n",
      "Epoch 14/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9829\n",
      "Epoch 00014: val_accuracy improved from 0.93891 to 0.94073, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0560 - accuracy: 0.9829 - val_loss: 0.2077 - val_accuracy: 0.9407\n",
      "Epoch 15/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9864\n",
      "Epoch 00015: val_accuracy improved from 0.94073 to 0.94218, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0467 - accuracy: 0.9865 - val_loss: 0.2040 - val_accuracy: 0.9422\n",
      "Epoch 16/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9875\n",
      "Epoch 00016: val_accuracy did not improve from 0.94218\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0409 - accuracy: 0.9875 - val_loss: 0.2101 - val_accuracy: 0.9407\n",
      "Epoch 17/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0357 - accuracy: 0.9900\n",
      "Epoch 00017: val_accuracy did not improve from 0.94218\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0358 - accuracy: 0.9899 - val_loss: 0.2124 - val_accuracy: 0.9415\n",
      "Epoch 18/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 0.9901\n",
      "Epoch 00018: val_accuracy did not improve from 0.94218\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0334 - accuracy: 0.9901 - val_loss: 0.2109 - val_accuracy: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0273 - accuracy: 0.9925\n",
      "Epoch 00019: val_accuracy improved from 0.94218 to 0.94255, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0273 - accuracy: 0.9925 - val_loss: 0.2144 - val_accuracy: 0.9425\n",
      "Epoch 20/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9932\n",
      "Epoch 00020: val_accuracy improved from 0.94255 to 0.94291, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0244 - accuracy: 0.9932 - val_loss: 0.2160 - val_accuracy: 0.9429\n",
      "Epoch 21/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9947\n",
      "Epoch 00021: val_accuracy improved from 0.94291 to 0.94400, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0200 - accuracy: 0.9947 - val_loss: 0.2236 - val_accuracy: 0.9440\n",
      "Epoch 22/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9955\n",
      "Epoch 00022: val_accuracy did not improve from 0.94400\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.2296 - val_accuracy: 0.9415\n",
      "Epoch 23/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9969\n",
      "Epoch 00023: val_accuracy improved from 0.94400 to 0.94691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0148 - accuracy: 0.9969 - val_loss: 0.2378 - val_accuracy: 0.9469\n",
      "Epoch 24/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9961\n",
      "Epoch 00024: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0144 - accuracy: 0.9962 - val_loss: 0.2344 - val_accuracy: 0.9436\n",
      "Epoch 25/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9969\n",
      "Epoch 00025: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 0.2297 - val_accuracy: 0.9422\n",
      "Epoch 26/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9970\n",
      "Epoch 00026: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.2314 - val_accuracy: 0.9465\n",
      "Epoch 27/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9972\n",
      "Epoch 00027: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0118 - accuracy: 0.9972 - val_loss: 0.2286 - val_accuracy: 0.9425\n",
      "Epoch 28/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9982\n",
      "Epoch 00028: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.2357 - val_accuracy: 0.9469\n",
      "Epoch 29/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9978\n",
      "Epoch 00029: val_accuracy improved from 0.94691 to 0.94909, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0091 - accuracy: 0.9979 - val_loss: 0.2422 - val_accuracy: 0.9491\n",
      "Epoch 30/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9979\n",
      "Epoch 00030: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 0.2437 - val_accuracy: 0.9455\n",
      "Epoch 31/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9973\n",
      "Epoch 00031: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.2458 - val_accuracy: 0.9447\n",
      "Epoch 32/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9980\n",
      "Epoch 00032: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.2393 - val_accuracy: 0.9476\n",
      "Epoch 33/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9983\n",
      "Epoch 00033: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.2519 - val_accuracy: 0.9480\n",
      "Epoch 34/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9986\n",
      "Epoch 00034: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 0.2534 - val_accuracy: 0.9484\n",
      "Epoch 35/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9986\n",
      "Epoch 00035: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9986 - val_loss: 0.2501 - val_accuracy: 0.9469\n",
      "Epoch 36/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9988\n",
      "Epoch 00036: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.2503 - val_accuracy: 0.9465\n",
      "Epoch 37/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989\n",
      "Epoch 00037: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 0.2588 - val_accuracy: 0.9455\n",
      "Epoch 38/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9989\n",
      "Epoch 00038: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 0.2605 - val_accuracy: 0.9462\n",
      "Epoch 39/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 00039: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.2673 - val_accuracy: 0.9458\n",
      "Epoch 40/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.2592 - val_accuracy: 0.9451\n",
      "Epoch 41/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9992\n",
      "Epoch 00041: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.2681 - val_accuracy: 0.9465\n",
      "Epoch 42/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9995\n",
      "Epoch 00042: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.2673 - val_accuracy: 0.9473\n",
      "Epoch 43/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 00043: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2713 - val_accuracy: 0.9469\n",
      "Epoch 44/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9992\n",
      "Epoch 00044: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.2779 - val_accuracy: 0.9469\n",
      "Epoch 45/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753/774 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9991\n",
      "Epoch 00045: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.2711 - val_accuracy: 0.9469\n",
      "Epoch 46/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 00046: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.2685 - val_accuracy: 0.9491\n",
      "Epoch 47/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 00047: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.2809 - val_accuracy: 0.9480\n",
      "Epoch 48/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 00048: val_accuracy improved from 0.94909 to 0.94945, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.2824 - val_accuracy: 0.9495\n",
      "Epoch 49/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9990\n",
      "Epoch 00049: val_accuracy did not improve from 0.94945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.2836 - val_accuracy: 0.9480\n",
      "Epoch 50/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00050: val_accuracy improved from 0.94945 to 0.95055, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2767 - val_accuracy: 0.9505\n",
      "Epoch 51/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0026 - accuracy: 0.9997\n",
      "Epoch 00051: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2770 - val_accuracy: 0.9480\n",
      "Epoch 52/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9996\n",
      "Epoch 00052: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2779 - val_accuracy: 0.9469\n",
      "Epoch 53/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00053: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.2800 - val_accuracy: 0.9495\n",
      "Epoch 54/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9996\n",
      "Epoch 00054: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2800 - val_accuracy: 0.9469\n",
      "Epoch 55/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2797 - val_accuracy: 0.9505\n",
      "Epoch 56/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 00056: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.2851 - val_accuracy: 0.9487\n",
      "Epoch 57/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00057: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2729 - val_accuracy: 0.9476\n",
      "Epoch 58/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.2763 - val_accuracy: 0.9495\n",
      "Epoch 59/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2841 - val_accuracy: 0.9495\n",
      "Epoch 60/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00060: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2856 - val_accuracy: 0.9476\n",
      "Epoch 61/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2837 - val_accuracy: 0.9498\n",
      "Epoch 62/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 00062: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.2917 - val_accuracy: 0.9465\n",
      "Epoch 63/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00063: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2854 - val_accuracy: 0.9495\n",
      "Epoch 64/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00064: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2875 - val_accuracy: 0.9491\n",
      "Epoch 65/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00065: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.2949 - val_accuracy: 0.9473\n",
      "Epoch 66/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00066: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2924 - val_accuracy: 0.9502\n",
      "Epoch 67/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00067: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2945 - val_accuracy: 0.9476\n",
      "Epoch 68/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.3004 - val_accuracy: 0.9502\n",
      "Epoch 69/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3000 - val_accuracy: 0.9487\n",
      "Epoch 70/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00070: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9498\n",
      "Epoch 71/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00071: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 9s 12ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3007 - val_accuracy: 0.9505\n",
      "Epoch 72/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00072: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 4s 6ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2977 - val_accuracy: 0.9502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00073: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2984 - val_accuracy: 0.9498\n",
      "Epoch 74/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 00074: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.3096 - val_accuracy: 0.9495\n",
      "Epoch 75/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 00075: val_accuracy improved from 0.95055 to 0.95164, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.2966 - val_accuracy: 0.9516\n",
      "Epoch 76/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00076: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2970 - val_accuracy: 0.9487\n",
      "Epoch 77/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 00077: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.3011 - val_accuracy: 0.9505\n",
      "Epoch 78/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 00078: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2959 - val_accuracy: 0.9484\n",
      "Epoch 79/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9993\n",
      "Epoch 00079: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.3073 - val_accuracy: 0.9484\n",
      "Epoch 80/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00080: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.3129 - val_accuracy: 0.9473\n",
      "Epoch 81/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00081: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3002 - val_accuracy: 0.9487\n",
      "Epoch 82/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 00082: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.2949 - val_accuracy: 0.9480\n",
      "Epoch 83/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00083: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2928 - val_accuracy: 0.9509\n",
      "Epoch 84/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00084: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2939 - val_accuracy: 0.9495\n",
      "Epoch 85/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00085: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9505\n",
      "Epoch 86/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00086: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2903 - val_accuracy: 0.9498\n",
      "Epoch 87/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2895 - val_accuracy: 0.9495\n",
      "Epoch 88/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00088: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2927 - val_accuracy: 0.9498\n",
      "Epoch 89/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00089: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2915 - val_accuracy: 0.9513\n",
      "Epoch 90/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00090: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2942 - val_accuracy: 0.9498\n",
      "Epoch 91/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00091: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2926 - val_accuracy: 0.9495\n",
      "Epoch 92/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 9.4045e-04 - accuracy: 0.9999\n",
      "Epoch 00092: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.3956e-04 - accuracy: 0.9999 - val_loss: 0.2970 - val_accuracy: 0.9495\n",
      "Epoch 93/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 9.2900e-04 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.2359e-04 - accuracy: 0.9998 - val_loss: 0.3104 - val_accuracy: 0.9476\n",
      "Epoch 94/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.3018 - val_accuracy: 0.9502\n",
      "Epoch 95/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 00095: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.2945 - val_accuracy: 0.9505\n",
      "Epoch 96/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3071 - val_accuracy: 0.9465\n",
      "Epoch 97/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9998\n",
      "Epoch 00097: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.3087 - val_accuracy: 0.9480\n",
      "Epoch 98/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 8.4612e-04 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.3823e-04 - accuracy: 1.0000 - val_loss: 0.3024 - val_accuracy: 0.9487\n",
      "Epoch 99/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 8.9195e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.8792e-04 - accuracy: 0.9998 - val_loss: 0.2985 - val_accuracy: 0.9484\n",
      "Epoch 100/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00100: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3008 - val_accuracy: 0.9487\n",
      "Epoch 101/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/774 [============================>.] - ETA: 0s - loss: 8.6876e-04 - accuracy: 0.9998\n",
      "Epoch 00101: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 8.5476e-04 - accuracy: 0.9998 - val_loss: 0.2961 - val_accuracy: 0.9498\n",
      "Epoch 102/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 8.3309e-04 - accuracy: 0.9998\n",
      "Epoch 00102: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.2661e-04 - accuracy: 0.9998 - val_loss: 0.2985 - val_accuracy: 0.9487\n",
      "Epoch 103/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 6.4217e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 6.3415e-04 - accuracy: 1.0000 - val_loss: 0.2971 - val_accuracy: 0.9509\n",
      "Epoch 104/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 8.0594e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 8.1530e-04 - accuracy: 0.9999 - val_loss: 0.3010 - val_accuracy: 0.9513\n",
      "Epoch 105/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 7.1510e-04 - accuracy: 0.9999\n",
      "Epoch 00105: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 7.2042e-04 - accuracy: 0.9999 - val_loss: 0.2972 - val_accuracy: 0.9516\n",
      "Epoch 106/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 9.4884e-04 - accuracy: 0.9998\n",
      "Epoch 00106: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 9.4229e-04 - accuracy: 0.9998 - val_loss: 0.3002 - val_accuracy: 0.9495\n",
      "Epoch 107/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 9.5456e-04 - accuracy: 0.9998\n",
      "Epoch 00107: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 9.4721e-04 - accuracy: 0.9998 - val_loss: 0.2976 - val_accuracy: 0.9516\n",
      "Epoch 108/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 7.4247e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 7.3437e-04 - accuracy: 0.9999 - val_loss: 0.2989 - val_accuracy: 0.9513\n",
      "Epoch 109/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 7.4481e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 7.4892e-04 - accuracy: 0.9999 - val_loss: 0.3037 - val_accuracy: 0.9509\n",
      "Epoch 110/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 6.8301e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 6.7830e-04 - accuracy: 0.9999 - val_loss: 0.3053 - val_accuracy: 0.9513\n",
      "Epoch 111/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 6.7810e-04 - accuracy: 0.9999\n",
      "Epoch 00111: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.6412e-04 - accuracy: 0.9999 - val_loss: 0.3032 - val_accuracy: 0.9502\n",
      "Epoch 112/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 8.9072e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.8035e-04 - accuracy: 0.9998 - val_loss: 0.3015 - val_accuracy: 0.9509\n",
      "Epoch 113/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 6.9413e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.9209e-04 - accuracy: 0.9999 - val_loss: 0.3062 - val_accuracy: 0.9498\n",
      "Epoch 114/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 4.4941e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.4871e-04 - accuracy: 1.0000 - val_loss: 0.3027 - val_accuracy: 0.9491\n",
      "Epoch 115/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 7.0670e-04 - accuracy: 0.9999\n",
      "Epoch 00115: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 11s 15ms/step - loss: 7.0554e-04 - accuracy: 0.9999 - val_loss: 0.3053 - val_accuracy: 0.9509\n",
      "Epoch 116/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 8.0874e-04 - accuracy: 0.9998\n",
      "Epoch 00116: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.0104e-04 - accuracy: 0.9998 - val_loss: 0.3125 - val_accuracy: 0.9509\n",
      "Epoch 117/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 5.5600e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5485e-04 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.9498\n",
      "Epoch 118/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 5.8558e-04 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.8643e-04 - accuracy: 1.0000 - val_loss: 0.3043 - val_accuracy: 0.9502\n",
      "Epoch 119/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 8.1697e-04 - accuracy: 0.9998\n",
      "Epoch 00119: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.1327e-04 - accuracy: 0.9998 - val_loss: 0.3005 - val_accuracy: 0.9513\n",
      "Epoch 120/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 6.0401e-04 - accuracy: 0.9998\n",
      "Epoch 00120: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.0548e-04 - accuracy: 0.9998 - val_loss: 0.3009 - val_accuracy: 0.9516\n",
      "Epoch 121/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 9.5034e-04 - accuracy: 0.9997\n",
      "Epoch 00121: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 0.3060 - val_accuracy: 0.9484\n",
      "Epoch 122/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 7.3797e-04 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.3806e-04 - accuracy: 0.9999 - val_loss: 0.3033 - val_accuracy: 0.9476\n",
      "Epoch 123/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 9.1324e-04 - accuracy: 0.9998\n",
      "Epoch 00123: val_accuracy improved from 0.95164 to 0.95200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.0775e-04 - accuracy: 0.9998 - val_loss: 0.3038 - val_accuracy: 0.9520\n",
      "Epoch 124/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 6.7440e-04 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy improved from 0.95200 to 0.95236, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8017e-04 - accuracy: 1.0000 - val_loss: 0.3071 - val_accuracy: 0.9524\n",
      "Epoch 125/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 6.5047e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.4779e-04 - accuracy: 0.9999 - val_loss: 0.3117 - val_accuracy: 0.9498\n",
      "Epoch 126/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 6.2325e-04 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.2291e-04 - accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 0.9524\n",
      "Epoch 127/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 7.8766e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.7550e-04 - accuracy: 0.9999 - val_loss: 0.3144 - val_accuracy: 0.9509\n",
      "Epoch 128/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745/774 [===========================>..] - ETA: 0s - loss: 6.1379e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.5040e-04 - accuracy: 0.9999 - val_loss: 0.3159 - val_accuracy: 0.9487\n",
      "Epoch 129/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 8.1760e-04 - accuracy: 0.9998\n",
      "Epoch 00129: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.0915e-04 - accuracy: 0.9998 - val_loss: 0.3170 - val_accuracy: 0.9495\n",
      "Epoch 130/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 4.2138e-04 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.2613e-04 - accuracy: 1.0000 - val_loss: 0.3179 - val_accuracy: 0.9487\n",
      "Epoch 131/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 5.5918e-04 - accuracy: 0.9999\n",
      "Epoch 00131: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5918e-04 - accuracy: 0.9999 - val_loss: 0.3215 - val_accuracy: 0.9502\n",
      "Epoch 132/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 5.7124e-04 - accuracy: 1.0000\n",
      "Epoch 00132: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.6541e-04 - accuracy: 1.0000 - val_loss: 0.3166 - val_accuracy: 0.9513\n",
      "Epoch 133/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999    \n",
      "Epoch 00133: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.3249 - val_accuracy: 0.9491\n",
      "Epoch 134/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 6.1199e-04 - accuracy: 0.9999\n",
      "Epoch 00134: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 6.2723e-04 - accuracy: 0.9999 - val_loss: 0.3209 - val_accuracy: 0.9516\n",
      "Epoch 135/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 5.7706e-04 - accuracy: 0.9999\n",
      "Epoch 00135: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.7683e-04 - accuracy: 0.9999 - val_loss: 0.3205 - val_accuracy: 0.9505\n",
      "Epoch 136/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 5.8151e-04 - accuracy: 0.9999\n",
      "Epoch 00136: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.7075e-04 - accuracy: 0.9999 - val_loss: 0.3246 - val_accuracy: 0.9520\n",
      "Epoch 137/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 4.2551e-04 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.4616e-04 - accuracy: 1.0000 - val_loss: 0.3203 - val_accuracy: 0.9516\n",
      "Epoch 138/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 5.5412e-04 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5346e-04 - accuracy: 1.0000 - val_loss: 0.3191 - val_accuracy: 0.9516\n",
      "Epoch 139/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 6.5319e-04 - accuracy: 0.9998\n",
      "Epoch 00139: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.5000e-04 - accuracy: 0.9998 - val_loss: 0.3143 - val_accuracy: 0.9505\n",
      "Epoch 140/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 7.0714e-04 - accuracy: 0.9998\n",
      "Epoch 00140: val_accuracy did not improve from 0.95236\n",
      "Restoring model weights from the end of the best epoch.\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.9907e-04 - accuracy: 0.9998 - val_loss: 0.3253 - val_accuracy: 0.9491\n",
      "Epoch 00140: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:59<03:59, 239.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.83      0.80      0.81       756\n",
      "        car_horn       0.83      0.92      0.87       252\n",
      "children_playing       0.77      0.78      0.77       700\n",
      "        dog_bark       0.77      0.84      0.80       700\n",
      "           siren       0.85      0.74      0.79       602\n",
      "\n",
      "        accuracy                           0.80      3010\n",
      "       macro avg       0.81      0.82      0.81      3010\n",
      "    weighted avg       0.81      0.80      0.80      3010\n",
      "\n",
      "Model: \"Model_CNN_1D_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 225, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 225, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 225, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 112, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 112, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                313650    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 324,691\n",
      "Trainable params: 324,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24746, 231, 1)\n",
      "Epoch 1/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.7394 - accuracy: 0.7625\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83091, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.7394 - accuracy: 0.7625 - val_loss: 0.5348 - val_accuracy: 0.8309\n",
      "Epoch 2/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.4969 - accuracy: 0.8501\n",
      "Epoch 00002: val_accuracy improved from 0.83091 to 0.83564, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.4970 - accuracy: 0.8501 - val_loss: 0.5185 - val_accuracy: 0.8356\n",
      "Epoch 3/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.4255 - accuracy: 0.8737\n",
      "Epoch 00003: val_accuracy improved from 0.83564 to 0.86582, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.4254 - accuracy: 0.8737 - val_loss: 0.4282 - val_accuracy: 0.8658\n",
      "Epoch 4/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8902\n",
      "Epoch 00004: val_accuracy improved from 0.86582 to 0.86836, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3766 - accuracy: 0.8902 - val_loss: 0.4096 - val_accuracy: 0.8684\n",
      "Epoch 5/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.3493 - accuracy: 0.8995\n",
      "Epoch 00005: val_accuracy improved from 0.86836 to 0.87600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3487 - accuracy: 0.8997 - val_loss: 0.3877 - val_accuracy: 0.8760\n",
      "Epoch 6/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.9102\n",
      "Epoch 00006: val_accuracy improved from 0.87600 to 0.88691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3171 - accuracy: 0.9105 - val_loss: 0.3628 - val_accuracy: 0.8869\n",
      "Epoch 7/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.9159\n",
      "Epoch 00007: val_accuracy did not improve from 0.88691\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2987 - accuracy: 0.9159 - val_loss: 0.3691 - val_accuracy: 0.8793\n",
      "Epoch 8/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.2786 - accuracy: 0.9220\n",
      "Epoch 00008: val_accuracy improved from 0.88691 to 0.88909, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2787 - accuracy: 0.9220 - val_loss: 0.3518 - val_accuracy: 0.8891\n",
      "Epoch 9/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.2606 - accuracy: 0.9290\n",
      "Epoch 00009: val_accuracy improved from 0.88909 to 0.90291, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2610 - accuracy: 0.9286 - val_loss: 0.3409 - val_accuracy: 0.9029\n",
      "Epoch 10/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.9324\n",
      "Epoch 00010: val_accuracy did not improve from 0.90291\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2489 - accuracy: 0.9323 - val_loss: 0.3669 - val_accuracy: 0.8873\n",
      "Epoch 11/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.2411 - accuracy: 0.9347\n",
      "Epoch 00011: val_accuracy did not improve from 0.90291\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2411 - accuracy: 0.9347 - val_loss: 0.3413 - val_accuracy: 0.9018\n",
      "Epoch 12/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9423\n",
      "Epoch 00012: val_accuracy did not improve from 0.90291\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2266 - accuracy: 0.9420 - val_loss: 0.3418 - val_accuracy: 0.8996\n",
      "Epoch 13/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.2194 - accuracy: 0.9430\n",
      "Epoch 00013: val_accuracy improved from 0.90291 to 0.90327, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2193 - accuracy: 0.9431 - val_loss: 0.3251 - val_accuracy: 0.9033\n",
      "Epoch 14/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.9448\n",
      "Epoch 00014: val_accuracy improved from 0.90327 to 0.90582, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2120 - accuracy: 0.9447 - val_loss: 0.3313 - val_accuracy: 0.9058\n",
      "Epoch 15/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.9487\n",
      "Epoch 00015: val_accuracy did not improve from 0.90582\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2018 - accuracy: 0.9487 - val_loss: 0.3522 - val_accuracy: 0.8964\n",
      "Epoch 16/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9501\n",
      "Epoch 00016: val_accuracy did not improve from 0.90582\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1970 - accuracy: 0.9500 - val_loss: 0.3376 - val_accuracy: 0.9051\n",
      "Epoch 17/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1929 - accuracy: 0.9527\n",
      "Epoch 00017: val_accuracy did not improve from 0.90582\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1932 - accuracy: 0.9526 - val_loss: 0.3473 - val_accuracy: 0.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9525\n",
      "Epoch 00018: val_accuracy improved from 0.90582 to 0.91091, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1847 - accuracy: 0.9525 - val_loss: 0.3313 - val_accuracy: 0.9109\n",
      "Epoch 19/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1769 - accuracy: 0.9559\n",
      "Epoch 00019: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1770 - accuracy: 0.9558 - val_loss: 0.3201 - val_accuracy: 0.9102\n",
      "Epoch 20/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9572\n",
      "Epoch 00020: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1719 - accuracy: 0.9572 - val_loss: 0.3350 - val_accuracy: 0.9084\n",
      "Epoch 21/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9589\n",
      "Epoch 00021: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1687 - accuracy: 0.9587 - val_loss: 0.3424 - val_accuracy: 0.9080\n",
      "Epoch 22/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9603\n",
      "Epoch 00022: val_accuracy improved from 0.91091 to 0.91200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1680 - accuracy: 0.9602 - val_loss: 0.3346 - val_accuracy: 0.9120\n",
      "Epoch 23/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9610\n",
      "Epoch 00023: val_accuracy improved from 0.91200 to 0.91345, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1622 - accuracy: 0.9606 - val_loss: 0.3264 - val_accuracy: 0.9135\n",
      "Epoch 24/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9620\n",
      "Epoch 00024: val_accuracy did not improve from 0.91345\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1598 - accuracy: 0.9619 - val_loss: 0.3273 - val_accuracy: 0.9120\n",
      "Epoch 25/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1595 - accuracy: 0.9617\n",
      "Epoch 00025: val_accuracy did not improve from 0.91345\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1596 - accuracy: 0.9617 - val_loss: 0.3341 - val_accuracy: 0.9109\n",
      "Epoch 26/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1498 - accuracy: 0.9648\n",
      "Epoch 00026: val_accuracy did not improve from 0.91345\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1496 - accuracy: 0.9650 - val_loss: 0.3286 - val_accuracy: 0.9135\n",
      "Epoch 27/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1502 - accuracy: 0.9647\n",
      "Epoch 00027: val_accuracy did not improve from 0.91345\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1505 - accuracy: 0.9647 - val_loss: 0.3425 - val_accuracy: 0.9084\n",
      "Epoch 28/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.9683\n",
      "Epoch 00028: val_accuracy improved from 0.91345 to 0.91600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1436 - accuracy: 0.9684 - val_loss: 0.3217 - val_accuracy: 0.9160\n",
      "Epoch 29/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9680\n",
      "Epoch 00029: val_accuracy did not improve from 0.91600\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1416 - accuracy: 0.9680 - val_loss: 0.3361 - val_accuracy: 0.9160\n",
      "Epoch 30/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9685\n",
      "Epoch 00030: val_accuracy did not improve from 0.91600\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1392 - accuracy: 0.9683 - val_loss: 0.3316 - val_accuracy: 0.9149\n",
      "Epoch 31/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.9686\n",
      "Epoch 00031: val_accuracy improved from 0.91600 to 0.91636, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1388 - accuracy: 0.9685 - val_loss: 0.3324 - val_accuracy: 0.9164\n",
      "Epoch 32/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.9696\n",
      "Epoch 00032: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1372 - accuracy: 0.9696 - val_loss: 0.3375 - val_accuracy: 0.9145\n",
      "Epoch 33/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1330 - accuracy: 0.9710\n",
      "Epoch 00033: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1325 - accuracy: 0.9711 - val_loss: 0.3232 - val_accuracy: 0.9105\n",
      "Epoch 34/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9697\n",
      "Epoch 00034: val_accuracy improved from 0.91636 to 0.91673, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1311 - accuracy: 0.9698 - val_loss: 0.3330 - val_accuracy: 0.9167\n",
      "Epoch 35/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9721\n",
      "Epoch 00035: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9723 - val_loss: 0.3529 - val_accuracy: 0.9109\n",
      "Epoch 36/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9718\n",
      "Epoch 00036: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1291 - accuracy: 0.9720 - val_loss: 0.3439 - val_accuracy: 0.9105\n",
      "Epoch 37/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9717\n",
      "Epoch 00037: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1252 - accuracy: 0.9717 - val_loss: 0.3425 - val_accuracy: 0.9131\n",
      "Epoch 38/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9724\n",
      "Epoch 00038: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1230 - accuracy: 0.9724 - val_loss: 0.3343 - val_accuracy: 0.9156\n",
      "Epoch 39/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9750\n",
      "Epoch 00039: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1193 - accuracy: 0.9749 - val_loss: 0.3584 - val_accuracy: 0.9116\n",
      "Epoch 40/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9735\n",
      "Epoch 00040: val_accuracy improved from 0.91673 to 0.91745, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1234 - accuracy: 0.9735 - val_loss: 0.3386 - val_accuracy: 0.9175\n",
      "Epoch 41/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9750\n",
      "Epoch 00041: val_accuracy did not improve from 0.91745\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1180 - accuracy: 0.9749 - val_loss: 0.3355 - val_accuracy: 0.9149\n",
      "Epoch 42/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9753\n",
      "Epoch 00042: val_accuracy did not improve from 0.91745\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1170 - accuracy: 0.9753 - val_loss: 0.3443 - val_accuracy: 0.9145\n",
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/774 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9746\n",
      "Epoch 00043: val_accuracy did not improve from 0.91745\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1181 - accuracy: 0.9747 - val_loss: 0.3442 - val_accuracy: 0.9084\n",
      "Epoch 44/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9747\n",
      "Epoch 00044: val_accuracy did not improve from 0.91745\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1178 - accuracy: 0.9747 - val_loss: 0.3407 - val_accuracy: 0.9105\n",
      "Epoch 45/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9745\n",
      "Epoch 00045: val_accuracy improved from 0.91745 to 0.91782, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1169 - accuracy: 0.9746 - val_loss: 0.3513 - val_accuracy: 0.9178\n",
      "Epoch 46/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9751\n",
      "Epoch 00046: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 6s 7ms/step - loss: 0.1124 - accuracy: 0.9751 - val_loss: 0.3440 - val_accuracy: 0.9178\n",
      "Epoch 47/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9762\n",
      "Epoch 00047: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 8s 11ms/step - loss: 0.1134 - accuracy: 0.9763 - val_loss: 0.3579 - val_accuracy: 0.9156\n",
      "Epoch 48/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9769\n",
      "Epoch 00048: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.1104 - accuracy: 0.9767 - val_loss: 0.3472 - val_accuracy: 0.9175\n",
      "Epoch 49/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1079 - accuracy: 0.9787\n",
      "Epoch 00049: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1078 - accuracy: 0.9787 - val_loss: 0.3515 - val_accuracy: 0.9167\n",
      "Epoch 50/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9768\n",
      "Epoch 00050: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1111 - accuracy: 0.9768 - val_loss: 0.3422 - val_accuracy: 0.9156\n",
      "Epoch 51/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9787\n",
      "Epoch 00051: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1057 - accuracy: 0.9787 - val_loss: 0.3431 - val_accuracy: 0.9138\n",
      "Epoch 52/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9786\n",
      "Epoch 00052: val_accuracy improved from 0.91782 to 0.91927, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1047 - accuracy: 0.9786 - val_loss: 0.3472 - val_accuracy: 0.9193\n",
      "Epoch 53/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9790\n",
      "Epoch 00053: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1020 - accuracy: 0.9787 - val_loss: 0.3446 - val_accuracy: 0.9178\n",
      "Epoch 54/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9780\n",
      "Epoch 00054: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1037 - accuracy: 0.9778 - val_loss: 0.3485 - val_accuracy: 0.9178\n",
      "Epoch 55/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9773\n",
      "Epoch 00055: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1056 - accuracy: 0.9775 - val_loss: 0.3460 - val_accuracy: 0.9160\n",
      "Epoch 56/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9795\n",
      "Epoch 00056: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1012 - accuracy: 0.9795 - val_loss: 0.3582 - val_accuracy: 0.9124\n",
      "Epoch 57/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0997 - accuracy: 0.9799\n",
      "Epoch 00057: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0997 - accuracy: 0.9799 - val_loss: 0.3427 - val_accuracy: 0.9135\n",
      "Epoch 58/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9792\n",
      "Epoch 00058: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0992 - accuracy: 0.9791 - val_loss: 0.3616 - val_accuracy: 0.9131\n",
      "Epoch 59/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9792\n",
      "Epoch 00059: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0992 - accuracy: 0.9793 - val_loss: 0.3547 - val_accuracy: 0.9160\n",
      "Epoch 60/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9802\n",
      "Epoch 00060: val_accuracy improved from 0.91927 to 0.91964, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0988 - accuracy: 0.9801 - val_loss: 0.3489 - val_accuracy: 0.9196\n",
      "Epoch 61/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0961 - accuracy: 0.9803\n",
      "Epoch 00061: val_accuracy did not improve from 0.91964\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0966 - accuracy: 0.9802 - val_loss: 0.3544 - val_accuracy: 0.9164\n",
      "Epoch 62/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0979 - accuracy: 0.9798\n",
      "Epoch 00062: val_accuracy did not improve from 0.91964\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0980 - accuracy: 0.9798 - val_loss: 0.3554 - val_accuracy: 0.9196\n",
      "Epoch 63/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 0.9816\n",
      "Epoch 00063: val_accuracy improved from 0.91964 to 0.92145, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9812 - val_loss: 0.3467 - val_accuracy: 0.9215\n",
      "Epoch 64/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0937 - accuracy: 0.9812\n",
      "Epoch 00064: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0935 - accuracy: 0.9812 - val_loss: 0.3463 - val_accuracy: 0.9207\n",
      "Epoch 65/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9805\n",
      "Epoch 00065: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0958 - accuracy: 0.9804 - val_loss: 0.3505 - val_accuracy: 0.9204\n",
      "Epoch 66/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0931 - accuracy: 0.9815\n",
      "Epoch 00066: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0929 - accuracy: 0.9816 - val_loss: 0.3518 - val_accuracy: 0.9193\n",
      "Epoch 67/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0946 - accuracy: 0.9802\n",
      "Epoch 00067: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0946 - accuracy: 0.9802 - val_loss: 0.3538 - val_accuracy: 0.9207\n",
      "Epoch 68/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9824\n",
      "Epoch 00068: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0901 - accuracy: 0.9823 - val_loss: 0.3350 - val_accuracy: 0.9185\n",
      "Epoch 69/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0888 - accuracy: 0.9831\n",
      "Epoch 00069: val_accuracy improved from 0.92145 to 0.92255, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0888 - accuracy: 0.9831 - val_loss: 0.3468 - val_accuracy: 0.9225\n",
      "Epoch 70/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9831\n",
      "Epoch 00070: val_accuracy did not improve from 0.92255\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0869 - accuracy: 0.9831 - val_loss: 0.3440 - val_accuracy: 0.9211\n",
      "Epoch 71/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9831\n",
      "Epoch 00071: val_accuracy improved from 0.92255 to 0.92400, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0868 - accuracy: 0.9832 - val_loss: 0.3645 - val_accuracy: 0.9240\n",
      "Epoch 72/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9810\n",
      "Epoch 00072: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9807 - val_loss: 0.3583 - val_accuracy: 0.9211\n",
      "Epoch 73/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9839\n",
      "Epoch 00073: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0852 - accuracy: 0.9838 - val_loss: 0.3682 - val_accuracy: 0.9156\n",
      "Epoch 74/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0854 - accuracy: 0.9845\n",
      "Epoch 00074: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0853 - accuracy: 0.9846 - val_loss: 0.3537 - val_accuracy: 0.9233\n",
      "Epoch 75/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9824\n",
      "Epoch 00075: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0879 - accuracy: 0.9823 - val_loss: 0.3570 - val_accuracy: 0.9185\n",
      "Epoch 76/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0844 - accuracy: 0.9838\n",
      "Epoch 00076: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0843 - accuracy: 0.9838 - val_loss: 0.3437 - val_accuracy: 0.9207\n",
      "Epoch 77/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0876 - accuracy: 0.9830\n",
      "Epoch 00077: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0880 - accuracy: 0.9828 - val_loss: 0.3658 - val_accuracy: 0.9196\n",
      "Epoch 78/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9844\n",
      "Epoch 00078: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 6s 7ms/step - loss: 0.0828 - accuracy: 0.9843 - val_loss: 0.3573 - val_accuracy: 0.9175\n",
      "Epoch 79/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9829\n",
      "Epoch 00079: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0864 - accuracy: 0.9828 - val_loss: 0.3656 - val_accuracy: 0.9189\n",
      "Epoch 80/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0825 - accuracy: 0.9838\n",
      "Epoch 00080: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 7s 9ms/step - loss: 0.0825 - accuracy: 0.9838 - val_loss: 0.3651 - val_accuracy: 0.9207\n",
      "Epoch 81/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9847\n",
      "Epoch 00081: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0831 - accuracy: 0.9847 - val_loss: 0.3414 - val_accuracy: 0.9218\n",
      "Epoch 82/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0821 - accuracy: 0.9841\n",
      "Epoch 00082: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0821 - accuracy: 0.9841 - val_loss: 0.3286 - val_accuracy: 0.9225\n",
      "Epoch 83/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9846\n",
      "Epoch 00083: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0825 - accuracy: 0.9846 - val_loss: 0.3626 - val_accuracy: 0.9175\n",
      "Epoch 84/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9846\n",
      "Epoch 00084: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0810 - accuracy: 0.9845 - val_loss: 0.3496 - val_accuracy: 0.9233\n",
      "Epoch 85/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9839\n",
      "Epoch 00085: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0820 - accuracy: 0.9840 - val_loss: 0.3601 - val_accuracy: 0.9189\n",
      "Epoch 86/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9848\n",
      "Epoch 00086: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0802 - accuracy: 0.9848 - val_loss: 0.3530 - val_accuracy: 0.9215\n",
      "Epoch 87/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9832\n",
      "Epoch 00087: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0836 - accuracy: 0.9832 - val_loss: 0.3536 - val_accuracy: 0.9175\n",
      "Epoch 88/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9847\n",
      "Epoch 00088: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0794 - accuracy: 0.9848 - val_loss: 0.3453 - val_accuracy: 0.9215\n",
      "Epoch 89/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9858\n",
      "Epoch 00089: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0766 - accuracy: 0.9858 - val_loss: 0.3676 - val_accuracy: 0.9182\n",
      "Epoch 90/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.9852\n",
      "Epoch 00090: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0791 - accuracy: 0.9853 - val_loss: 0.3517 - val_accuracy: 0.9185\n",
      "Epoch 91/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9852\n",
      "Epoch 00091: val_accuracy improved from 0.92400 to 0.92436, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0770 - accuracy: 0.9851 - val_loss: 0.3599 - val_accuracy: 0.9244\n",
      "Epoch 92/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9849\n",
      "Epoch 00092: val_accuracy did not improve from 0.92436\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0801 - accuracy: 0.9849 - val_loss: 0.3416 - val_accuracy: 0.9185\n",
      "Epoch 93/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9851\n",
      "Epoch 00093: val_accuracy did not improve from 0.92436\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0788 - accuracy: 0.9850 - val_loss: 0.3494 - val_accuracy: 0.9244\n",
      "Epoch 94/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9859\n",
      "Epoch 00094: val_accuracy did not improve from 0.92436\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0736 - accuracy: 0.9859 - val_loss: 0.3538 - val_accuracy: 0.9207\n",
      "Epoch 95/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9860\n",
      "Epoch 00095: val_accuracy improved from 0.92436 to 0.92509, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0763 - accuracy: 0.9861 - val_loss: 0.3482 - val_accuracy: 0.9251\n",
      "Epoch 96/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9858\n",
      "Epoch 00096: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0737 - accuracy: 0.9859 - val_loss: 0.3501 - val_accuracy: 0.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.9840\n",
      "Epoch 00097: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9840 - val_loss: 0.3587 - val_accuracy: 0.9149\n",
      "Epoch 98/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9863\n",
      "Epoch 00098: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0739 - accuracy: 0.9863 - val_loss: 0.3408 - val_accuracy: 0.9251\n",
      "Epoch 99/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9872\n",
      "Epoch 00099: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0736 - accuracy: 0.9871 - val_loss: 0.3589 - val_accuracy: 0.9171\n",
      "Epoch 100/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9863\n",
      "Epoch 00100: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0713 - accuracy: 0.9863 - val_loss: 0.3614 - val_accuracy: 0.9225\n",
      "Epoch 101/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9852\n",
      "Epoch 00101: val_accuracy improved from 0.92509 to 0.92618, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 6s 8ms/step - loss: 0.0784 - accuracy: 0.9851 - val_loss: 0.3517 - val_accuracy: 0.9262\n",
      "Epoch 102/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9870\n",
      "Epoch 00102: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 9s 12ms/step - loss: 0.0701 - accuracy: 0.9870 - val_loss: 0.3604 - val_accuracy: 0.9218\n",
      "Epoch 103/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9861\n",
      "Epoch 00103: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9860 - val_loss: 0.3496 - val_accuracy: 0.9233\n",
      "Epoch 104/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9865\n",
      "Epoch 00104: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0740 - accuracy: 0.9865 - val_loss: 0.3582 - val_accuracy: 0.9207\n",
      "Epoch 105/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9874\n",
      "Epoch 00105: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0716 - accuracy: 0.9874 - val_loss: 0.3577 - val_accuracy: 0.9200\n",
      "Epoch 106/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9849\n",
      "Epoch 00106: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0738 - accuracy: 0.9849 - val_loss: 0.3694 - val_accuracy: 0.9185\n",
      "Epoch 107/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9865\n",
      "Epoch 00107: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 13s 17ms/step - loss: 0.0717 - accuracy: 0.9865 - val_loss: 0.3493 - val_accuracy: 0.9233\n",
      "Epoch 108/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9859\n",
      "Epoch 00108: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0731 - accuracy: 0.9859 - val_loss: 0.3469 - val_accuracy: 0.9247\n",
      "Epoch 109/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9861\n",
      "Epoch 00109: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0733 - accuracy: 0.9860 - val_loss: 0.3570 - val_accuracy: 0.9258\n",
      "Epoch 110/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9880\n",
      "Epoch 00110: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0693 - accuracy: 0.9880 - val_loss: 0.3532 - val_accuracy: 0.9229\n",
      "Epoch 111/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9884\n",
      "Epoch 00111: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0682 - accuracy: 0.9882 - val_loss: 0.3466 - val_accuracy: 0.9247\n",
      "Epoch 112/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9865\n",
      "Epoch 00112: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9864 - val_loss: 0.3590 - val_accuracy: 0.9200\n",
      "Epoch 113/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9872\n",
      "Epoch 00113: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0711 - accuracy: 0.9871 - val_loss: 0.3443 - val_accuracy: 0.9262\n",
      "Epoch 114/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9881\n",
      "Epoch 00114: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9882 - val_loss: 0.3312 - val_accuracy: 0.9262\n",
      "Epoch 115/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9875\n",
      "Epoch 00115: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9875 - val_loss: 0.3432 - val_accuracy: 0.9211\n",
      "Epoch 116/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9878\n",
      "Epoch 00116: val_accuracy improved from 0.92618 to 0.92655, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0673 - accuracy: 0.9877 - val_loss: 0.3459 - val_accuracy: 0.9265\n",
      "Epoch 117/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9873\n",
      "Epoch 00117: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0703 - accuracy: 0.9873 - val_loss: 0.3353 - val_accuracy: 0.9247\n",
      "Epoch 118/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9883\n",
      "Epoch 00118: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9882 - val_loss: 0.3492 - val_accuracy: 0.9233\n",
      "Epoch 119/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9876\n",
      "Epoch 00119: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9877 - val_loss: 0.3495 - val_accuracy: 0.9204\n",
      "Epoch 120/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9874\n",
      "Epoch 00120: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0677 - accuracy: 0.9875 - val_loss: 0.3465 - val_accuracy: 0.9229\n",
      "Epoch 121/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9879\n",
      "Epoch 00121: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0669 - accuracy: 0.9879 - val_loss: 0.3536 - val_accuracy: 0.9233\n",
      "Epoch 122/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9891\n",
      "Epoch 00122: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9891 - val_loss: 0.3572 - val_accuracy: 0.9222\n",
      "Epoch 123/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9873\n",
      "Epoch 00123: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0679 - accuracy: 0.9873 - val_loss: 0.3564 - val_accuracy: 0.9233\n",
      "Epoch 124/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9890\n",
      "Epoch 00124: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9890 - val_loss: 0.3484 - val_accuracy: 0.9215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9892\n",
      "Epoch 00125: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0640 - accuracy: 0.9892 - val_loss: 0.3457 - val_accuracy: 0.9255\n",
      "Epoch 126/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9869\n",
      "Epoch 00126: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0668 - accuracy: 0.9869 - val_loss: 0.3456 - val_accuracy: 0.9258\n",
      "Epoch 127/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9868\n",
      "Epoch 00127: val_accuracy improved from 0.92655 to 0.92800, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9869 - val_loss: 0.3429 - val_accuracy: 0.9280\n",
      "Epoch 128/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9876\n",
      "Epoch 00128: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0656 - accuracy: 0.9876 - val_loss: 0.3449 - val_accuracy: 0.9273\n",
      "Epoch 129/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9879\n",
      "Epoch 00129: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9878 - val_loss: 0.3700 - val_accuracy: 0.9211\n",
      "Epoch 130/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9887\n",
      "Epoch 00130: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0638 - accuracy: 0.9886 - val_loss: 0.3486 - val_accuracy: 0.9247\n",
      "Epoch 131/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0632 - accuracy: 0.9892\n",
      "Epoch 00131: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9891 - val_loss: 0.3654 - val_accuracy: 0.9218\n",
      "Epoch 132/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9901\n",
      "Epoch 00132: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0614 - accuracy: 0.9900 - val_loss: 0.3605 - val_accuracy: 0.9236\n",
      "Epoch 133/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9880\n",
      "Epoch 00133: val_accuracy improved from 0.92800 to 0.92873, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0654 - accuracy: 0.9879 - val_loss: 0.3542 - val_accuracy: 0.9287\n",
      "Epoch 134/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9895\n",
      "Epoch 00134: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0611 - accuracy: 0.9893 - val_loss: 0.3677 - val_accuracy: 0.9182\n",
      "Epoch 135/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9880\n",
      "Epoch 00135: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9879 - val_loss: 0.3638 - val_accuracy: 0.9225\n",
      "Epoch 136/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9894\n",
      "Epoch 00136: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0625 - accuracy: 0.9893 - val_loss: 0.3603 - val_accuracy: 0.9247\n",
      "Epoch 137/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9887\n",
      "Epoch 00137: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9887 - val_loss: 0.3651 - val_accuracy: 0.9233\n",
      "Epoch 138/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9890\n",
      "Epoch 00138: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9890 - val_loss: 0.3554 - val_accuracy: 0.9229\n",
      "Epoch 139/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9888\n",
      "Epoch 00139: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0608 - accuracy: 0.9888 - val_loss: 0.3479 - val_accuracy: 0.9276\n",
      "Epoch 140/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9893\n",
      "Epoch 00140: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0603 - accuracy: 0.9893 - val_loss: 0.3549 - val_accuracy: 0.9240\n",
      "Epoch 141/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0591 - accuracy: 0.9897\n",
      "Epoch 00141: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0591 - accuracy: 0.9896 - val_loss: 0.3516 - val_accuracy: 0.9255\n",
      "Epoch 142/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9888\n",
      "Epoch 00142: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0640 - accuracy: 0.9887 - val_loss: 0.3403 - val_accuracy: 0.9273\n",
      "Epoch 143/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9886\n",
      "Epoch 00143: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9886 - val_loss: 0.3630 - val_accuracy: 0.9225\n",
      "Epoch 144/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9906\n",
      "Epoch 00144: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0562 - accuracy: 0.9906 - val_loss: 0.3615 - val_accuracy: 0.9251\n",
      "Epoch 145/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9893\n",
      "Epoch 00145: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0595 - accuracy: 0.9894 - val_loss: 0.3579 - val_accuracy: 0.9218\n",
      "Epoch 146/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9878\n",
      "Epoch 00146: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9879 - val_loss: 0.3662 - val_accuracy: 0.9211\n",
      "Epoch 147/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9887\n",
      "Epoch 00147: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0606 - accuracy: 0.9887 - val_loss: 0.3526 - val_accuracy: 0.9262\n",
      "Epoch 148/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9907\n",
      "Epoch 00148: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0568 - accuracy: 0.9906 - val_loss: 0.3553 - val_accuracy: 0.9244\n",
      "Epoch 149/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9896\n",
      "Epoch 00149: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0585 - accuracy: 0.9896 - val_loss: 0.3627 - val_accuracy: 0.9244\n",
      "Epoch 150/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9897\n",
      "Epoch 00150: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0588 - accuracy: 0.9898 - val_loss: 0.3664 - val_accuracy: 0.9265\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [10:45<00:00, 322.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.87      0.76      0.81       756\n",
      "        car_horn       0.81      0.90      0.85       252\n",
      "children_playing       0.73      0.80      0.76       700\n",
      "        dog_bark       0.74      0.83      0.78       700\n",
      "           siren       0.88      0.76      0.82       602\n",
      "\n",
      "        accuracy                           0.80      3010\n",
      "       macro avg       0.81      0.81      0.81      3010\n",
      "    weighted avg       0.81      0.80      0.80      3010\n",
      "\n",
      "Validation fold: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27573, 375)\n",
      "X_val_norm shape.....:(2933, 375)\n",
      "\n",
      "Sum of elements: 0.9800710149454922\n",
      "Number of elements summed: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 234)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               176250    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 289,985\n",
      "Trainable params: 289,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24815, 234)\n",
      "Epoch 1/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.7900 - accuracy: 0.7127\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83104, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 2ms/step - loss: 0.7846 - accuracy: 0.7150 - val_loss: 0.4820 - val_accuracy: 0.8310\n",
      "Epoch 2/350\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.4401 - accuracy: 0.8455\n",
      "Epoch 00002: val_accuracy improved from 0.83104 to 0.86403, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.4402 - accuracy: 0.8455 - val_loss: 0.3785 - val_accuracy: 0.8640\n",
      "Epoch 3/350\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.3426 - accuracy: 0.8779\n",
      "Epoch 00003: val_accuracy improved from 0.86403 to 0.88434, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.3427 - accuracy: 0.8777 - val_loss: 0.3245 - val_accuracy: 0.8843\n",
      "Epoch 4/350\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.8984\n",
      "Epoch 00004: val_accuracy improved from 0.88434 to 0.89521, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.2834 - accuracy: 0.8983 - val_loss: 0.2985 - val_accuracy: 0.8952\n",
      "Epoch 5/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.2363 - accuracy: 0.9164\n",
      "Epoch 00005: val_accuracy improved from 0.89521 to 0.90537, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.2365 - accuracy: 0.9164 - val_loss: 0.2640 - val_accuracy: 0.9054\n",
      "Epoch 6/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9331\n",
      "Epoch 00006: val_accuracy improved from 0.90537 to 0.91842, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.1969 - accuracy: 0.9333 - val_loss: 0.2382 - val_accuracy: 0.9184\n",
      "Epoch 7/350\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.1665 - accuracy: 0.9420\n",
      "Epoch 00007: val_accuracy improved from 0.91842 to 0.92059, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.1668 - accuracy: 0.9417 - val_loss: 0.2290 - val_accuracy: 0.9206\n",
      "Epoch 8/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9525\n",
      "Epoch 00008: val_accuracy improved from 0.92059 to 0.92748, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.1405 - accuracy: 0.9522 - val_loss: 0.2156 - val_accuracy: 0.9275\n",
      "Epoch 9/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9609\n",
      "Epoch 00009: val_accuracy improved from 0.92748 to 0.93038, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.1175 - accuracy: 0.9610 - val_loss: 0.2109 - val_accuracy: 0.9304\n",
      "Epoch 10/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9672\n",
      "Epoch 00010: val_accuracy improved from 0.93038 to 0.93510, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0996 - accuracy: 0.9674 - val_loss: 0.2002 - val_accuracy: 0.9351\n",
      "Epoch 11/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9729\n",
      "Epoch 00011: val_accuracy improved from 0.93510 to 0.93655, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0845 - accuracy: 0.9730 - val_loss: 0.1981 - val_accuracy: 0.9365\n",
      "Epoch 12/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9767\n",
      "Epoch 00012: val_accuracy improved from 0.93655 to 0.93945, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0724 - accuracy: 0.9767 - val_loss: 0.1992 - val_accuracy: 0.9394\n",
      "Epoch 13/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9799\n",
      "Epoch 00013: val_accuracy improved from 0.93945 to 0.94162, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0624 - accuracy: 0.9801 - val_loss: 0.1971 - val_accuracy: 0.9416\n",
      "Epoch 14/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9837\n",
      "Epoch 00014: val_accuracy improved from 0.94162 to 0.94271, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0522 - accuracy: 0.9834 - val_loss: 0.1909 - val_accuracy: 0.9427\n",
      "Epoch 15/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0475 - accuracy: 0.9853\n",
      "Epoch 00015: val_accuracy did not improve from 0.94271\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0475 - accuracy: 0.9854 - val_loss: 0.1947 - val_accuracy: 0.9420\n",
      "Epoch 16/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9889\n",
      "Epoch 00016: val_accuracy improved from 0.94271 to 0.94344, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0385 - accuracy: 0.9889 - val_loss: 0.1946 - val_accuracy: 0.9434\n",
      "Epoch 17/350\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.0333 - accuracy: 0.9907\n",
      "Epoch 00017: val_accuracy improved from 0.94344 to 0.94634, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9907 - val_loss: 0.1947 - val_accuracy: 0.9463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0284 - accuracy: 0.9924\n",
      "Epoch 00018: val_accuracy did not improve from 0.94634\n",
      "776/776 [==============================] - 2s 2ms/step - loss: 0.0284 - accuracy: 0.9923 - val_loss: 0.1951 - val_accuracy: 0.9453\n",
      "Epoch 19/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9926\n",
      "Epoch 00019: val_accuracy did not improve from 0.94634\n",
      "776/776 [==============================] - 2s 2ms/step - loss: 0.0255 - accuracy: 0.9925 - val_loss: 0.1976 - val_accuracy: 0.9456\n",
      "Epoch 20/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0213 - accuracy: 0.9943\n",
      "Epoch 00020: val_accuracy improved from 0.94634 to 0.94851, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9944 - val_loss: 0.1997 - val_accuracy: 0.9485\n",
      "Epoch 21/350\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9948\n",
      "Epoch 00021: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9948 - val_loss: 0.1997 - val_accuracy: 0.9482\n",
      "Epoch 22/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9948\n",
      "Epoch 00022: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9948 - val_loss: 0.2072 - val_accuracy: 0.9456\n",
      "Epoch 23/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9966\n",
      "Epoch 00023: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0141 - accuracy: 0.9966 - val_loss: 0.2020 - val_accuracy: 0.9453\n",
      "Epoch 24/350\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9966\n",
      "Epoch 00024: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9966 - val_loss: 0.2051 - val_accuracy: 0.9485\n",
      "Epoch 25/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9967\n",
      "Epoch 00025: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0136 - accuracy: 0.9967 - val_loss: 0.2076 - val_accuracy: 0.9474\n",
      "Epoch 26/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9970\n",
      "Epoch 00026: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.2137 - val_accuracy: 0.9467\n",
      "Epoch 27/350\n",
      "756/776 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9966\n",
      "Epoch 00027: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0121 - accuracy: 0.9967 - val_loss: 0.2140 - val_accuracy: 0.9460\n",
      "Epoch 28/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9974\n",
      "Epoch 00028: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0107 - accuracy: 0.9974 - val_loss: 0.2167 - val_accuracy: 0.9463\n",
      "Epoch 29/350\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9981\n",
      "Epoch 00029: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.2132 - val_accuracy: 0.9467\n",
      "Epoch 30/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9981\n",
      "Epoch 00030: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.2194 - val_accuracy: 0.9485\n",
      "Epoch 31/350\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9984\n",
      "Epoch 00031: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9984 - val_loss: 0.2159 - val_accuracy: 0.9478\n",
      "Epoch 32/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9983\n",
      "Epoch 00032: val_accuracy improved from 0.94851 to 0.94960, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9983 - val_loss: 0.2171 - val_accuracy: 0.9496\n",
      "Epoch 33/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9985\n",
      "Epoch 00033: val_accuracy did not improve from 0.94960\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.2215 - val_accuracy: 0.9485\n",
      "Epoch 34/350\n",
      "750/776 [===========================>..] - ETA: 0s - loss: 0.0061 - accuracy: 0.9986\n",
      "Epoch 00034: val_accuracy improved from 0.94960 to 0.94996, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.2245 - val_accuracy: 0.9500\n",
      "Epoch 35/350\n",
      "745/776 [===========================>..] - ETA: 0s - loss: 0.0054 - accuracy: 0.9990\n",
      "Epoch 00035: val_accuracy did not improve from 0.94996\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 0.2270 - val_accuracy: 0.9489\n",
      "Epoch 36/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9989\n",
      "Epoch 00036: val_accuracy improved from 0.94996 to 0.95141, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.2218 - val_accuracy: 0.9514\n",
      "Epoch 37/350\n",
      "751/776 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00037: val_accuracy improved from 0.95141 to 0.95286, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.2277 - val_accuracy: 0.9529\n",
      "Epoch 38/350\n",
      "745/776 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9993\n",
      "Epoch 00038: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.2296 - val_accuracy: 0.9503\n",
      "Epoch 39/350\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 00039: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.2388 - val_accuracy: 0.9492\n",
      "Epoch 40/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9989\n",
      "Epoch 00040: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.2420 - val_accuracy: 0.9482\n",
      "Epoch 41/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9990\n",
      "Epoch 00041: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.2352 - val_accuracy: 0.9500\n",
      "Epoch 42/350\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9992\n",
      "Epoch 00042: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.2416 - val_accuracy: 0.9492\n",
      "Epoch 43/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 00043: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.2364 - val_accuracy: 0.9489\n",
      "Epoch 44/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772/776 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9989\n",
      "Epoch 00044: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.2431 - val_accuracy: 0.9485\n",
      "Epoch 45/350\n",
      "747/776 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 00045: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.2360 - val_accuracy: 0.9514\n",
      "Epoch 46/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9996\n",
      "Epoch 00046: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2302 - val_accuracy: 0.9507\n",
      "Epoch 47/350\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 00047: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.2318 - val_accuracy: 0.9507\n",
      "Epoch 48/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 00048: val_accuracy improved from 0.95286 to 0.95323, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.2387 - val_accuracy: 0.9532\n",
      "Epoch 49/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 00049: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.2406 - val_accuracy: 0.9500\n",
      "Epoch 50/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9993\n",
      "Epoch 00050: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.2413 - val_accuracy: 0.9525\n",
      "Epoch 51/350\n",
      "750/776 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 0.9997\n",
      "Epoch 00051: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.2464 - val_accuracy: 0.9529\n",
      "Epoch 52/350\n",
      "749/776 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 0.9997\n",
      "Epoch 00052: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.2490 - val_accuracy: 0.9511\n",
      "Epoch 53/350\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 00053: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.2472 - val_accuracy: 0.9532\n",
      "Epoch 54/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 00054: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.2431 - val_accuracy: 0.9514\n",
      "Epoch 55/350\n",
      "751/776 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2439 - val_accuracy: 0.9507\n",
      "Epoch 56/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00056: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.2457 - val_accuracy: 0.9518\n",
      "Epoch 57/350\n",
      "748/776 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 00057: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2663 - val_accuracy: 0.9496\n",
      "Epoch 58/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 00058: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2536 - val_accuracy: 0.9511\n",
      "Epoch 59/350\n",
      "747/776 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 00059: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.2504 - val_accuracy: 0.9474\n",
      "Epoch 60/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 00060: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.2497 - val_accuracy: 0.9521\n",
      "Epoch 61/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy improved from 0.95323 to 0.95359, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2493 - val_accuracy: 0.9536\n",
      "Epoch 62/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00062: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2531 - val_accuracy: 0.9503\n",
      "Epoch 63/350\n",
      "751/776 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00063: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2585 - val_accuracy: 0.9532\n",
      "Epoch 64/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00064: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2556 - val_accuracy: 0.9532\n",
      "Epoch 65/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00065: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2526 - val_accuracy: 0.9532\n",
      "Epoch 66/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 00066: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.2589 - val_accuracy: 0.9532\n",
      "Epoch 67/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00067: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2541 - val_accuracy: 0.9529\n",
      "Epoch 68/350\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 00068: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.2578 - val_accuracy: 0.9536\n",
      "Epoch 69/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00069: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2595 - val_accuracy: 0.9511\n",
      "Epoch 70/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00070: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2554 - val_accuracy: 0.9511\n",
      "Epoch 71/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2545 - val_accuracy: 0.9529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00072: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2578 - val_accuracy: 0.9525\n",
      "Epoch 73/350\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00073: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.2571 - val_accuracy: 0.9532\n",
      "Epoch 74/350\n",
      "753/776 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2574 - val_accuracy: 0.9529\n",
      "Epoch 75/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00075: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2523 - val_accuracy: 0.9536\n",
      "Epoch 76/350\n",
      "752/776 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00076: val_accuracy improved from 0.95359 to 0.95395, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2567 - val_accuracy: 0.9540\n",
      "Epoch 77/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00077: val_accuracy did not improve from 0.95395\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2520 - val_accuracy: 0.9518\n",
      "Epoch 78/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00078: val_accuracy improved from 0.95395 to 0.95431, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2506 - val_accuracy: 0.9543\n",
      "Epoch 79/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95431\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2545 - val_accuracy: 0.9507\n",
      "Epoch 80/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00080: val_accuracy did not improve from 0.95431\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2553 - val_accuracy: 0.9529\n",
      "Epoch 81/350\n",
      "749/776 [===========================>..] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00081: val_accuracy did not improve from 0.95431\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2551 - val_accuracy: 0.9536\n",
      "Epoch 82/350\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00082: val_accuracy improved from 0.95431 to 0.95504, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2561 - val_accuracy: 0.9550\n",
      "Epoch 83/350\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00083: val_accuracy did not improve from 0.95504\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2569 - val_accuracy: 0.9514\n",
      "Epoch 84/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 00084: val_accuracy did not improve from 0.95504\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.2516 - val_accuracy: 0.9540\n",
      "Epoch 85/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00085: val_accuracy did not improve from 0.95504\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2525 - val_accuracy: 0.9543\n",
      "Epoch 86/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00086: val_accuracy improved from 0.95504 to 0.95540, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2576 - val_accuracy: 0.9554\n",
      "Epoch 87/350\n",
      "770/776 [============================>.] - ETA: 0s - loss: 9.3750e-04 - accuracy: 0.9999\n",
      "Epoch 00087: val_accuracy did not improve from 0.95540\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.3505e-04 - accuracy: 0.9999 - val_loss: 0.2626 - val_accuracy: 0.9543\n",
      "Epoch 88/350\n",
      "760/776 [============================>.] - ETA: 0s - loss: 9.4824e-04 - accuracy: 0.9999\n",
      "Epoch 00088: val_accuracy did not improve from 0.95540\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.5139e-04 - accuracy: 0.9999 - val_loss: 0.2617 - val_accuracy: 0.9550\n",
      "Epoch 89/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy improved from 0.95540 to 0.95577, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2614 - val_accuracy: 0.9558\n",
      "Epoch 90/350\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00090: val_accuracy improved from 0.95577 to 0.95649, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2604 - val_accuracy: 0.9565\n",
      "Epoch 91/350\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00091: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2664 - val_accuracy: 0.9532\n",
      "Epoch 92/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00092: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2630 - val_accuracy: 0.9525\n",
      "Epoch 93/350\n",
      "750/776 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2681 - val_accuracy: 0.9536\n",
      "Epoch 94/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 9.0326e-04 - accuracy: 1.0000\n",
      "Epoch 00094: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.9657e-04 - accuracy: 1.0000 - val_loss: 0.2702 - val_accuracy: 0.9536\n",
      "Epoch 95/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00095: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2716 - val_accuracy: 0.9532\n",
      "Epoch 96/350\n",
      "746/776 [===========================>..] - ETA: 0s - loss: 8.8391e-04 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.7649e-04 - accuracy: 0.9998 - val_loss: 0.2710 - val_accuracy: 0.9507\n",
      "Epoch 97/350\n",
      "745/776 [===========================>..] - ETA: 0s - loss: 7.6181e-04 - accuracy: 0.9999\n",
      "Epoch 00097: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.8058e-04 - accuracy: 0.9999 - val_loss: 0.2720 - val_accuracy: 0.9518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/350\n",
      "754/776 [============================>.] - ETA: 0s - loss: 7.7488e-04 - accuracy: 0.9999\n",
      "Epoch 00098: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.7267e-04 - accuracy: 0.9999 - val_loss: 0.2698 - val_accuracy: 0.9529\n",
      "Epoch 99/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 9.9292e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.8703e-04 - accuracy: 0.9998 - val_loss: 0.2662 - val_accuracy: 0.9554\n",
      "Epoch 100/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00100: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.9258e-04 - accuracy: 0.9998 - val_loss: 0.2662 - val_accuracy: 0.9547\n",
      "Epoch 101/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 9.8036e-04 - accuracy: 0.9997\n",
      "Epoch 00101: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.7969e-04 - accuracy: 0.9997 - val_loss: 0.2711 - val_accuracy: 0.9547\n",
      "Epoch 102/350\n",
      "758/776 [============================>.] - ETA: 0s - loss: 7.4438e-04 - accuracy: 0.9998\n",
      "Epoch 00102: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.4451e-04 - accuracy: 0.9998 - val_loss: 0.2734 - val_accuracy: 0.9543\n",
      "Epoch 103/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 9.5469e-04 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.4872e-04 - accuracy: 0.9998 - val_loss: 0.2702 - val_accuracy: 0.9543\n",
      "Epoch 104/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 7.2210e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.2087e-04 - accuracy: 0.9999 - val_loss: 0.2721 - val_accuracy: 0.9525\n",
      "Epoch 105/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 7.9323e-04 - accuracy: 0.9998\n",
      "Epoch 00105: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.8292e-04 - accuracy: 0.9998 - val_loss: 0.2776 - val_accuracy: 0.9507\n",
      "Epoch 106/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 8.2885e-04 - accuracy: 0.9998\n",
      "Epoch 00106: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.3165e-04 - accuracy: 0.9998 - val_loss: 0.2734 - val_accuracy: 0.9532\n",
      "Epoch 107/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 7.5584e-04 - accuracy: 0.9999\n",
      "Epoch 00107: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.5374e-04 - accuracy: 0.9999 - val_loss: 0.2665 - val_accuracy: 0.9536\n",
      "Epoch 108/350\n",
      "771/776 [============================>.] - ETA: 0s - loss: 6.4519e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.4189e-04 - accuracy: 0.9999 - val_loss: 0.2691 - val_accuracy: 0.9543\n",
      "Epoch 109/350\n",
      "772/776 [============================>.] - ETA: 0s - loss: 8.0081e-04 - accuracy: 0.9998\n",
      "Epoch 00109: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.9957e-04 - accuracy: 0.9998 - val_loss: 0.2758 - val_accuracy: 0.9550\n",
      "Epoch 110/350\n",
      "745/776 [===========================>..] - ETA: 0s - loss: 8.2919e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.2140e-04 - accuracy: 0.9999 - val_loss: 0.2692 - val_accuracy: 0.9540\n",
      "Epoch 111/350\n",
      "749/776 [===========================>..] - ETA: 0s - loss: 8.2616e-04 - accuracy: 0.9998\n",
      "Epoch 00111: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.3281e-04 - accuracy: 0.9998 - val_loss: 0.2642 - val_accuracy: 0.9532\n",
      "Epoch 112/350\n",
      "756/776 [============================>.] - ETA: 0s - loss: 6.0171e-04 - accuracy: 1.0000\n",
      "Epoch 00112: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 5.9872e-04 - accuracy: 1.0000 - val_loss: 0.2627 - val_accuracy: 0.9543\n",
      "Epoch 113/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 7.6883e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.6336e-04 - accuracy: 0.9999 - val_loss: 0.2673 - val_accuracy: 0.9550\n",
      "Epoch 114/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 6.6975e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.6817e-04 - accuracy: 1.0000 - val_loss: 0.2665 - val_accuracy: 0.9550\n",
      "Epoch 115/350\n",
      "765/776 [============================>.] - ETA: 0s - loss: 8.9503e-04 - accuracy: 0.9999\n",
      "Epoch 00115: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.8500e-04 - accuracy: 0.9999 - val_loss: 0.2632 - val_accuracy: 0.9565\n",
      "Epoch 116/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 6.9201e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.9077e-04 - accuracy: 1.0000 - val_loss: 0.2674 - val_accuracy: 0.9529\n",
      "Epoch 117/350\n",
      "757/776 [============================>.] - ETA: 0s - loss: 6.7184e-04 - accuracy: 0.9998\n",
      "Epoch 00117: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.6387e-04 - accuracy: 0.9998 - val_loss: 0.2739 - val_accuracy: 0.9529\n",
      "Epoch 118/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 7.1182e-04 - accuracy: 0.9999\n",
      "Epoch 00118: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.0507e-04 - accuracy: 0.9999 - val_loss: 0.2672 - val_accuracy: 0.9558\n",
      "Epoch 119/350\n",
      "750/776 [===========================>..] - ETA: 0s - loss: 6.6833e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.5967e-04 - accuracy: 0.9999 - val_loss: 0.2730 - val_accuracy: 0.9543\n",
      "Epoch 120/350\n",
      "755/776 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998  \n",
      "Epoch 00120: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2849 - val_accuracy: 0.9525\n",
      "Epoch 121/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 5.6476e-04 - accuracy: 0.9999\n",
      "Epoch 00121: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 5.6383e-04 - accuracy: 0.9999 - val_loss: 0.2765 - val_accuracy: 0.9518\n",
      "Epoch 122/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 8.3316e-04 - accuracy: 0.9998\n",
      "Epoch 00122: val_accuracy did not improve from 0.95649\n",
      "Restoring model weights from the end of the best epoch.\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.2685e-04 - accuracy: 0.9998 - val_loss: 0.2740 - val_accuracy: 0.9514\n",
      "Epoch 00122: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:50<02:50, 170.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.84      0.88      0.86       721\n",
      "        car_horn       0.79      0.80      0.79       231\n",
      "children_playing       0.68      0.80      0.74       700\n",
      "        dog_bark       0.73      0.77      0.75       700\n",
      "           siren       0.83      0.56      0.67       581\n",
      "\n",
      "        accuracy                           0.76      2933\n",
      "       macro avg       0.77      0.76      0.76      2933\n",
      "    weighted avg       0.77      0.76      0.76      2933\n",
      "\n",
      "Model: \"Model_CNN_1D_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 228, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 228, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 228, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6384)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                319250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 330,291\n",
      "Trainable params: 330,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24815, 234, 1)\n",
      "Epoch 1/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.7741\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82777, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 3s 4ms/step - loss: 0.7279 - accuracy: 0.7741 - val_loss: 0.5420 - val_accuracy: 0.8278\n",
      "Epoch 2/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.4891 - accuracy: 0.8517\n",
      "Epoch 00002: val_accuracy improved from 0.82777 to 0.85533, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.4877 - accuracy: 0.8524 - val_loss: 0.4537 - val_accuracy: 0.8553\n",
      "Epoch 3/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8757\n",
      "Epoch 00003: val_accuracy improved from 0.85533 to 0.87817, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.4174 - accuracy: 0.8761 - val_loss: 0.4066 - val_accuracy: 0.8782\n",
      "Epoch 4/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8918\n",
      "Epoch 00004: val_accuracy improved from 0.87817 to 0.88687, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.3774 - accuracy: 0.8916 - val_loss: 0.3818 - val_accuracy: 0.8869\n",
      "Epoch 5/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.9011\n",
      "Epoch 00005: val_accuracy improved from 0.88687 to 0.88941, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.3384 - accuracy: 0.9012 - val_loss: 0.3769 - val_accuracy: 0.8894\n",
      "Epoch 6/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.9126\n",
      "Epoch 00006: val_accuracy improved from 0.88941 to 0.89920, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.3140 - accuracy: 0.9126 - val_loss: 0.3537 - val_accuracy: 0.8992\n",
      "Epoch 7/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9174\n",
      "Epoch 00007: val_accuracy improved from 0.89920 to 0.90754, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2918 - accuracy: 0.9173 - val_loss: 0.3388 - val_accuracy: 0.9075\n",
      "Epoch 8/150\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.9239\n",
      "Epoch 00008: val_accuracy improved from 0.90754 to 0.90790, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2768 - accuracy: 0.9238 - val_loss: 0.3380 - val_accuracy: 0.9079\n",
      "Epoch 9/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.9310\n",
      "Epoch 00009: val_accuracy improved from 0.90790 to 0.91117, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2565 - accuracy: 0.9308 - val_loss: 0.3312 - val_accuracy: 0.9112\n",
      "Epoch 10/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.9351\n",
      "Epoch 00010: val_accuracy improved from 0.91117 to 0.91878, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2456 - accuracy: 0.9347 - val_loss: 0.3178 - val_accuracy: 0.9188\n",
      "Epoch 11/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.2353 - accuracy: 0.9382\n",
      "Epoch 00011: val_accuracy improved from 0.91878 to 0.92023, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2350 - accuracy: 0.9383 - val_loss: 0.3187 - val_accuracy: 0.9202\n",
      "Epoch 12/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9423\n",
      "Epoch 00012: val_accuracy did not improve from 0.92023\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2232 - accuracy: 0.9423 - val_loss: 0.3167 - val_accuracy: 0.9166\n",
      "Epoch 13/150\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.2159 - accuracy: 0.9429\n",
      "Epoch 00013: val_accuracy improved from 0.92023 to 0.92386, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2157 - accuracy: 0.9430 - val_loss: 0.3179 - val_accuracy: 0.9239\n",
      "Epoch 14/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9465\n",
      "Epoch 00014: val_accuracy did not improve from 0.92386\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2092 - accuracy: 0.9465 - val_loss: 0.3175 - val_accuracy: 0.9159\n",
      "Epoch 15/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.1960 - accuracy: 0.9518\n",
      "Epoch 00015: val_accuracy did not improve from 0.92386\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1960 - accuracy: 0.9517 - val_loss: 0.3161 - val_accuracy: 0.9228\n",
      "Epoch 16/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9513\n",
      "Epoch 00016: val_accuracy did not improve from 0.92386\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1961 - accuracy: 0.9513 - val_loss: 0.3213 - val_accuracy: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9544\n",
      "Epoch 00017: val_accuracy did not improve from 0.92386\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1850 - accuracy: 0.9542 - val_loss: 0.3175 - val_accuracy: 0.9191\n",
      "Epoch 18/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9547\n",
      "Epoch 00018: val_accuracy improved from 0.92386 to 0.92676, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1822 - accuracy: 0.9546 - val_loss: 0.3075 - val_accuracy: 0.9268\n",
      "Epoch 19/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9579\n",
      "Epoch 00019: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1744 - accuracy: 0.9579 - val_loss: 0.2983 - val_accuracy: 0.9231\n",
      "Epoch 20/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9591\n",
      "Epoch 00020: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1697 - accuracy: 0.9590 - val_loss: 0.3184 - val_accuracy: 0.9191\n",
      "Epoch 21/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1663 - accuracy: 0.9592\n",
      "Epoch 00021: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1664 - accuracy: 0.9591 - val_loss: 0.3164 - val_accuracy: 0.9220\n",
      "Epoch 22/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.9608\n",
      "Epoch 00022: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1655 - accuracy: 0.9607 - val_loss: 0.3156 - val_accuracy: 0.9210\n",
      "Epoch 23/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9618\n",
      "Epoch 00023: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1596 - accuracy: 0.9619 - val_loss: 0.3135 - val_accuracy: 0.9253\n",
      "Epoch 24/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1523 - accuracy: 0.9637\n",
      "Epoch 00024: val_accuracy improved from 0.92676 to 0.92712, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1524 - accuracy: 0.9635 - val_loss: 0.3175 - val_accuracy: 0.9271\n",
      "Epoch 25/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.1517 - accuracy: 0.9651\n",
      "Epoch 00025: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1517 - accuracy: 0.9651 - val_loss: 0.3278 - val_accuracy: 0.9210\n",
      "Epoch 26/150\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.9674\n",
      "Epoch 00026: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1464 - accuracy: 0.9676 - val_loss: 0.3204 - val_accuracy: 0.9213\n",
      "Epoch 27/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9649\n",
      "Epoch 00027: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1493 - accuracy: 0.9649 - val_loss: 0.3307 - val_accuracy: 0.9173\n",
      "Epoch 28/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9669\n",
      "Epoch 00028: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1416 - accuracy: 0.9668 - val_loss: 0.3232 - val_accuracy: 0.9260\n",
      "Epoch 29/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9696\n",
      "Epoch 00029: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1391 - accuracy: 0.9697 - val_loss: 0.3126 - val_accuracy: 0.9264\n",
      "Epoch 30/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1319 - accuracy: 0.9708\n",
      "Epoch 00030: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1318 - accuracy: 0.9709 - val_loss: 0.3128 - val_accuracy: 0.9206\n",
      "Epoch 31/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9690\n",
      "Epoch 00031: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1375 - accuracy: 0.9689 - val_loss: 0.3343 - val_accuracy: 0.9249\n",
      "Epoch 32/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9717\n",
      "Epoch 00032: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1304 - accuracy: 0.9717 - val_loss: 0.3111 - val_accuracy: 0.9228\n",
      "Epoch 33/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9717\n",
      "Epoch 00033: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1281 - accuracy: 0.9717 - val_loss: 0.3190 - val_accuracy: 0.9271\n",
      "Epoch 34/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9706\n",
      "Epoch 00034: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1299 - accuracy: 0.9705 - val_loss: 0.3284 - val_accuracy: 0.9249\n",
      "Epoch 35/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9724\n",
      "Epoch 00035: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1254 - accuracy: 0.9723 - val_loss: 0.3251 - val_accuracy: 0.9260\n",
      "Epoch 36/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9746\n",
      "Epoch 00036: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1208 - accuracy: 0.9746 - val_loss: 0.3168 - val_accuracy: 0.9264\n",
      "Epoch 37/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9726\n",
      "Epoch 00037: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1217 - accuracy: 0.9726 - val_loss: 0.3208 - val_accuracy: 0.9264\n",
      "Epoch 38/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9746\n",
      "Epoch 00038: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1198 - accuracy: 0.9745 - val_loss: 0.3195 - val_accuracy: 0.9228\n",
      "Epoch 39/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.1145 - accuracy: 0.9762\n",
      "Epoch 00039: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1146 - accuracy: 0.9762 - val_loss: 0.3289 - val_accuracy: 0.9257\n",
      "Epoch 40/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9734\n",
      "Epoch 00040: val_accuracy improved from 0.92712 to 0.92821, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1193 - accuracy: 0.9734 - val_loss: 0.3288 - val_accuracy: 0.9282\n",
      "Epoch 41/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9746\n",
      "Epoch 00041: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1163 - accuracy: 0.9746 - val_loss: 0.3352 - val_accuracy: 0.9264\n",
      "Epoch 42/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9752\n",
      "Epoch 00042: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1134 - accuracy: 0.9752 - val_loss: 0.3402 - val_accuracy: 0.9228\n",
      "Epoch 43/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9764\n",
      "Epoch 00043: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1114 - accuracy: 0.9763 - val_loss: 0.3323 - val_accuracy: 0.9278\n",
      "Epoch 44/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/776 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9773\n",
      "Epoch 00044: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1131 - accuracy: 0.9771 - val_loss: 0.3301 - val_accuracy: 0.9246\n",
      "Epoch 45/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9773\n",
      "Epoch 00045: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1091 - accuracy: 0.9774 - val_loss: 0.3276 - val_accuracy: 0.9278\n",
      "Epoch 46/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9767\n",
      "Epoch 00046: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1096 - accuracy: 0.9767 - val_loss: 0.3217 - val_accuracy: 0.9231\n",
      "Epoch 47/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9774\n",
      "Epoch 00047: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1077 - accuracy: 0.9773 - val_loss: 0.3454 - val_accuracy: 0.9213\n",
      "Epoch 48/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9771\n",
      "Epoch 00048: val_accuracy improved from 0.92821 to 0.92857, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1099 - accuracy: 0.9770 - val_loss: 0.3274 - val_accuracy: 0.9286\n",
      "Epoch 49/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.1048 - accuracy: 0.9792\n",
      "Epoch 00049: val_accuracy improved from 0.92857 to 0.92966, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1048 - accuracy: 0.9792 - val_loss: 0.3328 - val_accuracy: 0.9297\n",
      "Epoch 50/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9793\n",
      "Epoch 00050: val_accuracy did not improve from 0.92966\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1014 - accuracy: 0.9792 - val_loss: 0.3425 - val_accuracy: 0.9253\n",
      "Epoch 51/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0977 - accuracy: 0.9810\n",
      "Epoch 00051: val_accuracy did not improve from 0.92966\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0977 - accuracy: 0.9810 - val_loss: 0.3319 - val_accuracy: 0.9278\n",
      "Epoch 52/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9797\n",
      "Epoch 00052: val_accuracy did not improve from 0.92966\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1005 - accuracy: 0.9797 - val_loss: 0.3399 - val_accuracy: 0.9242\n",
      "Epoch 53/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9804\n",
      "Epoch 00053: val_accuracy did not improve from 0.92966\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0960 - accuracy: 0.9803 - val_loss: 0.3562 - val_accuracy: 0.9224\n",
      "Epoch 54/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9803\n",
      "Epoch 00054: val_accuracy improved from 0.92966 to 0.93002, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0980 - accuracy: 0.9803 - val_loss: 0.3403 - val_accuracy: 0.9300\n",
      "Epoch 55/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9790\n",
      "Epoch 00055: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1016 - accuracy: 0.9787 - val_loss: 0.3499 - val_accuracy: 0.9242\n",
      "Epoch 56/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9802\n",
      "Epoch 00056: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0983 - accuracy: 0.9801 - val_loss: 0.3417 - val_accuracy: 0.9253\n",
      "Epoch 57/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0905 - accuracy: 0.9830\n",
      "Epoch 00057: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0905 - accuracy: 0.9831 - val_loss: 0.3394 - val_accuracy: 0.9286\n",
      "Epoch 58/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0950 - accuracy: 0.9804\n",
      "Epoch 00058: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0952 - accuracy: 0.9805 - val_loss: 0.3332 - val_accuracy: 0.9297\n",
      "Epoch 59/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9798\n",
      "Epoch 00059: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0963 - accuracy: 0.9797 - val_loss: 0.3355 - val_accuracy: 0.9289\n",
      "Epoch 60/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9815\n",
      "Epoch 00060: val_accuracy improved from 0.93002 to 0.93075, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0947 - accuracy: 0.9815 - val_loss: 0.3341 - val_accuracy: 0.9307\n",
      "Epoch 61/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9804\n",
      "Epoch 00061: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0957 - accuracy: 0.9804 - val_loss: 0.3393 - val_accuracy: 0.9239\n",
      "Epoch 62/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9819\n",
      "Epoch 00062: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0955 - accuracy: 0.9819 - val_loss: 0.3402 - val_accuracy: 0.9264\n",
      "Epoch 63/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9829\n",
      "Epoch 00063: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0890 - accuracy: 0.9829 - val_loss: 0.3525 - val_accuracy: 0.9235\n",
      "Epoch 64/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9833\n",
      "Epoch 00064: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0888 - accuracy: 0.9832 - val_loss: 0.3429 - val_accuracy: 0.9282\n",
      "Epoch 65/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9811\n",
      "Epoch 00065: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.9811 - val_loss: 0.3482 - val_accuracy: 0.9289\n",
      "Epoch 66/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9829\n",
      "Epoch 00066: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0894 - accuracy: 0.9831 - val_loss: 0.3473 - val_accuracy: 0.9300\n",
      "Epoch 67/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9833\n",
      "Epoch 00067: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0873 - accuracy: 0.9833 - val_loss: 0.3491 - val_accuracy: 0.9246\n",
      "Epoch 68/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9822\n",
      "Epoch 00068: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9821 - val_loss: 0.3441 - val_accuracy: 0.9206\n",
      "Epoch 69/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9837\n",
      "Epoch 00069: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9836 - val_loss: 0.3427 - val_accuracy: 0.9282\n",
      "Epoch 70/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0879 - accuracy: 0.9817\n",
      "Epoch 00070: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0879 - accuracy: 0.9818 - val_loss: 0.3459 - val_accuracy: 0.9249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9827\n",
      "Epoch 00071: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9826 - val_loss: 0.3539 - val_accuracy: 0.9224\n",
      "Epoch 72/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0825 - accuracy: 0.9845\n",
      "Epoch 00072: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0827 - accuracy: 0.9844 - val_loss: 0.3405 - val_accuracy: 0.9253\n",
      "Epoch 73/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9831\n",
      "Epoch 00073: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9833 - val_loss: 0.3510 - val_accuracy: 0.9228\n",
      "Epoch 74/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9861\n",
      "Epoch 00074: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9861 - val_loss: 0.3532 - val_accuracy: 0.9242\n",
      "Epoch 75/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9853\n",
      "Epoch 00075: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0806 - accuracy: 0.9853 - val_loss: 0.3555 - val_accuracy: 0.9177\n",
      "Epoch 76/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9855\n",
      "Epoch 00076: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0799 - accuracy: 0.9855 - val_loss: 0.3270 - val_accuracy: 0.9307\n",
      "Epoch 77/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9840\n",
      "Epoch 00077: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.9839 - val_loss: 0.3306 - val_accuracy: 0.9286\n",
      "Epoch 78/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9844\n",
      "Epoch 00078: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0825 - accuracy: 0.9843 - val_loss: 0.3333 - val_accuracy: 0.9257\n",
      "Epoch 79/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9826\n",
      "Epoch 00079: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0849 - accuracy: 0.9826 - val_loss: 0.3343 - val_accuracy: 0.9268\n",
      "Epoch 80/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9833\n",
      "Epoch 00080: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0841 - accuracy: 0.9833 - val_loss: 0.3553 - val_accuracy: 0.9184\n",
      "Epoch 81/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9835\n",
      "Epoch 00081: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9836 - val_loss: 0.3541 - val_accuracy: 0.9271\n",
      "Epoch 82/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9853\n",
      "Epoch 00082: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0793 - accuracy: 0.9854 - val_loss: 0.3511 - val_accuracy: 0.9278\n",
      "Epoch 83/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9838\n",
      "Epoch 00083: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0829 - accuracy: 0.9837 - val_loss: 0.3403 - val_accuracy: 0.9304\n",
      "Epoch 84/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9842\n",
      "Epoch 00084: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0795 - accuracy: 0.9842 - val_loss: 0.3652 - val_accuracy: 0.9282\n",
      "Epoch 85/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9851\n",
      "Epoch 00085: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0764 - accuracy: 0.9851 - val_loss: 0.3349 - val_accuracy: 0.9271\n",
      "Epoch 86/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9854\n",
      "Epoch 00086: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9855 - val_loss: 0.3445 - val_accuracy: 0.9257\n",
      "Epoch 87/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9849\n",
      "Epoch 00087: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9848 - val_loss: 0.3363 - val_accuracy: 0.9282\n",
      "Epoch 88/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9861\n",
      "Epoch 00088: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0743 - accuracy: 0.9861 - val_loss: 0.3452 - val_accuracy: 0.9282\n",
      "Epoch 89/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9865\n",
      "Epoch 00089: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0748 - accuracy: 0.9865 - val_loss: 0.3481 - val_accuracy: 0.9282\n",
      "Epoch 90/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9857\n",
      "Epoch 00090: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0748 - accuracy: 0.9856 - val_loss: 0.3448 - val_accuracy: 0.9257\n",
      "Epoch 91/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9872\n",
      "Epoch 00091: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0730 - accuracy: 0.9871 - val_loss: 0.3628 - val_accuracy: 0.9224\n",
      "Epoch 92/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9868\n",
      "Epoch 00092: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9868 - val_loss: 0.3575 - val_accuracy: 0.9260\n",
      "Epoch 93/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9855\n",
      "Epoch 00093: val_accuracy improved from 0.93075 to 0.93328, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0756 - accuracy: 0.9857 - val_loss: 0.3569 - val_accuracy: 0.9333\n",
      "Epoch 94/150\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9867\n",
      "Epoch 00094: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0708 - accuracy: 0.9865 - val_loss: 0.3403 - val_accuracy: 0.9257\n",
      "Epoch 95/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9861\n",
      "Epoch 00095: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9859 - val_loss: 0.3371 - val_accuracy: 0.9286\n",
      "Epoch 96/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9869\n",
      "Epoch 00096: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9869 - val_loss: 0.3527 - val_accuracy: 0.9304\n",
      "Epoch 97/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9855\n",
      "Epoch 00097: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9854 - val_loss: 0.3449 - val_accuracy: 0.9275\n",
      "Epoch 98/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9842\n",
      "Epoch 00098: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0754 - accuracy: 0.9840 - val_loss: 0.3672 - val_accuracy: 0.9304\n",
      "Epoch 99/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770/776 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9873\n",
      "Epoch 00099: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9873 - val_loss: 0.3597 - val_accuracy: 0.9268\n",
      "Epoch 100/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9868\n",
      "Epoch 00100: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0693 - accuracy: 0.9868 - val_loss: 0.3478 - val_accuracy: 0.9253\n",
      "Epoch 101/150\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9858\n",
      "Epoch 00101: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9858 - val_loss: 0.3421 - val_accuracy: 0.9253\n",
      "Epoch 102/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9864\n",
      "Epoch 00102: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9864 - val_loss: 0.3447 - val_accuracy: 0.9264\n",
      "Epoch 103/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9857\n",
      "Epoch 00103: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0745 - accuracy: 0.9853 - val_loss: 0.3422 - val_accuracy: 0.9300\n",
      "Epoch 104/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9869\n",
      "Epoch 00104: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9869 - val_loss: 0.3452 - val_accuracy: 0.9264\n",
      "Epoch 105/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9871\n",
      "Epoch 00105: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9871 - val_loss: 0.3433 - val_accuracy: 0.9300\n",
      "Epoch 106/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9877\n",
      "Epoch 00106: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0664 - accuracy: 0.9877 - val_loss: 0.3678 - val_accuracy: 0.9271\n",
      "Epoch 107/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9868\n",
      "Epoch 00107: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0700 - accuracy: 0.9868 - val_loss: 0.3682 - val_accuracy: 0.9322\n",
      "Epoch 108/150\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9861\n",
      "Epoch 00108: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0702 - accuracy: 0.9861 - val_loss: 0.3562 - val_accuracy: 0.9275\n",
      "Epoch 109/150\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9878\n",
      "Epoch 00109: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9877 - val_loss: 0.3441 - val_accuracy: 0.9253\n",
      "Epoch 110/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9880\n",
      "Epoch 00110: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0650 - accuracy: 0.9880 - val_loss: 0.3485 - val_accuracy: 0.9260\n",
      "Epoch 111/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9876\n",
      "Epoch 00111: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0670 - accuracy: 0.9876 - val_loss: 0.3420 - val_accuracy: 0.9246\n",
      "Epoch 112/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9897\n",
      "Epoch 00112: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9898 - val_loss: 0.3632 - val_accuracy: 0.9286\n",
      "Epoch 113/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9882\n",
      "Epoch 00113: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9882 - val_loss: 0.3746 - val_accuracy: 0.9220\n",
      "Epoch 114/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9894\n",
      "Epoch 00114: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0642 - accuracy: 0.9895 - val_loss: 0.3453 - val_accuracy: 0.9286\n",
      "Epoch 115/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9881\n",
      "Epoch 00115: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0658 - accuracy: 0.9881 - val_loss: 0.3519 - val_accuracy: 0.9311\n",
      "Epoch 116/150\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9878\n",
      "Epoch 00116: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0665 - accuracy: 0.9879 - val_loss: 0.3450 - val_accuracy: 0.9275\n",
      "Epoch 117/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9885\n",
      "Epoch 00117: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0641 - accuracy: 0.9885 - val_loss: 0.3615 - val_accuracy: 0.9242\n",
      "Epoch 118/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9870\n",
      "Epoch 00118: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0664 - accuracy: 0.9871 - val_loss: 0.3519 - val_accuracy: 0.9293\n",
      "Epoch 119/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9879\n",
      "Epoch 00119: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9880 - val_loss: 0.3564 - val_accuracy: 0.9300\n",
      "Epoch 120/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9894\n",
      "Epoch 00120: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0603 - accuracy: 0.9895 - val_loss: 0.3548 - val_accuracy: 0.9322\n",
      "Epoch 121/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9895\n",
      "Epoch 00121: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0606 - accuracy: 0.9895 - val_loss: 0.3694 - val_accuracy: 0.9278\n",
      "Epoch 122/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9885\n",
      "Epoch 00122: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0645 - accuracy: 0.9885 - val_loss: 0.3523 - val_accuracy: 0.9275\n",
      "Epoch 123/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9895\n",
      "Epoch 00123: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0610 - accuracy: 0.9894 - val_loss: 0.3498 - val_accuracy: 0.9311\n",
      "Epoch 124/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9888\n",
      "Epoch 00124: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0632 - accuracy: 0.9888 - val_loss: 0.3383 - val_accuracy: 0.9293\n",
      "Epoch 125/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9873\n",
      "Epoch 00125: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9872 - val_loss: 0.3616 - val_accuracy: 0.9242\n",
      "Epoch 126/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9897\n",
      "Epoch 00126: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9898 - val_loss: 0.3491 - val_accuracy: 0.9307\n",
      "Epoch 127/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9889\n",
      "Epoch 00127: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0622 - accuracy: 0.9889 - val_loss: 0.3557 - val_accuracy: 0.9318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0581 - accuracy: 0.9899\n",
      "Epoch 00128: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0581 - accuracy: 0.9899 - val_loss: 0.3593 - val_accuracy: 0.9300\n",
      "Epoch 129/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9880\n",
      "Epoch 00129: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0631 - accuracy: 0.9880 - val_loss: 0.3571 - val_accuracy: 0.9278\n",
      "Epoch 130/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9878\n",
      "Epoch 00130: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9878 - val_loss: 0.3759 - val_accuracy: 0.9275\n",
      "Epoch 131/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9868\n",
      "Epoch 00131: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0665 - accuracy: 0.9868 - val_loss: 0.3511 - val_accuracy: 0.9307\n",
      "Epoch 132/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9880\n",
      "Epoch 00132: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0621 - accuracy: 0.9880 - val_loss: 0.3535 - val_accuracy: 0.9326\n",
      "Epoch 133/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9883\n",
      "Epoch 00133: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0603 - accuracy: 0.9883 - val_loss: 0.3677 - val_accuracy: 0.9318\n",
      "Epoch 134/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9881\n",
      "Epoch 00134: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9882 - val_loss: 0.3405 - val_accuracy: 0.9278\n",
      "Epoch 135/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9889\n",
      "Epoch 00135: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0621 - accuracy: 0.9890 - val_loss: 0.3438 - val_accuracy: 0.9300\n",
      "Epoch 136/150\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9889\n",
      "Epoch 00136: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9890 - val_loss: 0.3494 - val_accuracy: 0.9242\n",
      "Epoch 137/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9892\n",
      "Epoch 00137: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0626 - accuracy: 0.9891 - val_loss: 0.3509 - val_accuracy: 0.9304\n",
      "Epoch 138/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.0546 - accuracy: 0.9915\n",
      "Epoch 00138: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0547 - accuracy: 0.9915 - val_loss: 0.3459 - val_accuracy: 0.9293\n",
      "Epoch 139/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0603 - accuracy: 0.9886\n",
      "Epoch 00139: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0602 - accuracy: 0.9886 - val_loss: 0.3654 - val_accuracy: 0.9307\n",
      "Epoch 140/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9893\n",
      "Epoch 00140: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0608 - accuracy: 0.9893 - val_loss: 0.3556 - val_accuracy: 0.9311\n",
      "Epoch 141/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9904\n",
      "Epoch 00141: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0572 - accuracy: 0.9904 - val_loss: 0.3507 - val_accuracy: 0.9318\n",
      "Epoch 142/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0612 - accuracy: 0.9888\n",
      "Epoch 00142: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0611 - accuracy: 0.9888 - val_loss: 0.3451 - val_accuracy: 0.9333\n",
      "Epoch 143/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9903\n",
      "Epoch 00143: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0572 - accuracy: 0.9904 - val_loss: 0.3652 - val_accuracy: 0.9260\n",
      "Epoch 144/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9881\n",
      "Epoch 00144: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9881 - val_loss: 0.3438 - val_accuracy: 0.9307\n",
      "Epoch 145/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9877\n",
      "Epoch 00145: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9877 - val_loss: 0.3628 - val_accuracy: 0.9242\n",
      "Epoch 146/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9882\n",
      "Epoch 00146: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9881 - val_loss: 0.3539 - val_accuracy: 0.9300\n",
      "Epoch 147/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9886\n",
      "Epoch 00147: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0600 - accuracy: 0.9885 - val_loss: 0.3701 - val_accuracy: 0.9307\n",
      "Epoch 148/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0592 - accuracy: 0.9886\n",
      "Epoch 00148: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0592 - accuracy: 0.9886 - val_loss: 0.3598 - val_accuracy: 0.9282\n",
      "Epoch 149/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0580 - accuracy: 0.9900\n",
      "Epoch 00149: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 3s 3ms/step - loss: 0.0581 - accuracy: 0.9900 - val_loss: 0.3863 - val_accuracy: 0.9220\n",
      "Epoch 150/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9905\n",
      "Epoch 00150: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0576 - accuracy: 0.9904 - val_loss: 0.3650 - val_accuracy: 0.9307\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:38<00:00, 259.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.81      0.90      0.85       721\n",
      "        car_horn       0.87      0.76      0.81       231\n",
      "children_playing       0.62      0.82      0.71       700\n",
      "        dog_bark       0.77      0.74      0.76       700\n",
      "           siren       0.81      0.48      0.60       581\n",
      "\n",
      "        accuracy                           0.75      2933\n",
      "       macro avg       0.78      0.74      0.75      2933\n",
      "    weighted avg       0.76      0.75      0.74      2933\n",
      "\n",
      "Validation fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27503, 375)\n",
      "X_val_norm shape.....:(3003, 375)\n",
      "\n",
      "Sum of elements: 0.9802557517920008\n",
      "Number of elements summed: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 233)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               175500    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 288,299\n",
      "Trainable params: 288,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24752, 233)\n",
      "Epoch 1/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.7973 - accuracy: 0.7088\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.81716, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.7896 - accuracy: 0.7121 - val_loss: 0.4991 - val_accuracy: 0.8172\n",
      "Epoch 2/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.4473 - accuracy: 0.8391\n",
      "Epoch 00002: val_accuracy improved from 0.81716 to 0.85351, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 5s 7ms/step - loss: 0.4472 - accuracy: 0.8391 - val_loss: 0.4042 - val_accuracy: 0.8535\n",
      "Epoch 3/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.3492 - accuracy: 0.8751\n",
      "Epoch 00003: val_accuracy improved from 0.85351 to 0.87350, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 8s 10ms/step - loss: 0.3490 - accuracy: 0.8752 - val_loss: 0.3486 - val_accuracy: 0.8735\n",
      "Epoch 4/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.8997\n",
      "Epoch 00004: val_accuracy improved from 0.87350 to 0.88695, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.2890 - accuracy: 0.8996 - val_loss: 0.3132 - val_accuracy: 0.8870\n",
      "Epoch 5/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.2412 - accuracy: 0.9159\n",
      "Epoch 00005: val_accuracy improved from 0.88695 to 0.89531, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.2415 - accuracy: 0.9156 - val_loss: 0.2885 - val_accuracy: 0.8953\n",
      "Epoch 6/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.2051 - accuracy: 0.9292\n",
      "Epoch 00006: val_accuracy improved from 0.89531 to 0.90185, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.2037 - accuracy: 0.9295 - val_loss: 0.2698 - val_accuracy: 0.9019\n",
      "Epoch 7/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1715 - accuracy: 0.9416\n",
      "Epoch 00007: val_accuracy improved from 0.90185 to 0.90840, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.1713 - accuracy: 0.9416 - val_loss: 0.2533 - val_accuracy: 0.9084\n",
      "Epoch 8/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1511 - accuracy: 0.9477\n",
      "Epoch 00008: val_accuracy improved from 0.90840 to 0.91712, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1507 - accuracy: 0.9478 - val_loss: 0.2369 - val_accuracy: 0.9171\n",
      "Epoch 9/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9578\n",
      "Epoch 00009: val_accuracy improved from 0.91712 to 0.92185, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1266 - accuracy: 0.9576 - val_loss: 0.2312 - val_accuracy: 0.9218\n",
      "Epoch 10/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9647\n",
      "Epoch 00010: val_accuracy improved from 0.92185 to 0.92621, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1086 - accuracy: 0.9646 - val_loss: 0.2239 - val_accuracy: 0.9262\n",
      "Epoch 11/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0895 - accuracy: 0.9705\n",
      "Epoch 00011: val_accuracy improved from 0.92621 to 0.92948, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0896 - accuracy: 0.9704 - val_loss: 0.2196 - val_accuracy: 0.9295\n",
      "Epoch 12/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9744\n",
      "Epoch 00012: val_accuracy did not improve from 0.92948\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0791 - accuracy: 0.9744 - val_loss: 0.2187 - val_accuracy: 0.9295\n",
      "Epoch 13/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9796\n",
      "Epoch 00013: val_accuracy did not improve from 0.92948\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0640 - accuracy: 0.9796 - val_loss: 0.2290 - val_accuracy: 0.9277\n",
      "Epoch 14/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0557 - accuracy: 0.9823\n",
      "Epoch 00014: val_accuracy improved from 0.92948 to 0.93021, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0558 - accuracy: 0.9822 - val_loss: 0.2189 - val_accuracy: 0.9302\n",
      "Epoch 15/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9860\n",
      "Epoch 00015: val_accuracy improved from 0.93021 to 0.93384, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0482 - accuracy: 0.9861 - val_loss: 0.2190 - val_accuracy: 0.9338\n",
      "Epoch 16/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0405 - accuracy: 0.9883\n",
      "Epoch 00016: val_accuracy did not improve from 0.93384\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0404 - accuracy: 0.9884 - val_loss: 0.2323 - val_accuracy: 0.9309\n",
      "Epoch 17/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.9885\n",
      "Epoch 00017: val_accuracy improved from 0.93384 to 0.93566, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0368 - accuracy: 0.9884 - val_loss: 0.2186 - val_accuracy: 0.9357\n",
      "Epoch 18/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9910\n",
      "Epoch 00018: val_accuracy improved from 0.93566 to 0.93748, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0314 - accuracy: 0.9909 - val_loss: 0.2263 - val_accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0273 - accuracy: 0.9920\n",
      "Epoch 00019: val_accuracy improved from 0.93748 to 0.93893, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0275 - accuracy: 0.9918 - val_loss: 0.2233 - val_accuracy: 0.9389\n",
      "Epoch 20/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9934\n",
      "Epoch 00020: val_accuracy did not improve from 0.93893\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0228 - accuracy: 0.9934 - val_loss: 0.2408 - val_accuracy: 0.9375\n",
      "Epoch 21/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9945\n",
      "Epoch 00021: val_accuracy improved from 0.93893 to 0.94111, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0199 - accuracy: 0.9945 - val_loss: 0.2326 - val_accuracy: 0.9411\n",
      "Epoch 22/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9958\n",
      "Epoch 00022: val_accuracy did not improve from 0.94111\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0182 - accuracy: 0.9958 - val_loss: 0.2358 - val_accuracy: 0.9404\n",
      "Epoch 23/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9968\n",
      "Epoch 00023: val_accuracy did not improve from 0.94111\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0149 - accuracy: 0.9968 - val_loss: 0.2406 - val_accuracy: 0.9404\n",
      "Epoch 24/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9962\n",
      "Epoch 00024: val_accuracy did not improve from 0.94111\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0154 - accuracy: 0.9962 - val_loss: 0.2509 - val_accuracy: 0.9397\n",
      "Epoch 25/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9968\n",
      "Epoch 00025: val_accuracy improved from 0.94111 to 0.94475, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0135 - accuracy: 0.9967 - val_loss: 0.2359 - val_accuracy: 0.9447\n",
      "Epoch 26/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0115 - accuracy: 0.9975\n",
      "Epoch 00026: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0115 - accuracy: 0.9975 - val_loss: 0.2456 - val_accuracy: 0.9415\n",
      "Epoch 27/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9973\n",
      "Epoch 00027: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0106 - accuracy: 0.9973 - val_loss: 0.2398 - val_accuracy: 0.9422\n",
      "Epoch 28/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9979\n",
      "Epoch 00028: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0100 - accuracy: 0.9979 - val_loss: 0.2495 - val_accuracy: 0.9411\n",
      "Epoch 29/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0090 - accuracy: 0.9982\n",
      "Epoch 00029: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9981 - val_loss: 0.2532 - val_accuracy: 0.9404\n",
      "Epoch 30/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 00030: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 0.2448 - val_accuracy: 0.9422\n",
      "Epoch 31/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9988\n",
      "Epoch 00031: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.2534 - val_accuracy: 0.9418\n",
      "Epoch 32/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9982\n",
      "Epoch 00032: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9983 - val_loss: 0.2595 - val_accuracy: 0.9422\n",
      "Epoch 33/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n",
      "Epoch 00033: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.2884 - val_accuracy: 0.9411\n",
      "Epoch 34/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9985\n",
      "Epoch 00034: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0074 - accuracy: 0.9984 - val_loss: 0.2694 - val_accuracy: 0.9411\n",
      "Epoch 35/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9990\n",
      "Epoch 00035: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.2661 - val_accuracy: 0.9418\n",
      "Epoch 36/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9989\n",
      "Epoch 00036: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.2716 - val_accuracy: 0.9389\n",
      "Epoch 37/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9989\n",
      "Epoch 00037: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.2659 - val_accuracy: 0.9444\n",
      "Epoch 38/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9988\n",
      "Epoch 00038: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.2705 - val_accuracy: 0.9444\n",
      "Epoch 39/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9991\n",
      "Epoch 00039: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.2776 - val_accuracy: 0.9415\n",
      "Epoch 40/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9993\n",
      "Epoch 00040: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 0.2965 - val_accuracy: 0.9382\n",
      "Epoch 41/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 00041: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2839 - val_accuracy: 0.9389\n",
      "Epoch 42/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9996\n",
      "Epoch 00042: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.2806 - val_accuracy: 0.9418\n",
      "Epoch 43/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 00043: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.2891 - val_accuracy: 0.9444\n",
      "Epoch 44/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00044: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2723 - val_accuracy: 0.9437\n",
      "Epoch 45/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 00045: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.2805 - val_accuracy: 0.9415\n",
      "Epoch 46/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9997\n",
      "Epoch 00046: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.2743 - val_accuracy: 0.9433\n",
      "Epoch 47/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9995\n",
      "Epoch 00047: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.2828 - val_accuracy: 0.9433\n",
      "Epoch 48/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00048: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2916 - val_accuracy: 0.9433\n",
      "Epoch 49/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9995\n",
      "Epoch 00049: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.2880 - val_accuracy: 0.9429\n",
      "Epoch 50/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 00050: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.3037 - val_accuracy: 0.9426\n",
      "Epoch 51/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9996\n",
      "Epoch 00051: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.2913 - val_accuracy: 0.9415\n",
      "Epoch 52/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9998\n",
      "Epoch 00052: val_accuracy improved from 0.94475 to 0.94547, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.2946 - val_accuracy: 0.9455\n",
      "Epoch 53/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00053: val_accuracy did not improve from 0.94547\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2957 - val_accuracy: 0.9444\n",
      "Epoch 54/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00054: val_accuracy did not improve from 0.94547\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2930 - val_accuracy: 0.9451\n",
      "Epoch 55/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy improved from 0.94547 to 0.94620, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2939 - val_accuracy: 0.9462\n",
      "Epoch 56/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9995\n",
      "Epoch 00056: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.2911 - val_accuracy: 0.9447\n",
      "Epoch 57/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0019 - accuracy: 0.9998\n",
      "Epoch 00057: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.2968 - val_accuracy: 0.9440\n",
      "Epoch 58/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00058: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2935 - val_accuracy: 0.9458\n",
      "Epoch 59/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2992 - val_accuracy: 0.9458\n",
      "Epoch 60/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 00060: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2987 - val_accuracy: 0.9447\n",
      "Epoch 61/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00061: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.3022 - val_accuracy: 0.9444\n",
      "Epoch 62/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 00062: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.3034 - val_accuracy: 0.9455\n",
      "Epoch 63/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 00063: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.3061 - val_accuracy: 0.9451\n",
      "Epoch 64/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 00064: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.3017 - val_accuracy: 0.9447\n",
      "Epoch 65/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00065: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.3126 - val_accuracy: 0.9451\n",
      "Epoch 66/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00066: val_accuracy improved from 0.94620 to 0.94766, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.3009 - val_accuracy: 0.9477\n",
      "Epoch 67/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00067: val_accuracy did not improve from 0.94766\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2975 - val_accuracy: 0.9469\n",
      "Epoch 68/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00068: val_accuracy improved from 0.94766 to 0.94838, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2984 - val_accuracy: 0.9484\n",
      "Epoch 69/350\n",
      "742/774 [===========================>..] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00069: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.3029 - val_accuracy: 0.9473\n",
      "Epoch 70/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00070: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3024 - val_accuracy: 0.9469\n",
      "Epoch 71/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.3007 - val_accuracy: 0.9466\n",
      "Epoch 72/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00072: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.2996 - val_accuracy: 0.9477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 9.7374e-04 - accuracy: 0.9999\n",
      "Epoch 00073: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.7358e-04 - accuracy: 0.9999 - val_loss: 0.2967 - val_accuracy: 0.9466\n",
      "Epoch 74/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999  \n",
      "Epoch 00074: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3028 - val_accuracy: 0.9447\n",
      "Epoch 75/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00075: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3028 - val_accuracy: 0.9458\n",
      "Epoch 76/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00076: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3128 - val_accuracy: 0.9462\n",
      "Epoch 77/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00077: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3125 - val_accuracy: 0.9469\n",
      "Epoch 78/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.3108 - val_accuracy: 0.9477\n",
      "Epoch 79/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 9.9745e-04 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3066 - val_accuracy: 0.9466\n",
      "Epoch 80/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00080: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.3174 - val_accuracy: 0.9451\n",
      "Epoch 81/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00081: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3169 - val_accuracy: 0.9455\n",
      "Epoch 82/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 8.4576e-04 - accuracy: 0.9999\n",
      "Epoch 00082: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.4772e-04 - accuracy: 0.9999 - val_loss: 0.3157 - val_accuracy: 0.9477\n",
      "Epoch 83/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00083: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3120 - val_accuracy: 0.9480\n",
      "Epoch 84/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00084: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3170 - val_accuracy: 0.9466\n",
      "Epoch 85/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00085: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3188 - val_accuracy: 0.9462\n",
      "Epoch 86/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998  \n",
      "Epoch 00086: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3306 - val_accuracy: 0.9462\n",
      "Epoch 87/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3186 - val_accuracy: 0.9480\n",
      "Epoch 88/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3179 - val_accuracy: 0.9458\n",
      "Epoch 89/350\n",
      "741/774 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 00089: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.3214 - val_accuracy: 0.9484\n",
      "Epoch 90/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998    \n",
      "Epoch 00090: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3237 - val_accuracy: 0.9451\n",
      "Epoch 91/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3152 - val_accuracy: 0.9462\n",
      "Epoch 92/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00092: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3169 - val_accuracy: 0.9477\n",
      "Epoch 93/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 9.4696e-04 - accuracy: 0.9997\n",
      "Epoch 00093: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.4317e-04 - accuracy: 0.9997 - val_loss: 0.3217 - val_accuracy: 0.9469\n",
      "Epoch 94/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 8.4531e-04 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.4114e-04 - accuracy: 0.9999 - val_loss: 0.3276 - val_accuracy: 0.9477\n",
      "Epoch 95/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 6.8297e-04 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8117e-04 - accuracy: 1.0000 - val_loss: 0.3222 - val_accuracy: 0.9480\n",
      "Epoch 96/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 9.5361e-04 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.6033e-04 - accuracy: 0.9998 - val_loss: 0.3232 - val_accuracy: 0.9477\n",
      "Epoch 97/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 8.8482e-04 - accuracy: 0.9999\n",
      "Epoch 00097: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 8.7870e-04 - accuracy: 0.9999 - val_loss: 0.3258 - val_accuracy: 0.9469\n",
      "Epoch 98/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 7.8217e-04 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.8077e-04 - accuracy: 0.9998 - val_loss: 0.3284 - val_accuracy: 0.9477\n",
      "Epoch 99/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3212 - val_accuracy: 0.9477\n",
      "Epoch 100/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 7.7687e-04 - accuracy: 0.9999\n",
      "Epoch 00100: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.7499e-04 - accuracy: 0.9999 - val_loss: 0.3285 - val_accuracy: 0.9480\n",
      "Epoch 101/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 7.8357e-04 - accuracy: 0.9998\n",
      "Epoch 00101: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.6161e-04 - accuracy: 0.9998 - val_loss: 0.3281 - val_accuracy: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 8.4906e-04 - accuracy: 0.9999\n",
      "Epoch 00102: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.3986e-04 - accuracy: 0.9999 - val_loss: 0.3253 - val_accuracy: 0.9473\n",
      "Epoch 103/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3363 - val_accuracy: 0.9462\n",
      "Epoch 104/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00104: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3290 - val_accuracy: 0.9480\n",
      "Epoch 105/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 7.4116e-04 - accuracy: 0.9998\n",
      "Epoch 00105: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.5572e-04 - accuracy: 0.9998 - val_loss: 0.3390 - val_accuracy: 0.9466\n",
      "Epoch 106/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 8.2200e-04 - accuracy: 0.9999\n",
      "Epoch 00106: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.1486e-04 - accuracy: 0.9999 - val_loss: 0.3305 - val_accuracy: 0.9480\n",
      "Epoch 107/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 7.9671e-04 - accuracy: 0.9999\n",
      "Epoch 00107: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.1916e-04 - accuracy: 0.9999 - val_loss: 0.3321 - val_accuracy: 0.9480\n",
      "Epoch 108/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 7.7902e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.7615e-04 - accuracy: 0.9999 - val_loss: 0.3279 - val_accuracy: 0.9480\n",
      "Epoch 109/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 8.0860e-04 - accuracy: 0.9998\n",
      "Epoch 00109: val_accuracy improved from 0.94838 to 0.94911, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.0618e-04 - accuracy: 0.9998 - val_loss: 0.3259 - val_accuracy: 0.9491\n",
      "Epoch 110/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 7.0060e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8937e-04 - accuracy: 0.9999 - val_loss: 0.3264 - val_accuracy: 0.9487\n",
      "Epoch 111/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 6.2394e-04 - accuracy: 0.9998\n",
      "Epoch 00111: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.2009e-04 - accuracy: 0.9998 - val_loss: 0.3342 - val_accuracy: 0.9462\n",
      "Epoch 112/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.9926e-04 - accuracy: 0.9998 - val_loss: 0.3379 - val_accuracy: 0.9480\n",
      "Epoch 113/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 7.0525e-04 - accuracy: 0.9998\n",
      "Epoch 00113: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.0280e-04 - accuracy: 0.9998 - val_loss: 0.3386 - val_accuracy: 0.9491\n",
      "Epoch 114/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 6.9352e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.7853e-04 - accuracy: 1.0000 - val_loss: 0.3382 - val_accuracy: 0.9477\n",
      "Epoch 115/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 8.3131e-04 - accuracy: 0.9998\n",
      "Epoch 00115: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.1833e-04 - accuracy: 0.9998 - val_loss: 0.3344 - val_accuracy: 0.9473\n",
      "Epoch 116/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 4.8985e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5264e-04 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.9477\n",
      "Epoch 117/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 7.4091e-04 - accuracy: 0.9998\n",
      "Epoch 00117: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.3433e-04 - accuracy: 0.9998 - val_loss: 0.3386 - val_accuracy: 0.9480\n",
      "Epoch 118/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 6.8442e-04 - accuracy: 0.9998\n",
      "Epoch 00118: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8349e-04 - accuracy: 0.9998 - val_loss: 0.3408 - val_accuracy: 0.9473\n",
      "Epoch 119/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 6.7650e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.6815e-04 - accuracy: 0.9999 - val_loss: 0.3384 - val_accuracy: 0.9455\n",
      "Epoch 120/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 4.8300e-04 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7888e-04 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.9469\n",
      "Epoch 121/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 6.8129e-04 - accuracy: 0.9998\n",
      "Epoch 00121: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.7298e-04 - accuracy: 0.9998 - val_loss: 0.3438 - val_accuracy: 0.9484\n",
      "Epoch 122/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 4.7579e-04 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7579e-04 - accuracy: 1.0000 - val_loss: 0.3408 - val_accuracy: 0.9491\n",
      "Epoch 123/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 8.4642e-04 - accuracy: 0.9998\n",
      "Epoch 00123: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.4406e-04 - accuracy: 0.9998 - val_loss: 0.3408 - val_accuracy: 0.9466\n",
      "Epoch 124/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 6.2533e-04 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.2312e-04 - accuracy: 1.0000 - val_loss: 0.3524 - val_accuracy: 0.9466\n",
      "Epoch 125/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 5.5665e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.4897e-04 - accuracy: 0.9999 - val_loss: 0.3451 - val_accuracy: 0.9466\n",
      "Epoch 126/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 7.0781e-04 - accuracy: 0.9998\n",
      "Epoch 00126: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.0606e-04 - accuracy: 0.9998 - val_loss: 0.3299 - val_accuracy: 0.9466\n",
      "Epoch 127/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 5.0050e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.9986e-04 - accuracy: 0.9999 - val_loss: 0.3353 - val_accuracy: 0.9484\n",
      "Epoch 128/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 5.2962e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.2796e-04 - accuracy: 0.9999 - val_loss: 0.3328 - val_accuracy: 0.9462\n",
      "Epoch 129/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 4.5882e-04 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.5722e-04 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 5.0176e-04 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.2114e-04 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.9473\n",
      "Epoch 131/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 5.4787e-04 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.4023e-04 - accuracy: 1.0000 - val_loss: 0.3374 - val_accuracy: 0.9469\n",
      "Epoch 132/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 6.8644e-04 - accuracy: 0.9998\n",
      "Epoch 00132: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8093e-04 - accuracy: 0.9998 - val_loss: 0.3379 - val_accuracy: 0.9462\n",
      "Epoch 133/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 5.4224e-04 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.3574e-04 - accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.9469\n",
      "Epoch 134/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 6.5291e-04 - accuracy: 0.9999\n",
      "Epoch 00134: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.4534e-04 - accuracy: 0.9999 - val_loss: 0.3384 - val_accuracy: 0.9477\n",
      "Epoch 135/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 4.1053e-04 - accuracy: 1.0000\n",
      "Epoch 00135: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.1053e-04 - accuracy: 1.0000 - val_loss: 0.3321 - val_accuracy: 0.9469\n",
      "Epoch 136/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 4.6666e-04 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.5950e-04 - accuracy: 1.0000 - val_loss: 0.3409 - val_accuracy: 0.9458\n",
      "Epoch 137/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 4.7896e-04 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7021e-04 - accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9473\n",
      "Epoch 138/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 6.3898e-04 - accuracy: 0.9999\n",
      "Epoch 00138: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.2921e-04 - accuracy: 0.9999 - val_loss: 0.3361 - val_accuracy: 0.9473\n",
      "Epoch 139/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 5.3260e-04 - accuracy: 0.9999\n",
      "Epoch 00139: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5646e-04 - accuracy: 0.9998 - val_loss: 0.3360 - val_accuracy: 0.9484\n",
      "Epoch 140/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 7.2416e-04 - accuracy: 0.9998\n",
      "Epoch 00140: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.2020e-04 - accuracy: 0.9998 - val_loss: 0.3408 - val_accuracy: 0.9473\n",
      "Epoch 141/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 6.2621e-04 - accuracy: 0.9998\n",
      "Epoch 00141: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.5232e-04 - accuracy: 0.9998 - val_loss: 0.3443 - val_accuracy: 0.9480\n",
      "Epoch 142/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 5.5419e-04 - accuracy: 0.9998\n",
      "Epoch 00142: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.8138e-04 - accuracy: 0.9998 - val_loss: 0.3432 - val_accuracy: 0.9458\n",
      "Epoch 143/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 5.9109e-04 - accuracy: 0.9998\n",
      "Epoch 00143: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.8878e-04 - accuracy: 0.9998 - val_loss: 0.3535 - val_accuracy: 0.9462\n",
      "Epoch 144/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 6.2635e-04 - accuracy: 0.9998\n",
      "Epoch 00144: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.1622e-04 - accuracy: 0.9998 - val_loss: 0.3477 - val_accuracy: 0.9484\n",
      "Epoch 145/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 6.6610e-04 - accuracy: 0.9999\n",
      "Epoch 00145: val_accuracy did not improve from 0.94911\n",
      "Restoring model weights from the end of the best epoch.\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.6290e-04 - accuracy: 0.9999 - val_loss: 0.3447 - val_accuracy: 0.9484\n",
      "Epoch 00145: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:39<03:39, 219.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.70      0.61      0.66       672\n",
      "        car_horn       0.68      0.56      0.62       294\n",
      "children_playing       0.77      0.89      0.83       700\n",
      "        dog_bark       0.79      0.85      0.82       700\n",
      "           siren       0.70      0.68      0.69       637\n",
      "\n",
      "        accuracy                           0.74      3003\n",
      "       macro avg       0.73      0.72      0.72      3003\n",
      "    weighted avg       0.74      0.74      0.74      3003\n",
      "\n",
      "Model: \"Model_CNN_1D_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 227, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 227, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 227, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6328)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                316450    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 327,491\n",
      "Trainable params: 327,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24752, 233, 1)\n",
      "Epoch 1/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.7579 - accuracy: 0.7592\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82552, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.7579 - accuracy: 0.7592 - val_loss: 0.5663 - val_accuracy: 0.8255\n",
      "Epoch 2/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.4928 - accuracy: 0.8492\n",
      "Epoch 00002: val_accuracy improved from 0.82552 to 0.85896, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.4924 - accuracy: 0.8493 - val_loss: 0.4702 - val_accuracy: 0.8590\n",
      "Epoch 3/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.4218 - accuracy: 0.8728\n",
      "Epoch 00003: val_accuracy did not improve from 0.85896\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.4214 - accuracy: 0.8731 - val_loss: 0.4698 - val_accuracy: 0.8582\n",
      "Epoch 4/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.3748 - accuracy: 0.8916\n",
      "Epoch 00004: val_accuracy improved from 0.85896 to 0.87714, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3748 - accuracy: 0.8918 - val_loss: 0.4225 - val_accuracy: 0.8771\n",
      "Epoch 5/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.9041\n",
      "Epoch 00005: val_accuracy improved from 0.87714 to 0.88259, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3399 - accuracy: 0.9041 - val_loss: 0.4083 - val_accuracy: 0.8826\n",
      "Epoch 6/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.3182 - accuracy: 0.9086\n",
      "Epoch 00006: val_accuracy improved from 0.88259 to 0.88477, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3182 - accuracy: 0.9086 - val_loss: 0.4105 - val_accuracy: 0.8848\n",
      "Epoch 7/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.9213\n",
      "Epoch 00007: val_accuracy improved from 0.88477 to 0.89059, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2900 - accuracy: 0.9213 - val_loss: 0.3979 - val_accuracy: 0.8906\n",
      "Epoch 8/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2683 - accuracy: 0.9260\n",
      "Epoch 00008: val_accuracy improved from 0.89059 to 0.89168, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2685 - accuracy: 0.9259 - val_loss: 0.4073 - val_accuracy: 0.8917\n",
      "Epoch 9/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.9306\n",
      "Epoch 00009: val_accuracy improved from 0.89168 to 0.89531, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2547 - accuracy: 0.9307 - val_loss: 0.3844 - val_accuracy: 0.8953\n",
      "Epoch 10/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.2454 - accuracy: 0.9357\n",
      "Epoch 00010: val_accuracy did not improve from 0.89531\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2454 - accuracy: 0.9359 - val_loss: 0.3781 - val_accuracy: 0.8935\n",
      "Epoch 11/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.2302 - accuracy: 0.9409\n",
      "Epoch 00011: val_accuracy did not improve from 0.89531\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2303 - accuracy: 0.9409 - val_loss: 0.3790 - val_accuracy: 0.8946\n",
      "Epoch 12/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9430\n",
      "Epoch 00012: val_accuracy improved from 0.89531 to 0.89749, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2247 - accuracy: 0.9431 - val_loss: 0.3817 - val_accuracy: 0.8975\n",
      "Epoch 13/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.2123 - accuracy: 0.9457\n",
      "Epoch 00013: val_accuracy improved from 0.89749 to 0.90513, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2136 - accuracy: 0.9453 - val_loss: 0.3692 - val_accuracy: 0.9051\n",
      "Epoch 14/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9480\n",
      "Epoch 00014: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2070 - accuracy: 0.9480 - val_loss: 0.3760 - val_accuracy: 0.9029\n",
      "Epoch 15/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1950 - accuracy: 0.9500\n",
      "Epoch 00015: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1953 - accuracy: 0.9497 - val_loss: 0.3779 - val_accuracy: 0.9019\n",
      "Epoch 16/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9532\n",
      "Epoch 00016: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1917 - accuracy: 0.9530 - val_loss: 0.3665 - val_accuracy: 0.9015\n",
      "Epoch 17/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1835 - accuracy: 0.9548\n",
      "Epoch 00017: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1833 - accuracy: 0.9548 - val_loss: 0.3801 - val_accuracy: 0.8997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9561\n",
      "Epoch 00018: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1798 - accuracy: 0.9561 - val_loss: 0.3785 - val_accuracy: 0.9019\n",
      "Epoch 19/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9594\n",
      "Epoch 00019: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1751 - accuracy: 0.9592 - val_loss: 0.3812 - val_accuracy: 0.9040\n",
      "Epoch 20/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.9598\n",
      "Epoch 00020: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1699 - accuracy: 0.9597 - val_loss: 0.4189 - val_accuracy: 0.8982\n",
      "Epoch 21/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.1632 - accuracy: 0.9616\n",
      "Epoch 00021: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1635 - accuracy: 0.9616 - val_loss: 0.3745 - val_accuracy: 0.9029\n",
      "Epoch 22/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9630\n",
      "Epoch 00022: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1578 - accuracy: 0.9629 - val_loss: 0.3908 - val_accuracy: 0.8997\n",
      "Epoch 23/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9621\n",
      "Epoch 00023: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1599 - accuracy: 0.9622 - val_loss: 0.3714 - val_accuracy: 0.9037\n",
      "Epoch 24/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9646\n",
      "Epoch 00024: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1500 - accuracy: 0.9649 - val_loss: 0.3921 - val_accuracy: 0.9000\n",
      "Epoch 25/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1494 - accuracy: 0.9652\n",
      "Epoch 00025: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1492 - accuracy: 0.9653 - val_loss: 0.3797 - val_accuracy: 0.9033\n",
      "Epoch 26/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9660\n",
      "Epoch 00026: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1492 - accuracy: 0.9659 - val_loss: 0.3877 - val_accuracy: 0.9051\n",
      "Epoch 27/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.9676\n",
      "Epoch 00027: val_accuracy improved from 0.90513 to 0.90585, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1433 - accuracy: 0.9675 - val_loss: 0.3871 - val_accuracy: 0.9059\n",
      "Epoch 28/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.9671\n",
      "Epoch 00028: val_accuracy improved from 0.90585 to 0.90912, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1452 - accuracy: 0.9672 - val_loss: 0.3889 - val_accuracy: 0.9091\n",
      "Epoch 29/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9718\n",
      "Epoch 00029: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1366 - accuracy: 0.9714 - val_loss: 0.4013 - val_accuracy: 0.9048\n",
      "Epoch 30/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9694\n",
      "Epoch 00030: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1360 - accuracy: 0.9693 - val_loss: 0.3876 - val_accuracy: 0.9077\n",
      "Epoch 31/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9705\n",
      "Epoch 00031: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1335 - accuracy: 0.9705 - val_loss: 0.4009 - val_accuracy: 0.9051\n",
      "Epoch 32/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9716\n",
      "Epoch 00032: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1312 - accuracy: 0.9715 - val_loss: 0.3915 - val_accuracy: 0.9088\n",
      "Epoch 33/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.9715\n",
      "Epoch 00033: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1298 - accuracy: 0.9717 - val_loss: 0.3990 - val_accuracy: 0.9051\n",
      "Epoch 34/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9746\n",
      "Epoch 00034: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1253 - accuracy: 0.9745 - val_loss: 0.4102 - val_accuracy: 0.9069\n",
      "Epoch 35/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9717\n",
      "Epoch 00035: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1289 - accuracy: 0.9718 - val_loss: 0.4024 - val_accuracy: 0.9066\n",
      "Epoch 36/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9755\n",
      "Epoch 00036: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1209 - accuracy: 0.9756 - val_loss: 0.3957 - val_accuracy: 0.9051\n",
      "Epoch 37/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9763\n",
      "Epoch 00037: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1189 - accuracy: 0.9762 - val_loss: 0.3988 - val_accuracy: 0.9026\n",
      "Epoch 38/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9758\n",
      "Epoch 00038: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1174 - accuracy: 0.9758 - val_loss: 0.4269 - val_accuracy: 0.9073\n",
      "Epoch 39/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9749\n",
      "Epoch 00039: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1178 - accuracy: 0.9750 - val_loss: 0.4169 - val_accuracy: 0.9088\n",
      "Epoch 40/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9740\n",
      "Epoch 00040: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1194 - accuracy: 0.9740 - val_loss: 0.3998 - val_accuracy: 0.9051\n",
      "Epoch 41/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1152 - accuracy: 0.9771\n",
      "Epoch 00041: val_accuracy improved from 0.90912 to 0.91167, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1150 - accuracy: 0.9770 - val_loss: 0.3890 - val_accuracy: 0.9117\n",
      "Epoch 42/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9770\n",
      "Epoch 00042: val_accuracy did not improve from 0.91167\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1122 - accuracy: 0.9771 - val_loss: 0.3917 - val_accuracy: 0.9113\n",
      "Epoch 43/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9770\n",
      "Epoch 00043: val_accuracy improved from 0.91167 to 0.91203, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1102 - accuracy: 0.9771 - val_loss: 0.3844 - val_accuracy: 0.9120\n",
      "Epoch 44/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9769\n",
      "Epoch 00044: val_accuracy did not improve from 0.91203\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1127 - accuracy: 0.9771 - val_loss: 0.3984 - val_accuracy: 0.9059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9814\n",
      "Epoch 00045: val_accuracy did not improve from 0.91203\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1026 - accuracy: 0.9814 - val_loss: 0.4033 - val_accuracy: 0.9080\n",
      "Epoch 46/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9776\n",
      "Epoch 00046: val_accuracy improved from 0.91203 to 0.91240, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9776 - val_loss: 0.3983 - val_accuracy: 0.9124\n",
      "Epoch 47/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1058 - accuracy: 0.9790\n",
      "Epoch 00047: val_accuracy did not improve from 0.91240\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1058 - accuracy: 0.9790 - val_loss: 0.4051 - val_accuracy: 0.9106\n",
      "Epoch 48/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1028 - accuracy: 0.9807\n",
      "Epoch 00048: val_accuracy did not improve from 0.91240\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1027 - accuracy: 0.9807 - val_loss: 0.4081 - val_accuracy: 0.9088\n",
      "Epoch 49/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9793\n",
      "Epoch 00049: val_accuracy did not improve from 0.91240\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1054 - accuracy: 0.9795 - val_loss: 0.4063 - val_accuracy: 0.9099\n",
      "Epoch 50/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9791\n",
      "Epoch 00050: val_accuracy improved from 0.91240 to 0.91385, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1074 - accuracy: 0.9792 - val_loss: 0.3825 - val_accuracy: 0.9138\n",
      "Epoch 51/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9803\n",
      "Epoch 00051: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1020 - accuracy: 0.9804 - val_loss: 0.4202 - val_accuracy: 0.9051\n",
      "Epoch 52/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.1019 - accuracy: 0.9802\n",
      "Epoch 00052: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9801 - val_loss: 0.3960 - val_accuracy: 0.9080\n",
      "Epoch 53/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9813\n",
      "Epoch 00053: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9814 - val_loss: 0.3890 - val_accuracy: 0.9120\n",
      "Epoch 54/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9806\n",
      "Epoch 00054: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0985 - accuracy: 0.9806 - val_loss: 0.4162 - val_accuracy: 0.9080\n",
      "Epoch 55/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9806\n",
      "Epoch 00055: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 8s 10ms/step - loss: 0.0984 - accuracy: 0.9806 - val_loss: 0.4137 - val_accuracy: 0.9138\n",
      "Epoch 56/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9811\n",
      "Epoch 00056: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 5s 6ms/step - loss: 0.0987 - accuracy: 0.9810 - val_loss: 0.4003 - val_accuracy: 0.9113\n",
      "Epoch 57/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0949 - accuracy: 0.9824\n",
      "Epoch 00057: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0951 - accuracy: 0.9823 - val_loss: 0.4140 - val_accuracy: 0.9080\n",
      "Epoch 58/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9796\n",
      "Epoch 00058: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0989 - accuracy: 0.9796 - val_loss: 0.4163 - val_accuracy: 0.9088\n",
      "Epoch 59/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9809\n",
      "Epoch 00059: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0979 - accuracy: 0.9809 - val_loss: 0.4119 - val_accuracy: 0.9080\n",
      "Epoch 60/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9840\n",
      "Epoch 00060: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9840 - val_loss: 0.3992 - val_accuracy: 0.9106\n",
      "Epoch 61/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0917 - accuracy: 0.9824\n",
      "Epoch 00061: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9824 - val_loss: 0.4161 - val_accuracy: 0.9091\n",
      "Epoch 62/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9820\n",
      "Epoch 00062: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0937 - accuracy: 0.9820 - val_loss: 0.4100 - val_accuracy: 0.9062\n",
      "Epoch 63/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0912 - accuracy: 0.9822\n",
      "Epoch 00063: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0912 - accuracy: 0.9822 - val_loss: 0.4133 - val_accuracy: 0.9128\n",
      "Epoch 64/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9837\n",
      "Epoch 00064: val_accuracy improved from 0.91385 to 0.91458, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0882 - accuracy: 0.9837 - val_loss: 0.4158 - val_accuracy: 0.9146\n",
      "Epoch 65/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9846\n",
      "Epoch 00065: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 5s 6ms/step - loss: 0.0866 - accuracy: 0.9846 - val_loss: 0.4121 - val_accuracy: 0.9135\n",
      "Epoch 66/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0884 - accuracy: 0.9831\n",
      "Epoch 00066: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 11s 14ms/step - loss: 0.0883 - accuracy: 0.9832 - val_loss: 0.4198 - val_accuracy: 0.9138\n",
      "Epoch 67/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0874 - accuracy: 0.9843\n",
      "Epoch 00067: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0873 - accuracy: 0.9844 - val_loss: 0.4069 - val_accuracy: 0.9120\n",
      "Epoch 68/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9827\n",
      "Epoch 00068: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0883 - accuracy: 0.9827 - val_loss: 0.4161 - val_accuracy: 0.9146\n",
      "Epoch 69/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0887 - accuracy: 0.9825\n",
      "Epoch 00069: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0887 - accuracy: 0.9825 - val_loss: 0.4047 - val_accuracy: 0.9135\n",
      "Epoch 70/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9857\n",
      "Epoch 00070: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0828 - accuracy: 0.9856 - val_loss: 0.4198 - val_accuracy: 0.9131\n",
      "Epoch 71/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9850\n",
      "Epoch 00071: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0851 - accuracy: 0.9849 - val_loss: 0.4251 - val_accuracy: 0.9117\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/774 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9844\n",
      "Epoch 00072: val_accuracy improved from 0.91458 to 0.91567, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0854 - accuracy: 0.9842 - val_loss: 0.4176 - val_accuracy: 0.9157\n",
      "Epoch 73/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9835\n",
      "Epoch 00073: val_accuracy did not improve from 0.91567\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0873 - accuracy: 0.9835 - val_loss: 0.4226 - val_accuracy: 0.9135\n",
      "Epoch 74/150\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9834\n",
      "Epoch 00074: val_accuracy did not improve from 0.91567\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0858 - accuracy: 0.9832 - val_loss: 0.4155 - val_accuracy: 0.9131\n",
      "Epoch 75/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0817 - accuracy: 0.9850\n",
      "Epoch 00075: val_accuracy did not improve from 0.91567\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9851 - val_loss: 0.4400 - val_accuracy: 0.9124\n",
      "Epoch 76/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0821 - accuracy: 0.9858\n",
      "Epoch 00076: val_accuracy did not improve from 0.91567\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0820 - accuracy: 0.9859 - val_loss: 0.4310 - val_accuracy: 0.9142\n",
      "Epoch 77/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.9855\n",
      "Epoch 00077: val_accuracy improved from 0.91567 to 0.91858, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0801 - accuracy: 0.9857 - val_loss: 0.4079 - val_accuracy: 0.9186\n",
      "Epoch 78/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9845\n",
      "Epoch 00078: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0832 - accuracy: 0.9846 - val_loss: 0.4202 - val_accuracy: 0.9135\n",
      "Epoch 79/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9839\n",
      "Epoch 00079: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9839 - val_loss: 0.4070 - val_accuracy: 0.9142\n",
      "Epoch 80/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9847\n",
      "Epoch 00080: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0821 - accuracy: 0.9848 - val_loss: 0.4026 - val_accuracy: 0.9146\n",
      "Epoch 81/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9867\n",
      "Epoch 00081: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0772 - accuracy: 0.9868 - val_loss: 0.4376 - val_accuracy: 0.9099\n",
      "Epoch 82/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9869\n",
      "Epoch 00082: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0783 - accuracy: 0.9868 - val_loss: 0.4033 - val_accuracy: 0.9142\n",
      "Epoch 83/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9849\n",
      "Epoch 00083: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0799 - accuracy: 0.9849 - val_loss: 0.4016 - val_accuracy: 0.9168\n",
      "Epoch 84/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9845\n",
      "Epoch 00084: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9844 - val_loss: 0.4119 - val_accuracy: 0.9149\n",
      "Epoch 85/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9865\n",
      "Epoch 00085: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0749 - accuracy: 0.9865 - val_loss: 0.4179 - val_accuracy: 0.9178\n",
      "Epoch 86/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.9846\n",
      "Epoch 00086: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9846 - val_loss: 0.4264 - val_accuracy: 0.9091\n",
      "Epoch 87/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9865\n",
      "Epoch 00087: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9864 - val_loss: 0.4013 - val_accuracy: 0.9171\n",
      "Epoch 88/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9893\n",
      "Epoch 00088: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9893 - val_loss: 0.4085 - val_accuracy: 0.9157\n",
      "Epoch 89/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9864\n",
      "Epoch 00089: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0765 - accuracy: 0.9861 - val_loss: 0.4326 - val_accuracy: 0.9131\n",
      "Epoch 90/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9864\n",
      "Epoch 00090: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9863 - val_loss: 0.4175 - val_accuracy: 0.9153\n",
      "Epoch 91/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9877\n",
      "Epoch 00091: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0735 - accuracy: 0.9877 - val_loss: 0.4181 - val_accuracy: 0.9138\n",
      "Epoch 92/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9871\n",
      "Epoch 00092: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9871 - val_loss: 0.4133 - val_accuracy: 0.9142\n",
      "Epoch 93/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9891\n",
      "Epoch 00093: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9891 - val_loss: 0.4268 - val_accuracy: 0.9146\n",
      "Epoch 94/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9867\n",
      "Epoch 00094: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9867 - val_loss: 0.4412 - val_accuracy: 0.9106\n",
      "Epoch 95/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9872\n",
      "Epoch 00095: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0736 - accuracy: 0.9873 - val_loss: 0.4218 - val_accuracy: 0.9117\n",
      "Epoch 96/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9870\n",
      "Epoch 00096: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0741 - accuracy: 0.9871 - val_loss: 0.4399 - val_accuracy: 0.9113\n",
      "Epoch 97/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9860\n",
      "Epoch 00097: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9860 - val_loss: 0.4088 - val_accuracy: 0.9146\n",
      "Epoch 98/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9865\n",
      "Epoch 00098: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0746 - accuracy: 0.9865 - val_loss: 0.4416 - val_accuracy: 0.9091\n",
      "Epoch 99/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9883\n",
      "Epoch 00099: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0687 - accuracy: 0.9883 - val_loss: 0.4296 - val_accuracy: 0.9146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9860\n",
      "Epoch 00100: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9860 - val_loss: 0.4268 - val_accuracy: 0.9124\n",
      "Epoch 101/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.9872\n",
      "Epoch 00101: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9872 - val_loss: 0.4160 - val_accuracy: 0.9149\n",
      "Epoch 102/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9870\n",
      "Epoch 00102: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0729 - accuracy: 0.9870 - val_loss: 0.4248 - val_accuracy: 0.9157\n",
      "Epoch 103/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9869\n",
      "Epoch 00103: val_accuracy improved from 0.91858 to 0.92076, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0725 - accuracy: 0.9870 - val_loss: 0.4006 - val_accuracy: 0.9208\n",
      "Epoch 104/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9876\n",
      "Epoch 00104: val_accuracy did not improve from 0.92076\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9876 - val_loss: 0.4039 - val_accuracy: 0.9171\n",
      "Epoch 105/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9874\n",
      "Epoch 00105: val_accuracy did not improve from 0.92076\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0710 - accuracy: 0.9874 - val_loss: 0.4241 - val_accuracy: 0.9128\n",
      "Epoch 106/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9879\n",
      "Epoch 00106: val_accuracy did not improve from 0.92076\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0701 - accuracy: 0.9877 - val_loss: 0.4212 - val_accuracy: 0.9168\n",
      "Epoch 107/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.9876\n",
      "Epoch 00107: val_accuracy did not improve from 0.92076\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0691 - accuracy: 0.9875 - val_loss: 0.4258 - val_accuracy: 0.9128\n",
      "Epoch 108/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9885\n",
      "Epoch 00108: val_accuracy improved from 0.92076 to 0.92112, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0674 - accuracy: 0.9886 - val_loss: 0.4063 - val_accuracy: 0.9211\n",
      "Epoch 109/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9874\n",
      "Epoch 00109: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0696 - accuracy: 0.9873 - val_loss: 0.4128 - val_accuracy: 0.9142\n",
      "Epoch 110/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9880\n",
      "Epoch 00110: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0672 - accuracy: 0.9881 - val_loss: 0.4130 - val_accuracy: 0.9171\n",
      "Epoch 111/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9882\n",
      "Epoch 00111: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0668 - accuracy: 0.9882 - val_loss: 0.4133 - val_accuracy: 0.9149\n",
      "Epoch 112/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9886\n",
      "Epoch 00112: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0670 - accuracy: 0.9886 - val_loss: 0.4104 - val_accuracy: 0.9138\n",
      "Epoch 113/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9895\n",
      "Epoch 00113: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9896 - val_loss: 0.3952 - val_accuracy: 0.9157\n",
      "Epoch 114/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9899\n",
      "Epoch 00114: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9897 - val_loss: 0.4137 - val_accuracy: 0.9160\n",
      "Epoch 115/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9875\n",
      "Epoch 00115: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9873 - val_loss: 0.4182 - val_accuracy: 0.9164\n",
      "Epoch 116/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9894\n",
      "Epoch 00116: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9893 - val_loss: 0.4406 - val_accuracy: 0.9135\n",
      "Epoch 117/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9863\n",
      "Epoch 00117: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0692 - accuracy: 0.9864 - val_loss: 0.4331 - val_accuracy: 0.9175\n",
      "Epoch 118/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9872\n",
      "Epoch 00118: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9872 - val_loss: 0.4014 - val_accuracy: 0.9146\n",
      "Epoch 119/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9888\n",
      "Epoch 00119: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9888 - val_loss: 0.4295 - val_accuracy: 0.9128\n",
      "Epoch 120/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9887\n",
      "Epoch 00120: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0664 - accuracy: 0.9887 - val_loss: 0.4078 - val_accuracy: 0.9160\n",
      "Epoch 121/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9884\n",
      "Epoch 00121: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9884 - val_loss: 0.4263 - val_accuracy: 0.9131\n",
      "Epoch 122/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9886\n",
      "Epoch 00122: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9886 - val_loss: 0.4250 - val_accuracy: 0.9157\n",
      "Epoch 123/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0629 - accuracy: 0.9889\n",
      "Epoch 00123: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0628 - accuracy: 0.9889 - val_loss: 0.4100 - val_accuracy: 0.9157\n",
      "Epoch 124/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9879\n",
      "Epoch 00124: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0656 - accuracy: 0.9879 - val_loss: 0.4223 - val_accuracy: 0.9146\n",
      "Epoch 125/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9891\n",
      "Epoch 00125: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9892 - val_loss: 0.4233 - val_accuracy: 0.9182\n",
      "Epoch 126/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9895\n",
      "Epoch 00126: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0623 - accuracy: 0.9896 - val_loss: 0.4354 - val_accuracy: 0.9160\n",
      "Epoch 127/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9887\n",
      "Epoch 00127: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9887 - val_loss: 0.4294 - val_accuracy: 0.9128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9875\n",
      "Epoch 00128: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9874 - val_loss: 0.4185 - val_accuracy: 0.9153\n",
      "Epoch 129/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9899\n",
      "Epoch 00129: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0615 - accuracy: 0.9899 - val_loss: 0.4355 - val_accuracy: 0.9113\n",
      "Epoch 130/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9883\n",
      "Epoch 00130: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9883 - val_loss: 0.4370 - val_accuracy: 0.9113\n",
      "Epoch 131/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9873\n",
      "Epoch 00131: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0655 - accuracy: 0.9873 - val_loss: 0.4345 - val_accuracy: 0.9135\n",
      "Epoch 132/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9900\n",
      "Epoch 00132: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0618 - accuracy: 0.9900 - val_loss: 0.4431 - val_accuracy: 0.9171\n",
      "Epoch 133/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9893\n",
      "Epoch 00133: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0622 - accuracy: 0.9893 - val_loss: 0.4440 - val_accuracy: 0.9164\n",
      "Epoch 134/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9884\n",
      "Epoch 00134: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9884 - val_loss: 0.4405 - val_accuracy: 0.9160\n",
      "Epoch 135/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9884\n",
      "Epoch 00135: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9884 - val_loss: 0.4478 - val_accuracy: 0.9102\n",
      "Epoch 136/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9900\n",
      "Epoch 00136: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9899 - val_loss: 0.4410 - val_accuracy: 0.9211\n",
      "Epoch 137/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9902\n",
      "Epoch 00137: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0593 - accuracy: 0.9902 - val_loss: 0.4488 - val_accuracy: 0.9175\n",
      "Epoch 138/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9898\n",
      "Epoch 00138: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9897 - val_loss: 0.4242 - val_accuracy: 0.9208\n",
      "Epoch 139/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9883\n",
      "Epoch 00139: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9884 - val_loss: 0.4323 - val_accuracy: 0.9208\n",
      "Epoch 140/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9893\n",
      "Epoch 00140: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0603 - accuracy: 0.9894 - val_loss: 0.4361 - val_accuracy: 0.9164\n",
      "Epoch 141/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9911\n",
      "Epoch 00141: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0586 - accuracy: 0.9910 - val_loss: 0.4211 - val_accuracy: 0.9178\n",
      "Epoch 142/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9902\n",
      "Epoch 00142: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0582 - accuracy: 0.9903 - val_loss: 0.4444 - val_accuracy: 0.9128\n",
      "Epoch 143/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9903\n",
      "Epoch 00143: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0574 - accuracy: 0.9903 - val_loss: 0.4271 - val_accuracy: 0.9168\n",
      "Epoch 144/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9902\n",
      "Epoch 00144: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0589 - accuracy: 0.9903 - val_loss: 0.4346 - val_accuracy: 0.9193\n",
      "Epoch 145/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0559 - accuracy: 0.9911\n",
      "Epoch 00145: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0556 - accuracy: 0.9912 - val_loss: 0.4498 - val_accuracy: 0.9164\n",
      "Epoch 146/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0562 - accuracy: 0.9903\n",
      "Epoch 00146: val_accuracy improved from 0.92112 to 0.92330, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0562 - accuracy: 0.9903 - val_loss: 0.4278 - val_accuracy: 0.9233\n",
      "Epoch 147/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9891\n",
      "Epoch 00147: val_accuracy did not improve from 0.92330\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0605 - accuracy: 0.9891 - val_loss: 0.4484 - val_accuracy: 0.9164\n",
      "Epoch 148/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9895\n",
      "Epoch 00148: val_accuracy did not improve from 0.92330\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0586 - accuracy: 0.9896 - val_loss: 0.4356 - val_accuracy: 0.9186\n",
      "Epoch 149/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0611 - accuracy: 0.9897\n",
      "Epoch 00149: val_accuracy did not improve from 0.92330\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0610 - accuracy: 0.9898 - val_loss: 0.4100 - val_accuracy: 0.9197\n",
      "Epoch 150/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9911\n",
      "Epoch 00150: val_accuracy did not improve from 0.92330\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0554 - accuracy: 0.9911 - val_loss: 0.4146 - val_accuracy: 0.9164\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [09:56<00:00, 298.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.65      0.66      0.66       672\n",
      "        car_horn       0.65      0.50      0.56       294\n",
      "children_playing       0.74      0.85      0.79       700\n",
      "        dog_bark       0.81      0.82      0.81       700\n",
      "           siren       0.76      0.69      0.72       637\n",
      "\n",
      "        accuracy                           0.73      3003\n",
      "       macro avg       0.72      0.70      0.71      3003\n",
      "    weighted avg       0.73      0.73      0.73      3003\n",
      "\n",
      "Validation fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27132, 375)\n",
      "X_val_norm shape.....:(3374, 375)\n",
      "\n",
      "Sum of elements: 0.9802037458487682\n",
      "Number of elements summed: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 233)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               175500    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 288,299\n",
      "Trainable params: 288,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24418, 233)\n",
      "Epoch 1/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.8154 - accuracy: 0.6989\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82461, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.8093 - accuracy: 0.7013 - val_loss: 0.4915 - val_accuracy: 0.8246\n",
      "Epoch 2/350\n",
      "736/764 [===========================>..] - ETA: 0s - loss: 0.4630 - accuracy: 0.8339\n",
      "Epoch 00002: val_accuracy improved from 0.82461 to 0.85298, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.4616 - accuracy: 0.8347 - val_loss: 0.3934 - val_accuracy: 0.8530\n",
      "Epoch 3/350\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.3606 - accuracy: 0.8720\n",
      "Epoch 00003: val_accuracy improved from 0.85298 to 0.87693, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.3591 - accuracy: 0.8727 - val_loss: 0.3357 - val_accuracy: 0.8769\n",
      "Epoch 4/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8945\n",
      "Epoch 00004: val_accuracy improved from 0.87693 to 0.89130, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.2974 - accuracy: 0.8946 - val_loss: 0.3007 - val_accuracy: 0.8913\n",
      "Epoch 5/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.9121\n",
      "Epoch 00005: val_accuracy improved from 0.89130 to 0.90015, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.2489 - accuracy: 0.9120 - val_loss: 0.2725 - val_accuracy: 0.9001\n",
      "Epoch 6/350\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9265\n",
      "Epoch 00006: val_accuracy improved from 0.90015 to 0.90789, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.2071 - accuracy: 0.9267 - val_loss: 0.2548 - val_accuracy: 0.9079\n",
      "Epoch 7/350\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9377\n",
      "Epoch 00007: val_accuracy improved from 0.90789 to 0.91489, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.9376 - val_loss: 0.2408 - val_accuracy: 0.9149\n",
      "Epoch 8/350\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.1503 - accuracy: 0.9478\n",
      "Epoch 00008: val_accuracy improved from 0.91489 to 0.91746, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.1504 - accuracy: 0.9477 - val_loss: 0.2267 - val_accuracy: 0.9175\n",
      "Epoch 9/350\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9555\n",
      "Epoch 00009: val_accuracy improved from 0.91746 to 0.92410, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.1298 - accuracy: 0.9554 - val_loss: 0.2223 - val_accuracy: 0.9241\n",
      "Epoch 10/350\n",
      "743/764 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9622\n",
      "Epoch 00010: val_accuracy improved from 0.92410 to 0.92962, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.1107 - accuracy: 0.9620 - val_loss: 0.2195 - val_accuracy: 0.9296\n",
      "Epoch 11/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9701\n",
      "Epoch 00011: val_accuracy improved from 0.92962 to 0.93073, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0920 - accuracy: 0.9700 - val_loss: 0.2118 - val_accuracy: 0.9307\n",
      "Epoch 12/350\n",
      "736/764 [===========================>..] - ETA: 0s - loss: 0.0792 - accuracy: 0.9746\n",
      "Epoch 00012: val_accuracy improved from 0.93073 to 0.93441, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.9746 - val_loss: 0.2117 - val_accuracy: 0.9344\n",
      "Epoch 13/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9787\n",
      "Epoch 00013: val_accuracy did not improve from 0.93441\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0670 - accuracy: 0.9788 - val_loss: 0.2100 - val_accuracy: 0.9344\n",
      "Epoch 14/350\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0572 - accuracy: 0.9831\n",
      "Epoch 00014: val_accuracy improved from 0.93441 to 0.93515, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0572 - accuracy: 0.9830 - val_loss: 0.2105 - val_accuracy: 0.9352\n",
      "Epoch 15/350\n",
      "741/764 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9853\n",
      "Epoch 00015: val_accuracy improved from 0.93515 to 0.93884, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0481 - accuracy: 0.9853 - val_loss: 0.2077 - val_accuracy: 0.9388\n",
      "Epoch 16/350\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9864\n",
      "Epoch 00016: val_accuracy improved from 0.93884 to 0.93957, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0427 - accuracy: 0.9864 - val_loss: 0.2104 - val_accuracy: 0.9396\n",
      "Epoch 17/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0369 - accuracy: 0.9882\n",
      "Epoch 00017: val_accuracy improved from 0.93957 to 0.93994, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0368 - accuracy: 0.9882 - val_loss: 0.2213 - val_accuracy: 0.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/350\n",
      "742/764 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9904\n",
      "Epoch 00018: val_accuracy improved from 0.93994 to 0.94068, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9904 - val_loss: 0.2198 - val_accuracy: 0.9407\n",
      "Epoch 19/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9920\n",
      "Epoch 00019: val_accuracy improved from 0.94068 to 0.94289, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0284 - accuracy: 0.9919 - val_loss: 0.2194 - val_accuracy: 0.9429\n",
      "Epoch 20/350\n",
      "733/764 [===========================>..] - ETA: 0s - loss: 0.0238 - accuracy: 0.9934\n",
      "Epoch 00020: val_accuracy did not improve from 0.94289\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0240 - accuracy: 0.9933 - val_loss: 0.2292 - val_accuracy: 0.9396\n",
      "Epoch 21/350\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9943\n",
      "Epoch 00021: val_accuracy did not improve from 0.94289\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9943 - val_loss: 0.2232 - val_accuracy: 0.9429\n",
      "Epoch 22/350\n",
      "737/764 [===========================>..] - ETA: 0s - loss: 0.0206 - accuracy: 0.9948\n",
      "Epoch 00022: val_accuracy improved from 0.94289 to 0.94547, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9948 - val_loss: 0.2265 - val_accuracy: 0.9455\n",
      "Epoch 23/350\n",
      "744/764 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9961\n",
      "Epoch 00023: val_accuracy improved from 0.94547 to 0.94768, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9961 - val_loss: 0.2346 - val_accuracy: 0.9477\n",
      "Epoch 24/350\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.0166 - accuracy: 0.9956\n",
      "Epoch 00024: val_accuracy improved from 0.94768 to 0.94805, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9956 - val_loss: 0.2333 - val_accuracy: 0.9480\n",
      "Epoch 25/350\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9967\n",
      "Epoch 00025: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9967 - val_loss: 0.2408 - val_accuracy: 0.9458\n",
      "Epoch 26/350\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9972\n",
      "Epoch 00026: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0126 - accuracy: 0.9971 - val_loss: 0.2449 - val_accuracy: 0.9469\n",
      "Epoch 27/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9972\n",
      "Epoch 00027: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9973 - val_loss: 0.2480 - val_accuracy: 0.9433\n",
      "Epoch 28/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9977\n",
      "Epoch 00028: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.2504 - val_accuracy: 0.9444\n",
      "Epoch 29/350\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9977\n",
      "Epoch 00029: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.2400 - val_accuracy: 0.9469\n",
      "Epoch 30/350\n",
      "736/764 [===========================>..] - ETA: 0s - loss: 0.0087 - accuracy: 0.9983\n",
      "Epoch 00030: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.2501 - val_accuracy: 0.9451\n",
      "Epoch 31/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9988\n",
      "Epoch 00031: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.2527 - val_accuracy: 0.9469\n",
      "Epoch 32/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9979\n",
      "Epoch 00032: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.2609 - val_accuracy: 0.9469\n",
      "Epoch 33/350\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9983\n",
      "Epoch 00033: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.2592 - val_accuracy: 0.9458\n",
      "Epoch 34/350\n",
      "738/764 [===========================>..] - ETA: 0s - loss: 0.0065 - accuracy: 0.9988\n",
      "Epoch 00034: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 0.2831 - val_accuracy: 0.9433\n",
      "Epoch 35/350\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9977\n",
      "Epoch 00035: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0097 - accuracy: 0.9977 - val_loss: 0.2749 - val_accuracy: 0.9469\n",
      "Epoch 36/350\n",
      "735/764 [===========================>..] - ETA: 0s - loss: 0.0064 - accuracy: 0.9987\n",
      "Epoch 00036: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.2666 - val_accuracy: 0.9451\n",
      "Epoch 37/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9988\n",
      "Epoch 00037: val_accuracy improved from 0.94805 to 0.94915, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 0.2690 - val_accuracy: 0.9492\n",
      "Epoch 38/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9995\n",
      "Epoch 00038: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.2742 - val_accuracy: 0.9469\n",
      "Epoch 39/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9992\n",
      "Epoch 00039: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.2732 - val_accuracy: 0.9466\n",
      "Epoch 40/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.2742 - val_accuracy: 0.9458\n",
      "Epoch 41/350\n",
      "736/764 [===========================>..] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989\n",
      "Epoch 00041: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.2707 - val_accuracy: 0.9477\n",
      "Epoch 42/350\n",
      "742/764 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 00042: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2810 - val_accuracy: 0.9480\n",
      "Epoch 43/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9992\n",
      "Epoch 00043: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 0.2755 - val_accuracy: 0.9477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/350\n",
      "734/764 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9993\n",
      "Epoch 00044: val_accuracy improved from 0.94915 to 0.94989, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.2737 - val_accuracy: 0.9499\n",
      "Epoch 45/350\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9990\n",
      "Epoch 00045: val_accuracy did not improve from 0.94989\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.2722 - val_accuracy: 0.9499\n",
      "Epoch 46/350\n",
      "731/764 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 00046: val_accuracy did not improve from 0.94989\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.2759 - val_accuracy: 0.9466\n",
      "Epoch 47/350\n",
      "739/764 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9995\n",
      "Epoch 00047: val_accuracy improved from 0.94989 to 0.95063, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.2785 - val_accuracy: 0.9506\n",
      "Epoch 48/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 00048: val_accuracy did not improve from 0.95063\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.2828 - val_accuracy: 0.9499\n",
      "Epoch 49/350\n",
      "739/764 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9992\n",
      "Epoch 00049: val_accuracy improved from 0.95063 to 0.95099, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.2806 - val_accuracy: 0.9510\n",
      "Epoch 50/350\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 00050: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.2855 - val_accuracy: 0.9510\n",
      "Epoch 51/350\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 00051: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.2784 - val_accuracy: 0.9492\n",
      "Epoch 52/350\n",
      "741/764 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 00052: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.2849 - val_accuracy: 0.9495\n",
      "Epoch 53/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9993\n",
      "Epoch 00053: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.2880 - val_accuracy: 0.9499\n",
      "Epoch 54/350\n",
      "734/764 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 00054: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.2937 - val_accuracy: 0.9469\n",
      "Epoch 55/350\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00055: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2956 - val_accuracy: 0.9480\n",
      "Epoch 56/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9996\n",
      "Epoch 00056: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2946 - val_accuracy: 0.9499\n",
      "Epoch 57/350\n",
      "743/764 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00057: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.3004 - val_accuracy: 0.9455\n",
      "Epoch 58/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.3004 - val_accuracy: 0.9484\n",
      "Epoch 59/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00059: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2965 - val_accuracy: 0.9488\n",
      "Epoch 60/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9996\n",
      "Epoch 00060: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2953 - val_accuracy: 0.9499\n",
      "Epoch 61/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9996\n",
      "Epoch 00061: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2949 - val_accuracy: 0.9480\n",
      "Epoch 62/350\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00062: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2933 - val_accuracy: 0.9484\n",
      "Epoch 63/350\n",
      "740/764 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00063: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2964 - val_accuracy: 0.9484\n",
      "Epoch 64/350\n",
      "741/764 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00064: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2996 - val_accuracy: 0.9462\n",
      "Epoch 65/350\n",
      "734/764 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00065: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2951 - val_accuracy: 0.9488\n",
      "Epoch 66/350\n",
      "744/764 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00066: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2953 - val_accuracy: 0.9492\n",
      "Epoch 67/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00067: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2945 - val_accuracy: 0.9480\n",
      "Epoch 68/350\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00068: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2975 - val_accuracy: 0.9503\n",
      "Epoch 69/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2963 - val_accuracy: 0.9499\n",
      "Epoch 70/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00070: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2996 - val_accuracy: 0.9506\n",
      "Epoch 71/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743/764 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3039 - val_accuracy: 0.9495\n",
      "Epoch 72/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 00072: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3049 - val_accuracy: 0.9506\n",
      "Epoch 73/350\n",
      "762/764 [============================>.] - ETA: 0s - loss: 9.9363e-04 - accuracy: 0.9999\n",
      "Epoch 00073: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.9288e-04 - accuracy: 0.9999 - val_loss: 0.3067 - val_accuracy: 0.9492\n",
      "Epoch 74/350\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3134 - val_accuracy: 0.9503\n",
      "Epoch 75/350\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00075: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.3111 - val_accuracy: 0.9477\n",
      "Epoch 76/350\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00076: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.3161 - val_accuracy: 0.9473\n",
      "Epoch 77/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00077: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.3108 - val_accuracy: 0.9492\n",
      "Epoch 78/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.3127 - val_accuracy: 0.9462\n",
      "Epoch 79/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3117 - val_accuracy: 0.9484\n",
      "Epoch 80/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 00080: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.3130 - val_accuracy: 0.9495\n",
      "Epoch 81/350\n",
      "734/764 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00081: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3098 - val_accuracy: 0.9499\n",
      "Epoch 82/350\n",
      "743/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 00082: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.3084 - val_accuracy: 0.9503\n",
      "Epoch 83/350\n",
      "739/764 [============================>.] - ETA: 0s - loss: 9.8283e-04 - accuracy: 0.9999\n",
      "Epoch 00083: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3124 - val_accuracy: 0.9510\n",
      "Epoch 84/350\n",
      "744/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00084: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3162 - val_accuracy: 0.9488\n",
      "Epoch 85/350\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 00085: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3059 - val_accuracy: 0.9495\n",
      "Epoch 86/350\n",
      "739/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00086: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3072 - val_accuracy: 0.9499\n",
      "Epoch 87/350\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3049 - val_accuracy: 0.9488\n",
      "Epoch 88/350\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3073 - val_accuracy: 0.9495\n",
      "Epoch 89/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00089: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3110 - val_accuracy: 0.9492\n",
      "Epoch 90/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00090: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3144 - val_accuracy: 0.9506\n",
      "Epoch 91/350\n",
      "741/764 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00091: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3145 - val_accuracy: 0.9503\n",
      "Epoch 92/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00092: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3278 - val_accuracy: 0.9495\n",
      "Epoch 93/350\n",
      "755/764 [============================>.] - ETA: 0s - loss: 9.2258e-04 - accuracy: 0.9999\n",
      "Epoch 00093: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.1965e-04 - accuracy: 0.9999 - val_loss: 0.3214 - val_accuracy: 0.9488\n",
      "Epoch 94/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00094: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.3167 - val_accuracy: 0.9503\n",
      "Epoch 95/350\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 00095: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.3089 - val_accuracy: 0.9506\n",
      "Epoch 96/350\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00096: val_accuracy improved from 0.95099 to 0.95173, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3184 - val_accuracy: 0.9517\n",
      "Epoch 97/350\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00097: val_accuracy did not improve from 0.95173\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3218 - val_accuracy: 0.9499\n",
      "Epoch 98/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy improved from 0.95173 to 0.95210, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3225 - val_accuracy: 0.9521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 8.7839e-04 - accuracy: 0.9999\n",
      "Epoch 00099: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 8.7096e-04 - accuracy: 0.9999 - val_loss: 0.3212 - val_accuracy: 0.9521\n",
      "Epoch 100/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 9.5819e-04 - accuracy: 0.9999\n",
      "Epoch 00100: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 9.5811e-04 - accuracy: 0.9999 - val_loss: 0.3252 - val_accuracy: 0.9514\n",
      "Epoch 101/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 7.3124e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 7.2800e-04 - accuracy: 1.0000 - val_loss: 0.3260 - val_accuracy: 0.9503\n",
      "Epoch 102/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00102: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.3229 - val_accuracy: 0.9499\n",
      "Epoch 103/350\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3273 - val_accuracy: 0.9506\n",
      "Epoch 104/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 6.8103e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 6.8797e-04 - accuracy: 0.9999 - val_loss: 0.3287 - val_accuracy: 0.9517\n",
      "Epoch 105/350\n",
      "750/764 [============================>.] - ETA: 0s - loss: 8.8775e-04 - accuracy: 0.9999\n",
      "Epoch 00105: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 8.8259e-04 - accuracy: 0.9999 - val_loss: 0.3295 - val_accuracy: 0.9514\n",
      "Epoch 106/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 8.8855e-04 - accuracy: 0.9998\n",
      "Epoch 00106: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 8.8843e-04 - accuracy: 0.9998 - val_loss: 0.3262 - val_accuracy: 0.9521\n",
      "Epoch 107/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00107: val_accuracy improved from 0.95210 to 0.95247, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.9877e-04 - accuracy: 0.9998 - val_loss: 0.3241 - val_accuracy: 0.9525\n",
      "Epoch 108/350\n",
      "733/764 [===========================>..] - ETA: 0s - loss: 7.5342e-04 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.5786e-04 - accuracy: 1.0000 - val_loss: 0.3258 - val_accuracy: 0.9503\n",
      "Epoch 109/350\n",
      "738/764 [===========================>..] - ETA: 0s - loss: 9.1189e-04 - accuracy: 0.9998\n",
      "Epoch 00109: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 8.9264e-04 - accuracy: 0.9998 - val_loss: 0.3263 - val_accuracy: 0.9506\n",
      "Epoch 110/350\n",
      "752/764 [============================>.] - ETA: 0s - loss: 7.6014e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 7.5461e-04 - accuracy: 0.9999 - val_loss: 0.3285 - val_accuracy: 0.9492\n",
      "Epoch 111/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 5.7749e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 5.8197e-04 - accuracy: 1.0000 - val_loss: 0.3283 - val_accuracy: 0.9480\n",
      "Epoch 112/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 7.6135e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.6109e-04 - accuracy: 0.9998 - val_loss: 0.3280 - val_accuracy: 0.9495\n",
      "Epoch 113/350\n",
      "743/764 [============================>.] - ETA: 0s - loss: 9.7460e-04 - accuracy: 0.9998\n",
      "Epoch 00113: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.7290e-04 - accuracy: 0.9998 - val_loss: 0.3251 - val_accuracy: 0.9480\n",
      "Epoch 114/350\n",
      "754/764 [============================>.] - ETA: 0s - loss: 6.9651e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.2860e-04 - accuracy: 0.9999 - val_loss: 0.3286 - val_accuracy: 0.9484\n",
      "Epoch 115/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 9.1768e-04 - accuracy: 0.9997\n",
      "Epoch 00115: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.0590e-04 - accuracy: 0.9998 - val_loss: 0.3275 - val_accuracy: 0.9495\n",
      "Epoch 116/350\n",
      "742/764 [============================>.] - ETA: 0s - loss: 9.5076e-04 - accuracy: 0.9998\n",
      "Epoch 00116: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.3935e-04 - accuracy: 0.9998 - val_loss: 0.3316 - val_accuracy: 0.9473\n",
      "Epoch 117/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00117: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3233 - val_accuracy: 0.9484\n",
      "Epoch 118/350\n",
      "738/764 [===========================>..] - ETA: 0s - loss: 8.8365e-04 - accuracy: 0.9999\n",
      "Epoch 00118: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.1470e-04 - accuracy: 0.9998 - val_loss: 0.3304 - val_accuracy: 0.9492\n",
      "Epoch 119/350\n",
      "737/764 [===========================>..] - ETA: 0s - loss: 7.2382e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.1051e-04 - accuracy: 0.9999 - val_loss: 0.3257 - val_accuracy: 0.9492\n",
      "Epoch 120/350\n",
      "752/764 [============================>.] - ETA: 0s - loss: 6.2829e-04 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 6.2325e-04 - accuracy: 1.0000 - val_loss: 0.3263 - val_accuracy: 0.9506\n",
      "Epoch 121/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 8.1448e-04 - accuracy: 0.9999\n",
      "Epoch 00121: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 8.1176e-04 - accuracy: 0.9999 - val_loss: 0.3284 - val_accuracy: 0.9506\n",
      "Epoch 122/350\n",
      "756/764 [============================>.] - ETA: 0s - loss: 7.7739e-04 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.7177e-04 - accuracy: 0.9999 - val_loss: 0.3315 - val_accuracy: 0.9499\n",
      "Epoch 123/350\n",
      "753/764 [============================>.] - ETA: 0s - loss: 5.4531e-04 - accuracy: 0.9999\n",
      "Epoch 00123: val_accuracy did not improve from 0.95247\n",
      "Restoring model weights from the end of the best epoch.\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 5.4452e-04 - accuracy: 0.9999 - val_loss: 0.3299 - val_accuracy: 0.9506\n",
      "Epoch 00123: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:50<02:50, 170.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.78      0.81      0.80       840\n",
      "        car_horn       0.86      0.98      0.91       301\n",
      "children_playing       0.78      0.78      0.78       700\n",
      "        dog_bark       0.84      0.80      0.82       700\n",
      "           siren       0.88      0.84      0.86       833\n",
      "\n",
      "        accuracy                           0.82      3374\n",
      "       macro avg       0.83      0.84      0.83      3374\n",
      "    weighted avg       0.82      0.82      0.82      3374\n",
      "\n",
      "Model: \"Model_CNN_1D_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 227, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 227, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 227, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6328)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                316450    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 327,491\n",
      "Trainable params: 327,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24418, 233, 1)\n",
      "Epoch 1/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.7465 - accuracy: 0.7609\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83014, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 4s 5ms/step - loss: 0.7465 - accuracy: 0.7609 - val_loss: 0.5227 - val_accuracy: 0.8301\n",
      "Epoch 2/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.4914 - accuracy: 0.8483\n",
      "Epoch 00002: val_accuracy improved from 0.83014 to 0.85814, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.4905 - accuracy: 0.8485 - val_loss: 0.4525 - val_accuracy: 0.8581\n",
      "Epoch 3/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8752\n",
      "Epoch 00003: val_accuracy improved from 0.85814 to 0.86035, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.4213 - accuracy: 0.8751 - val_loss: 0.4483 - val_accuracy: 0.8604\n",
      "Epoch 4/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.3722 - accuracy: 0.8917\n",
      "Epoch 00004: val_accuracy improved from 0.86035 to 0.86809, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.3727 - accuracy: 0.8915 - val_loss: 0.4307 - val_accuracy: 0.8681\n",
      "Epoch 5/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.9020\n",
      "Epoch 00005: val_accuracy improved from 0.86809 to 0.88025, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 3s 4ms/step - loss: 0.3439 - accuracy: 0.9019 - val_loss: 0.4044 - val_accuracy: 0.8803\n",
      "Epoch 6/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.3205 - accuracy: 0.9110\n",
      "Epoch 00006: val_accuracy did not improve from 0.88025\n",
      "764/764 [==============================] - 6s 8ms/step - loss: 0.3205 - accuracy: 0.9110 - val_loss: 0.3984 - val_accuracy: 0.8755\n",
      "Epoch 7/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9201\n",
      "Epoch 00007: val_accuracy did not improve from 0.88025\n",
      "764/764 [==============================] - 4s 5ms/step - loss: 0.2958 - accuracy: 0.9201 - val_loss: 0.4128 - val_accuracy: 0.8721\n",
      "Epoch 8/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.9234\n",
      "Epoch 00008: val_accuracy improved from 0.88025 to 0.89278, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2805 - accuracy: 0.9235 - val_loss: 0.3608 - val_accuracy: 0.8928\n",
      "Epoch 9/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.9267\n",
      "Epoch 00009: val_accuracy did not improve from 0.89278\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2697 - accuracy: 0.9266 - val_loss: 0.3755 - val_accuracy: 0.8880\n",
      "Epoch 10/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9325\n",
      "Epoch 00010: val_accuracy improved from 0.89278 to 0.89462, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2579 - accuracy: 0.9322 - val_loss: 0.3572 - val_accuracy: 0.8946\n",
      "Epoch 11/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.2386 - accuracy: 0.9386\n",
      "Epoch 00011: val_accuracy improved from 0.89462 to 0.89757, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2382 - accuracy: 0.9389 - val_loss: 0.3481 - val_accuracy: 0.8976\n",
      "Epoch 12/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9388\n",
      "Epoch 00012: val_accuracy improved from 0.89757 to 0.89867, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2356 - accuracy: 0.9386 - val_loss: 0.3564 - val_accuracy: 0.8987\n",
      "Epoch 13/150\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.2179 - accuracy: 0.9441\n",
      "Epoch 00013: val_accuracy did not improve from 0.89867\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2179 - accuracy: 0.9441 - val_loss: 0.3646 - val_accuracy: 0.8972\n",
      "Epoch 14/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9467\n",
      "Epoch 00014: val_accuracy improved from 0.89867 to 0.89978, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2130 - accuracy: 0.9466 - val_loss: 0.3497 - val_accuracy: 0.8998\n",
      "Epoch 15/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.2111 - accuracy: 0.9454\n",
      "Epoch 00015: val_accuracy improved from 0.89978 to 0.90052, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 3s 3ms/step - loss: 0.2108 - accuracy: 0.9455 - val_loss: 0.3472 - val_accuracy: 0.9005\n",
      "Epoch 16/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9488\n",
      "Epoch 00016: val_accuracy did not improve from 0.90052\n",
      "764/764 [==============================] - 3s 3ms/step - loss: 0.2006 - accuracy: 0.9488 - val_loss: 0.3839 - val_accuracy: 0.8917\n",
      "Epoch 17/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751/764 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9522\n",
      "Epoch 00017: val_accuracy improved from 0.90052 to 0.90125, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1930 - accuracy: 0.9521 - val_loss: 0.3539 - val_accuracy: 0.9013\n",
      "Epoch 18/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.1851 - accuracy: 0.9553\n",
      "Epoch 00018: val_accuracy improved from 0.90125 to 0.90567, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1853 - accuracy: 0.9552 - val_loss: 0.3535 - val_accuracy: 0.9057\n",
      "Epoch 19/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9543\n",
      "Epoch 00019: val_accuracy improved from 0.90567 to 0.90789, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1848 - accuracy: 0.9542 - val_loss: 0.3405 - val_accuracy: 0.9079\n",
      "Epoch 20/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9563\n",
      "Epoch 00020: val_accuracy did not improve from 0.90789\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1793 - accuracy: 0.9563 - val_loss: 0.3566 - val_accuracy: 0.9001\n",
      "Epoch 21/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.1738 - accuracy: 0.9589\n",
      "Epoch 00021: val_accuracy improved from 0.90789 to 0.90825, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1738 - accuracy: 0.9589 - val_loss: 0.3391 - val_accuracy: 0.9083\n",
      "Epoch 22/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.9613\n",
      "Epoch 00022: val_accuracy did not improve from 0.90825\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1648 - accuracy: 0.9614 - val_loss: 0.3558 - val_accuracy: 0.9038\n",
      "Epoch 23/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.1629 - accuracy: 0.9633\n",
      "Epoch 00023: val_accuracy did not improve from 0.90825\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1626 - accuracy: 0.9635 - val_loss: 0.3554 - val_accuracy: 0.9024\n",
      "Epoch 24/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1601 - accuracy: 0.9629\n",
      "Epoch 00024: val_accuracy improved from 0.90825 to 0.91304, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1607 - accuracy: 0.9627 - val_loss: 0.3384 - val_accuracy: 0.9130\n",
      "Epoch 25/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9637\n",
      "Epoch 00025: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1569 - accuracy: 0.9636 - val_loss: 0.3406 - val_accuracy: 0.9031\n",
      "Epoch 26/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1508 - accuracy: 0.9657\n",
      "Epoch 00026: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1510 - accuracy: 0.9658 - val_loss: 0.3613 - val_accuracy: 0.9083\n",
      "Epoch 27/150\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9652\n",
      "Epoch 00027: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1504 - accuracy: 0.9652 - val_loss: 0.3489 - val_accuracy: 0.9071\n",
      "Epoch 28/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.9648\n",
      "Epoch 00028: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1525 - accuracy: 0.9649 - val_loss: 0.3559 - val_accuracy: 0.9035\n",
      "Epoch 29/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9661\n",
      "Epoch 00029: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1478 - accuracy: 0.9660 - val_loss: 0.3542 - val_accuracy: 0.9086\n",
      "Epoch 30/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9691\n",
      "Epoch 00030: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1406 - accuracy: 0.9692 - val_loss: 0.3426 - val_accuracy: 0.9123\n",
      "Epoch 31/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.1456 - accuracy: 0.9674\n",
      "Epoch 00031: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1457 - accuracy: 0.9673 - val_loss: 0.3382 - val_accuracy: 0.9108\n",
      "Epoch 32/150\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9677\n",
      "Epoch 00032: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1394 - accuracy: 0.9679 - val_loss: 0.3325 - val_accuracy: 0.9097\n",
      "Epoch 33/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.1346 - accuracy: 0.9705\n",
      "Epoch 00033: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1345 - accuracy: 0.9706 - val_loss: 0.3556 - val_accuracy: 0.9086\n",
      "Epoch 34/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9710\n",
      "Epoch 00034: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1352 - accuracy: 0.9710 - val_loss: 0.3410 - val_accuracy: 0.9119\n",
      "Epoch 35/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9704\n",
      "Epoch 00035: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1362 - accuracy: 0.9703 - val_loss: 0.3413 - val_accuracy: 0.9112\n",
      "Epoch 36/150\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.9708\n",
      "Epoch 00036: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.9708 - val_loss: 0.3550 - val_accuracy: 0.9060\n",
      "Epoch 37/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9750\n",
      "Epoch 00037: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1227 - accuracy: 0.9750 - val_loss: 0.3531 - val_accuracy: 0.9119\n",
      "Epoch 38/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9704\n",
      "Epoch 00038: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9706 - val_loss: 0.3507 - val_accuracy: 0.9071\n",
      "Epoch 39/150\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9715\n",
      "Epoch 00039: val_accuracy improved from 0.91304 to 0.91341, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9712 - val_loss: 0.3496 - val_accuracy: 0.9134\n",
      "Epoch 40/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9732\n",
      "Epoch 00040: val_accuracy did not improve from 0.91341\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1234 - accuracy: 0.9733 - val_loss: 0.3571 - val_accuracy: 0.9123\n",
      "Epoch 41/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9748\n",
      "Epoch 00041: val_accuracy improved from 0.91341 to 0.91378, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1213 - accuracy: 0.9746 - val_loss: 0.3423 - val_accuracy: 0.9138\n",
      "Epoch 42/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759/764 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9765\n",
      "Epoch 00042: val_accuracy did not improve from 0.91378\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1170 - accuracy: 0.9766 - val_loss: 0.3623 - val_accuracy: 0.9108\n",
      "Epoch 43/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9754\n",
      "Epoch 00043: val_accuracy improved from 0.91378 to 0.91525, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1161 - accuracy: 0.9755 - val_loss: 0.3575 - val_accuracy: 0.9153\n",
      "Epoch 44/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9733\n",
      "Epoch 00044: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1207 - accuracy: 0.9735 - val_loss: 0.3734 - val_accuracy: 0.9075\n",
      "Epoch 45/150\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9755\n",
      "Epoch 00045: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1171 - accuracy: 0.9753 - val_loss: 0.3696 - val_accuracy: 0.9112\n",
      "Epoch 46/150\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9766\n",
      "Epoch 00046: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1156 - accuracy: 0.9765 - val_loss: 0.3504 - val_accuracy: 0.9090\n",
      "Epoch 47/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9778\n",
      "Epoch 00047: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1127 - accuracy: 0.9777 - val_loss: 0.3613 - val_accuracy: 0.9075\n",
      "Epoch 48/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.1102 - accuracy: 0.9768\n",
      "Epoch 00048: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1103 - accuracy: 0.9768 - val_loss: 0.3633 - val_accuracy: 0.9108\n",
      "Epoch 49/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9752\n",
      "Epoch 00049: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1148 - accuracy: 0.9751 - val_loss: 0.3585 - val_accuracy: 0.9119\n",
      "Epoch 50/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.1078 - accuracy: 0.9787\n",
      "Epoch 00050: val_accuracy improved from 0.91525 to 0.91746, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1077 - accuracy: 0.9787 - val_loss: 0.3424 - val_accuracy: 0.9175\n",
      "Epoch 51/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9772\n",
      "Epoch 00051: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1083 - accuracy: 0.9775 - val_loss: 0.3759 - val_accuracy: 0.9153\n",
      "Epoch 52/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9777\n",
      "Epoch 00052: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1089 - accuracy: 0.9775 - val_loss: 0.3581 - val_accuracy: 0.9156\n",
      "Epoch 53/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9795\n",
      "Epoch 00053: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1042 - accuracy: 0.9795 - val_loss: 0.3719 - val_accuracy: 0.9083\n",
      "Epoch 54/150\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.1056 - accuracy: 0.9791\n",
      "Epoch 00054: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1057 - accuracy: 0.9791 - val_loss: 0.3524 - val_accuracy: 0.9153\n",
      "Epoch 55/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9801\n",
      "Epoch 00055: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1006 - accuracy: 0.9801 - val_loss: 0.3720 - val_accuracy: 0.9127\n",
      "Epoch 56/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9791\n",
      "Epoch 00056: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1034 - accuracy: 0.9790 - val_loss: 0.3536 - val_accuracy: 0.9119\n",
      "Epoch 57/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9804\n",
      "Epoch 00057: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9803 - val_loss: 0.3682 - val_accuracy: 0.9094\n",
      "Epoch 58/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1018 - accuracy: 0.9803\n",
      "Epoch 00058: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1014 - accuracy: 0.9805 - val_loss: 0.3537 - val_accuracy: 0.9112\n",
      "Epoch 59/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.9820\n",
      "Epoch 00059: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0954 - accuracy: 0.9820 - val_loss: 0.3593 - val_accuracy: 0.9130\n",
      "Epoch 60/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0965 - accuracy: 0.9816\n",
      "Epoch 00060: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0966 - accuracy: 0.9816 - val_loss: 0.3748 - val_accuracy: 0.9112\n",
      "Epoch 61/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0978 - accuracy: 0.9814\n",
      "Epoch 00061: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0977 - accuracy: 0.9815 - val_loss: 0.3528 - val_accuracy: 0.9127\n",
      "Epoch 62/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0969 - accuracy: 0.9810\n",
      "Epoch 00062: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0967 - accuracy: 0.9812 - val_loss: 0.3675 - val_accuracy: 0.9130\n",
      "Epoch 63/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9800\n",
      "Epoch 00063: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0983 - accuracy: 0.9800 - val_loss: 0.3744 - val_accuracy: 0.9108\n",
      "Epoch 64/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.0927 - accuracy: 0.9822\n",
      "Epoch 00064: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0932 - accuracy: 0.9821 - val_loss: 0.3623 - val_accuracy: 0.9116\n",
      "Epoch 65/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9810\n",
      "Epoch 00065: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0988 - accuracy: 0.9811 - val_loss: 0.3525 - val_accuracy: 0.9149\n",
      "Epoch 66/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0934 - accuracy: 0.9818\n",
      "Epoch 00066: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0933 - accuracy: 0.9819 - val_loss: 0.3545 - val_accuracy: 0.9141\n",
      "Epoch 67/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9813\n",
      "Epoch 00067: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0943 - accuracy: 0.9815 - val_loss: 0.3739 - val_accuracy: 0.9116\n",
      "Epoch 68/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9813\n",
      "Epoch 00068: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0931 - accuracy: 0.9814 - val_loss: 0.3565 - val_accuracy: 0.9108\n",
      "Epoch 69/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9825\n",
      "Epoch 00069: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0933 - accuracy: 0.9825 - val_loss: 0.3528 - val_accuracy: 0.9105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9833\n",
      "Epoch 00070: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0889 - accuracy: 0.9834 - val_loss: 0.3716 - val_accuracy: 0.9127\n",
      "Epoch 71/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9829\n",
      "Epoch 00071: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0909 - accuracy: 0.9828 - val_loss: 0.3647 - val_accuracy: 0.9141\n",
      "Epoch 72/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9830\n",
      "Epoch 00072: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0897 - accuracy: 0.9828 - val_loss: 0.3797 - val_accuracy: 0.9068\n",
      "Epoch 73/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9841\n",
      "Epoch 00073: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0871 - accuracy: 0.9842 - val_loss: 0.3767 - val_accuracy: 0.9097\n",
      "Epoch 74/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9822\n",
      "Epoch 00074: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0891 - accuracy: 0.9821 - val_loss: 0.3596 - val_accuracy: 0.9156\n",
      "Epoch 75/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9823\n",
      "Epoch 00075: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0886 - accuracy: 0.9823 - val_loss: 0.3597 - val_accuracy: 0.9141\n",
      "Epoch 76/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9828\n",
      "Epoch 00076: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0897 - accuracy: 0.9828 - val_loss: 0.3743 - val_accuracy: 0.9127\n",
      "Epoch 77/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9826\n",
      "Epoch 00077: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0905 - accuracy: 0.9826 - val_loss: 0.3664 - val_accuracy: 0.9153\n",
      "Epoch 78/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9832\n",
      "Epoch 00078: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9833 - val_loss: 0.3671 - val_accuracy: 0.9116\n",
      "Epoch 79/150\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9825\n",
      "Epoch 00079: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9824 - val_loss: 0.3708 - val_accuracy: 0.9119\n",
      "Epoch 80/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9840\n",
      "Epoch 00080: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0859 - accuracy: 0.9840 - val_loss: 0.3890 - val_accuracy: 0.9086\n",
      "Epoch 81/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9840\n",
      "Epoch 00081: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0843 - accuracy: 0.9840 - val_loss: 0.3628 - val_accuracy: 0.9112\n",
      "Epoch 82/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0806 - accuracy: 0.9851\n",
      "Epoch 00082: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0808 - accuracy: 0.9849 - val_loss: 0.3532 - val_accuracy: 0.9160\n",
      "Epoch 83/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9855\n",
      "Epoch 00083: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9855 - val_loss: 0.3663 - val_accuracy: 0.9153\n",
      "Epoch 84/150\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9820\n",
      "Epoch 00084: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0882 - accuracy: 0.9820 - val_loss: 0.3906 - val_accuracy: 0.9116\n",
      "Epoch 85/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9832\n",
      "Epoch 00085: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0843 - accuracy: 0.9831 - val_loss: 0.3688 - val_accuracy: 0.9138\n",
      "Epoch 86/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0830 - accuracy: 0.9842\n",
      "Epoch 00086: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0829 - accuracy: 0.9843 - val_loss: 0.3676 - val_accuracy: 0.9149\n",
      "Epoch 87/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9852\n",
      "Epoch 00087: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9852 - val_loss: 0.3689 - val_accuracy: 0.9145\n",
      "Epoch 88/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9856\n",
      "Epoch 00088: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0790 - accuracy: 0.9857 - val_loss: 0.3901 - val_accuracy: 0.9079\n",
      "Epoch 89/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9862\n",
      "Epoch 00089: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9863 - val_loss: 0.3778 - val_accuracy: 0.9153\n",
      "Epoch 90/150\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9869\n",
      "Epoch 00090: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0762 - accuracy: 0.9869 - val_loss: 0.3565 - val_accuracy: 0.9145\n",
      "Epoch 91/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9867\n",
      "Epoch 00091: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9866 - val_loss: 0.3644 - val_accuracy: 0.9145\n",
      "Epoch 92/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9869\n",
      "Epoch 00092: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9870 - val_loss: 0.3704 - val_accuracy: 0.9116\n",
      "Epoch 93/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9850\n",
      "Epoch 00093: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0822 - accuracy: 0.9850 - val_loss: 0.3753 - val_accuracy: 0.9167\n",
      "Epoch 94/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9850\n",
      "Epoch 00094: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0784 - accuracy: 0.9851 - val_loss: 0.3823 - val_accuracy: 0.9145\n",
      "Epoch 95/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9867\n",
      "Epoch 00095: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9867 - val_loss: 0.3874 - val_accuracy: 0.9105\n",
      "Epoch 96/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9852\n",
      "Epoch 00096: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0783 - accuracy: 0.9851 - val_loss: 0.3758 - val_accuracy: 0.9112\n",
      "Epoch 97/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9874\n",
      "Epoch 00097: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0735 - accuracy: 0.9873 - val_loss: 0.3783 - val_accuracy: 0.9123\n",
      "Epoch 98/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9870\n",
      "Epoch 00098: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9871 - val_loss: 0.3870 - val_accuracy: 0.9156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9843\n",
      "Epoch 00099: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9844 - val_loss: 0.3903 - val_accuracy: 0.9123\n",
      "Epoch 100/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9867\n",
      "Epoch 00100: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0747 - accuracy: 0.9866 - val_loss: 0.3732 - val_accuracy: 0.9149\n",
      "Epoch 101/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9876\n",
      "Epoch 00101: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0720 - accuracy: 0.9877 - val_loss: 0.3844 - val_accuracy: 0.9123\n",
      "Epoch 102/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9881\n",
      "Epoch 00102: val_accuracy improved from 0.91746 to 0.91968, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9882 - val_loss: 0.3739 - val_accuracy: 0.9197\n",
      "Epoch 103/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9877\n",
      "Epoch 00103: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9877 - val_loss: 0.3618 - val_accuracy: 0.9141\n",
      "Epoch 104/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9868\n",
      "Epoch 00104: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9867 - val_loss: 0.3746 - val_accuracy: 0.9171\n",
      "Epoch 105/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9873\n",
      "Epoch 00105: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9873 - val_loss: 0.3573 - val_accuracy: 0.9175\n",
      "Epoch 106/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9865\n",
      "Epoch 00106: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9865 - val_loss: 0.3769 - val_accuracy: 0.9153\n",
      "Epoch 107/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9854\n",
      "Epoch 00107: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9854 - val_loss: 0.3673 - val_accuracy: 0.9156\n",
      "Epoch 108/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9865\n",
      "Epoch 00108: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9865 - val_loss: 0.3836 - val_accuracy: 0.9160\n",
      "Epoch 109/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9865\n",
      "Epoch 00109: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0725 - accuracy: 0.9865 - val_loss: 0.3744 - val_accuracy: 0.9130\n",
      "Epoch 110/150\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9853\n",
      "Epoch 00110: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0762 - accuracy: 0.9853 - val_loss: 0.3682 - val_accuracy: 0.9134\n",
      "Epoch 111/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9868\n",
      "Epoch 00111: val_accuracy improved from 0.91968 to 0.92004, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0722 - accuracy: 0.9868 - val_loss: 0.3661 - val_accuracy: 0.9200\n",
      "Epoch 112/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9863\n",
      "Epoch 00112: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9864 - val_loss: 0.3795 - val_accuracy: 0.9079\n",
      "Epoch 113/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9877\n",
      "Epoch 00113: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9878 - val_loss: 0.3934 - val_accuracy: 0.9145\n",
      "Epoch 114/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9877\n",
      "Epoch 00114: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0717 - accuracy: 0.9877 - val_loss: 0.3774 - val_accuracy: 0.9160\n",
      "Epoch 115/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9881\n",
      "Epoch 00115: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0682 - accuracy: 0.9880 - val_loss: 0.3739 - val_accuracy: 0.9145\n",
      "Epoch 116/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9870\n",
      "Epoch 00116: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0708 - accuracy: 0.9870 - val_loss: 0.3761 - val_accuracy: 0.9200\n",
      "Epoch 117/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9871\n",
      "Epoch 00117: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0702 - accuracy: 0.9871 - val_loss: 0.3742 - val_accuracy: 0.9156\n",
      "Epoch 118/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9869\n",
      "Epoch 00118: val_accuracy improved from 0.92004 to 0.92115, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0712 - accuracy: 0.9869 - val_loss: 0.3606 - val_accuracy: 0.9211\n",
      "Epoch 119/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9877\n",
      "Epoch 00119: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9876 - val_loss: 0.3649 - val_accuracy: 0.9189\n",
      "Epoch 120/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9906\n",
      "Epoch 00120: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0615 - accuracy: 0.9906 - val_loss: 0.3717 - val_accuracy: 0.9186\n",
      "Epoch 121/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9892\n",
      "Epoch 00121: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0642 - accuracy: 0.9892 - val_loss: 0.3785 - val_accuracy: 0.9178\n",
      "Epoch 122/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9886\n",
      "Epoch 00122: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0653 - accuracy: 0.9887 - val_loss: 0.3729 - val_accuracy: 0.9145\n",
      "Epoch 123/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9891\n",
      "Epoch 00123: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0645 - accuracy: 0.9891 - val_loss: 0.3862 - val_accuracy: 0.9149\n",
      "Epoch 124/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9895\n",
      "Epoch 00124: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9896 - val_loss: 0.3938 - val_accuracy: 0.9171\n",
      "Epoch 125/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9880\n",
      "Epoch 00125: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9880 - val_loss: 0.3821 - val_accuracy: 0.9153\n",
      "Epoch 126/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/764 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9892\n",
      "Epoch 00126: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0641 - accuracy: 0.9893 - val_loss: 0.3867 - val_accuracy: 0.9127\n",
      "Epoch 127/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9881\n",
      "Epoch 00127: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9881 - val_loss: 0.4080 - val_accuracy: 0.9134\n",
      "Epoch 128/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9873\n",
      "Epoch 00128: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0693 - accuracy: 0.9872 - val_loss: 0.3694 - val_accuracy: 0.9153\n",
      "Epoch 129/150\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9884\n",
      "Epoch 00129: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0653 - accuracy: 0.9884 - val_loss: 0.3890 - val_accuracy: 0.9130\n",
      "Epoch 130/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9900\n",
      "Epoch 00130: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0616 - accuracy: 0.9900 - val_loss: 0.3814 - val_accuracy: 0.9156\n",
      "Epoch 131/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9881\n",
      "Epoch 00131: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9881 - val_loss: 0.3861 - val_accuracy: 0.9156\n",
      "Epoch 132/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9886\n",
      "Epoch 00132: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0637 - accuracy: 0.9886 - val_loss: 0.3969 - val_accuracy: 0.9130\n",
      "Epoch 133/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9881\n",
      "Epoch 00133: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0679 - accuracy: 0.9881 - val_loss: 0.3788 - val_accuracy: 0.9193\n",
      "Epoch 134/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9892\n",
      "Epoch 00134: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9892 - val_loss: 0.3760 - val_accuracy: 0.9175\n",
      "Epoch 135/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9877\n",
      "Epoch 00135: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0667 - accuracy: 0.9878 - val_loss: 0.4076 - val_accuracy: 0.9141\n",
      "Epoch 136/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9889\n",
      "Epoch 00136: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0626 - accuracy: 0.9889 - val_loss: 0.3936 - val_accuracy: 0.9119\n",
      "Epoch 137/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9897\n",
      "Epoch 00137: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0616 - accuracy: 0.9897 - val_loss: 0.3803 - val_accuracy: 0.9171\n",
      "Epoch 138/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9892\n",
      "Epoch 00138: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9892 - val_loss: 0.3825 - val_accuracy: 0.9193\n",
      "Epoch 139/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9889\n",
      "Epoch 00139: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0646 - accuracy: 0.9889 - val_loss: 0.3731 - val_accuracy: 0.9164\n",
      "Epoch 140/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9900\n",
      "Epoch 00140: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9901 - val_loss: 0.3859 - val_accuracy: 0.9145\n",
      "Epoch 141/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9896\n",
      "Epoch 00141: val_accuracy improved from 0.92115 to 0.92189, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0623 - accuracy: 0.9896 - val_loss: 0.3830 - val_accuracy: 0.9219\n",
      "Epoch 142/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9899\n",
      "Epoch 00142: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9899 - val_loss: 0.3943 - val_accuracy: 0.9193\n",
      "Epoch 143/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9879\n",
      "Epoch 00143: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9879 - val_loss: 0.3847 - val_accuracy: 0.9141\n",
      "Epoch 144/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9890\n",
      "Epoch 00144: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0627 - accuracy: 0.9889 - val_loss: 0.3733 - val_accuracy: 0.9186\n",
      "Epoch 145/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9888\n",
      "Epoch 00145: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0631 - accuracy: 0.9887 - val_loss: 0.3830 - val_accuracy: 0.9189\n",
      "Epoch 146/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0579 - accuracy: 0.9905\n",
      "Epoch 00146: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0579 - accuracy: 0.9905 - val_loss: 0.3885 - val_accuracy: 0.9171\n",
      "Epoch 147/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9871\n",
      "Epoch 00147: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0654 - accuracy: 0.9871 - val_loss: 0.3860 - val_accuracy: 0.9171\n",
      "Epoch 148/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9892\n",
      "Epoch 00148: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0629 - accuracy: 0.9890 - val_loss: 0.3990 - val_accuracy: 0.9167\n",
      "Epoch 149/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9909\n",
      "Epoch 00149: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0583 - accuracy: 0.9907 - val_loss: 0.3927 - val_accuracy: 0.9156\n",
      "Epoch 150/150\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9889\n",
      "Epoch 00150: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0626 - accuracy: 0.9888 - val_loss: 0.4004 - val_accuracy: 0.9108\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:42<00:00, 261.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.76      0.83      0.79       840\n",
      "        car_horn       0.87      0.96      0.91       301\n",
      "children_playing       0.75      0.79      0.77       700\n",
      "        dog_bark       0.81      0.77      0.79       700\n",
      "           siren       0.90      0.78      0.84       833\n",
      "\n",
      "        accuracy                           0.81      3374\n",
      "       macro avg       0.82      0.83      0.82      3374\n",
      "    weighted avg       0.81      0.81      0.81      3374\n",
      "\n",
      "Validation fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(26733, 375)\n",
      "X_val_norm shape.....:(3773, 375)\n",
      "\n",
      "Sum of elements: 0.9803348031146931\n",
      "Number of elements summed: 237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 237)               56406     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 237)               56406     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 237)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               178500    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 295,067\n",
      "Trainable params: 295,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24059, 237)\n",
      "Epoch 1/350\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.7819 - accuracy: 0.7090\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83583, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 2ms/step - loss: 0.7787 - accuracy: 0.7105 - val_loss: 0.4704 - val_accuracy: 0.8358\n",
      "Epoch 2/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.4350 - accuracy: 0.8470\n",
      "Epoch 00002: val_accuracy improved from 0.83583 to 0.87360, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.4338 - accuracy: 0.8475 - val_loss: 0.3601 - val_accuracy: 0.8736\n",
      "Epoch 3/350\n",
      "722/752 [===========================>..] - ETA: 0s - loss: 0.3344 - accuracy: 0.8806\n",
      "Epoch 00003: val_accuracy improved from 0.87360 to 0.89155, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.8807 - val_loss: 0.3039 - val_accuracy: 0.8915\n",
      "Epoch 4/350\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.9037\n",
      "Epoch 00004: val_accuracy improved from 0.89155 to 0.90726, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.2772 - accuracy: 0.9037 - val_loss: 0.2643 - val_accuracy: 0.9073\n",
      "Epoch 5/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.2256 - accuracy: 0.9217\n",
      "Epoch 00005: val_accuracy improved from 0.90726 to 0.91324, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.2249 - accuracy: 0.9219 - val_loss: 0.2416 - val_accuracy: 0.9132\n",
      "Epoch 6/350\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.9347\n",
      "Epoch 00006: val_accuracy improved from 0.91324 to 0.92221, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9347 - val_loss: 0.2234 - val_accuracy: 0.9222\n",
      "Epoch 7/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9458\n",
      "Epoch 00007: val_accuracy improved from 0.92221 to 0.93231, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.1602 - accuracy: 0.9459 - val_loss: 0.2060 - val_accuracy: 0.9323\n",
      "Epoch 8/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.9563\n",
      "Epoch 00008: val_accuracy improved from 0.93231 to 0.93455, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.1314 - accuracy: 0.9562 - val_loss: 0.1985 - val_accuracy: 0.9346\n",
      "Epoch 9/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.1104 - accuracy: 0.9640\n",
      "Epoch 00009: val_accuracy improved from 0.93455 to 0.93680, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.1094 - accuracy: 0.9645 - val_loss: 0.1920 - val_accuracy: 0.9368\n",
      "Epoch 10/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9705\n",
      "Epoch 00010: val_accuracy improved from 0.93680 to 0.93979, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0947 - accuracy: 0.9702 - val_loss: 0.1874 - val_accuracy: 0.9398\n",
      "Epoch 11/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9742\n",
      "Epoch 00011: val_accuracy improved from 0.93979 to 0.94241, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0802 - accuracy: 0.9742 - val_loss: 0.1768 - val_accuracy: 0.9424\n",
      "Epoch 12/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9799\n",
      "Epoch 00012: val_accuracy did not improve from 0.94241\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0654 - accuracy: 0.9800 - val_loss: 0.1815 - val_accuracy: 0.9405\n",
      "Epoch 13/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9830\n",
      "Epoch 00013: val_accuracy improved from 0.94241 to 0.94428, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0561 - accuracy: 0.9830 - val_loss: 0.1773 - val_accuracy: 0.9443\n",
      "Epoch 14/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9867\n",
      "Epoch 00014: val_accuracy improved from 0.94428 to 0.94615, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0457 - accuracy: 0.9866 - val_loss: 0.1793 - val_accuracy: 0.9461\n",
      "Epoch 15/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9875\n",
      "Epoch 00015: val_accuracy improved from 0.94615 to 0.94914, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9874 - val_loss: 0.1797 - val_accuracy: 0.9491\n",
      "Epoch 16/350\n",
      "723/752 [===========================>..] - ETA: 0s - loss: 0.0358 - accuracy: 0.9898\n",
      "Epoch 00016: val_accuracy did not improve from 0.94914\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0355 - accuracy: 0.9901 - val_loss: 0.1805 - val_accuracy: 0.9491\n",
      "Epoch 17/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9925\n",
      "Epoch 00017: val_accuracy improved from 0.94914 to 0.94951, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0279 - accuracy: 0.9924 - val_loss: 0.1817 - val_accuracy: 0.9495\n",
      "Epoch 18/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737/752 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9930\n",
      "Epoch 00018: val_accuracy did not improve from 0.94951\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0256 - accuracy: 0.9931 - val_loss: 0.1881 - val_accuracy: 0.9480\n",
      "Epoch 19/350\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0205 - accuracy: 0.9941\n",
      "Epoch 00019: val_accuracy improved from 0.94951 to 0.95101, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9941 - val_loss: 0.1926 - val_accuracy: 0.9510\n",
      "Epoch 20/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9949\n",
      "Epoch 00020: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9949 - val_loss: 0.1881 - val_accuracy: 0.9488\n",
      "Epoch 21/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9953\n",
      "Epoch 00021: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1966 - val_accuracy: 0.9506\n",
      "Epoch 22/350\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9966\n",
      "Epoch 00022: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 0.2042 - val_accuracy: 0.9499\n",
      "Epoch 23/350\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9967\n",
      "Epoch 00023: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0143 - accuracy: 0.9966 - val_loss: 0.2042 - val_accuracy: 0.9469\n",
      "Epoch 24/350\n",
      "729/752 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9973\n",
      "Epoch 00024: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0121 - accuracy: 0.9973 - val_loss: 0.2025 - val_accuracy: 0.9503\n",
      "Epoch 25/350\n",
      "727/752 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9967\n",
      "Epoch 00025: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9968 - val_loss: 0.2036 - val_accuracy: 0.9488\n",
      "Epoch 26/350\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9976\n",
      "Epoch 00026: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9975 - val_loss: 0.2140 - val_accuracy: 0.9491\n",
      "Epoch 27/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9979\n",
      "Epoch 00027: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.2091 - val_accuracy: 0.9499\n",
      "Epoch 28/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9984\n",
      "Epoch 00028: val_accuracy improved from 0.95101 to 0.95251, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.2108 - val_accuracy: 0.9525\n",
      "Epoch 29/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9984\n",
      "Epoch 00029: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.2196 - val_accuracy: 0.9521\n",
      "Epoch 30/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9980\n",
      "Epoch 00030: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.2173 - val_accuracy: 0.9514\n",
      "Epoch 31/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9988\n",
      "Epoch 00031: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.2230 - val_accuracy: 0.9506\n",
      "Epoch 32/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9980\n",
      "Epoch 00032: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.2210 - val_accuracy: 0.9495\n",
      "Epoch 33/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9988\n",
      "Epoch 00033: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 0.2161 - val_accuracy: 0.9518\n",
      "Epoch 34/350\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9991\n",
      "Epoch 00034: val_accuracy improved from 0.95251 to 0.95325, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.2125 - val_accuracy: 0.9533\n",
      "Epoch 35/350\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9992\n",
      "Epoch 00035: val_accuracy did not improve from 0.95325\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.2135 - val_accuracy: 0.9518\n",
      "Epoch 36/350\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9991\n",
      "Epoch 00036: val_accuracy did not improve from 0.95325\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.2133 - val_accuracy: 0.9514\n",
      "Epoch 37/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 00037: val_accuracy improved from 0.95325 to 0.95400, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.2203 - val_accuracy: 0.9540\n",
      "Epoch 38/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9989\n",
      "Epoch 00038: val_accuracy did not improve from 0.95400\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2247 - val_accuracy: 0.9529\n",
      "Epoch 39/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9997\n",
      "Epoch 00039: val_accuracy did not improve from 0.95400\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 0.2240 - val_accuracy: 0.9529\n",
      "Epoch 40/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.95400\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.2262 - val_accuracy: 0.9518\n",
      "Epoch 41/350\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 00041: val_accuracy improved from 0.95400 to 0.95475, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.2234 - val_accuracy: 0.9547\n",
      "Epoch 42/350\n",
      "732/752 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9993\n",
      "Epoch 00042: val_accuracy did not improve from 0.95475\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.2223 - val_accuracy: 0.9521\n",
      "Epoch 43/350\n",
      "727/752 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00043: val_accuracy did not improve from 0.95475\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.2224 - val_accuracy: 0.9529\n",
      "Epoch 44/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726/752 [===========================>..] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00044: val_accuracy did not improve from 0.95475\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2223 - val_accuracy: 0.9529\n",
      "Epoch 45/350\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9998\n",
      "Epoch 00045: val_accuracy improved from 0.95475 to 0.95587, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.2249 - val_accuracy: 0.9559\n",
      "Epoch 46/350\n",
      "725/752 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 00046: val_accuracy did not improve from 0.95587\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.2310 - val_accuracy: 0.9510\n",
      "Epoch 47/350\n",
      "726/752 [===========================>..] - ETA: 0s - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 00047: val_accuracy did not improve from 0.95587\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.2241 - val_accuracy: 0.9525\n",
      "Epoch 48/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00048: val_accuracy improved from 0.95587 to 0.95662, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2236 - val_accuracy: 0.9566\n",
      "Epoch 49/350\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9997\n",
      "Epoch 00049: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.2305 - val_accuracy: 0.9555\n",
      "Epoch 50/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 00050: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2308 - val_accuracy: 0.9555\n",
      "Epoch 51/350\n",
      "722/752 [===========================>..] - ETA: 0s - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 00051: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.2299 - val_accuracy: 0.9555\n",
      "Epoch 52/350\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9998\n",
      "Epoch 00052: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.2340 - val_accuracy: 0.9544\n",
      "Epoch 53/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9997\n",
      "Epoch 00053: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.2396 - val_accuracy: 0.9544\n",
      "Epoch 54/350\n",
      "724/752 [===========================>..] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00054: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.2367 - val_accuracy: 0.9540\n",
      "Epoch 55/350\n",
      "724/752 [===========================>..] - ETA: 0s - loss: 0.0017 - accuracy: 0.9999\n",
      "Epoch 00055: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.2387 - val_accuracy: 0.9544\n",
      "Epoch 56/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00056: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2475 - val_accuracy: 0.9510\n",
      "Epoch 57/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00057: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2433 - val_accuracy: 0.9559\n",
      "Epoch 58/350\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2426 - val_accuracy: 0.9547\n",
      "Epoch 59/350\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2436 - val_accuracy: 0.9559\n",
      "Epoch 60/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00060: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.2546 - val_accuracy: 0.9551\n",
      "Epoch 61/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00061: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2499 - val_accuracy: 0.9547\n",
      "Epoch 62/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9996\n",
      "Epoch 00062: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2456 - val_accuracy: 0.9559\n",
      "Epoch 63/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00063: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2444 - val_accuracy: 0.9547\n",
      "Epoch 64/350\n",
      "727/752 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 00064: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2538 - val_accuracy: 0.9540\n",
      "Epoch 65/350\n",
      "720/752 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00065: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2498 - val_accuracy: 0.9547\n",
      "Epoch 66/350\n",
      "724/752 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00066: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2513 - val_accuracy: 0.9547\n",
      "Epoch 67/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 00067: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.2536 - val_accuracy: 0.9544\n",
      "Epoch 68/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.2560 - val_accuracy: 0.9521\n",
      "Epoch 69/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2566 - val_accuracy: 0.9533\n",
      "Epoch 70/350\n",
      "724/752 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00070: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2572 - val_accuracy: 0.9529\n",
      "Epoch 71/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00071: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2603 - val_accuracy: 0.9547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00072: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2529 - val_accuracy: 0.9544\n",
      "Epoch 73/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00073: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2542 - val_accuracy: 0.9529\n",
      "Epoch 74/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2559 - val_accuracy: 0.9525\n",
      "Epoch 75/350\n",
      "732/752 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00075: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.2612 - val_accuracy: 0.9529\n",
      "Epoch 76/350\n",
      "740/752 [============================>.] - ETA: 0s - loss: 9.3636e-04 - accuracy: 0.9999\n",
      "Epoch 00076: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.7911e-04 - accuracy: 0.9999 - val_loss: 0.2552 - val_accuracy: 0.9525\n",
      "Epoch 77/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00077: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2547 - val_accuracy: 0.9529\n",
      "Epoch 78/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 7.7684e-04 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.7569e-04 - accuracy: 0.9999 - val_loss: 0.2528 - val_accuracy: 0.9521\n",
      "Epoch 79/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00079: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2587 - val_accuracy: 0.9533\n",
      "Epoch 80/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00080: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2631 - val_accuracy: 0.9536\n",
      "Epoch 81/350\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00081: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2556 - val_accuracy: 0.9533\n",
      "Epoch 82/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00082: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2536 - val_accuracy: 0.9529\n",
      "Epoch 83/350\n",
      "720/752 [===========================>..] - ETA: 0s - loss: 8.9969e-04 - accuracy: 0.9999\n",
      "Epoch 00083: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.0779e-04 - accuracy: 0.9999 - val_loss: 0.2567 - val_accuracy: 0.9533\n",
      "Epoch 84/350\n",
      "721/752 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998  \n",
      "Epoch 00084: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2594 - val_accuracy: 0.9544\n",
      "Epoch 85/350\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00085: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2581 - val_accuracy: 0.9544\n",
      "Epoch 86/350\n",
      "739/752 [============================>.] - ETA: 0s - loss: 9.3011e-04 - accuracy: 0.9999\n",
      "Epoch 00086: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.3475e-04 - accuracy: 0.9999 - val_loss: 0.2606 - val_accuracy: 0.9529\n",
      "Epoch 87/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 9.5449e-04 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.5361e-04 - accuracy: 0.9998 - val_loss: 0.2639 - val_accuracy: 0.9536\n",
      "Epoch 88/350\n",
      "743/752 [============================>.] - ETA: 0s - loss: 9.0137e-04 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.9457e-04 - accuracy: 0.9998 - val_loss: 0.2633 - val_accuracy: 0.9544\n",
      "Epoch 89/350\n",
      "719/752 [===========================>..] - ETA: 0s - loss: 9.2043e-04 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.0831e-04 - accuracy: 0.9998 - val_loss: 0.2608 - val_accuracy: 0.9540\n",
      "Epoch 90/350\n",
      "751/752 [============================>.] - ETA: 0s - loss: 8.6417e-04 - accuracy: 0.9999\n",
      "Epoch 00090: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.6333e-04 - accuracy: 0.9999 - val_loss: 0.2591 - val_accuracy: 0.9566\n",
      "Epoch 91/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 9.4409e-04 - accuracy: 0.9998\n",
      "Epoch 00091: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.4071e-04 - accuracy: 0.9998 - val_loss: 0.2644 - val_accuracy: 0.9551\n",
      "Epoch 92/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 7.3997e-04 - accuracy: 1.0000\n",
      "Epoch 00092: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.3488e-04 - accuracy: 1.0000 - val_loss: 0.2582 - val_accuracy: 0.9544\n",
      "Epoch 93/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 8.6813e-04 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.6743e-04 - accuracy: 0.9998 - val_loss: 0.2574 - val_accuracy: 0.9566\n",
      "Epoch 94/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 9.4020e-04 - accuracy: 0.9998\n",
      "Epoch 00094: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.3513e-04 - accuracy: 0.9998 - val_loss: 0.2559 - val_accuracy: 0.9547\n",
      "Epoch 95/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 8.2032e-04 - accuracy: 0.9998\n",
      "Epoch 00095: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.1385e-04 - accuracy: 0.9998 - val_loss: 0.2580 - val_accuracy: 0.9559\n",
      "Epoch 96/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2560 - val_accuracy: 0.9555\n",
      "Epoch 97/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 7.3361e-04 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.9007e-04 - accuracy: 0.9999 - val_loss: 0.2539 - val_accuracy: 0.9547\n",
      "Epoch 98/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 7.0821e-04 - accuracy: 0.9999\n",
      "Epoch 00098: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.4546e-04 - accuracy: 0.9999 - val_loss: 0.2513 - val_accuracy: 0.9551\n",
      "Epoch 99/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 9.1096e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.0452e-04 - accuracy: 0.9998 - val_loss: 0.2572 - val_accuracy: 0.9551\n",
      "Epoch 100/350\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00100: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2571 - val_accuracy: 0.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/350\n",
      "746/752 [============================>.] - ETA: 0s - loss: 7.7267e-04 - accuracy: 0.9999\n",
      "Epoch 00101: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.6910e-04 - accuracy: 0.9999 - val_loss: 0.2536 - val_accuracy: 0.9540\n",
      "Epoch 102/350\n",
      "749/752 [============================>.] - ETA: 0s - loss: 8.0665e-04 - accuracy: 0.9999\n",
      "Epoch 00102: val_accuracy improved from 0.95662 to 0.95699, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.0448e-04 - accuracy: 0.9999 - val_loss: 0.2488 - val_accuracy: 0.9570\n",
      "Epoch 103/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 7.0428e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 0.95699\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.0140e-04 - accuracy: 1.0000 - val_loss: 0.2515 - val_accuracy: 0.9544\n",
      "Epoch 104/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 6.5164e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95699\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 6.6094e-04 - accuracy: 0.9999 - val_loss: 0.2562 - val_accuracy: 0.9540\n",
      "Epoch 105/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 6.0028e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.95699\n",
      "Restoring model weights from the end of the best epoch.\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 5.9939e-04 - accuracy: 1.0000 - val_loss: 0.2632 - val_accuracy: 0.9544\n",
      "Epoch 00105: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:15<02:15, 135.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.71      0.79      0.75       798\n",
      "        car_horn       0.82      0.59      0.68       413\n",
      "children_playing       0.66      0.64      0.65       700\n",
      "        dog_bark       0.69      0.84      0.76       700\n",
      "           siren       0.91      0.83      0.87      1162\n",
      "\n",
      "        accuracy                           0.76      3773\n",
      "       macro avg       0.76      0.74      0.74      3773\n",
      "    weighted avg       0.77      0.76      0.76      3773\n",
      "\n",
      "Model: \"Model_CNN_1D_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 231, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 231, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 231, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 115, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 115, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6440)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                322050    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 333,091\n",
      "Trainable params: 333,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24059, 237, 1)\n",
      "Epoch 1/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.7161 - accuracy: 0.7722\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85116, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 4s 5ms/step - loss: 0.7161 - accuracy: 0.7722 - val_loss: 0.5091 - val_accuracy: 0.8512\n",
      "Epoch 2/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.8568\n",
      "Epoch 00002: val_accuracy improved from 0.85116 to 0.87472, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.4741 - accuracy: 0.8569 - val_loss: 0.4358 - val_accuracy: 0.8747\n",
      "Epoch 3/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8791\n",
      "Epoch 00003: val_accuracy did not improve from 0.87472\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.4073 - accuracy: 0.8787 - val_loss: 0.4249 - val_accuracy: 0.8672\n",
      "Epoch 4/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.8946\n",
      "Epoch 00004: val_accuracy improved from 0.87472 to 0.89043, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.3653 - accuracy: 0.8947 - val_loss: 0.3876 - val_accuracy: 0.8904\n",
      "Epoch 5/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.9049\n",
      "Epoch 00005: val_accuracy did not improve from 0.89043\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.3328 - accuracy: 0.9049 - val_loss: 0.3917 - val_accuracy: 0.8882\n",
      "Epoch 6/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.9136\n",
      "Epoch 00006: val_accuracy improved from 0.89043 to 0.89604, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.3102 - accuracy: 0.9137 - val_loss: 0.3555 - val_accuracy: 0.8960\n",
      "Epoch 7/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9196\n",
      "Epoch 00007: val_accuracy improved from 0.89604 to 0.90202, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2892 - accuracy: 0.9197 - val_loss: 0.3530 - val_accuracy: 0.9020\n",
      "Epoch 8/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.2726 - accuracy: 0.9270\n",
      "Epoch 00008: val_accuracy did not improve from 0.90202\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2726 - accuracy: 0.9269 - val_loss: 0.3497 - val_accuracy: 0.9016\n",
      "Epoch 9/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.2611 - accuracy: 0.9306\n",
      "Epoch 00009: val_accuracy improved from 0.90202 to 0.90800, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2617 - accuracy: 0.9305 - val_loss: 0.3363 - val_accuracy: 0.9080\n",
      "Epoch 10/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.9322\n",
      "Epoch 00010: val_accuracy did not improve from 0.90800\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2504 - accuracy: 0.9320 - val_loss: 0.3453 - val_accuracy: 0.9035\n",
      "Epoch 11/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9373\n",
      "Epoch 00011: val_accuracy did not improve from 0.90800\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2371 - accuracy: 0.9369 - val_loss: 0.3394 - val_accuracy: 0.9024\n",
      "Epoch 12/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9416\n",
      "Epoch 00012: val_accuracy improved from 0.90800 to 0.90987, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2261 - accuracy: 0.9419 - val_loss: 0.3320 - val_accuracy: 0.9099\n",
      "Epoch 13/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9443\n",
      "Epoch 00013: val_accuracy improved from 0.90987 to 0.91062, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2161 - accuracy: 0.9444 - val_loss: 0.3328 - val_accuracy: 0.9106\n",
      "Epoch 14/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.2110 - accuracy: 0.9465\n",
      "Epoch 00014: val_accuracy did not improve from 0.91062\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2110 - accuracy: 0.9465 - val_loss: 0.3464 - val_accuracy: 0.8998\n",
      "Epoch 15/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9499\n",
      "Epoch 00015: val_accuracy improved from 0.91062 to 0.91548, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2015 - accuracy: 0.9496 - val_loss: 0.3258 - val_accuracy: 0.9155\n",
      "Epoch 16/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9501\n",
      "Epoch 00016: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1972 - accuracy: 0.9502 - val_loss: 0.3278 - val_accuracy: 0.9106\n",
      "Epoch 17/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9531\n",
      "Epoch 00017: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1895 - accuracy: 0.9530 - val_loss: 0.3259 - val_accuracy: 0.9132\n",
      "Epoch 18/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748/752 [============================>.] - ETA: 0s - loss: 0.1832 - accuracy: 0.9544\n",
      "Epoch 00018: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1834 - accuracy: 0.9542 - val_loss: 0.3326 - val_accuracy: 0.9106\n",
      "Epoch 19/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9544\n",
      "Epoch 00019: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1815 - accuracy: 0.9544 - val_loss: 0.3300 - val_accuracy: 0.9144\n",
      "Epoch 20/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.1738 - accuracy: 0.9570\n",
      "Epoch 00020: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1738 - accuracy: 0.9571 - val_loss: 0.3358 - val_accuracy: 0.9132\n",
      "Epoch 21/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.1674 - accuracy: 0.9611\n",
      "Epoch 00021: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1677 - accuracy: 0.9608 - val_loss: 0.3446 - val_accuracy: 0.9069\n",
      "Epoch 22/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.9608\n",
      "Epoch 00022: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1642 - accuracy: 0.9609 - val_loss: 0.3320 - val_accuracy: 0.9144\n",
      "Epoch 23/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.1616 - accuracy: 0.9606\n",
      "Epoch 00023: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1615 - accuracy: 0.9606 - val_loss: 0.3312 - val_accuracy: 0.9117\n",
      "Epoch 24/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9621\n",
      "Epoch 00024: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1603 - accuracy: 0.9619 - val_loss: 0.3226 - val_accuracy: 0.9155\n",
      "Epoch 25/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.1569 - accuracy: 0.9622\n",
      "Epoch 00025: val_accuracy improved from 0.91548 to 0.91586, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1568 - accuracy: 0.9621 - val_loss: 0.3342 - val_accuracy: 0.9159\n",
      "Epoch 26/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.9627\n",
      "Epoch 00026: val_accuracy improved from 0.91586 to 0.91623, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1526 - accuracy: 0.9627 - val_loss: 0.3383 - val_accuracy: 0.9162\n",
      "Epoch 27/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.9666\n",
      "Epoch 00027: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1449 - accuracy: 0.9665 - val_loss: 0.3354 - val_accuracy: 0.9114\n",
      "Epoch 28/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.1440 - accuracy: 0.9664\n",
      "Epoch 00028: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1445 - accuracy: 0.9663 - val_loss: 0.3437 - val_accuracy: 0.9114\n",
      "Epoch 29/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.1414 - accuracy: 0.9683\n",
      "Epoch 00029: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1417 - accuracy: 0.9680 - val_loss: 0.3484 - val_accuracy: 0.9091\n",
      "Epoch 30/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9671\n",
      "Epoch 00030: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1416 - accuracy: 0.9671 - val_loss: 0.3445 - val_accuracy: 0.9132\n",
      "Epoch 31/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9703\n",
      "Epoch 00031: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1362 - accuracy: 0.9704 - val_loss: 0.3328 - val_accuracy: 0.9159\n",
      "Epoch 32/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9699\n",
      "Epoch 00032: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1325 - accuracy: 0.9697 - val_loss: 0.3382 - val_accuracy: 0.9129\n",
      "Epoch 33/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9716\n",
      "Epoch 00033: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1307 - accuracy: 0.9712 - val_loss: 0.3379 - val_accuracy: 0.9159\n",
      "Epoch 34/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9697\n",
      "Epoch 00034: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1309 - accuracy: 0.9695 - val_loss: 0.3431 - val_accuracy: 0.9125\n",
      "Epoch 35/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9728\n",
      "Epoch 00035: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1266 - accuracy: 0.9729 - val_loss: 0.3426 - val_accuracy: 0.9117\n",
      "Epoch 36/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9719\n",
      "Epoch 00036: val_accuracy improved from 0.91623 to 0.91997, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1272 - accuracy: 0.9720 - val_loss: 0.3443 - val_accuracy: 0.9200\n",
      "Epoch 37/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9713\n",
      "Epoch 00037: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1251 - accuracy: 0.9712 - val_loss: 0.3498 - val_accuracy: 0.9121\n",
      "Epoch 38/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9740\n",
      "Epoch 00038: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1207 - accuracy: 0.9739 - val_loss: 0.3435 - val_accuracy: 0.9177\n",
      "Epoch 39/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9739\n",
      "Epoch 00039: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1207 - accuracy: 0.9736 - val_loss: 0.3386 - val_accuracy: 0.9140\n",
      "Epoch 40/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9750\n",
      "Epoch 00040: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1182 - accuracy: 0.9749 - val_loss: 0.3390 - val_accuracy: 0.9159\n",
      "Epoch 41/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9739\n",
      "Epoch 00041: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1193 - accuracy: 0.9738 - val_loss: 0.3538 - val_accuracy: 0.9147\n",
      "Epoch 42/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9730\n",
      "Epoch 00042: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1208 - accuracy: 0.9727 - val_loss: 0.3407 - val_accuracy: 0.9151\n",
      "Epoch 43/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1117 - accuracy: 0.9775\n",
      "Epoch 00043: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1118 - accuracy: 0.9774 - val_loss: 0.3555 - val_accuracy: 0.9155\n",
      "Epoch 44/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9767\n",
      "Epoch 00044: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1110 - accuracy: 0.9767 - val_loss: 0.3516 - val_accuracy: 0.9185\n",
      "Epoch 45/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/752 [============================>.] - ETA: 0s - loss: 0.1116 - accuracy: 0.9771\n",
      "Epoch 00045: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1116 - accuracy: 0.9770 - val_loss: 0.3684 - val_accuracy: 0.9185\n",
      "Epoch 46/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9774\n",
      "Epoch 00046: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1102 - accuracy: 0.9773 - val_loss: 0.3585 - val_accuracy: 0.9151\n",
      "Epoch 47/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9771\n",
      "Epoch 00047: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1059 - accuracy: 0.9772 - val_loss: 0.3693 - val_accuracy: 0.9110\n",
      "Epoch 48/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9776\n",
      "Epoch 00048: val_accuracy improved from 0.91997 to 0.92147, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1091 - accuracy: 0.9775 - val_loss: 0.3551 - val_accuracy: 0.9215\n",
      "Epoch 49/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9776\n",
      "Epoch 00049: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1083 - accuracy: 0.9776 - val_loss: 0.3674 - val_accuracy: 0.9166\n",
      "Epoch 50/150\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.1056 - accuracy: 0.9784\n",
      "Epoch 00050: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1057 - accuracy: 0.9784 - val_loss: 0.3660 - val_accuracy: 0.9155\n",
      "Epoch 51/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1016 - accuracy: 0.9794\n",
      "Epoch 00051: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9794 - val_loss: 0.3659 - val_accuracy: 0.9170\n",
      "Epoch 52/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9798\n",
      "Epoch 00052: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9796 - val_loss: 0.3546 - val_accuracy: 0.9211\n",
      "Epoch 53/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.1028 - accuracy: 0.9791\n",
      "Epoch 00053: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1026 - accuracy: 0.9793 - val_loss: 0.3663 - val_accuracy: 0.9174\n",
      "Epoch 54/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9806\n",
      "Epoch 00054: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0996 - accuracy: 0.9806 - val_loss: 0.3608 - val_accuracy: 0.9207\n",
      "Epoch 55/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9787\n",
      "Epoch 00055: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1005 - accuracy: 0.9789 - val_loss: 0.3671 - val_accuracy: 0.9177\n",
      "Epoch 56/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9797\n",
      "Epoch 00056: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1015 - accuracy: 0.9797 - val_loss: 0.3680 - val_accuracy: 0.9174\n",
      "Epoch 57/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9795\n",
      "Epoch 00057: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0983 - accuracy: 0.9796 - val_loss: 0.3660 - val_accuracy: 0.9192\n",
      "Epoch 58/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9796\n",
      "Epoch 00058: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0988 - accuracy: 0.9796 - val_loss: 0.3556 - val_accuracy: 0.9215\n",
      "Epoch 59/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0953 - accuracy: 0.9807\n",
      "Epoch 00059: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0952 - accuracy: 0.9807 - val_loss: 0.3603 - val_accuracy: 0.9144\n",
      "Epoch 60/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9784\n",
      "Epoch 00060: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1011 - accuracy: 0.9783 - val_loss: 0.3601 - val_accuracy: 0.9181\n",
      "Epoch 61/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9806\n",
      "Epoch 00061: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0981 - accuracy: 0.9807 - val_loss: 0.3583 - val_accuracy: 0.9196\n",
      "Epoch 62/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9815\n",
      "Epoch 00062: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0933 - accuracy: 0.9815 - val_loss: 0.3658 - val_accuracy: 0.9125\n",
      "Epoch 63/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9801\n",
      "Epoch 00063: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0951 - accuracy: 0.9803 - val_loss: 0.3623 - val_accuracy: 0.9155\n",
      "Epoch 64/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9819\n",
      "Epoch 00064: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9819 - val_loss: 0.3815 - val_accuracy: 0.9106\n",
      "Epoch 65/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9805\n",
      "Epoch 00065: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9805 - val_loss: 0.3756 - val_accuracy: 0.9185\n",
      "Epoch 66/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 0.9808\n",
      "Epoch 00066: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0923 - accuracy: 0.9808 - val_loss: 0.3693 - val_accuracy: 0.9177\n",
      "Epoch 67/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9825\n",
      "Epoch 00067: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0900 - accuracy: 0.9826 - val_loss: 0.3683 - val_accuracy: 0.9196\n",
      "Epoch 68/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0889 - accuracy: 0.9833\n",
      "Epoch 00068: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0889 - accuracy: 0.9832 - val_loss: 0.3668 - val_accuracy: 0.9181\n",
      "Epoch 69/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9817\n",
      "Epoch 00069: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0907 - accuracy: 0.9817 - val_loss: 0.3661 - val_accuracy: 0.9181\n",
      "Epoch 70/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9828\n",
      "Epoch 00070: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9828 - val_loss: 0.3583 - val_accuracy: 0.9162\n",
      "Epoch 71/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9825\n",
      "Epoch 00071: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0883 - accuracy: 0.9825 - val_loss: 0.3713 - val_accuracy: 0.9203\n",
      "Epoch 72/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9856\n",
      "Epoch 00072: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0804 - accuracy: 0.9856 - val_loss: 0.3745 - val_accuracy: 0.9200\n",
      "Epoch 73/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9849\n",
      "Epoch 00073: val_accuracy improved from 0.92147 to 0.92221, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0852 - accuracy: 0.9847 - val_loss: 0.3535 - val_accuracy: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9844\n",
      "Epoch 00074: val_accuracy improved from 0.92221 to 0.92334, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0853 - accuracy: 0.9842 - val_loss: 0.3525 - val_accuracy: 0.9233\n",
      "Epoch 75/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9855\n",
      "Epoch 00075: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9854 - val_loss: 0.3727 - val_accuracy: 0.9218\n",
      "Epoch 76/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0836 - accuracy: 0.9835\n",
      "Epoch 00076: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0836 - accuracy: 0.9835 - val_loss: 0.3713 - val_accuracy: 0.9185\n",
      "Epoch 77/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9834\n",
      "Epoch 00077: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0831 - accuracy: 0.9835 - val_loss: 0.3844 - val_accuracy: 0.9174\n",
      "Epoch 78/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9833\n",
      "Epoch 00078: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0842 - accuracy: 0.9832 - val_loss: 0.3655 - val_accuracy: 0.9185\n",
      "Epoch 79/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9851\n",
      "Epoch 00079: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9852 - val_loss: 0.3712 - val_accuracy: 0.9188\n",
      "Epoch 80/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9846\n",
      "Epoch 00080: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0808 - accuracy: 0.9847 - val_loss: 0.3678 - val_accuracy: 0.9188\n",
      "Epoch 81/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9849\n",
      "Epoch 00081: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0788 - accuracy: 0.9849 - val_loss: 0.3711 - val_accuracy: 0.9226\n",
      "Epoch 82/150\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9838\n",
      "Epoch 00082: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9838 - val_loss: 0.3682 - val_accuracy: 0.9222\n",
      "Epoch 83/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9842\n",
      "Epoch 00083: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0797 - accuracy: 0.9842 - val_loss: 0.3645 - val_accuracy: 0.9200\n",
      "Epoch 84/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9857\n",
      "Epoch 00084: val_accuracy improved from 0.92334 to 0.92371, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0787 - accuracy: 0.9856 - val_loss: 0.3627 - val_accuracy: 0.9237\n",
      "Epoch 85/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9841\n",
      "Epoch 00085: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0806 - accuracy: 0.9841 - val_loss: 0.3732 - val_accuracy: 0.9215\n",
      "Epoch 86/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0831 - accuracy: 0.9838\n",
      "Epoch 00086: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0831 - accuracy: 0.9838 - val_loss: 0.3754 - val_accuracy: 0.9200\n",
      "Epoch 87/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9880\n",
      "Epoch 00087: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9879 - val_loss: 0.3794 - val_accuracy: 0.9188\n",
      "Epoch 88/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9852\n",
      "Epoch 00088: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9853 - val_loss: 0.3627 - val_accuracy: 0.9181\n",
      "Epoch 89/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9861\n",
      "Epoch 00089: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9859 - val_loss: 0.3655 - val_accuracy: 0.9188\n",
      "Epoch 90/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9863\n",
      "Epoch 00090: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0750 - accuracy: 0.9862 - val_loss: 0.3783 - val_accuracy: 0.9174\n",
      "Epoch 91/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9865\n",
      "Epoch 00091: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0749 - accuracy: 0.9866 - val_loss: 0.3842 - val_accuracy: 0.9181\n",
      "Epoch 92/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9873\n",
      "Epoch 00092: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9872 - val_loss: 0.3809 - val_accuracy: 0.9222\n",
      "Epoch 93/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9850\n",
      "Epoch 00093: val_accuracy improved from 0.92371 to 0.92670, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0777 - accuracy: 0.9848 - val_loss: 0.3774 - val_accuracy: 0.9267\n",
      "Epoch 94/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9857\n",
      "Epoch 00094: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0754 - accuracy: 0.9857 - val_loss: 0.3784 - val_accuracy: 0.9196\n",
      "Epoch 95/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9877\n",
      "Epoch 00095: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0731 - accuracy: 0.9877 - val_loss: 0.3941 - val_accuracy: 0.9218\n",
      "Epoch 96/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9873\n",
      "Epoch 00096: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0719 - accuracy: 0.9875 - val_loss: 0.3841 - val_accuracy: 0.9211\n",
      "Epoch 97/150\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9864\n",
      "Epoch 00097: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0723 - accuracy: 0.9864 - val_loss: 0.3848 - val_accuracy: 0.9226\n",
      "Epoch 98/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9877\n",
      "Epoch 00098: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9877 - val_loss: 0.3732 - val_accuracy: 0.9230\n",
      "Epoch 99/150\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9868\n",
      "Epoch 00099: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9867 - val_loss: 0.3763 - val_accuracy: 0.9192\n",
      "Epoch 100/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9862\n",
      "Epoch 00100: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0731 - accuracy: 0.9862 - val_loss: 0.3811 - val_accuracy: 0.9196\n",
      "Epoch 101/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/752 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9881\n",
      "Epoch 00101: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0677 - accuracy: 0.9881 - val_loss: 0.3686 - val_accuracy: 0.9215\n",
      "Epoch 102/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9860\n",
      "Epoch 00102: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0737 - accuracy: 0.9861 - val_loss: 0.3866 - val_accuracy: 0.9177\n",
      "Epoch 103/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9873\n",
      "Epoch 00103: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9872 - val_loss: 0.3691 - val_accuracy: 0.9230\n",
      "Epoch 104/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9875\n",
      "Epoch 00104: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9877 - val_loss: 0.3723 - val_accuracy: 0.9237\n",
      "Epoch 105/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 0.9881\n",
      "Epoch 00105: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0691 - accuracy: 0.9880 - val_loss: 0.3606 - val_accuracy: 0.9218\n",
      "Epoch 106/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9880\n",
      "Epoch 00106: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9879 - val_loss: 0.3938 - val_accuracy: 0.9211\n",
      "Epoch 107/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9857\n",
      "Epoch 00107: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0719 - accuracy: 0.9856 - val_loss: 0.3969 - val_accuracy: 0.9196\n",
      "Epoch 108/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9874\n",
      "Epoch 00108: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0690 - accuracy: 0.9874 - val_loss: 0.3643 - val_accuracy: 0.9267\n",
      "Epoch 109/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9877\n",
      "Epoch 00109: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9876 - val_loss: 0.3844 - val_accuracy: 0.9211\n",
      "Epoch 110/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9871\n",
      "Epoch 00110: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0704 - accuracy: 0.9872 - val_loss: 0.3520 - val_accuracy: 0.9248\n",
      "Epoch 111/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9878\n",
      "Epoch 00111: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9878 - val_loss: 0.3638 - val_accuracy: 0.9233\n",
      "Epoch 112/150\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9858\n",
      "Epoch 00112: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9857 - val_loss: 0.3568 - val_accuracy: 0.9203\n",
      "Epoch 113/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9866\n",
      "Epoch 00113: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0705 - accuracy: 0.9867 - val_loss: 0.3643 - val_accuracy: 0.9211\n",
      "Epoch 114/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9870\n",
      "Epoch 00114: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0687 - accuracy: 0.9870 - val_loss: 0.3651 - val_accuracy: 0.9237\n",
      "Epoch 115/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9871\n",
      "Epoch 00115: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0692 - accuracy: 0.9870 - val_loss: 0.3854 - val_accuracy: 0.9207\n",
      "Epoch 116/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9876\n",
      "Epoch 00116: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0672 - accuracy: 0.9875 - val_loss: 0.3809 - val_accuracy: 0.9218\n",
      "Epoch 117/150\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9881\n",
      "Epoch 00117: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9881 - val_loss: 0.3634 - val_accuracy: 0.9267\n",
      "Epoch 118/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9895\n",
      "Epoch 00118: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0638 - accuracy: 0.9894 - val_loss: 0.3745 - val_accuracy: 0.9237\n",
      "Epoch 119/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9871\n",
      "Epoch 00119: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9872 - val_loss: 0.3772 - val_accuracy: 0.9230\n",
      "Epoch 120/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9887\n",
      "Epoch 00120: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0628 - accuracy: 0.9887 - val_loss: 0.3683 - val_accuracy: 0.9252\n",
      "Epoch 121/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9870\n",
      "Epoch 00121: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9869 - val_loss: 0.3802 - val_accuracy: 0.9218\n",
      "Epoch 122/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9892\n",
      "Epoch 00122: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0636 - accuracy: 0.9890 - val_loss: 0.3772 - val_accuracy: 0.9211\n",
      "Epoch 123/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0632 - accuracy: 0.9886\n",
      "Epoch 00123: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0630 - accuracy: 0.9887 - val_loss: 0.3787 - val_accuracy: 0.9241\n",
      "Epoch 124/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9894\n",
      "Epoch 00124: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0624 - accuracy: 0.9894 - val_loss: 0.3848 - val_accuracy: 0.9252\n",
      "Epoch 125/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9898\n",
      "Epoch 00125: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0596 - accuracy: 0.9898 - val_loss: 0.4079 - val_accuracy: 0.9237\n",
      "Epoch 126/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9877\n",
      "Epoch 00126: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9877 - val_loss: 0.3727 - val_accuracy: 0.9252\n",
      "Epoch 127/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9897\n",
      "Epoch 00127: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9897 - val_loss: 0.3962 - val_accuracy: 0.9181\n",
      "Epoch 128/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9888\n",
      "Epoch 00128: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9889 - val_loss: 0.4032 - val_accuracy: 0.9192\n",
      "Epoch 129/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9886\n",
      "Epoch 00129: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0631 - accuracy: 0.9887 - val_loss: 0.3838 - val_accuracy: 0.9241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9895\n",
      "Epoch 00130: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0615 - accuracy: 0.9894 - val_loss: 0.3782 - val_accuracy: 0.9263\n",
      "Epoch 131/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9902\n",
      "Epoch 00131: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0593 - accuracy: 0.9901 - val_loss: 0.3880 - val_accuracy: 0.9222\n",
      "Epoch 132/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9895\n",
      "Epoch 00132: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9893 - val_loss: 0.3859 - val_accuracy: 0.9196\n",
      "Epoch 133/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9888\n",
      "Epoch 00133: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0624 - accuracy: 0.9889 - val_loss: 0.3793 - val_accuracy: 0.9215\n",
      "Epoch 134/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9889\n",
      "Epoch 00134: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0626 - accuracy: 0.9889 - val_loss: 0.3669 - val_accuracy: 0.9200\n",
      "Epoch 135/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9883\n",
      "Epoch 00135: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0621 - accuracy: 0.9883 - val_loss: 0.3925 - val_accuracy: 0.9196\n",
      "Epoch 136/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9913\n",
      "Epoch 00136: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0577 - accuracy: 0.9913 - val_loss: 0.3863 - val_accuracy: 0.9245\n",
      "Epoch 137/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9888\n",
      "Epoch 00137: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9889 - val_loss: 0.3861 - val_accuracy: 0.9230\n",
      "Epoch 138/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9889\n",
      "Epoch 00138: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0614 - accuracy: 0.9889 - val_loss: 0.3881 - val_accuracy: 0.9207\n",
      "Epoch 139/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9900\n",
      "Epoch 00139: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9900 - val_loss: 0.3742 - val_accuracy: 0.9248\n",
      "Epoch 140/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0568 - accuracy: 0.9898\n",
      "Epoch 00140: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0567 - accuracy: 0.9899 - val_loss: 0.3826 - val_accuracy: 0.9248\n",
      "Epoch 141/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9892\n",
      "Epoch 00141: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9892 - val_loss: 0.3892 - val_accuracy: 0.9211\n",
      "Epoch 142/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9900\n",
      "Epoch 00142: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0594 - accuracy: 0.9900 - val_loss: 0.3844 - val_accuracy: 0.9260\n",
      "Epoch 143/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9894\n",
      "Epoch 00143: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9893 - val_loss: 0.3921 - val_accuracy: 0.9237\n",
      "Epoch 144/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0559 - accuracy: 0.9910\n",
      "Epoch 00144: val_accuracy improved from 0.92670 to 0.92857, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0560 - accuracy: 0.9910 - val_loss: 0.3761 - val_accuracy: 0.9286\n",
      "Epoch 145/150\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9899\n",
      "Epoch 00145: val_accuracy improved from 0.92857 to 0.92932, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 3s 3ms/step - loss: 0.0593 - accuracy: 0.9899 - val_loss: 0.3751 - val_accuracy: 0.9293\n",
      "Epoch 146/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9897\n",
      "Epoch 00146: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0589 - accuracy: 0.9897 - val_loss: 0.3805 - val_accuracy: 0.9260\n",
      "Epoch 147/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0564 - accuracy: 0.9901\n",
      "Epoch 00147: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0563 - accuracy: 0.9901 - val_loss: 0.3871 - val_accuracy: 0.9260\n",
      "Epoch 148/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9896\n",
      "Epoch 00148: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0609 - accuracy: 0.9897 - val_loss: 0.3985 - val_accuracy: 0.9241\n",
      "Epoch 149/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9891\n",
      "Epoch 00149: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0585 - accuracy: 0.9890 - val_loss: 0.3961 - val_accuracy: 0.9218\n",
      "Epoch 150/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9876\n",
      "Epoch 00150: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0623 - accuracy: 0.9877 - val_loss: 0.4001 - val_accuracy: 0.9256\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [07:53<00:00, 236.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.68      0.75      0.71       798\n",
      "        car_horn       0.88      0.50      0.63       413\n",
      "children_playing       0.64      0.64      0.64       700\n",
      "        dog_bark       0.68      0.84      0.75       700\n",
      "           siren       0.93      0.86      0.89      1162\n",
      "\n",
      "        accuracy                           0.76      3773\n",
      "       macro avg       0.76      0.72      0.73      3773\n",
      "    weighted avg       0.77      0.76      0.75      3773\n",
      "\n",
      "Validation fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27230, 375)\n",
      "X_val_norm shape.....:(3276, 375)\n",
      "\n",
      "Sum of elements: 0.9802872505448041\n",
      "Number of elements summed: 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 239)               57360     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 239)               57360     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 239)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               180000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 298,475\n",
      "Trainable params: 298,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24507, 239)\n",
      "Epoch 1/350\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.7898 - accuracy: 0.7081\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82336, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 2ms/step - loss: 0.7840 - accuracy: 0.7105 - val_loss: 0.4833 - val_accuracy: 0.8234\n",
      "Epoch 2/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.4366 - accuracy: 0.8465\n",
      "Epoch 00002: val_accuracy improved from 0.82336 to 0.86669, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.4366 - accuracy: 0.8465 - val_loss: 0.3799 - val_accuracy: 0.8667\n",
      "Epoch 3/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.8770\n",
      "Epoch 00003: val_accuracy improved from 0.86669 to 0.88469, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.3403 - accuracy: 0.8773 - val_loss: 0.3259 - val_accuracy: 0.8847\n",
      "Epoch 4/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.9022\n",
      "Epoch 00004: val_accuracy improved from 0.88469 to 0.90158, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.2781 - accuracy: 0.9022 - val_loss: 0.2904 - val_accuracy: 0.9016\n",
      "Epoch 5/350\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.2275 - accuracy: 0.9212\n",
      "Epoch 00005: val_accuracy improved from 0.90158 to 0.90378, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.2271 - accuracy: 0.9212 - val_loss: 0.2640 - val_accuracy: 0.9038\n",
      "Epoch 6/350\n",
      "742/766 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9361\n",
      "Epoch 00006: val_accuracy improved from 0.90378 to 0.91223, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9363 - val_loss: 0.2433 - val_accuracy: 0.9122\n",
      "Epoch 7/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 0.1613 - accuracy: 0.9468\n",
      "Epoch 00007: val_accuracy improved from 0.91223 to 0.91921, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.1600 - accuracy: 0.9473 - val_loss: 0.2245 - val_accuracy: 0.9192\n",
      "Epoch 8/350\n",
      "743/766 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9535\n",
      "Epoch 00008: val_accuracy improved from 0.91921 to 0.92104, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.1340 - accuracy: 0.9532 - val_loss: 0.2158 - val_accuracy: 0.9210\n",
      "Epoch 9/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9630\n",
      "Epoch 00009: val_accuracy improved from 0.92104 to 0.93169, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.1126 - accuracy: 0.9632 - val_loss: 0.2135 - val_accuracy: 0.9317\n",
      "Epoch 10/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 0.0964 - accuracy: 0.9675\n",
      "Epoch 00010: val_accuracy improved from 0.93169 to 0.93316, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0960 - accuracy: 0.9676 - val_loss: 0.1991 - val_accuracy: 0.9332\n",
      "Epoch 11/350\n",
      "737/766 [===========================>..] - ETA: 0s - loss: 0.0817 - accuracy: 0.9733\n",
      "Epoch 00011: val_accuracy did not improve from 0.93316\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0814 - accuracy: 0.9735 - val_loss: 0.2069 - val_accuracy: 0.9332\n",
      "Epoch 12/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.9783\n",
      "Epoch 00012: val_accuracy improved from 0.93316 to 0.93573, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0681 - accuracy: 0.9783 - val_loss: 0.2012 - val_accuracy: 0.9357\n",
      "Epoch 13/350\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0584 - accuracy: 0.9808\n",
      "Epoch 00013: val_accuracy improved from 0.93573 to 0.93977, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0581 - accuracy: 0.9809 - val_loss: 0.1986 - val_accuracy: 0.9398\n",
      "Epoch 14/350\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9862\n",
      "Epoch 00014: val_accuracy did not improve from 0.93977\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0458 - accuracy: 0.9862 - val_loss: 0.1993 - val_accuracy: 0.9398\n",
      "Epoch 15/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0423 - accuracy: 0.9871\n",
      "Epoch 00015: val_accuracy improved from 0.93977 to 0.94161, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9871 - val_loss: 0.1937 - val_accuracy: 0.9416\n",
      "Epoch 16/350\n",
      "747/766 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9899\n",
      "Epoch 00016: val_accuracy improved from 0.94161 to 0.94565, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9899 - val_loss: 0.1942 - val_accuracy: 0.9456\n",
      "Epoch 17/350\n",
      "738/766 [===========================>..] - ETA: 0s - loss: 0.0299 - accuracy: 0.9915\n",
      "Epoch 00017: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0299 - accuracy: 0.9914 - val_loss: 0.1978 - val_accuracy: 0.9427\n",
      "Epoch 18/350\n",
      "747/766 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9926\n",
      "Epoch 00018: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0257 - accuracy: 0.9925 - val_loss: 0.2022 - val_accuracy: 0.9442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "735/766 [===========================>..] - ETA: 0s - loss: 0.0221 - accuracy: 0.9948\n",
      "Epoch 00019: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9949 - val_loss: 0.2095 - val_accuracy: 0.9423\n",
      "Epoch 20/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9953\n",
      "Epoch 00020: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9953 - val_loss: 0.2033 - val_accuracy: 0.9431\n",
      "Epoch 21/350\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9960\n",
      "Epoch 00021: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9959 - val_loss: 0.2076 - val_accuracy: 0.9453\n",
      "Epoch 22/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9970\n",
      "Epoch 00022: val_accuracy improved from 0.94565 to 0.94638, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 0.2143 - val_accuracy: 0.9464\n",
      "Epoch 23/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9971\n",
      "Epoch 00023: val_accuracy did not improve from 0.94638\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.2168 - val_accuracy: 0.9456\n",
      "Epoch 24/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 0.0125 - accuracy: 0.9964\n",
      "Epoch 00024: val_accuracy did not improve from 0.94638\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.2168 - val_accuracy: 0.9445\n",
      "Epoch 25/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 0.0111 - accuracy: 0.9975\n",
      "Epoch 00025: val_accuracy improved from 0.94638 to 0.94712, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9976 - val_loss: 0.2188 - val_accuracy: 0.9471\n",
      "Epoch 26/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 0.0096 - accuracy: 0.9977\n",
      "Epoch 00026: val_accuracy improved from 0.94712 to 0.94785, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 0.2140 - val_accuracy: 0.9479\n",
      "Epoch 27/350\n",
      "737/766 [===========================>..] - ETA: 0s - loss: 0.0080 - accuracy: 0.9985\n",
      "Epoch 00027: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.2326 - val_accuracy: 0.9471\n",
      "Epoch 28/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9980\n",
      "Epoch 00028: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.2364 - val_accuracy: 0.9456\n",
      "Epoch 29/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0081 - accuracy: 0.9982\n",
      "Epoch 00029: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9982 - val_loss: 0.2321 - val_accuracy: 0.9464\n",
      "Epoch 30/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 0.0069 - accuracy: 0.9989\n",
      "Epoch 00030: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0068 - accuracy: 0.9989 - val_loss: 0.2365 - val_accuracy: 0.9471\n",
      "Epoch 31/350\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9986\n",
      "Epoch 00031: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.2337 - val_accuracy: 0.9471\n",
      "Epoch 32/350\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9990\n",
      "Epoch 00032: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 0.2356 - val_accuracy: 0.9475\n",
      "Epoch 33/350\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9985\n",
      "Epoch 00033: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.2440 - val_accuracy: 0.9467\n",
      "Epoch 34/350\n",
      "746/766 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9990\n",
      "Epoch 00034: val_accuracy improved from 0.94785 to 0.94859, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.2394 - val_accuracy: 0.9486\n",
      "Epoch 35/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9990\n",
      "Epoch 00035: val_accuracy did not improve from 0.94859\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.2442 - val_accuracy: 0.9464\n",
      "Epoch 36/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9991\n",
      "Epoch 00036: val_accuracy did not improve from 0.94859\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.2438 - val_accuracy: 0.9467\n",
      "Epoch 37/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00037: val_accuracy improved from 0.94859 to 0.94969, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.2353 - val_accuracy: 0.9497\n",
      "Epoch 38/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 00038: val_accuracy did not improve from 0.94969\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.2484 - val_accuracy: 0.9479\n",
      "Epoch 39/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9988\n",
      "Epoch 00039: val_accuracy improved from 0.94969 to 0.95006, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.2455 - val_accuracy: 0.9501\n",
      "Epoch 40/350\n",
      "746/766 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00040: val_accuracy did not improve from 0.95006\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.2493 - val_accuracy: 0.9490\n",
      "Epoch 41/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9990\n",
      "Epoch 00041: val_accuracy improved from 0.95006 to 0.95152, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 0.2422 - val_accuracy: 0.9515\n",
      "Epoch 42/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9993\n",
      "Epoch 00042: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2507 - val_accuracy: 0.9486\n",
      "Epoch 43/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 00043: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.2427 - val_accuracy: 0.9486\n",
      "Epoch 44/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762/766 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9993\n",
      "Epoch 00044: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.2503 - val_accuracy: 0.9464\n",
      "Epoch 45/350\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9995\n",
      "Epoch 00045: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.2581 - val_accuracy: 0.9467\n",
      "Epoch 46/350\n",
      "745/766 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00046: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.2606 - val_accuracy: 0.9467\n",
      "Epoch 47/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 0.9997\n",
      "Epoch 00047: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2654 - val_accuracy: 0.9475\n",
      "Epoch 48/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9996\n",
      "Epoch 00048: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2572 - val_accuracy: 0.9486\n",
      "Epoch 49/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00049: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.2582 - val_accuracy: 0.9490\n",
      "Epoch 50/350\n",
      "733/766 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00050: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2588 - val_accuracy: 0.9490\n",
      "Epoch 51/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00051: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2575 - val_accuracy: 0.9490\n",
      "Epoch 52/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9997\n",
      "Epoch 00052: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.2639 - val_accuracy: 0.9475\n",
      "Epoch 53/350\n",
      "741/766 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 00053: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2682 - val_accuracy: 0.9479\n",
      "Epoch 54/350\n",
      "735/766 [===========================>..] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00054: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2718 - val_accuracy: 0.9456\n",
      "Epoch 55/350\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2772 - val_accuracy: 0.9456\n",
      "Epoch 56/350\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00056: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2767 - val_accuracy: 0.9467\n",
      "Epoch 57/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 00057: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2814 - val_accuracy: 0.9501\n",
      "Epoch 58/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.2752 - val_accuracy: 0.9493\n",
      "Epoch 59/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2693 - val_accuracy: 0.9467\n",
      "Epoch 60/350\n",
      "737/766 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 00060: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.2723 - val_accuracy: 0.9475\n",
      "Epoch 61/350\n",
      "746/766 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2749 - val_accuracy: 0.9490\n",
      "Epoch 62/350\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00062: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2651 - val_accuracy: 0.9504\n",
      "Epoch 63/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00063: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2664 - val_accuracy: 0.9508\n",
      "Epoch 64/350\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00064: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2748 - val_accuracy: 0.9497\n",
      "Epoch 65/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00065: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2702 - val_accuracy: 0.9471\n",
      "Epoch 66/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00066: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2710 - val_accuracy: 0.9479\n",
      "Epoch 67/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00067: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2710 - val_accuracy: 0.9497\n",
      "Epoch 68/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2698 - val_accuracy: 0.9490\n",
      "Epoch 69/350\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2705 - val_accuracy: 0.9493\n",
      "Epoch 70/350\n",
      "738/766 [===========================>..] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00070: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2718 - val_accuracy: 0.9486\n",
      "Epoch 71/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2700 - val_accuracy: 0.9490\n",
      "Epoch 72/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00072: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2812 - val_accuracy: 0.9479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00073: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2723 - val_accuracy: 0.9493\n",
      "Epoch 74/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2778 - val_accuracy: 0.9471\n",
      "Epoch 75/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00075: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2802 - val_accuracy: 0.9515\n",
      "Epoch 76/350\n",
      "738/766 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00076: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2847 - val_accuracy: 0.9508\n",
      "Epoch 77/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00077: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2799 - val_accuracy: 0.9504\n",
      "Epoch 78/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 9.8091e-04 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 9.7198e-04 - accuracy: 0.9999 - val_loss: 0.2832 - val_accuracy: 0.9515\n",
      "Epoch 79/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2825 - val_accuracy: 0.9497\n",
      "Epoch 80/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 9.9272e-04 - accuracy: 0.9999\n",
      "Epoch 00080: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2857 - val_accuracy: 0.9508\n",
      "Epoch 81/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00081: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2832 - val_accuracy: 0.9501\n",
      "Epoch 82/350\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00082: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.2877 - val_accuracy: 0.9504\n",
      "Epoch 83/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 8.8393e-04 - accuracy: 0.9998\n",
      "Epoch 00083: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.8371e-04 - accuracy: 0.9998 - val_loss: 0.2868 - val_accuracy: 0.9482\n",
      "Epoch 84/350\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2869 - val_accuracy: 0.9497\n",
      "Epoch 85/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00085: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2815 - val_accuracy: 0.9501\n",
      "Epoch 86/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 9.9674e-04 - accuracy: 0.9998\n",
      "Epoch 00086: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 9.8979e-04 - accuracy: 0.9998 - val_loss: 0.2821 - val_accuracy: 0.9493\n",
      "Epoch 87/350\n",
      "741/766 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2843 - val_accuracy: 0.9512\n",
      "Epoch 88/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 9.1661e-04 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 9.1462e-04 - accuracy: 0.9998 - val_loss: 0.2919 - val_accuracy: 0.9493\n",
      "Epoch 89/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 8.8378e-04 - accuracy: 0.9999\n",
      "Epoch 00089: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.8326e-04 - accuracy: 0.9999 - val_loss: 0.2882 - val_accuracy: 0.9508\n",
      "Epoch 90/350\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00090: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2874 - val_accuracy: 0.9504\n",
      "Epoch 91/350\n",
      "735/766 [===========================>..] - ETA: 0s - loss: 7.3540e-04 - accuracy: 1.0000\n",
      "Epoch 00091: val_accuracy improved from 0.95152 to 0.95189, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.2899e-04 - accuracy: 1.0000 - val_loss: 0.2852 - val_accuracy: 0.9519\n",
      "Epoch 92/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00092: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.2869 - val_accuracy: 0.9519\n",
      "Epoch 93/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00093: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2937 - val_accuracy: 0.9512\n",
      "Epoch 94/350\n",
      "743/766 [============================>.] - ETA: 0s - loss: 8.0667e-04 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.0454e-04 - accuracy: 0.9999 - val_loss: 0.2923 - val_accuracy: 0.9515\n",
      "Epoch 95/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00095: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3039 - val_accuracy: 0.9464\n",
      "Epoch 96/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2985 - val_accuracy: 0.9508\n",
      "Epoch 97/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9995  \n",
      "Epoch 00097: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.2986 - val_accuracy: 0.9460\n",
      "Epoch 98/350\n",
      "752/766 [============================>.] - ETA: 0s - loss: 8.9448e-04 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.8530e-04 - accuracy: 0.9998 - val_loss: 0.2937 - val_accuracy: 0.9475\n",
      "Epoch 99/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 7.8468e-04 - accuracy: 0.9999\n",
      "Epoch 00099: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.1082e-04 - accuracy: 0.9999 - val_loss: 0.3016 - val_accuracy: 0.9482\n",
      "Epoch 100/350\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00100: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2857 - val_accuracy: 0.9493\n",
      "Epoch 101/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737/766 [===========================>..] - ETA: 0s - loss: 9.4818e-04 - accuracy: 0.9998\n",
      "Epoch 00101: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 9.2151e-04 - accuracy: 0.9998 - val_loss: 0.2880 - val_accuracy: 0.9493\n",
      "Epoch 102/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 6.6016e-04 - accuracy: 1.0000\n",
      "Epoch 00102: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.2298e-04 - accuracy: 0.9999 - val_loss: 0.2858 - val_accuracy: 0.9508\n",
      "Epoch 103/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 8.9973e-04 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.9973e-04 - accuracy: 0.9998 - val_loss: 0.2965 - val_accuracy: 0.9493\n",
      "Epoch 104/350\n",
      "759/766 [============================>.] - ETA: 0s - loss: 5.7360e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.7383e-04 - accuracy: 1.0000 - val_loss: 0.2886 - val_accuracy: 0.9490\n",
      "Epoch 105/350\n",
      "759/766 [============================>.] - ETA: 0s - loss: 7.8251e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.8566e-04 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.9471\n",
      "Epoch 106/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 7.6376e-04 - accuracy: 0.9999\n",
      "Epoch 00106: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.6367e-04 - accuracy: 0.9999 - val_loss: 0.2884 - val_accuracy: 0.9497\n",
      "Epoch 107/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 6.2115e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.2115e-04 - accuracy: 1.0000 - val_loss: 0.2890 - val_accuracy: 0.9490\n",
      "Epoch 108/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 6.5839e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.5343e-04 - accuracy: 0.9999 - val_loss: 0.2976 - val_accuracy: 0.9486\n",
      "Epoch 109/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 6.3489e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.3523e-04 - accuracy: 0.9999 - val_loss: 0.2970 - val_accuracy: 0.9482\n",
      "Epoch 110/350\n",
      "735/766 [===========================>..] - ETA: 0s - loss: 5.2929e-04 - accuracy: 1.0000\n",
      "Epoch 00110: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.2720e-04 - accuracy: 1.0000 - val_loss: 0.3004 - val_accuracy: 0.9490\n",
      "Epoch 111/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 4.2693e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.2514e-04 - accuracy: 1.0000 - val_loss: 0.2993 - val_accuracy: 0.9490\n",
      "Epoch 112/350\n",
      "762/766 [============================>.] - ETA: 0s - loss: 8.3499e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.3427e-04 - accuracy: 0.9998 - val_loss: 0.3050 - val_accuracy: 0.9482\n",
      "Epoch 113/350\n",
      "733/766 [===========================>..] - ETA: 0s - loss: 6.1263e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.0058e-04 - accuracy: 0.9999 - val_loss: 0.2983 - val_accuracy: 0.9479\n",
      "Epoch 114/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 5.4240e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.4043e-04 - accuracy: 1.0000 - val_loss: 0.2976 - val_accuracy: 0.9482\n",
      "Epoch 115/350\n",
      "745/766 [============================>.] - ETA: 0s - loss: 5.2920e-04 - accuracy: 0.9999\n",
      "Epoch 00115: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.2283e-04 - accuracy: 0.9999 - val_loss: 0.2980 - val_accuracy: 0.9475\n",
      "Epoch 116/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 5.9168e-04 - accuracy: 0.9999\n",
      "Epoch 00116: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.9068e-04 - accuracy: 0.9999 - val_loss: 0.3006 - val_accuracy: 0.9504\n",
      "Epoch 117/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 5.7980e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.7645e-04 - accuracy: 1.0000 - val_loss: 0.2958 - val_accuracy: 0.9512\n",
      "Epoch 118/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 5.3260e-04 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.1963e-04 - accuracy: 1.0000 - val_loss: 0.2960 - val_accuracy: 0.9515\n",
      "Epoch 119/350\n",
      "742/766 [============================>.] - ETA: 0s - loss: 4.8446e-04 - accuracy: 1.0000\n",
      "Epoch 00119: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.7864e-04 - accuracy: 1.0000 - val_loss: 0.2982 - val_accuracy: 0.9515\n",
      "Epoch 120/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 4.2600e-04 - accuracy: 0.9999\n",
      "Epoch 00120: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.2600e-04 - accuracy: 0.9999 - val_loss: 0.2997 - val_accuracy: 0.9519\n",
      "Epoch 121/350\n",
      "742/766 [============================>.] - ETA: 0s - loss: 5.4810e-04 - accuracy: 0.9998\n",
      "Epoch 00121: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.3448e-04 - accuracy: 0.9998 - val_loss: 0.3009 - val_accuracy: 0.9515\n",
      "Epoch 122/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 0.0018 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.2984 - val_accuracy: 0.9497\n",
      "Epoch 123/350\n",
      "745/766 [============================>.] - ETA: 0s - loss: 6.1383e-04 - accuracy: 0.9998\n",
      "Epoch 00123: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.0562e-04 - accuracy: 0.9998 - val_loss: 0.3017 - val_accuracy: 0.9504\n",
      "Epoch 124/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 7.6714e-04 - accuracy: 0.9999\n",
      "Epoch 00124: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.6039e-04 - accuracy: 0.9999 - val_loss: 0.3046 - val_accuracy: 0.9501\n",
      "Epoch 125/350\n",
      "750/766 [============================>.] - ETA: 0s - loss: 5.5230e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.4818e-04 - accuracy: 0.9999 - val_loss: 0.3018 - val_accuracy: 0.9512\n",
      "Epoch 126/350\n",
      "752/766 [============================>.] - ETA: 0s - loss: 5.8212e-04 - accuracy: 0.9999\n",
      "Epoch 00126: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.3303e-04 - accuracy: 0.9999 - val_loss: 0.3064 - val_accuracy: 0.9508\n",
      "Epoch 127/350\n",
      "750/766 [============================>.] - ETA: 0s - loss: 4.9929e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.9237e-04 - accuracy: 0.9999 - val_loss: 0.3087 - val_accuracy: 0.9497\n",
      "Epoch 128/350\n",
      "748/766 [============================>.] - ETA: 0s - loss: 8.2013e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.0539e-04 - accuracy: 0.9999 - val_loss: 0.3112 - val_accuracy: 0.9497\n",
      "Epoch 129/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/766 [============================>.] - ETA: 0s - loss: 4.2174e-04 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.1764e-04 - accuracy: 1.0000 - val_loss: 0.3105 - val_accuracy: 0.9482\n",
      "Epoch 130/350\n",
      "760/766 [============================>.] - ETA: 0s - loss: 5.0147e-04 - accuracy: 0.9999\n",
      "Epoch 00130: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.0188e-04 - accuracy: 0.9999 - val_loss: 0.3209 - val_accuracy: 0.9512\n",
      "Epoch 131/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 3.4587e-04 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 3.4587e-04 - accuracy: 1.0000 - val_loss: 0.3140 - val_accuracy: 0.9479\n",
      "Epoch 132/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 5.6295e-04 - accuracy: 0.9999\n",
      "Epoch 00132: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.5150e-04 - accuracy: 0.9999 - val_loss: 0.3026 - val_accuracy: 0.9490\n",
      "Epoch 133/350\n",
      "747/766 [============================>.] - ETA: 0s - loss: 4.7487e-04 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.6714e-04 - accuracy: 1.0000 - val_loss: 0.3121 - val_accuracy: 0.9486\n",
      "Epoch 134/350\n",
      "737/766 [===========================>..] - ETA: 0s - loss: 4.0578e-04 - accuracy: 0.9999\n",
      "Epoch 00134: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.2505e-04 - accuracy: 0.9999 - val_loss: 0.3076 - val_accuracy: 0.9486\n",
      "Epoch 135/350\n",
      "765/766 [============================>.] - ETA: 0s - loss: 5.0308e-04 - accuracy: 0.9999\n",
      "Epoch 00135: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.0292e-04 - accuracy: 0.9999 - val_loss: 0.3055 - val_accuracy: 0.9515\n",
      "Epoch 136/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 4.3965e-04 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.4522e-04 - accuracy: 1.0000 - val_loss: 0.3046 - val_accuracy: 0.9501\n",
      "Epoch 137/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 8.2346e-04 - accuracy: 0.9998\n",
      "Epoch 00137: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.4782e-04 - accuracy: 0.9998 - val_loss: 0.3020 - val_accuracy: 0.9501\n",
      "Epoch 138/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 7.4980e-04 - accuracy: 0.9998\n",
      "Epoch 00138: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.4410e-04 - accuracy: 0.9998 - val_loss: 0.3027 - val_accuracy: 0.9490\n",
      "Epoch 139/350\n",
      "754/766 [============================>.] - ETA: 0s - loss: 4.9177e-04 - accuracy: 1.0000\n",
      "Epoch 00139: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.8601e-04 - accuracy: 1.0000 - val_loss: 0.3048 - val_accuracy: 0.9493\n",
      "Epoch 140/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 4.2164e-04 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.1179e-04 - accuracy: 1.0000 - val_loss: 0.3027 - val_accuracy: 0.9490\n",
      "Epoch 141/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 3.8838e-04 - accuracy: 1.0000\n",
      "Epoch 00141: val_accuracy did not improve from 0.95189\n",
      "Restoring model weights from the end of the best epoch.\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 3.8838e-04 - accuracy: 1.0000 - val_loss: 0.3064 - val_accuracy: 0.9493\n",
      "Epoch 00141: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:06<03:06, 186.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.68      0.85      0.76       693\n",
      "        car_horn       0.93      0.67      0.78       686\n",
      "children_playing       0.76      0.74      0.75       700\n",
      "        dog_bark       0.74      0.77      0.75       700\n",
      "           siren       0.86      0.88      0.87       497\n",
      "\n",
      "        accuracy                           0.77      3276\n",
      "       macro avg       0.79      0.78      0.78      3276\n",
      "    weighted avg       0.79      0.77      0.77      3276\n",
      "\n",
      "Model: \"Model_CNN_1D_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 233, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 233, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 233, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 116, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 116, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6496)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                324850    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 335,891\n",
      "Trainable params: 335,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24507, 239, 1)\n",
      "Epoch 1/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.7523 - accuracy: 0.7622\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83548, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 4s 5ms/step - loss: 0.7523 - accuracy: 0.7622 - val_loss: 0.5443 - val_accuracy: 0.8355\n",
      "Epoch 2/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.4832 - accuracy: 0.8542\n",
      "Epoch 00002: val_accuracy improved from 0.83548 to 0.86926, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 3s 3ms/step - loss: 0.4836 - accuracy: 0.8539 - val_loss: 0.4540 - val_accuracy: 0.8693\n",
      "Epoch 3/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8764\n",
      "Epoch 00003: val_accuracy improved from 0.86926 to 0.88028, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.4168 - accuracy: 0.8766 - val_loss: 0.4128 - val_accuracy: 0.8803\n",
      "Epoch 4/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.8950\n",
      "Epoch 00004: val_accuracy improved from 0.88028 to 0.88322, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.3692 - accuracy: 0.8951 - val_loss: 0.4037 - val_accuracy: 0.8832\n",
      "Epoch 5/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.9040\n",
      "Epoch 00005: val_accuracy improved from 0.88322 to 0.89093, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.3380 - accuracy: 0.9038 - val_loss: 0.3984 - val_accuracy: 0.8909\n",
      "Epoch 6/150\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.9154\n",
      "Epoch 00006: val_accuracy improved from 0.89093 to 0.89974, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.3097 - accuracy: 0.9152 - val_loss: 0.3665 - val_accuracy: 0.8997\n",
      "Epoch 7/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9201\n",
      "Epoch 00007: val_accuracy did not improve from 0.89974\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2884 - accuracy: 0.9201 - val_loss: 0.3656 - val_accuracy: 0.8968\n",
      "Epoch 8/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.2681 - accuracy: 0.9291\n",
      "Epoch 00008: val_accuracy did not improve from 0.89974\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2686 - accuracy: 0.9290 - val_loss: 0.3678 - val_accuracy: 0.8920\n",
      "Epoch 9/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.9355\n",
      "Epoch 00009: val_accuracy improved from 0.89974 to 0.90562, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2528 - accuracy: 0.9351 - val_loss: 0.3541 - val_accuracy: 0.9056\n",
      "Epoch 10/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.2411 - accuracy: 0.9366\n",
      "Epoch 00010: val_accuracy did not improve from 0.90562\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2408 - accuracy: 0.9368 - val_loss: 0.3508 - val_accuracy: 0.9038\n",
      "Epoch 11/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9416\n",
      "Epoch 00011: val_accuracy improved from 0.90562 to 0.90709, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2269 - accuracy: 0.9417 - val_loss: 0.3421 - val_accuracy: 0.9071\n",
      "Epoch 12/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.2211 - accuracy: 0.9430\n",
      "Epoch 00012: val_accuracy did not improve from 0.90709\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2211 - accuracy: 0.9430 - val_loss: 0.3525 - val_accuracy: 0.9030\n",
      "Epoch 13/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.2114 - accuracy: 0.9469\n",
      "Epoch 00013: val_accuracy improved from 0.90709 to 0.91076, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2118 - accuracy: 0.9467 - val_loss: 0.3535 - val_accuracy: 0.9108\n",
      "Epoch 14/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.2072 - accuracy: 0.9475\n",
      "Epoch 00014: val_accuracy did not improve from 0.91076\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2071 - accuracy: 0.9475 - val_loss: 0.3422 - val_accuracy: 0.9100\n",
      "Epoch 15/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9505\n",
      "Epoch 00015: val_accuracy improved from 0.91076 to 0.91149, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1992 - accuracy: 0.9505 - val_loss: 0.3340 - val_accuracy: 0.9115\n",
      "Epoch 16/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.9565\n",
      "Epoch 00016: val_accuracy improved from 0.91149 to 0.91443, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1870 - accuracy: 0.9565 - val_loss: 0.3365 - val_accuracy: 0.9144\n",
      "Epoch 17/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751/766 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9554\n",
      "Epoch 00017: val_accuracy did not improve from 0.91443\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1837 - accuracy: 0.9555 - val_loss: 0.3970 - val_accuracy: 0.9008\n",
      "Epoch 18/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9565\n",
      "Epoch 00018: val_accuracy did not improve from 0.91443\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1807 - accuracy: 0.9565 - val_loss: 0.3347 - val_accuracy: 0.9141\n",
      "Epoch 19/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.9590\n",
      "Epoch 00019: val_accuracy did not improve from 0.91443\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1718 - accuracy: 0.9588 - val_loss: 0.3474 - val_accuracy: 0.9111\n",
      "Epoch 20/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.9601\n",
      "Epoch 00020: val_accuracy improved from 0.91443 to 0.91664, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1694 - accuracy: 0.9601 - val_loss: 0.3351 - val_accuracy: 0.9166\n",
      "Epoch 21/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.1601 - accuracy: 0.9631\n",
      "Epoch 00021: val_accuracy did not improve from 0.91664\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1603 - accuracy: 0.9631 - val_loss: 0.3390 - val_accuracy: 0.9152\n",
      "Epoch 22/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1593 - accuracy: 0.9623\n",
      "Epoch 00022: val_accuracy did not improve from 0.91664\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1593 - accuracy: 0.9623 - val_loss: 0.3461 - val_accuracy: 0.9144\n",
      "Epoch 23/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.1592 - accuracy: 0.9612\n",
      "Epoch 00023: val_accuracy did not improve from 0.91664\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1591 - accuracy: 0.9613 - val_loss: 0.3459 - val_accuracy: 0.9144\n",
      "Epoch 24/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.9631\n",
      "Epoch 00024: val_accuracy improved from 0.91664 to 0.91884, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1538 - accuracy: 0.9627 - val_loss: 0.3372 - val_accuracy: 0.9188\n",
      "Epoch 25/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.9658\n",
      "Epoch 00025: val_accuracy did not improve from 0.91884\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1501 - accuracy: 0.9657 - val_loss: 0.3414 - val_accuracy: 0.9170\n",
      "Epoch 26/150\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.1477 - accuracy: 0.9650\n",
      "Epoch 00026: val_accuracy did not improve from 0.91884\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1477 - accuracy: 0.9651 - val_loss: 0.3527 - val_accuracy: 0.9166\n",
      "Epoch 27/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.9660\n",
      "Epoch 00027: val_accuracy did not improve from 0.91884\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1468 - accuracy: 0.9659 - val_loss: 0.3393 - val_accuracy: 0.9185\n",
      "Epoch 28/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.9706\n",
      "Epoch 00028: val_accuracy did not improve from 0.91884\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1374 - accuracy: 0.9706 - val_loss: 0.3368 - val_accuracy: 0.9174\n",
      "Epoch 29/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9684\n",
      "Epoch 00029: val_accuracy improved from 0.91884 to 0.92104, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1408 - accuracy: 0.9685 - val_loss: 0.3361 - val_accuracy: 0.9210\n",
      "Epoch 30/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9697\n",
      "Epoch 00030: val_accuracy did not improve from 0.92104\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1372 - accuracy: 0.9698 - val_loss: 0.3567 - val_accuracy: 0.9137\n",
      "Epoch 31/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9708\n",
      "Epoch 00031: val_accuracy improved from 0.92104 to 0.92361, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.9708 - val_loss: 0.3322 - val_accuracy: 0.9236\n",
      "Epoch 32/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9727\n",
      "Epoch 00032: val_accuracy did not improve from 0.92361\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1297 - accuracy: 0.9727 - val_loss: 0.3281 - val_accuracy: 0.9214\n",
      "Epoch 33/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9718\n",
      "Epoch 00033: val_accuracy improved from 0.92361 to 0.92435, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1295 - accuracy: 0.9718 - val_loss: 0.3400 - val_accuracy: 0.9243\n",
      "Epoch 34/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9719\n",
      "Epoch 00034: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1285 - accuracy: 0.9719 - val_loss: 0.3485 - val_accuracy: 0.9177\n",
      "Epoch 35/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9716\n",
      "Epoch 00035: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1263 - accuracy: 0.9717 - val_loss: 0.3350 - val_accuracy: 0.9174\n",
      "Epoch 36/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9724\n",
      "Epoch 00036: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1246 - accuracy: 0.9724 - val_loss: 0.3363 - val_accuracy: 0.9181\n",
      "Epoch 37/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9734\n",
      "Epoch 00037: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1248 - accuracy: 0.9732 - val_loss: 0.3492 - val_accuracy: 0.9210\n",
      "Epoch 38/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9745\n",
      "Epoch 00038: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1200 - accuracy: 0.9745 - val_loss: 0.3386 - val_accuracy: 0.9218\n",
      "Epoch 39/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9746\n",
      "Epoch 00039: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1203 - accuracy: 0.9742 - val_loss: 0.3496 - val_accuracy: 0.9185\n",
      "Epoch 40/150\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9754\n",
      "Epoch 00040: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1180 - accuracy: 0.9751 - val_loss: 0.3448 - val_accuracy: 0.9229\n",
      "Epoch 41/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.1152 - accuracy: 0.9759\n",
      "Epoch 00041: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1150 - accuracy: 0.9759 - val_loss: 0.3587 - val_accuracy: 0.9218\n",
      "Epoch 42/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9757\n",
      "Epoch 00042: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1162 - accuracy: 0.9757 - val_loss: 0.3460 - val_accuracy: 0.9210\n",
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/766 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9761\n",
      "Epoch 00043: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1123 - accuracy: 0.9760 - val_loss: 0.3391 - val_accuracy: 0.9214\n",
      "Epoch 44/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9765\n",
      "Epoch 00044: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1099 - accuracy: 0.9766 - val_loss: 0.3518 - val_accuracy: 0.9170\n",
      "Epoch 45/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9785\n",
      "Epoch 00045: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1107 - accuracy: 0.9780 - val_loss: 0.3479 - val_accuracy: 0.9207\n",
      "Epoch 46/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9777\n",
      "Epoch 00046: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1100 - accuracy: 0.9777 - val_loss: 0.3442 - val_accuracy: 0.9218\n",
      "Epoch 47/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.1095 - accuracy: 0.9782\n",
      "Epoch 00047: val_accuracy improved from 0.92435 to 0.92692, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1092 - accuracy: 0.9784 - val_loss: 0.3362 - val_accuracy: 0.9269\n",
      "Epoch 48/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9797\n",
      "Epoch 00048: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1023 - accuracy: 0.9798 - val_loss: 0.3485 - val_accuracy: 0.9196\n",
      "Epoch 49/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9779\n",
      "Epoch 00049: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1082 - accuracy: 0.9780 - val_loss: 0.3557 - val_accuracy: 0.9196\n",
      "Epoch 50/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9780\n",
      "Epoch 00050: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1039 - accuracy: 0.9780 - val_loss: 0.3798 - val_accuracy: 0.9170\n",
      "Epoch 51/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9782\n",
      "Epoch 00051: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1071 - accuracy: 0.9781 - val_loss: 0.3558 - val_accuracy: 0.9225\n",
      "Epoch 52/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9799\n",
      "Epoch 00052: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 4ms/step - loss: 0.1012 - accuracy: 0.9800 - val_loss: 0.3558 - val_accuracy: 0.9181\n",
      "Epoch 53/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0974 - accuracy: 0.9814\n",
      "Epoch 00053: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 4ms/step - loss: 0.0974 - accuracy: 0.9814 - val_loss: 0.3712 - val_accuracy: 0.9185\n",
      "Epoch 54/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0978 - accuracy: 0.9804\n",
      "Epoch 00054: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 4ms/step - loss: 0.0977 - accuracy: 0.9805 - val_loss: 0.3654 - val_accuracy: 0.9192\n",
      "Epoch 55/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9813\n",
      "Epoch 00055: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0964 - accuracy: 0.9813 - val_loss: 0.3601 - val_accuracy: 0.9240\n",
      "Epoch 56/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0966 - accuracy: 0.9810\n",
      "Epoch 00056: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0965 - accuracy: 0.9811 - val_loss: 0.3642 - val_accuracy: 0.9203\n",
      "Epoch 57/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0940 - accuracy: 0.9816\n",
      "Epoch 00057: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0944 - accuracy: 0.9815 - val_loss: 0.3635 - val_accuracy: 0.9174\n",
      "Epoch 58/150\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.0962 - accuracy: 0.9810\n",
      "Epoch 00058: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0963 - accuracy: 0.9809 - val_loss: 0.3778 - val_accuracy: 0.9210\n",
      "Epoch 59/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0942 - accuracy: 0.9829\n",
      "Epoch 00059: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0946 - accuracy: 0.9827 - val_loss: 0.3656 - val_accuracy: 0.9207\n",
      "Epoch 60/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9837\n",
      "Epoch 00060: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9836 - val_loss: 0.3655 - val_accuracy: 0.9258\n",
      "Epoch 61/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9810\n",
      "Epoch 00061: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9810 - val_loss: 0.3672 - val_accuracy: 0.9229\n",
      "Epoch 62/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9820\n",
      "Epoch 00062: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0923 - accuracy: 0.9819 - val_loss: 0.3736 - val_accuracy: 0.9218\n",
      "Epoch 63/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9816\n",
      "Epoch 00063: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0950 - accuracy: 0.9817 - val_loss: 0.3643 - val_accuracy: 0.9225\n",
      "Epoch 64/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9822\n",
      "Epoch 00064: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0903 - accuracy: 0.9820 - val_loss: 0.3740 - val_accuracy: 0.9210\n",
      "Epoch 65/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9814\n",
      "Epoch 00065: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0907 - accuracy: 0.9814 - val_loss: 0.3713 - val_accuracy: 0.9203\n",
      "Epoch 66/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9827\n",
      "Epoch 00066: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0881 - accuracy: 0.9827 - val_loss: 0.3722 - val_accuracy: 0.9203\n",
      "Epoch 67/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0931 - accuracy: 0.9803\n",
      "Epoch 00067: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0931 - accuracy: 0.9801 - val_loss: 0.3605 - val_accuracy: 0.9214\n",
      "Epoch 68/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0854 - accuracy: 0.9839\n",
      "Epoch 00068: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0852 - accuracy: 0.9840 - val_loss: 0.3578 - val_accuracy: 0.9218\n",
      "Epoch 69/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9847\n",
      "Epoch 00069: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0833 - accuracy: 0.9847 - val_loss: 0.3664 - val_accuracy: 0.9192\n",
      "Epoch 70/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9827\n",
      "Epoch 00070: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0875 - accuracy: 0.9827 - val_loss: 0.3501 - val_accuracy: 0.9243\n",
      "Epoch 71/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9852\n",
      "Epoch 00071: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0804 - accuracy: 0.9851 - val_loss: 0.3796 - val_accuracy: 0.9229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9835\n",
      "Epoch 00072: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0843 - accuracy: 0.9835 - val_loss: 0.3694 - val_accuracy: 0.9218\n",
      "Epoch 73/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9840\n",
      "Epoch 00073: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0839 - accuracy: 0.9840 - val_loss: 0.3784 - val_accuracy: 0.9188\n",
      "Epoch 74/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9824\n",
      "Epoch 00074: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0871 - accuracy: 0.9824 - val_loss: 0.3618 - val_accuracy: 0.9207\n",
      "Epoch 75/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9826\n",
      "Epoch 00075: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0876 - accuracy: 0.9825 - val_loss: 0.3715 - val_accuracy: 0.9218\n",
      "Epoch 76/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9849\n",
      "Epoch 00076: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0812 - accuracy: 0.9850 - val_loss: 0.3687 - val_accuracy: 0.9243\n",
      "Epoch 77/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0829 - accuracy: 0.9834\n",
      "Epoch 00077: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0828 - accuracy: 0.9835 - val_loss: 0.3751 - val_accuracy: 0.9221\n",
      "Epoch 78/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9829\n",
      "Epoch 00078: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0857 - accuracy: 0.9830 - val_loss: 0.3780 - val_accuracy: 0.9166\n",
      "Epoch 79/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9833\n",
      "Epoch 00079: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0815 - accuracy: 0.9834 - val_loss: 0.3678 - val_accuracy: 0.9214\n",
      "Epoch 80/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9845\n",
      "Epoch 00080: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9845 - val_loss: 0.3647 - val_accuracy: 0.9243\n",
      "Epoch 81/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9856\n",
      "Epoch 00081: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9854 - val_loss: 0.3624 - val_accuracy: 0.9225\n",
      "Epoch 82/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9836\n",
      "Epoch 00082: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9836 - val_loss: 0.3719 - val_accuracy: 0.9221\n",
      "Epoch 83/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9854\n",
      "Epoch 00083: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9854 - val_loss: 0.3636 - val_accuracy: 0.9269\n",
      "Epoch 84/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9854\n",
      "Epoch 00084: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0775 - accuracy: 0.9856 - val_loss: 0.3620 - val_accuracy: 0.9218\n",
      "Epoch 85/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9855\n",
      "Epoch 00085: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9856 - val_loss: 0.3756 - val_accuracy: 0.9221\n",
      "Epoch 86/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9862\n",
      "Epoch 00086: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0763 - accuracy: 0.9863 - val_loss: 0.3879 - val_accuracy: 0.9185\n",
      "Epoch 87/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9849\n",
      "Epoch 00087: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0784 - accuracy: 0.9849 - val_loss: 0.3617 - val_accuracy: 0.9258\n",
      "Epoch 88/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9852\n",
      "Epoch 00088: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 3ms/step - loss: 0.0760 - accuracy: 0.9849 - val_loss: 0.3774 - val_accuracy: 0.9214\n",
      "Epoch 89/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9874\n",
      "Epoch 00089: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0727 - accuracy: 0.9873 - val_loss: 0.3802 - val_accuracy: 0.9229\n",
      "Epoch 90/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9864\n",
      "Epoch 00090: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0750 - accuracy: 0.9862 - val_loss: 0.3703 - val_accuracy: 0.9229\n",
      "Epoch 91/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9861\n",
      "Epoch 00091: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9861 - val_loss: 0.3723 - val_accuracy: 0.9221\n",
      "Epoch 92/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9859\n",
      "Epoch 00092: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0755 - accuracy: 0.9858 - val_loss: 0.3834 - val_accuracy: 0.9214\n",
      "Epoch 93/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9856\n",
      "Epoch 00093: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9856 - val_loss: 0.3660 - val_accuracy: 0.9251\n",
      "Epoch 94/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9855\n",
      "Epoch 00094: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0744 - accuracy: 0.9856 - val_loss: 0.3949 - val_accuracy: 0.9207\n",
      "Epoch 95/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9862\n",
      "Epoch 00095: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0761 - accuracy: 0.9862 - val_loss: 0.3805 - val_accuracy: 0.9207\n",
      "Epoch 96/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9868\n",
      "Epoch 00096: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0706 - accuracy: 0.9867 - val_loss: 0.3780 - val_accuracy: 0.9203\n",
      "Epoch 97/150\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9873\n",
      "Epoch 00097: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0723 - accuracy: 0.9874 - val_loss: 0.3667 - val_accuracy: 0.9232\n",
      "Epoch 98/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.9884\n",
      "Epoch 00098: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0679 - accuracy: 0.9885 - val_loss: 0.3940 - val_accuracy: 0.9192\n",
      "Epoch 99/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9865\n",
      "Epoch 00099: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9866 - val_loss: 0.3833 - val_accuracy: 0.9232\n",
      "Epoch 100/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9851\n",
      "Epoch 00100: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9850 - val_loss: 0.3681 - val_accuracy: 0.9240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9872\n",
      "Epoch 00101: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9871 - val_loss: 0.3625 - val_accuracy: 0.9262\n",
      "Epoch 102/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0699 - accuracy: 0.9874\n",
      "Epoch 00102: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9874 - val_loss: 0.3770 - val_accuracy: 0.9243\n",
      "Epoch 103/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9853\n",
      "Epoch 00103: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9852 - val_loss: 0.3792 - val_accuracy: 0.9170\n",
      "Epoch 104/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9871\n",
      "Epoch 00104: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0691 - accuracy: 0.9871 - val_loss: 0.3879 - val_accuracy: 0.9225\n",
      "Epoch 105/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9879\n",
      "Epoch 00105: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0677 - accuracy: 0.9879 - val_loss: 0.3773 - val_accuracy: 0.9214\n",
      "Epoch 106/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9878\n",
      "Epoch 00106: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0712 - accuracy: 0.9878 - val_loss: 0.3733 - val_accuracy: 0.9254\n",
      "Epoch 107/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9894\n",
      "Epoch 00107: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0651 - accuracy: 0.9893 - val_loss: 0.3697 - val_accuracy: 0.9236\n",
      "Epoch 108/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9881\n",
      "Epoch 00108: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9882 - val_loss: 0.3667 - val_accuracy: 0.9269\n",
      "Epoch 109/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.9876\n",
      "Epoch 00109: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9876 - val_loss: 0.3923 - val_accuracy: 0.9236\n",
      "Epoch 110/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9876\n",
      "Epoch 00110: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0674 - accuracy: 0.9875 - val_loss: 0.3901 - val_accuracy: 0.9221\n",
      "Epoch 111/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9876\n",
      "Epoch 00111: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9876 - val_loss: 0.3891 - val_accuracy: 0.9203\n",
      "Epoch 112/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9877\n",
      "Epoch 00112: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9878 - val_loss: 0.3658 - val_accuracy: 0.9218\n",
      "Epoch 113/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9885\n",
      "Epoch 00113: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9886 - val_loss: 0.3712 - val_accuracy: 0.9232\n",
      "Epoch 114/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9900\n",
      "Epoch 00114: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9900 - val_loss: 0.3842 - val_accuracy: 0.9243\n",
      "Epoch 115/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9895\n",
      "Epoch 00115: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 3ms/step - loss: 0.0644 - accuracy: 0.9895 - val_loss: 0.3905 - val_accuracy: 0.9192\n",
      "Epoch 116/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9877\n",
      "Epoch 00116: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0672 - accuracy: 0.9876 - val_loss: 0.3658 - val_accuracy: 0.9196\n",
      "Epoch 117/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9887\n",
      "Epoch 00117: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0637 - accuracy: 0.9887 - val_loss: 0.3721 - val_accuracy: 0.9232\n",
      "Epoch 118/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9881\n",
      "Epoch 00118: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9879 - val_loss: 0.3645 - val_accuracy: 0.9240\n",
      "Epoch 119/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9870\n",
      "Epoch 00119: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9870 - val_loss: 0.3753 - val_accuracy: 0.9221\n",
      "Epoch 120/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.9883\n",
      "Epoch 00120: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0655 - accuracy: 0.9880 - val_loss: 0.3611 - val_accuracy: 0.9240\n",
      "Epoch 121/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9893\n",
      "Epoch 00121: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0638 - accuracy: 0.9891 - val_loss: 0.4126 - val_accuracy: 0.9210\n",
      "Epoch 122/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9878\n",
      "Epoch 00122: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9878 - val_loss: 0.3858 - val_accuracy: 0.9243\n",
      "Epoch 123/150\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9879\n",
      "Epoch 00123: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0650 - accuracy: 0.9878 - val_loss: 0.3743 - val_accuracy: 0.9232\n",
      "Epoch 124/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9884\n",
      "Epoch 00124: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9884 - val_loss: 0.3802 - val_accuracy: 0.9243\n",
      "Epoch 125/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0588 - accuracy: 0.9903\n",
      "Epoch 00125: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0586 - accuracy: 0.9904 - val_loss: 0.3635 - val_accuracy: 0.9269\n",
      "Epoch 126/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9883\n",
      "Epoch 00126: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0623 - accuracy: 0.9885 - val_loss: 0.3791 - val_accuracy: 0.9214\n",
      "Epoch 127/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9885\n",
      "Epoch 00127: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0630 - accuracy: 0.9887 - val_loss: 0.3746 - val_accuracy: 0.9210\n",
      "Epoch 128/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9880\n",
      "Epoch 00128: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9880 - val_loss: 0.3800 - val_accuracy: 0.9232\n",
      "Epoch 129/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9899\n",
      "Epoch 00129: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0581 - accuracy: 0.9900 - val_loss: 0.3744 - val_accuracy: 0.9229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0597 - accuracy: 0.9896\n",
      "Epoch 00130: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9896 - val_loss: 0.3758 - val_accuracy: 0.9214\n",
      "Epoch 131/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9897\n",
      "Epoch 00131: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0594 - accuracy: 0.9898 - val_loss: 0.4008 - val_accuracy: 0.9199\n",
      "Epoch 132/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9882\n",
      "Epoch 00132: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0627 - accuracy: 0.9882 - val_loss: 0.3997 - val_accuracy: 0.9210\n",
      "Epoch 133/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9876\n",
      "Epoch 00133: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0642 - accuracy: 0.9877 - val_loss: 0.3982 - val_accuracy: 0.9199\n",
      "Epoch 134/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9882\n",
      "Epoch 00134: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9883 - val_loss: 0.3908 - val_accuracy: 0.9221\n",
      "Epoch 135/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9902\n",
      "Epoch 00135: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0577 - accuracy: 0.9902 - val_loss: 0.3870 - val_accuracy: 0.9221\n",
      "Epoch 136/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9889\n",
      "Epoch 00136: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0637 - accuracy: 0.9889 - val_loss: 0.3972 - val_accuracy: 0.9192\n",
      "Epoch 137/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9893\n",
      "Epoch 00137: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0598 - accuracy: 0.9893 - val_loss: 0.3964 - val_accuracy: 0.9188\n",
      "Epoch 138/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9887\n",
      "Epoch 00138: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0629 - accuracy: 0.9886 - val_loss: 0.3819 - val_accuracy: 0.9185\n",
      "Epoch 139/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9898\n",
      "Epoch 00139: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0593 - accuracy: 0.9898 - val_loss: 0.3776 - val_accuracy: 0.9243\n",
      "Epoch 140/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9894\n",
      "Epoch 00140: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0602 - accuracy: 0.9893 - val_loss: 0.3831 - val_accuracy: 0.9221\n",
      "Epoch 141/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0579 - accuracy: 0.9900\n",
      "Epoch 00141: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0578 - accuracy: 0.9901 - val_loss: 0.3936 - val_accuracy: 0.9203\n",
      "Epoch 142/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9891\n",
      "Epoch 00142: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0598 - accuracy: 0.9890 - val_loss: 0.3863 - val_accuracy: 0.9236\n",
      "Epoch 143/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0591 - accuracy: 0.9904\n",
      "Epoch 00143: val_accuracy improved from 0.92692 to 0.92729, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0593 - accuracy: 0.9903 - val_loss: 0.3800 - val_accuracy: 0.9273\n",
      "Epoch 144/150\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0566 - accuracy: 0.9903\n",
      "Epoch 00144: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 3s 3ms/step - loss: 0.0564 - accuracy: 0.9903 - val_loss: 0.3927 - val_accuracy: 0.9221\n",
      "Epoch 145/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9903\n",
      "Epoch 00145: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0575 - accuracy: 0.9903 - val_loss: 0.3910 - val_accuracy: 0.9214\n",
      "Epoch 146/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9894\n",
      "Epoch 00146: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0581 - accuracy: 0.9895 - val_loss: 0.3739 - val_accuracy: 0.9221\n",
      "Epoch 147/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9899\n",
      "Epoch 00147: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0554 - accuracy: 0.9899 - val_loss: 0.3771 - val_accuracy: 0.9254\n",
      "Epoch 148/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9887\n",
      "Epoch 00148: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0589 - accuracy: 0.9887 - val_loss: 0.3675 - val_accuracy: 0.9232\n",
      "Epoch 149/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9885\n",
      "Epoch 00149: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9884 - val_loss: 0.3779 - val_accuracy: 0.9225\n",
      "Epoch 150/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9908\n",
      "Epoch 00150: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0563 - accuracy: 0.9908 - val_loss: 0.3884 - val_accuracy: 0.9229\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [09:03<00:00, 271.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.64      0.85      0.73       693\n",
      "        car_horn       0.93      0.67      0.78       686\n",
      "children_playing       0.69      0.71      0.70       700\n",
      "        dog_bark       0.78      0.74      0.76       700\n",
      "           siren       0.82      0.79      0.80       497\n",
      "\n",
      "        accuracy                           0.75      3276\n",
      "       macro avg       0.77      0.75      0.75      3276\n",
      "    weighted avg       0.77      0.75      0.75      3276\n",
      "\n",
      "Validation fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27706, 375)\n",
      "X_val_norm shape.....:(2800, 375)\n",
      "\n",
      "Sum of elements: 0.9803388072660862\n",
      "Number of elements summed: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 233)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               175500    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 288,299\n",
      "Trainable params: 288,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24935, 233)\n",
      "Epoch 1/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.7935 - accuracy: 0.7109\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82533, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 2ms/step - loss: 0.7892 - accuracy: 0.7127 - val_loss: 0.4813 - val_accuracy: 0.8253\n",
      "Epoch 2/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.4496 - accuracy: 0.8399\n",
      "Epoch 00002: val_accuracy improved from 0.82533 to 0.87044, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.4493 - accuracy: 0.8400 - val_loss: 0.3700 - val_accuracy: 0.8704\n",
      "Epoch 3/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.3493 - accuracy: 0.8750\n",
      "Epoch 00003: val_accuracy improved from 0.87044 to 0.88813, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.3492 - accuracy: 0.8751 - val_loss: 0.3171 - val_accuracy: 0.8881\n",
      "Epoch 4/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9006\n",
      "Epoch 00004: val_accuracy improved from 0.88813 to 0.89643, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.2847 - accuracy: 0.9011 - val_loss: 0.2851 - val_accuracy: 0.8964\n",
      "Epoch 5/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.9158\n",
      "Epoch 00005: val_accuracy improved from 0.89643 to 0.90870, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.2433 - accuracy: 0.9157 - val_loss: 0.2539 - val_accuracy: 0.9087\n",
      "Epoch 6/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.2010 - accuracy: 0.9314\n",
      "Epoch 00006: val_accuracy improved from 0.90870 to 0.91519, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.2008 - accuracy: 0.9314 - val_loss: 0.2428 - val_accuracy: 0.9152\n",
      "Epoch 7/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.9407\n",
      "Epoch 00007: val_accuracy improved from 0.91519 to 0.92277, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.1694 - accuracy: 0.9411 - val_loss: 0.2222 - val_accuracy: 0.9228\n",
      "Epoch 8/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9516\n",
      "Epoch 00008: val_accuracy improved from 0.92277 to 0.92494, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.1424 - accuracy: 0.9513 - val_loss: 0.2147 - val_accuracy: 0.9249\n",
      "Epoch 9/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9590\n",
      "Epoch 00009: val_accuracy improved from 0.92494 to 0.93035, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.1225 - accuracy: 0.9590 - val_loss: 0.1985 - val_accuracy: 0.9304\n",
      "Epoch 10/350\n",
      "747/780 [===========================>..] - ETA: 0s - loss: 0.1063 - accuracy: 0.9636\n",
      "Epoch 00010: val_accuracy improved from 0.93035 to 0.93396, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.1060 - accuracy: 0.9641 - val_loss: 0.1946 - val_accuracy: 0.9340\n",
      "Epoch 11/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9715\n",
      "Epoch 00011: val_accuracy improved from 0.93396 to 0.93432, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.9716 - val_loss: 0.1925 - val_accuracy: 0.9343\n",
      "Epoch 12/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9762\n",
      "Epoch 00012: val_accuracy did not improve from 0.93432\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0771 - accuracy: 0.9763 - val_loss: 0.1908 - val_accuracy: 0.9322\n",
      "Epoch 13/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0647 - accuracy: 0.9794\n",
      "Epoch 00013: val_accuracy did not improve from 0.93432\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0646 - accuracy: 0.9795 - val_loss: 0.1902 - val_accuracy: 0.9329\n",
      "Epoch 14/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0524 - accuracy: 0.9839\n",
      "Epoch 00014: val_accuracy improved from 0.93432 to 0.94262, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0527 - accuracy: 0.9839 - val_loss: 0.1860 - val_accuracy: 0.9426\n",
      "Epoch 15/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9858\n",
      "Epoch 00015: val_accuracy did not improve from 0.94262\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0464 - accuracy: 0.9858 - val_loss: 0.1830 - val_accuracy: 0.9412\n",
      "Epoch 16/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9882\n",
      "Epoch 00016: val_accuracy did not improve from 0.94262\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9883 - val_loss: 0.1818 - val_accuracy: 0.9405\n",
      "Epoch 17/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0348 - accuracy: 0.9900\n",
      "Epoch 00017: val_accuracy improved from 0.94262 to 0.94406, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0348 - accuracy: 0.9900 - val_loss: 0.1842 - val_accuracy: 0.9441\n",
      "Epoch 18/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0287 - accuracy: 0.9925\n",
      "Epoch 00018: val_accuracy did not improve from 0.94406\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0287 - accuracy: 0.9924 - val_loss: 0.1890 - val_accuracy: 0.9419\n",
      "Epoch 19/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/780 [============================>.] - ETA: 0s - loss: 0.0268 - accuracy: 0.9927\n",
      "Epoch 00019: val_accuracy improved from 0.94406 to 0.94695, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9928 - val_loss: 0.1896 - val_accuracy: 0.9470\n",
      "Epoch 20/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 0.0220 - accuracy: 0.9947\n",
      "Epoch 00020: val_accuracy did not improve from 0.94695\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9947 - val_loss: 0.1966 - val_accuracy: 0.9423\n",
      "Epoch 21/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9948\n",
      "Epoch 00021: val_accuracy did not improve from 0.94695\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9948 - val_loss: 0.1981 - val_accuracy: 0.9444\n",
      "Epoch 22/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0189 - accuracy: 0.9955\n",
      "Epoch 00022: val_accuracy did not improve from 0.94695\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9955 - val_loss: 0.1998 - val_accuracy: 0.9462\n",
      "Epoch 23/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9971\n",
      "Epoch 00023: val_accuracy improved from 0.94695 to 0.94767, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0151 - accuracy: 0.9971 - val_loss: 0.2024 - val_accuracy: 0.9477\n",
      "Epoch 24/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9971\n",
      "Epoch 00024: val_accuracy improved from 0.94767 to 0.95020, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0140 - accuracy: 0.9971 - val_loss: 0.2008 - val_accuracy: 0.9502\n",
      "Epoch 25/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9967\n",
      "Epoch 00025: val_accuracy did not improve from 0.95020\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0129 - accuracy: 0.9967 - val_loss: 0.2069 - val_accuracy: 0.9488\n",
      "Epoch 26/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9972\n",
      "Epoch 00026: val_accuracy improved from 0.95020 to 0.95092, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.2019 - val_accuracy: 0.9509\n",
      "Epoch 27/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 0.0101 - accuracy: 0.9979\n",
      "Epoch 00027: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0101 - accuracy: 0.9979 - val_loss: 0.2070 - val_accuracy: 0.9509\n",
      "Epoch 28/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0092 - accuracy: 0.9977\n",
      "Epoch 00028: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0092 - accuracy: 0.9978 - val_loss: 0.2048 - val_accuracy: 0.9448\n",
      "Epoch 29/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0092 - accuracy: 0.9977\n",
      "Epoch 00029: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.2064 - val_accuracy: 0.9462\n",
      "Epoch 30/350\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9984\n",
      "Epoch 00030: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 0.2221 - val_accuracy: 0.9444\n",
      "Epoch 31/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9980\n",
      "Epoch 00031: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 0.2111 - val_accuracy: 0.9488\n",
      "Epoch 32/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9983\n",
      "Epoch 00032: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9983 - val_loss: 0.2168 - val_accuracy: 0.9462\n",
      "Epoch 33/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9988\n",
      "Epoch 00033: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.2193 - val_accuracy: 0.9488\n",
      "Epoch 34/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0062 - accuracy: 0.9988\n",
      "Epoch 00034: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 0.2160 - val_accuracy: 0.9491\n",
      "Epoch 35/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9990\n",
      "Epoch 00035: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.2210 - val_accuracy: 0.9470\n",
      "Epoch 36/350\n",
      "751/780 [===========================>..] - ETA: 0s - loss: 0.0048 - accuracy: 0.9993\n",
      "Epoch 00036: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.2255 - val_accuracy: 0.9462\n",
      "Epoch 37/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9989\n",
      "Epoch 00037: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9990 - val_loss: 0.2266 - val_accuracy: 0.9506\n",
      "Epoch 38/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 00038: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.2264 - val_accuracy: 0.9484\n",
      "Epoch 39/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9990\n",
      "Epoch 00039: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.2203 - val_accuracy: 0.9488\n",
      "Epoch 40/350\n",
      "747/780 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 00040: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.2236 - val_accuracy: 0.9509\n",
      "Epoch 41/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 00041: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.2242 - val_accuracy: 0.9509\n",
      "Epoch 42/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 00042: val_accuracy improved from 0.95092 to 0.95200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.2220 - val_accuracy: 0.9520\n",
      "Epoch 43/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9991\n",
      "Epoch 00043: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.2217 - val_accuracy: 0.9498\n",
      "Epoch 44/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9991\n",
      "Epoch 00044: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.2313 - val_accuracy: 0.9491\n",
      "Epoch 45/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749/780 [===========================>..] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00045: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2292 - val_accuracy: 0.9491\n",
      "Epoch 46/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 00046: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.2332 - val_accuracy: 0.9480\n",
      "Epoch 47/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 00047: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.2344 - val_accuracy: 0.9506\n",
      "Epoch 48/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 00048: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.2384 - val_accuracy: 0.9498\n",
      "Epoch 49/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9996\n",
      "Epoch 00049: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2364 - val_accuracy: 0.9484\n",
      "Epoch 50/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 00050: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.2365 - val_accuracy: 0.9509\n",
      "Epoch 51/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9996\n",
      "Epoch 00051: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.2367 - val_accuracy: 0.9498\n",
      "Epoch 52/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00052: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2400 - val_accuracy: 0.9506\n",
      "Epoch 53/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9998\n",
      "Epoch 00053: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.2426 - val_accuracy: 0.9502\n",
      "Epoch 54/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9996\n",
      "Epoch 00054: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.2381 - val_accuracy: 0.9506\n",
      "Epoch 55/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9997\n",
      "Epoch 00055: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2377 - val_accuracy: 0.9495\n",
      "Epoch 56/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9996\n",
      "Epoch 00056: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2353 - val_accuracy: 0.9520\n",
      "Epoch 57/350\n",
      "747/780 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9997\n",
      "Epoch 00057: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.2346 - val_accuracy: 0.9502\n",
      "Epoch 58/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00058: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2393 - val_accuracy: 0.9520\n",
      "Epoch 59/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00059: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2384 - val_accuracy: 0.9516\n",
      "Epoch 60/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00060: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2361 - val_accuracy: 0.9520\n",
      "Epoch 61/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00061: val_accuracy improved from 0.95200 to 0.95345, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2386 - val_accuracy: 0.9534\n",
      "Epoch 62/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9997\n",
      "Epoch 00062: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.2364 - val_accuracy: 0.9516\n",
      "Epoch 63/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00063: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2404 - val_accuracy: 0.9534\n",
      "Epoch 64/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00064: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2419 - val_accuracy: 0.9513\n",
      "Epoch 65/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00065: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2471 - val_accuracy: 0.9491\n",
      "Epoch 66/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 00066: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.2500 - val_accuracy: 0.9495\n",
      "Epoch 67/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 00067: val_accuracy improved from 0.95345 to 0.95381, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.2472 - val_accuracy: 0.9538\n",
      "Epoch 68/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2492 - val_accuracy: 0.9524\n",
      "Epoch 69/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00069: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2476 - val_accuracy: 0.9534\n",
      "Epoch 70/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00070: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.2450 - val_accuracy: 0.9516\n",
      "Epoch 71/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00071: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2461 - val_accuracy: 0.9531\n",
      "Epoch 72/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9994\n",
      "Epoch 00072: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.2431 - val_accuracy: 0.9513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00073: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2448 - val_accuracy: 0.9531\n",
      "Epoch 74/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 00074: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.2453 - val_accuracy: 0.9498\n",
      "Epoch 75/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00075: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2415 - val_accuracy: 0.9520\n",
      "Epoch 76/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9998\n",
      "Epoch 00076: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.2447 - val_accuracy: 0.9516\n",
      "Epoch 77/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00077: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2498 - val_accuracy: 0.9513\n",
      "Epoch 78/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00078: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2466 - val_accuracy: 0.9520\n",
      "Epoch 79/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2476 - val_accuracy: 0.9538\n",
      "Epoch 80/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00080: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2466 - val_accuracy: 0.9538\n",
      "Epoch 81/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00081: val_accuracy improved from 0.95381 to 0.95417, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2451 - val_accuracy: 0.9542\n",
      "Epoch 82/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00082: val_accuracy did not improve from 0.95417\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2464 - val_accuracy: 0.9542\n",
      "Epoch 83/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00083: val_accuracy did not improve from 0.95417\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2485 - val_accuracy: 0.9542\n",
      "Epoch 84/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.95417\n",
      "780/780 [==============================] - 2s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2447 - val_accuracy: 0.9516\n",
      "Epoch 85/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00085: val_accuracy improved from 0.95417 to 0.95669, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2480 - val_accuracy: 0.9567\n",
      "Epoch 86/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00086: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2465 - val_accuracy: 0.9545\n",
      "Epoch 87/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 9.3253e-04 - accuracy: 0.9999\n",
      "Epoch 00087: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.7960e-04 - accuracy: 0.9998 - val_loss: 0.2493 - val_accuracy: 0.9520\n",
      "Epoch 88/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00088: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2520 - val_accuracy: 0.9524\n",
      "Epoch 89/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2538 - val_accuracy: 0.9553\n",
      "Epoch 90/350\n",
      "751/780 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00090: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2582 - val_accuracy: 0.9538\n",
      "Epoch 91/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 9.2646e-04 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.5668e-04 - accuracy: 0.9999 - val_loss: 0.2518 - val_accuracy: 0.9531\n",
      "Epoch 92/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 9.0188e-04 - accuracy: 0.9999\n",
      "Epoch 00092: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.0356e-04 - accuracy: 0.9999 - val_loss: 0.2541 - val_accuracy: 0.9531\n",
      "Epoch 93/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 9.8097e-04 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2544 - val_accuracy: 0.9542\n",
      "Epoch 94/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 7.1187e-04 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.1963e-04 - accuracy: 0.9999 - val_loss: 0.2553 - val_accuracy: 0.9534\n",
      "Epoch 95/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00095: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2564 - val_accuracy: 0.9534\n",
      "Epoch 96/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 9.6102e-04 - accuracy: 0.9999\n",
      "Epoch 00096: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.5234e-04 - accuracy: 0.9999 - val_loss: 0.2555 - val_accuracy: 0.9534\n",
      "Epoch 97/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 6.3338e-04 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.2553e-04 - accuracy: 1.0000 - val_loss: 0.2568 - val_accuracy: 0.9553\n",
      "Epoch 98/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 9.2115e-04 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.1297e-04 - accuracy: 0.9998 - val_loss: 0.2564 - val_accuracy: 0.9545\n",
      "Epoch 99/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 8.3270e-04 - accuracy: 0.9999\n",
      "Epoch 00099: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 8.2746e-04 - accuracy: 0.9999 - val_loss: 0.2573 - val_accuracy: 0.9549\n",
      "Epoch 100/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 7.6936e-04 - accuracy: 1.0000\n",
      "Epoch 00100: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.6506e-04 - accuracy: 1.0000 - val_loss: 0.2600 - val_accuracy: 0.9542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 7.6234e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.6733e-04 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 0.9531\n",
      "Epoch 102/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 7.1378e-04 - accuracy: 0.9998\n",
      "Epoch 00102: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.0584e-04 - accuracy: 0.9998 - val_loss: 0.2653 - val_accuracy: 0.9534\n",
      "Epoch 103/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 6.8157e-04 - accuracy: 0.9999\n",
      "Epoch 00103: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.8374e-04 - accuracy: 0.9999 - val_loss: 0.2593 - val_accuracy: 0.9524\n",
      "Epoch 104/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00104: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2580 - val_accuracy: 0.9542\n",
      "Epoch 105/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 8.5139e-04 - accuracy: 0.9998\n",
      "Epoch 00105: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.1012e-04 - accuracy: 0.9998 - val_loss: 0.2560 - val_accuracy: 0.9545\n",
      "Epoch 106/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 9.0172e-04 - accuracy: 0.9998\n",
      "Epoch 00106: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.1502e-04 - accuracy: 0.9998 - val_loss: 0.2543 - val_accuracy: 0.9542\n",
      "Epoch 107/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 6.9474e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.9455e-04 - accuracy: 1.0000 - val_loss: 0.2532 - val_accuracy: 0.9563\n",
      "Epoch 108/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 6.6340e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy improved from 0.95669 to 0.95706, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.5450e-04 - accuracy: 0.9999 - val_loss: 0.2535 - val_accuracy: 0.9571\n",
      "Epoch 109/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 6.9743e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy improved from 0.95706 to 0.95778, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.9743e-04 - accuracy: 0.9999 - val_loss: 0.2555 - val_accuracy: 0.9578\n",
      "Epoch 110/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 8.0544e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.9386e-04 - accuracy: 0.9999 - val_loss: 0.2585 - val_accuracy: 0.9571\n",
      "Epoch 111/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 5.8293e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.7453e-04 - accuracy: 1.0000 - val_loss: 0.2626 - val_accuracy: 0.9567\n",
      "Epoch 112/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 5.8116e-04 - accuracy: 0.9999\n",
      "Epoch 00112: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.8041e-04 - accuracy: 0.9999 - val_loss: 0.2617 - val_accuracy: 0.9556\n",
      "Epoch 113/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 8.1534e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 8.1344e-04 - accuracy: 0.9999 - val_loss: 0.2626 - val_accuracy: 0.9563\n",
      "Epoch 114/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 7.6083e-04 - accuracy: 0.9999\n",
      "Epoch 00114: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.5057e-04 - accuracy: 0.9999 - val_loss: 0.2664 - val_accuracy: 0.9553\n",
      "Epoch 115/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 4.9412e-04 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.9399e-04 - accuracy: 1.0000 - val_loss: 0.2719 - val_accuracy: 0.9549\n",
      "Epoch 116/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 4.6711e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.6093e-04 - accuracy: 1.0000 - val_loss: 0.2704 - val_accuracy: 0.9567\n",
      "Epoch 117/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 4.7336e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.6921e-04 - accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 0.9560\n",
      "Epoch 118/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 4.1190e-04 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.1038e-04 - accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 0.9560\n",
      "Epoch 119/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 5.8303e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.8288e-04 - accuracy: 0.9999 - val_loss: 0.2736 - val_accuracy: 0.9571\n",
      "Epoch 120/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 5.8408e-04 - accuracy: 0.9999\n",
      "Epoch 00120: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.9158e-04 - accuracy: 0.9999 - val_loss: 0.2764 - val_accuracy: 0.9553\n",
      "Epoch 121/350\n",
      "746/780 [===========================>..] - ETA: 0s - loss: 6.8764e-04 - accuracy: 0.9998\n",
      "Epoch 00121: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.7241e-04 - accuracy: 0.9998 - val_loss: 0.2735 - val_accuracy: 0.9556\n",
      "Epoch 122/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 6.4510e-04 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.2948e-04 - accuracy: 0.9999 - val_loss: 0.2739 - val_accuracy: 0.9563\n",
      "Epoch 123/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 6.0940e-04 - accuracy: 0.9999\n",
      "Epoch 00123: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.1512e-04 - accuracy: 0.9999 - val_loss: 0.2741 - val_accuracy: 0.9531\n",
      "Epoch 124/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 7.9925e-04 - accuracy: 0.9998\n",
      "Epoch 00124: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.9086e-04 - accuracy: 0.9998 - val_loss: 0.2706 - val_accuracy: 0.9556\n",
      "Epoch 125/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 5.6709e-04 - accuracy: 1.0000\n",
      "Epoch 00125: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.5610e-04 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 0.9556\n",
      "Epoch 126/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 5.6227e-04 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.5969e-04 - accuracy: 1.0000 - val_loss: 0.2684 - val_accuracy: 0.9553\n",
      "Epoch 127/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 6.4342e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.4132e-04 - accuracy: 0.9999 - val_loss: 0.2691 - val_accuracy: 0.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 4.1995e-04 - accuracy: 1.0000\n",
      "Epoch 00128: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.1910e-04 - accuracy: 1.0000 - val_loss: 0.2686 - val_accuracy: 0.9563\n",
      "Epoch 129/350\n",
      "751/780 [===========================>..] - ETA: 0s - loss: 7.5277e-04 - accuracy: 0.9999\n",
      "Epoch 00129: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.3603e-04 - accuracy: 0.9999 - val_loss: 0.2713 - val_accuracy: 0.9553\n",
      "Epoch 130/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 5.2124e-04 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.4392e-04 - accuracy: 0.9999 - val_loss: 0.2762 - val_accuracy: 0.9560\n",
      "Epoch 131/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 4.7297e-04 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.7367e-04 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9567\n",
      "Epoch 132/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 5.5865e-04 - accuracy: 0.9999\n",
      "Epoch 00132: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.5724e-04 - accuracy: 0.9999 - val_loss: 0.2694 - val_accuracy: 0.9549\n",
      "Epoch 133/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 4.6089e-04 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.5814e-04 - accuracy: 1.0000 - val_loss: 0.2702 - val_accuracy: 0.9567\n",
      "Epoch 134/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 7.0386e-04 - accuracy: 0.9998\n",
      "Epoch 00134: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.9014e-04 - accuracy: 0.9998 - val_loss: 0.2762 - val_accuracy: 0.9563\n",
      "Epoch 135/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 7.1177e-04 - accuracy: 0.9999\n",
      "Epoch 00135: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.0166e-04 - accuracy: 0.9999 - val_loss: 0.2745 - val_accuracy: 0.9560\n",
      "Epoch 136/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 6.5052e-04 - accuracy: 0.9998\n",
      "Epoch 00136: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.5003e-04 - accuracy: 0.9998 - val_loss: 0.2742 - val_accuracy: 0.9563\n",
      "Epoch 137/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 7.1489e-04 - accuracy: 0.9998\n",
      "Epoch 00137: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.0297e-04 - accuracy: 0.9998 - val_loss: 0.2738 - val_accuracy: 0.9549\n",
      "Epoch 138/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 4.0678e-04 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.0799e-04 - accuracy: 1.0000 - val_loss: 0.2764 - val_accuracy: 0.9542\n",
      "Epoch 139/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 3.9605e-04 - accuracy: 1.0000\n",
      "Epoch 00139: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 3.9866e-04 - accuracy: 1.0000 - val_loss: 0.2761 - val_accuracy: 0.9553\n",
      "Epoch 140/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 4.6124e-04 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.5853e-04 - accuracy: 1.0000 - val_loss: 0.2781 - val_accuracy: 0.9556\n",
      "Epoch 141/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 7.8900e-04 - accuracy: 0.9998\n",
      "Epoch 00141: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.7060e-04 - accuracy: 0.9998 - val_loss: 0.2786 - val_accuracy: 0.9560\n",
      "Epoch 142/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 6.0213e-04 - accuracy: 0.9999\n",
      "Epoch 00142: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.0096e-04 - accuracy: 0.9999 - val_loss: 0.2753 - val_accuracy: 0.9567\n",
      "Epoch 143/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 6.8498e-04 - accuracy: 0.9999\n",
      "Epoch 00143: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.0306e-04 - accuracy: 0.9999 - val_loss: 0.2809 - val_accuracy: 0.9556\n",
      "Epoch 144/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 3.3956e-04 - accuracy: 1.0000\n",
      "Epoch 00144: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 3.3785e-04 - accuracy: 1.0000 - val_loss: 0.2793 - val_accuracy: 0.9549\n",
      "Epoch 145/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 4.8673e-04 - accuracy: 1.0000\n",
      "Epoch 00145: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.8652e-04 - accuracy: 1.0000 - val_loss: 0.2793 - val_accuracy: 0.9567\n",
      "Epoch 146/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 4.0518e-04 - accuracy: 1.0000\n",
      "Epoch 00146: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.2379e-04 - accuracy: 1.0000 - val_loss: 0.2790 - val_accuracy: 0.9563\n",
      "Epoch 147/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 5.1547e-04 - accuracy: 0.9999\n",
      "Epoch 00147: val_accuracy did not improve from 0.95778\n",
      "Restoring model weights from the end of the best epoch.\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.0790e-04 - accuracy: 0.9999 - val_loss: 0.2771 - val_accuracy: 0.9571\n",
      "Epoch 00147: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:17<03:17, 197.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.73      0.75      0.74       686\n",
      "        car_horn       0.74      0.96      0.84       196\n",
      "children_playing       0.76      0.83      0.79       700\n",
      "        dog_bark       0.82      0.76      0.79       700\n",
      "           siren       0.81      0.68      0.74       518\n",
      "\n",
      "        accuracy                           0.77      2800\n",
      "       macro avg       0.77      0.79      0.78      2800\n",
      "    weighted avg       0.78      0.77      0.77      2800\n",
      "\n",
      "Model: \"Model_CNN_1D_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 227, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 227, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 227, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6328)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                316450    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 327,491\n",
      "Trainable params: 327,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24935, 233, 1)\n",
      "Epoch 1/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.7483 - accuracy: 0.7631\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83255, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 4s 5ms/step - loss: 0.7483 - accuracy: 0.7631 - val_loss: 0.5507 - val_accuracy: 0.8326\n",
      "Epoch 2/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.4938 - accuracy: 0.8504\n",
      "Epoch 00002: val_accuracy improved from 0.83255 to 0.86250, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.4926 - accuracy: 0.8509 - val_loss: 0.4323 - val_accuracy: 0.8625\n",
      "Epoch 3/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.8742\n",
      "Epoch 00003: val_accuracy improved from 0.86250 to 0.88307, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.4207 - accuracy: 0.8744 - val_loss: 0.3843 - val_accuracy: 0.8831\n",
      "Epoch 4/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.3748 - accuracy: 0.8925\n",
      "Epoch 00004: val_accuracy improved from 0.88307 to 0.89282, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3753 - accuracy: 0.8923 - val_loss: 0.3686 - val_accuracy: 0.8928\n",
      "Epoch 5/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.9018\n",
      "Epoch 00005: val_accuracy improved from 0.89282 to 0.89895, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3444 - accuracy: 0.9014 - val_loss: 0.3511 - val_accuracy: 0.8990\n",
      "Epoch 6/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.9120\n",
      "Epoch 00006: val_accuracy did not improve from 0.89895\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3172 - accuracy: 0.9121 - val_loss: 0.3652 - val_accuracy: 0.8881\n",
      "Epoch 7/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9177\n",
      "Epoch 00007: val_accuracy improved from 0.89895 to 0.90978, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2968 - accuracy: 0.9177 - val_loss: 0.3187 - val_accuracy: 0.9098\n",
      "Epoch 8/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.9265\n",
      "Epoch 00008: val_accuracy did not improve from 0.90978\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2754 - accuracy: 0.9263 - val_loss: 0.3225 - val_accuracy: 0.9087\n",
      "Epoch 9/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.2609 - accuracy: 0.9292\n",
      "Epoch 00009: val_accuracy did not improve from 0.90978\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2608 - accuracy: 0.9292 - val_loss: 0.3214 - val_accuracy: 0.9073\n",
      "Epoch 10/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9339\n",
      "Epoch 00010: val_accuracy improved from 0.90978 to 0.91050, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2494 - accuracy: 0.9339 - val_loss: 0.3107 - val_accuracy: 0.9105\n",
      "Epoch 11/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.2335 - accuracy: 0.9406\n",
      "Epoch 00011: val_accuracy improved from 0.91050 to 0.91231, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2340 - accuracy: 0.9404 - val_loss: 0.3175 - val_accuracy: 0.9123\n",
      "Epoch 12/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.2282 - accuracy: 0.9404\n",
      "Epoch 00012: val_accuracy improved from 0.91231 to 0.91772, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2283 - accuracy: 0.9404 - val_loss: 0.3088 - val_accuracy: 0.9177\n",
      "Epoch 13/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9436\n",
      "Epoch 00013: val_accuracy improved from 0.91772 to 0.92097, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2183 - accuracy: 0.9437 - val_loss: 0.3060 - val_accuracy: 0.9210\n",
      "Epoch 14/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.9459\n",
      "Epoch 00014: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2094 - accuracy: 0.9459 - val_loss: 0.3121 - val_accuracy: 0.9141\n",
      "Epoch 15/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.2001 - accuracy: 0.9490\n",
      "Epoch 00015: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2000 - accuracy: 0.9491 - val_loss: 0.3032 - val_accuracy: 0.9199\n",
      "Epoch 16/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9490\n",
      "Epoch 00016: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2014 - accuracy: 0.9490 - val_loss: 0.3263 - val_accuracy: 0.9130\n",
      "Epoch 17/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9511\n",
      "Epoch 00017: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1922 - accuracy: 0.9511 - val_loss: 0.2986 - val_accuracy: 0.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1847 - accuracy: 0.9547\n",
      "Epoch 00018: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1847 - accuracy: 0.9547 - val_loss: 0.3007 - val_accuracy: 0.9170\n",
      "Epoch 19/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.1782 - accuracy: 0.9563\n",
      "Epoch 00019: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1783 - accuracy: 0.9562 - val_loss: 0.3017 - val_accuracy: 0.9174\n",
      "Epoch 20/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9568\n",
      "Epoch 00020: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.1770 - accuracy: 0.9567 - val_loss: 0.3135 - val_accuracy: 0.9148\n",
      "Epoch 21/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9590\n",
      "Epoch 00021: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1717 - accuracy: 0.9589 - val_loss: 0.3058 - val_accuracy: 0.9188\n",
      "Epoch 22/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9630\n",
      "Epoch 00022: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1609 - accuracy: 0.9631 - val_loss: 0.3100 - val_accuracy: 0.9181\n",
      "Epoch 23/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.1631 - accuracy: 0.9609\n",
      "Epoch 00023: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1634 - accuracy: 0.9609 - val_loss: 0.2949 - val_accuracy: 0.9181\n",
      "Epoch 24/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9632\n",
      "Epoch 00024: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1586 - accuracy: 0.9630 - val_loss: 0.3102 - val_accuracy: 0.9206\n",
      "Epoch 25/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9651\n",
      "Epoch 00025: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1552 - accuracy: 0.9652 - val_loss: 0.3127 - val_accuracy: 0.9184\n",
      "Epoch 26/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9658\n",
      "Epoch 00026: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1517 - accuracy: 0.9659 - val_loss: 0.3094 - val_accuracy: 0.9192\n",
      "Epoch 27/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1483 - accuracy: 0.9671\n",
      "Epoch 00027: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1481 - accuracy: 0.9673 - val_loss: 0.3199 - val_accuracy: 0.9188\n",
      "Epoch 28/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9664\n",
      "Epoch 00028: val_accuracy improved from 0.92097 to 0.92494, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1483 - accuracy: 0.9664 - val_loss: 0.3089 - val_accuracy: 0.9249\n",
      "Epoch 29/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9670\n",
      "Epoch 00029: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1481 - accuracy: 0.9668 - val_loss: 0.3089 - val_accuracy: 0.9159\n",
      "Epoch 30/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9703\n",
      "Epoch 00030: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1394 - accuracy: 0.9702 - val_loss: 0.3100 - val_accuracy: 0.9206\n",
      "Epoch 31/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9684\n",
      "Epoch 00031: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1390 - accuracy: 0.9685 - val_loss: 0.3109 - val_accuracy: 0.9199\n",
      "Epoch 32/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1370 - accuracy: 0.9695\n",
      "Epoch 00032: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1373 - accuracy: 0.9693 - val_loss: 0.3139 - val_accuracy: 0.9206\n",
      "Epoch 33/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9688\n",
      "Epoch 00033: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1391 - accuracy: 0.9690 - val_loss: 0.3198 - val_accuracy: 0.9119\n",
      "Epoch 34/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1321 - accuracy: 0.9701\n",
      "Epoch 00034: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1322 - accuracy: 0.9701 - val_loss: 0.3108 - val_accuracy: 0.9181\n",
      "Epoch 35/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1310 - accuracy: 0.9710\n",
      "Epoch 00035: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.1309 - accuracy: 0.9710 - val_loss: 0.3165 - val_accuracy: 0.9217\n",
      "Epoch 36/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9711\n",
      "Epoch 00036: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1311 - accuracy: 0.9711 - val_loss: 0.3181 - val_accuracy: 0.9224\n",
      "Epoch 37/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9705\n",
      "Epoch 00037: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1308 - accuracy: 0.9704 - val_loss: 0.3246 - val_accuracy: 0.9206\n",
      "Epoch 38/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9729\n",
      "Epoch 00038: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1259 - accuracy: 0.9726 - val_loss: 0.3159 - val_accuracy: 0.9181\n",
      "Epoch 39/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9726\n",
      "Epoch 00039: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1230 - accuracy: 0.9726 - val_loss: 0.3136 - val_accuracy: 0.9195\n",
      "Epoch 40/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9738\n",
      "Epoch 00040: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1222 - accuracy: 0.9738 - val_loss: 0.3106 - val_accuracy: 0.9249\n",
      "Epoch 41/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9741\n",
      "Epoch 00041: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1203 - accuracy: 0.9741 - val_loss: 0.3146 - val_accuracy: 0.9199\n",
      "Epoch 42/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9736\n",
      "Epoch 00042: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1213 - accuracy: 0.9736 - val_loss: 0.3242 - val_accuracy: 0.9206\n",
      "Epoch 43/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9731\n",
      "Epoch 00043: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1222 - accuracy: 0.9732 - val_loss: 0.3088 - val_accuracy: 0.9184\n",
      "Epoch 44/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9778\n",
      "Epoch 00044: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1125 - accuracy: 0.9778 - val_loss: 0.3090 - val_accuracy: 0.9210\n",
      "Epoch 45/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9756\n",
      "Epoch 00045: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1169 - accuracy: 0.9756 - val_loss: 0.3102 - val_accuracy: 0.9246\n",
      "Epoch 46/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/780 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9790\n",
      "Epoch 00046: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1073 - accuracy: 0.9790 - val_loss: 0.3148 - val_accuracy: 0.9249\n",
      "Epoch 47/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9785\n",
      "Epoch 00047: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1105 - accuracy: 0.9784 - val_loss: 0.3181 - val_accuracy: 0.9231\n",
      "Epoch 48/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9761\n",
      "Epoch 00048: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1130 - accuracy: 0.9760 - val_loss: 0.3169 - val_accuracy: 0.9235\n",
      "Epoch 49/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1067 - accuracy: 0.9786\n",
      "Epoch 00049: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1070 - accuracy: 0.9785 - val_loss: 0.3232 - val_accuracy: 0.9220\n",
      "Epoch 50/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9777\n",
      "Epoch 00050: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1066 - accuracy: 0.9777 - val_loss: 0.3316 - val_accuracy: 0.9206\n",
      "Epoch 51/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9781\n",
      "Epoch 00051: val_accuracy improved from 0.92494 to 0.92602, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1084 - accuracy: 0.9781 - val_loss: 0.3135 - val_accuracy: 0.9260\n",
      "Epoch 52/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9795\n",
      "Epoch 00052: val_accuracy did not improve from 0.92602\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1041 - accuracy: 0.9796 - val_loss: 0.3303 - val_accuracy: 0.9213\n",
      "Epoch 53/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9777\n",
      "Epoch 00053: val_accuracy did not improve from 0.92602\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9777 - val_loss: 0.3282 - val_accuracy: 0.9235\n",
      "Epoch 54/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9795\n",
      "Epoch 00054: val_accuracy improved from 0.92602 to 0.92638, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1019 - accuracy: 0.9793 - val_loss: 0.3120 - val_accuracy: 0.9264\n",
      "Epoch 55/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.1045 - accuracy: 0.9791\n",
      "Epoch 00055: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1046 - accuracy: 0.9790 - val_loss: 0.3204 - val_accuracy: 0.9192\n",
      "Epoch 56/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9799\n",
      "Epoch 00056: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1010 - accuracy: 0.9799 - val_loss: 0.3199 - val_accuracy: 0.9199\n",
      "Epoch 57/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9784\n",
      "Epoch 00057: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1034 - accuracy: 0.9783 - val_loss: 0.3275 - val_accuracy: 0.9181\n",
      "Epoch 58/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9798\n",
      "Epoch 00058: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0994 - accuracy: 0.9798 - val_loss: 0.3251 - val_accuracy: 0.9202\n",
      "Epoch 59/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9816\n",
      "Epoch 00059: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0962 - accuracy: 0.9814 - val_loss: 0.3204 - val_accuracy: 0.9217\n",
      "Epoch 60/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0967 - accuracy: 0.9817\n",
      "Epoch 00060: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0965 - accuracy: 0.9818 - val_loss: 0.3396 - val_accuracy: 0.9148\n",
      "Epoch 61/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9803\n",
      "Epoch 00061: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0972 - accuracy: 0.9803 - val_loss: 0.3344 - val_accuracy: 0.9192\n",
      "Epoch 62/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.9813\n",
      "Epoch 00062: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0954 - accuracy: 0.9814 - val_loss: 0.3267 - val_accuracy: 0.9217\n",
      "Epoch 63/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9825\n",
      "Epoch 00063: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0941 - accuracy: 0.9822 - val_loss: 0.3192 - val_accuracy: 0.9231\n",
      "Epoch 64/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9803\n",
      "Epoch 00064: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0963 - accuracy: 0.9803 - val_loss: 0.3182 - val_accuracy: 0.9242\n",
      "Epoch 65/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0931 - accuracy: 0.9819\n",
      "Epoch 00065: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9819 - val_loss: 0.3137 - val_accuracy: 0.9257\n",
      "Epoch 66/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0936 - accuracy: 0.9816\n",
      "Epoch 00066: val_accuracy improved from 0.92638 to 0.92710, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0936 - accuracy: 0.9816 - val_loss: 0.3229 - val_accuracy: 0.9271\n",
      "Epoch 67/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9796\n",
      "Epoch 00067: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0956 - accuracy: 0.9796 - val_loss: 0.3250 - val_accuracy: 0.9224\n",
      "Epoch 68/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9832\n",
      "Epoch 00068: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0912 - accuracy: 0.9833 - val_loss: 0.3259 - val_accuracy: 0.9239\n",
      "Epoch 69/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9820\n",
      "Epoch 00069: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0926 - accuracy: 0.9821 - val_loss: 0.3149 - val_accuracy: 0.9228\n",
      "Epoch 70/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9809\n",
      "Epoch 00070: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0924 - accuracy: 0.9807 - val_loss: 0.3201 - val_accuracy: 0.9206\n",
      "Epoch 71/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9823\n",
      "Epoch 00071: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0901 - accuracy: 0.9823 - val_loss: 0.3187 - val_accuracy: 0.9224\n",
      "Epoch 72/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9823\n",
      "Epoch 00072: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0907 - accuracy: 0.9823 - val_loss: 0.3444 - val_accuracy: 0.9210\n",
      "Epoch 73/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767/780 [============================>.] - ETA: 0s - loss: 0.0903 - accuracy: 0.9814\n",
      "Epoch 00073: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0904 - accuracy: 0.9813 - val_loss: 0.3313 - val_accuracy: 0.9228\n",
      "Epoch 74/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9838\n",
      "Epoch 00074: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0874 - accuracy: 0.9838 - val_loss: 0.3264 - val_accuracy: 0.9202\n",
      "Epoch 75/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0872 - accuracy: 0.9828\n",
      "Epoch 00075: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0873 - accuracy: 0.9827 - val_loss: 0.3138 - val_accuracy: 0.9217\n",
      "Epoch 76/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9825\n",
      "Epoch 00076: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0872 - accuracy: 0.9824 - val_loss: 0.3238 - val_accuracy: 0.9217\n",
      "Epoch 77/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9833\n",
      "Epoch 00077: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0865 - accuracy: 0.9833 - val_loss: 0.3259 - val_accuracy: 0.9249\n",
      "Epoch 78/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9815\n",
      "Epoch 00078: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0900 - accuracy: 0.9814 - val_loss: 0.3291 - val_accuracy: 0.9235\n",
      "Epoch 79/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.9834\n",
      "Epoch 00079: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0878 - accuracy: 0.9834 - val_loss: 0.3240 - val_accuracy: 0.9249\n",
      "Epoch 80/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0844 - accuracy: 0.9824\n",
      "Epoch 00080: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9824 - val_loss: 0.3120 - val_accuracy: 0.9260\n",
      "Epoch 81/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9856\n",
      "Epoch 00081: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0822 - accuracy: 0.9856 - val_loss: 0.3171 - val_accuracy: 0.9246\n",
      "Epoch 82/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9840\n",
      "Epoch 00082: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9840 - val_loss: 0.3348 - val_accuracy: 0.9228\n",
      "Epoch 83/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9833\n",
      "Epoch 00083: val_accuracy improved from 0.92710 to 0.92927, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0850 - accuracy: 0.9833 - val_loss: 0.3195 - val_accuracy: 0.9293\n",
      "Epoch 84/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9830\n",
      "Epoch 00084: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9831 - val_loss: 0.3296 - val_accuracy: 0.9257\n",
      "Epoch 85/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9849\n",
      "Epoch 00085: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9848 - val_loss: 0.3300 - val_accuracy: 0.9282\n",
      "Epoch 86/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9849\n",
      "Epoch 00086: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9849 - val_loss: 0.3157 - val_accuracy: 0.9293\n",
      "Epoch 87/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0814 - accuracy: 0.9845\n",
      "Epoch 00087: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9844 - val_loss: 0.3307 - val_accuracy: 0.9213\n",
      "Epoch 88/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9847\n",
      "Epoch 00088: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0802 - accuracy: 0.9847 - val_loss: 0.3272 - val_accuracy: 0.9242\n",
      "Epoch 89/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9838\n",
      "Epoch 00089: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0825 - accuracy: 0.9838 - val_loss: 0.3263 - val_accuracy: 0.9271\n",
      "Epoch 90/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9858\n",
      "Epoch 00090: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0790 - accuracy: 0.9858 - val_loss: 0.3237 - val_accuracy: 0.9231\n",
      "Epoch 91/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9855\n",
      "Epoch 00091: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9855 - val_loss: 0.3337 - val_accuracy: 0.9235\n",
      "Epoch 92/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9837\n",
      "Epoch 00092: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9838 - val_loss: 0.3292 - val_accuracy: 0.9210\n",
      "Epoch 93/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9849\n",
      "Epoch 00093: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9850 - val_loss: 0.3256 - val_accuracy: 0.9278\n",
      "Epoch 94/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9846\n",
      "Epoch 00094: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0783 - accuracy: 0.9847 - val_loss: 0.3348 - val_accuracy: 0.9231\n",
      "Epoch 95/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9856\n",
      "Epoch 00095: val_accuracy improved from 0.92927 to 0.92963, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9856 - val_loss: 0.3096 - val_accuracy: 0.9296\n",
      "Epoch 96/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9848\n",
      "Epoch 00096: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0774 - accuracy: 0.9848 - val_loss: 0.3213 - val_accuracy: 0.9285\n",
      "Epoch 97/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9852\n",
      "Epoch 00097: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0768 - accuracy: 0.9853 - val_loss: 0.3326 - val_accuracy: 0.9202\n",
      "Epoch 98/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9856\n",
      "Epoch 00098: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0774 - accuracy: 0.9855 - val_loss: 0.3417 - val_accuracy: 0.9199\n",
      "Epoch 99/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9852\n",
      "Epoch 00099: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9852 - val_loss: 0.3194 - val_accuracy: 0.9235\n",
      "Epoch 100/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9854\n",
      "Epoch 00100: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0754 - accuracy: 0.9854 - val_loss: 0.3261 - val_accuracy: 0.9235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9874\n",
      "Epoch 00101: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9874 - val_loss: 0.3229 - val_accuracy: 0.9253\n",
      "Epoch 102/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9871\n",
      "Epoch 00102: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0733 - accuracy: 0.9870 - val_loss: 0.3178 - val_accuracy: 0.9235\n",
      "Epoch 103/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9851\n",
      "Epoch 00103: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0786 - accuracy: 0.9852 - val_loss: 0.3415 - val_accuracy: 0.9188\n",
      "Epoch 104/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9851\n",
      "Epoch 00104: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9849 - val_loss: 0.3323 - val_accuracy: 0.9271\n",
      "Epoch 105/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9861\n",
      "Epoch 00105: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0752 - accuracy: 0.9862 - val_loss: 0.3306 - val_accuracy: 0.9278\n",
      "Epoch 106/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9869\n",
      "Epoch 00106: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0743 - accuracy: 0.9869 - val_loss: 0.3434 - val_accuracy: 0.9220\n",
      "Epoch 107/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9877\n",
      "Epoch 00107: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9876 - val_loss: 0.3402 - val_accuracy: 0.9249\n",
      "Epoch 108/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9880\n",
      "Epoch 00108: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9879 - val_loss: 0.3526 - val_accuracy: 0.9195\n",
      "Epoch 109/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9865\n",
      "Epoch 00109: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0716 - accuracy: 0.9865 - val_loss: 0.3390 - val_accuracy: 0.9210\n",
      "Epoch 110/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9858\n",
      "Epoch 00110: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.0747 - accuracy: 0.9859 - val_loss: 0.3352 - val_accuracy: 0.9246\n",
      "Epoch 111/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9854\n",
      "Epoch 00111: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9855 - val_loss: 0.3300 - val_accuracy: 0.9239\n",
      "Epoch 112/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9872\n",
      "Epoch 00112: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0717 - accuracy: 0.9870 - val_loss: 0.3250 - val_accuracy: 0.9224\n",
      "Epoch 113/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9850\n",
      "Epoch 00113: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.0745 - accuracy: 0.9850 - val_loss: 0.3405 - val_accuracy: 0.9257\n",
      "Epoch 114/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9885\n",
      "Epoch 00114: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0686 - accuracy: 0.9885 - val_loss: 0.3255 - val_accuracy: 0.9242\n",
      "Epoch 115/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9890\n",
      "Epoch 00115: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0673 - accuracy: 0.9890 - val_loss: 0.3181 - val_accuracy: 0.9275\n",
      "Epoch 116/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9882\n",
      "Epoch 00116: val_accuracy improved from 0.92963 to 0.92999, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9882 - val_loss: 0.3101 - val_accuracy: 0.9300\n",
      "Epoch 117/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9873\n",
      "Epoch 00117: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0698 - accuracy: 0.9873 - val_loss: 0.3433 - val_accuracy: 0.9220\n",
      "Epoch 118/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9871\n",
      "Epoch 00118: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9870 - val_loss: 0.3464 - val_accuracy: 0.9184\n",
      "Epoch 119/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9870\n",
      "Epoch 00119: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0706 - accuracy: 0.9871 - val_loss: 0.3263 - val_accuracy: 0.9220\n",
      "Epoch 120/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9889\n",
      "Epoch 00120: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0664 - accuracy: 0.9888 - val_loss: 0.3276 - val_accuracy: 0.9220\n",
      "Epoch 121/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9873\n",
      "Epoch 00121: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0695 - accuracy: 0.9874 - val_loss: 0.3167 - val_accuracy: 0.9249\n",
      "Epoch 122/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9880\n",
      "Epoch 00122: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9879 - val_loss: 0.3221 - val_accuracy: 0.9264\n",
      "Epoch 123/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9855\n",
      "Epoch 00123: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0717 - accuracy: 0.9855 - val_loss: 0.3296 - val_accuracy: 0.9257\n",
      "Epoch 124/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9884\n",
      "Epoch 00124: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0650 - accuracy: 0.9884 - val_loss: 0.3230 - val_accuracy: 0.9249\n",
      "Epoch 125/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9878\n",
      "Epoch 00125: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0651 - accuracy: 0.9879 - val_loss: 0.3362 - val_accuracy: 0.9253\n",
      "Epoch 126/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9876\n",
      "Epoch 00126: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0670 - accuracy: 0.9876 - val_loss: 0.3294 - val_accuracy: 0.9260\n",
      "Epoch 127/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9882\n",
      "Epoch 00127: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0671 - accuracy: 0.9882 - val_loss: 0.3470 - val_accuracy: 0.9239\n",
      "Epoch 128/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9881\n",
      "Epoch 00128: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0677 - accuracy: 0.9882 - val_loss: 0.3384 - val_accuracy: 0.9282\n",
      "Epoch 129/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9895\n",
      "Epoch 00129: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9895 - val_loss: 0.3374 - val_accuracy: 0.9231\n",
      "Epoch 130/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9876\n",
      "Epoch 00130: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9876 - val_loss: 0.3456 - val_accuracy: 0.9239\n",
      "Epoch 131/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9873\n",
      "Epoch 00131: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0672 - accuracy: 0.9873 - val_loss: 0.3260 - val_accuracy: 0.9267\n",
      "Epoch 132/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9883\n",
      "Epoch 00132: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0655 - accuracy: 0.9881 - val_loss: 0.3173 - val_accuracy: 0.9239\n",
      "Epoch 133/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9889\n",
      "Epoch 00133: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9889 - val_loss: 0.3381 - val_accuracy: 0.9213\n",
      "Epoch 134/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9884\n",
      "Epoch 00134: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9882 - val_loss: 0.3378 - val_accuracy: 0.9239\n",
      "Epoch 135/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9870\n",
      "Epoch 00135: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0673 - accuracy: 0.9870 - val_loss: 0.3325 - val_accuracy: 0.9224\n",
      "Epoch 136/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9886\n",
      "Epoch 00136: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0638 - accuracy: 0.9886 - val_loss: 0.3236 - val_accuracy: 0.9228\n",
      "Epoch 137/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9871\n",
      "Epoch 00137: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9871 - val_loss: 0.3340 - val_accuracy: 0.9235\n",
      "Epoch 138/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9874\n",
      "Epoch 00138: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9874 - val_loss: 0.3218 - val_accuracy: 0.9271\n",
      "Epoch 139/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9889\n",
      "Epoch 00139: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0627 - accuracy: 0.9889 - val_loss: 0.3110 - val_accuracy: 0.9285\n",
      "Epoch 140/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9878\n",
      "Epoch 00140: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9878 - val_loss: 0.3326 - val_accuracy: 0.9253\n",
      "Epoch 141/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9880\n",
      "Epoch 00141: val_accuracy improved from 0.92999 to 0.93143, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0649 - accuracy: 0.9881 - val_loss: 0.3230 - val_accuracy: 0.9314\n",
      "Epoch 142/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9888\n",
      "Epoch 00142: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9887 - val_loss: 0.3261 - val_accuracy: 0.9289\n",
      "Epoch 143/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9889\n",
      "Epoch 00143: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0628 - accuracy: 0.9889 - val_loss: 0.3445 - val_accuracy: 0.9217\n",
      "Epoch 144/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9883\n",
      "Epoch 00144: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9883 - val_loss: 0.3338 - val_accuracy: 0.9257\n",
      "Epoch 145/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9884\n",
      "Epoch 00145: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0643 - accuracy: 0.9884 - val_loss: 0.3372 - val_accuracy: 0.9264\n",
      "Epoch 146/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9879\n",
      "Epoch 00146: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0656 - accuracy: 0.9880 - val_loss: 0.3190 - val_accuracy: 0.9300\n",
      "Epoch 147/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0590 - accuracy: 0.9899\n",
      "Epoch 00147: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0596 - accuracy: 0.9897 - val_loss: 0.3173 - val_accuracy: 0.9285\n",
      "Epoch 148/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9897\n",
      "Epoch 00148: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0606 - accuracy: 0.9897 - val_loss: 0.3251 - val_accuracy: 0.9260\n",
      "Epoch 149/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9905\n",
      "Epoch 00149: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0575 - accuracy: 0.9906 - val_loss: 0.3357 - val_accuracy: 0.9271\n",
      "Epoch 150/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9901\n",
      "Epoch 00150: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9901 - val_loss: 0.3457 - val_accuracy: 0.9239\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [09:16<00:00, 278.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.68      0.73      0.70       686\n",
      "        car_horn       0.80      0.88      0.83       196\n",
      "children_playing       0.74      0.83      0.78       700\n",
      "        dog_bark       0.80      0.72      0.76       700\n",
      "           siren       0.77      0.65      0.70       518\n",
      "\n",
      "        accuracy                           0.75      2800\n",
      "       macro avg       0.76      0.76      0.76      2800\n",
      "    weighted avg       0.75      0.75      0.75      2800\n",
      "\n",
      "Validation fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27671, 375)\n",
      "X_val_norm shape.....:(2835, 375)\n",
      "\n",
      "Sum of elements: 0.9802503685859969\n",
      "Number of elements summed: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 234)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               176250    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 289,985\n",
      "Trainable params: 289,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24903, 234)\n",
      "Epoch 1/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.7829 - accuracy: 0.7130\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82767, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 2ms/step - loss: 0.7823 - accuracy: 0.7133 - val_loss: 0.4944 - val_accuracy: 0.8277\n",
      "Epoch 2/350\n",
      "759/779 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.8428\n",
      "Epoch 00002: val_accuracy improved from 0.82767 to 0.86163, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.4414 - accuracy: 0.8435 - val_loss: 0.3863 - val_accuracy: 0.8616\n",
      "Epoch 3/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.3449 - accuracy: 0.8767\n",
      "Epoch 00003: val_accuracy improved from 0.86163 to 0.88439, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.3451 - accuracy: 0.8767 - val_loss: 0.3315 - val_accuracy: 0.8844\n",
      "Epoch 4/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.2792 - accuracy: 0.9002\n",
      "Epoch 00004: val_accuracy improved from 0.88439 to 0.89704, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.2792 - accuracy: 0.9003 - val_loss: 0.2919 - val_accuracy: 0.8970\n",
      "Epoch 5/350\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.2335 - accuracy: 0.9165\n",
      "Epoch 00005: val_accuracy improved from 0.89704 to 0.89993, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.2330 - accuracy: 0.9167 - val_loss: 0.2702 - val_accuracy: 0.8999\n",
      "Epoch 6/350\n",
      "759/779 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9318\n",
      "Epoch 00006: val_accuracy improved from 0.89993 to 0.91329, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.1950 - accuracy: 0.9317 - val_loss: 0.2477 - val_accuracy: 0.9133\n",
      "Epoch 7/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.9434\n",
      "Epoch 00007: val_accuracy improved from 0.91329 to 0.91727, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.1639 - accuracy: 0.9434 - val_loss: 0.2314 - val_accuracy: 0.9173\n",
      "Epoch 8/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.9525\n",
      "Epoch 00008: val_accuracy improved from 0.91727 to 0.92305, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.1385 - accuracy: 0.9524 - val_loss: 0.2177 - val_accuracy: 0.9230\n",
      "Epoch 9/350\n",
      "751/779 [===========================>..] - ETA: 0s - loss: 0.1190 - accuracy: 0.9596\n",
      "Epoch 00009: val_accuracy improved from 0.92305 to 0.93208, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.1190 - accuracy: 0.9598 - val_loss: 0.2062 - val_accuracy: 0.9321\n",
      "Epoch 10/350\n",
      "745/779 [===========================>..] - ETA: 0s - loss: 0.0981 - accuracy: 0.9676\n",
      "Epoch 00010: val_accuracy did not improve from 0.93208\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0974 - accuracy: 0.9679 - val_loss: 0.2085 - val_accuracy: 0.9270\n",
      "Epoch 11/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9731\n",
      "Epoch 00011: val_accuracy improved from 0.93208 to 0.93353, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0832 - accuracy: 0.9731 - val_loss: 0.1998 - val_accuracy: 0.9335\n",
      "Epoch 12/350\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9762\n",
      "Epoch 00012: val_accuracy improved from 0.93353 to 0.93461, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0707 - accuracy: 0.9763 - val_loss: 0.1941 - val_accuracy: 0.9346\n",
      "Epoch 13/350\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9798\n",
      "Epoch 00013: val_accuracy did not improve from 0.93461\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0614 - accuracy: 0.9797 - val_loss: 0.1944 - val_accuracy: 0.9346\n",
      "Epoch 14/350\n",
      "751/779 [===========================>..] - ETA: 0s - loss: 0.0510 - accuracy: 0.9829\n",
      "Epoch 00014: val_accuracy improved from 0.93461 to 0.93931, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0513 - accuracy: 0.9828 - val_loss: 0.1933 - val_accuracy: 0.9393\n",
      "Epoch 15/350\n",
      "749/779 [===========================>..] - ETA: 0s - loss: 0.0444 - accuracy: 0.9870\n",
      "Epoch 00015: val_accuracy improved from 0.93931 to 0.94003, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0446 - accuracy: 0.9867 - val_loss: 0.1983 - val_accuracy: 0.9400\n",
      "Epoch 16/350\n",
      "750/779 [===========================>..] - ETA: 0s - loss: 0.0369 - accuracy: 0.9893\n",
      "Epoch 00016: val_accuracy did not improve from 0.94003\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9894 - val_loss: 0.2021 - val_accuracy: 0.9400\n",
      "Epoch 17/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9903\n",
      "Epoch 00017: val_accuracy improved from 0.94003 to 0.94147, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9903 - val_loss: 0.2059 - val_accuracy: 0.9415\n",
      "Epoch 18/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9924\n",
      "Epoch 00018: val_accuracy improved from 0.94147 to 0.94256, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9925 - val_loss: 0.2025 - val_accuracy: 0.9426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "752/779 [===========================>..] - ETA: 0s - loss: 0.0239 - accuracy: 0.9939\n",
      "Epoch 00019: val_accuracy improved from 0.94256 to 0.94364, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0239 - accuracy: 0.9938 - val_loss: 0.2021 - val_accuracy: 0.9436\n",
      "Epoch 20/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9950\n",
      "Epoch 00020: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9951 - val_loss: 0.2115 - val_accuracy: 0.9408\n",
      "Epoch 21/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9956\n",
      "Epoch 00021: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9956 - val_loss: 0.2070 - val_accuracy: 0.9436\n",
      "Epoch 22/350\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9967\n",
      "Epoch 00022: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9967 - val_loss: 0.2157 - val_accuracy: 0.9415\n",
      "Epoch 23/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9962\n",
      "Epoch 00023: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0150 - accuracy: 0.9962 - val_loss: 0.2125 - val_accuracy: 0.9436\n",
      "Epoch 24/350\n",
      "756/779 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9965\n",
      "Epoch 00024: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9965 - val_loss: 0.2239 - val_accuracy: 0.9426\n",
      "Epoch 25/350\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9975\n",
      "Epoch 00025: val_accuracy improved from 0.94364 to 0.94617, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9976 - val_loss: 0.2173 - val_accuracy: 0.9462\n",
      "Epoch 26/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9972\n",
      "Epoch 00026: val_accuracy did not improve from 0.94617\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9973 - val_loss: 0.2238 - val_accuracy: 0.9451\n",
      "Epoch 27/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9973\n",
      "Epoch 00027: val_accuracy improved from 0.94617 to 0.94689, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9973 - val_loss: 0.2236 - val_accuracy: 0.9469\n",
      "Epoch 28/350\n",
      "754/779 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9979\n",
      "Epoch 00028: val_accuracy improved from 0.94689 to 0.94762, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.2246 - val_accuracy: 0.9476\n",
      "Epoch 29/350\n",
      "759/779 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9981\n",
      "Epoch 00029: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.2286 - val_accuracy: 0.9465\n",
      "Epoch 30/350\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9979\n",
      "Epoch 00030: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9978 - val_loss: 0.2344 - val_accuracy: 0.9465\n",
      "Epoch 31/350\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n",
      "Epoch 00031: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.2302 - val_accuracy: 0.9454\n",
      "Epoch 32/350\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9989\n",
      "Epoch 00032: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 0.2284 - val_accuracy: 0.9465\n",
      "Epoch 33/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9989\n",
      "Epoch 00033: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9989 - val_loss: 0.2343 - val_accuracy: 0.9476\n",
      "Epoch 34/350\n",
      "753/779 [===========================>..] - ETA: 0s - loss: 0.0058 - accuracy: 0.9988\n",
      "Epoch 00034: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9989 - val_loss: 0.2365 - val_accuracy: 0.9447\n",
      "Epoch 35/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989\n",
      "Epoch 00035: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.2353 - val_accuracy: 0.9447\n",
      "Epoch 36/350\n",
      "758/779 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9985\n",
      "Epoch 00036: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.2432 - val_accuracy: 0.9436\n",
      "Epoch 37/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9991\n",
      "Epoch 00037: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.2408 - val_accuracy: 0.9429\n",
      "Epoch 38/350\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 00038: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.2440 - val_accuracy: 0.9465\n",
      "Epoch 39/350\n",
      "748/779 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9989\n",
      "Epoch 00039: val_accuracy improved from 0.94762 to 0.94798, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.2381 - val_accuracy: 0.9480\n",
      "Epoch 40/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2451 - val_accuracy: 0.9473\n",
      "Epoch 41/350\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00041: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.2550 - val_accuracy: 0.9465\n",
      "Epoch 42/350\n",
      "753/779 [===========================>..] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00042: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.2510 - val_accuracy: 0.9462\n",
      "Epoch 43/350\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9991\n",
      "Epoch 00043: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.2493 - val_accuracy: 0.9473\n",
      "Epoch 44/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00044: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2568 - val_accuracy: 0.9451\n",
      "Epoch 45/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758/779 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00045: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.2549 - val_accuracy: 0.9465\n",
      "Epoch 46/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9995\n",
      "Epoch 00046: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.2564 - val_accuracy: 0.9476\n",
      "Epoch 47/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9992\n",
      "Epoch 00047: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.2601 - val_accuracy: 0.9480\n",
      "Epoch 48/350\n",
      "754/779 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 00048: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.2636 - val_accuracy: 0.9476\n",
      "Epoch 49/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 00049: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.2661 - val_accuracy: 0.9462\n",
      "Epoch 50/350\n",
      "757/779 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 00050: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2701 - val_accuracy: 0.9469\n",
      "Epoch 51/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9996\n",
      "Epoch 00051: val_accuracy improved from 0.94798 to 0.94978, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2659 - val_accuracy: 0.9498\n",
      "Epoch 52/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00052: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2645 - val_accuracy: 0.9494\n",
      "Epoch 53/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00053: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2644 - val_accuracy: 0.9494\n",
      "Epoch 54/350\n",
      "746/779 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 00054: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2652 - val_accuracy: 0.9494\n",
      "Epoch 55/350\n",
      "752/779 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2595 - val_accuracy: 0.9480\n",
      "Epoch 56/350\n",
      "756/779 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9996\n",
      "Epoch 00056: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2633 - val_accuracy: 0.9494\n",
      "Epoch 57/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00057: val_accuracy improved from 0.94978 to 0.95087, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2599 - val_accuracy: 0.9509\n",
      "Epoch 58/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy improved from 0.95087 to 0.95195, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2659 - val_accuracy: 0.9520\n",
      "Epoch 59/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2698 - val_accuracy: 0.9469\n",
      "Epoch 60/350\n",
      "746/779 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00060: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2763 - val_accuracy: 0.9487\n",
      "Epoch 61/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 00061: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.2685 - val_accuracy: 0.9487\n",
      "Epoch 62/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00062: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2748 - val_accuracy: 0.9462\n",
      "Epoch 63/350\n",
      "746/779 [===========================>..] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00063: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2735 - val_accuracy: 0.9469\n",
      "Epoch 64/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00064: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.2727 - val_accuracy: 0.9465\n",
      "Epoch 65/350\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 00065: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.2825 - val_accuracy: 0.9480\n",
      "Epoch 66/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00066: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2843 - val_accuracy: 0.9483\n",
      "Epoch 67/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00067: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2825 - val_accuracy: 0.9487\n",
      "Epoch 68/350\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2836 - val_accuracy: 0.9476\n",
      "Epoch 69/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2807 - val_accuracy: 0.9480\n",
      "Epoch 70/350\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00070: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2841 - val_accuracy: 0.9476\n",
      "Epoch 71/350\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00071: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2820 - val_accuracy: 0.9473\n",
      "Epoch 72/350\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00072: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2788 - val_accuracy: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00073: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2862 - val_accuracy: 0.9469\n",
      "Epoch 74/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2835 - val_accuracy: 0.9473\n",
      "Epoch 75/350\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00075: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2842 - val_accuracy: 0.9501\n",
      "Epoch 76/350\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 00076: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.2830 - val_accuracy: 0.9469\n",
      "Epoch 77/350\n",
      "758/779 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 00077: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2904 - val_accuracy: 0.9494\n",
      "Epoch 78/350\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00078: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2911 - val_accuracy: 0.9491\n",
      "Epoch 79/350\n",
      "758/779 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2848 - val_accuracy: 0.9473\n",
      "Epoch 80/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00080: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2901 - val_accuracy: 0.9491\n",
      "Epoch 81/350\n",
      "748/779 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 00081: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.2968 - val_accuracy: 0.9473\n",
      "Epoch 82/350\n",
      "748/779 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00082: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2928 - val_accuracy: 0.9491\n",
      "Epoch 83/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 9.6116e-04 - accuracy: 0.9999\n",
      "Epoch 00083: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.5890e-04 - accuracy: 0.9999 - val_loss: 0.2932 - val_accuracy: 0.9501\n",
      "Epoch 84/350\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2971 - val_accuracy: 0.9509\n",
      "Epoch 85/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00085: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2890 - val_accuracy: 0.9509\n",
      "Epoch 86/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00086: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2825 - val_accuracy: 0.9505\n",
      "Epoch 87/350\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy improved from 0.95195 to 0.95267, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2865 - val_accuracy: 0.9527\n",
      "Epoch 88/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2866 - val_accuracy: 0.9480\n",
      "Epoch 89/350\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2876 - val_accuracy: 0.9501\n",
      "Epoch 90/350\n",
      "768/779 [============================>.] - ETA: 0s - loss: 6.5594e-04 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.5374e-04 - accuracy: 1.0000 - val_loss: 0.2882 - val_accuracy: 0.9505\n",
      "Epoch 91/350\n",
      "747/779 [===========================>..] - ETA: 0s - loss: 7.3749e-04 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.7024e-04 - accuracy: 0.9999 - val_loss: 0.2907 - val_accuracy: 0.9512\n",
      "Epoch 92/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 6.8097e-04 - accuracy: 0.9999\n",
      "Epoch 00092: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.7484e-04 - accuracy: 0.9999 - val_loss: 0.2920 - val_accuracy: 0.9491\n",
      "Epoch 93/350\n",
      "751/779 [===========================>..] - ETA: 0s - loss: 8.8508e-04 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.8310e-04 - accuracy: 0.9998 - val_loss: 0.2919 - val_accuracy: 0.9491\n",
      "Epoch 94/350\n",
      "756/779 [============================>.] - ETA: 0s - loss: 8.4343e-04 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.2753e-04 - accuracy: 0.9999 - val_loss: 0.2894 - val_accuracy: 0.9498\n",
      "Epoch 95/350\n",
      "751/779 [===========================>..] - ETA: 0s - loss: 9.0789e-04 - accuracy: 0.9998\n",
      "Epoch 00095: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.9077e-04 - accuracy: 0.9998 - val_loss: 0.2807 - val_accuracy: 0.9498\n",
      "Epoch 96/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 7.7053e-04 - accuracy: 0.9999\n",
      "Epoch 00096: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.7525e-04 - accuracy: 0.9999 - val_loss: 0.2802 - val_accuracy: 0.9509\n",
      "Epoch 97/350\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00097: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2820 - val_accuracy: 0.9512\n",
      "Epoch 98/350\n",
      "762/779 [============================>.] - ETA: 0s - loss: 7.7438e-04 - accuracy: 0.9999\n",
      "Epoch 00098: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.6313e-04 - accuracy: 0.9999 - val_loss: 0.2837 - val_accuracy: 0.9512\n",
      "Epoch 99/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 9.6537e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy improved from 0.95267 to 0.95340, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.5857e-04 - accuracy: 0.9998 - val_loss: 0.2833 - val_accuracy: 0.9534\n",
      "Epoch 100/350\n",
      "752/779 [===========================>..] - ETA: 0s - loss: 9.8481e-04 - accuracy: 0.9998\n",
      "Epoch 00100: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.2779 - val_accuracy: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 7.0540e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.0416e-04 - accuracy: 1.0000 - val_loss: 0.2837 - val_accuracy: 0.9512\n",
      "Epoch 102/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 9.0493e-04 - accuracy: 0.9998\n",
      "Epoch 00102: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.1504e-04 - accuracy: 0.9998 - val_loss: 0.2897 - val_accuracy: 0.9512\n",
      "Epoch 103/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 8.8748e-04 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.8790e-04 - accuracy: 0.9998 - val_loss: 0.2924 - val_accuracy: 0.9501\n",
      "Epoch 104/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 6.2698e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.2560e-04 - accuracy: 0.9999 - val_loss: 0.2863 - val_accuracy: 0.9520\n",
      "Epoch 105/350\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 00105: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.2888 - val_accuracy: 0.9523\n",
      "Epoch 106/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 6.0772e-04 - accuracy: 1.0000\n",
      "Epoch 00106: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.0678e-04 - accuracy: 1.0000 - val_loss: 0.2914 - val_accuracy: 0.9512\n",
      "Epoch 107/350\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997    \n",
      "Epoch 00107: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.2939 - val_accuracy: 0.9505\n",
      "Epoch 108/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 7.7835e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.7589e-04 - accuracy: 0.9999 - val_loss: 0.2908 - val_accuracy: 0.9505\n",
      "Epoch 109/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 7.5469e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.5378e-04 - accuracy: 0.9999 - val_loss: 0.2922 - val_accuracy: 0.9512\n",
      "Epoch 110/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 6.4234e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.3255e-04 - accuracy: 0.9999 - val_loss: 0.2955 - val_accuracy: 0.9505\n",
      "Epoch 111/350\n",
      "762/779 [============================>.] - ETA: 0s - loss: 9.4776e-04 - accuracy: 0.9998\n",
      "Epoch 00111: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.3174e-04 - accuracy: 0.9998 - val_loss: 0.2925 - val_accuracy: 0.9516\n",
      "Epoch 112/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 8.0397e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.9370e-04 - accuracy: 0.9998 - val_loss: 0.3015 - val_accuracy: 0.9480\n",
      "Epoch 113/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 5.8683e-04 - accuracy: 1.0000\n",
      "Epoch 00113: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.8397e-04 - accuracy: 1.0000 - val_loss: 0.2944 - val_accuracy: 0.9498\n",
      "Epoch 114/350\n",
      "757/779 [============================>.] - ETA: 0s - loss: 7.7985e-04 - accuracy: 0.9998\n",
      "Epoch 00114: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.6828e-04 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9498\n",
      "Epoch 115/350\n",
      "768/779 [============================>.] - ETA: 0s - loss: 7.5491e-04 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.4996e-04 - accuracy: 1.0000 - val_loss: 0.2973 - val_accuracy: 0.9509\n",
      "Epoch 116/350\n",
      "779/779 [==============================] - ETA: 0s - loss: 8.4037e-04 - accuracy: 0.9998\n",
      "Epoch 00116: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.4037e-04 - accuracy: 0.9998 - val_loss: 0.2930 - val_accuracy: 0.9512\n",
      "Epoch 117/350\n",
      "759/779 [============================>.] - ETA: 0s - loss: 7.7162e-04 - accuracy: 0.9999\n",
      "Epoch 00117: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.8684e-04 - accuracy: 0.9999 - val_loss: 0.2890 - val_accuracy: 0.9498\n",
      "Epoch 118/350\n",
      "763/779 [============================>.] - ETA: 0s - loss: 5.6959e-04 - accuracy: 0.9998\n",
      "Epoch 00118: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.9848e-04 - accuracy: 0.9998 - val_loss: 0.2930 - val_accuracy: 0.9509\n",
      "Epoch 119/350\n",
      "757/779 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 00119: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.3033 - val_accuracy: 0.9509\n",
      "Epoch 120/350\n",
      "757/779 [============================>.] - ETA: 0s - loss: 6.7667e-04 - accuracy: 0.9999\n",
      "Epoch 00120: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.6394e-04 - accuracy: 0.9999 - val_loss: 0.2984 - val_accuracy: 0.9516\n",
      "Epoch 121/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 7.0531e-04 - accuracy: 0.9998\n",
      "Epoch 00121: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.0376e-04 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9523\n",
      "Epoch 122/350\n",
      "758/779 [============================>.] - ETA: 0s - loss: 6.9755e-04 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.9440e-04 - accuracy: 0.9999 - val_loss: 0.2963 - val_accuracy: 0.9505\n",
      "Epoch 123/350\n",
      "763/779 [============================>.] - ETA: 0s - loss: 6.2099e-04 - accuracy: 0.9999\n",
      "Epoch 00123: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.1423e-04 - accuracy: 0.9999 - val_loss: 0.3045 - val_accuracy: 0.9523\n",
      "Epoch 124/350\n",
      "753/779 [===========================>..] - ETA: 0s - loss: 9.2865e-04 - accuracy: 0.9998\n",
      "Epoch 00124: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.3207e-04 - accuracy: 0.9998 - val_loss: 0.3018 - val_accuracy: 0.9509\n",
      "Epoch 125/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 7.2572e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.1765e-04 - accuracy: 0.9999 - val_loss: 0.3004 - val_accuracy: 0.9494\n",
      "Epoch 126/350\n",
      "767/779 [============================>.] - ETA: 0s - loss: 5.4412e-04 - accuracy: 0.9999\n",
      "Epoch 00126: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.4165e-04 - accuracy: 0.9999 - val_loss: 0.3003 - val_accuracy: 0.9501\n",
      "Epoch 127/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 8.9342e-04 - accuracy: 0.9998\n",
      "Epoch 00127: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.8136e-04 - accuracy: 0.9998 - val_loss: 0.3062 - val_accuracy: 0.9520\n",
      "Epoch 128/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 5.6326e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.5545e-04 - accuracy: 0.9999 - val_loss: 0.3005 - val_accuracy: 0.9498\n",
      "Epoch 129/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/779 [============================>.] - ETA: 0s - loss: 6.6663e-04 - accuracy: 0.9999\n",
      "Epoch 00129: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.6199e-04 - accuracy: 0.9999 - val_loss: 0.3022 - val_accuracy: 0.9494\n",
      "Epoch 130/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 7.3118e-04 - accuracy: 0.9999\n",
      "Epoch 00130: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.2504e-04 - accuracy: 0.9999 - val_loss: 0.3024 - val_accuracy: 0.9498\n",
      "Epoch 131/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 7.3354e-04 - accuracy: 0.9998\n",
      "Epoch 00131: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.3240e-04 - accuracy: 0.9998 - val_loss: 0.2998 - val_accuracy: 0.9509\n",
      "Epoch 132/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 7.2816e-04 - accuracy: 0.9999\n",
      "Epoch 00132: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.2707e-04 - accuracy: 0.9999 - val_loss: 0.3025 - val_accuracy: 0.9498\n",
      "Epoch 133/350\n",
      "750/779 [===========================>..] - ETA: 0s - loss: 7.8338e-04 - accuracy: 0.9998\n",
      "Epoch 00133: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.7269e-04 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9505\n",
      "Epoch 134/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 5.4981e-04 - accuracy: 0.9999\n",
      "Epoch 00134: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.5048e-04 - accuracy: 0.9999 - val_loss: 0.3010 - val_accuracy: 0.9512\n",
      "Epoch 135/350\n",
      "767/779 [============================>.] - ETA: 0s - loss: 6.9401e-04 - accuracy: 0.9999\n",
      "Epoch 00135: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.8814e-04 - accuracy: 0.9999 - val_loss: 0.3014 - val_accuracy: 0.9498\n",
      "Epoch 136/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 4.1285e-04 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 4.1030e-04 - accuracy: 1.0000 - val_loss: 0.3021 - val_accuracy: 0.9487\n",
      "Epoch 137/350\n",
      "770/779 [============================>.] - ETA: 0s - loss: 4.9723e-04 - accuracy: 0.9999\n",
      "Epoch 00137: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 4.9295e-04 - accuracy: 0.9999 - val_loss: 0.3041 - val_accuracy: 0.9494\n",
      "Epoch 138/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 7.2816e-04 - accuracy: 0.9999\n",
      "Epoch 00138: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.1796e-04 - accuracy: 0.9999 - val_loss: 0.3029 - val_accuracy: 0.9480\n",
      "Epoch 139/350\n",
      "760/779 [============================>.] - ETA: 0s - loss: 5.8825e-04 - accuracy: 0.9999\n",
      "Epoch 00139: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.8017e-04 - accuracy: 0.9999 - val_loss: 0.3082 - val_accuracy: 0.9498\n",
      "Epoch 140/350\n",
      "762/779 [============================>.] - ETA: 0s - loss: 5.5055e-04 - accuracy: 0.9999\n",
      "Epoch 00140: val_accuracy did not improve from 0.95340\n",
      "Restoring model weights from the end of the best epoch.\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.4320e-04 - accuracy: 0.9999 - val_loss: 0.3047 - val_accuracy: 0.9505\n",
      "Epoch 00140: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:04<03:04, 184.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.71      0.73      0.72       700\n",
      "        car_horn       0.82      0.69      0.75       196\n",
      "children_playing       0.74      0.72      0.73       700\n",
      "        dog_bark       0.75      0.86      0.80       700\n",
      "           siren       0.73      0.63      0.67       539\n",
      "\n",
      "        accuracy                           0.74      2835\n",
      "       macro avg       0.75      0.73      0.74      2835\n",
      "    weighted avg       0.74      0.74      0.74      2835\n",
      "\n",
      "Model: \"Model_CNN_1D_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 228, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 228, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 228, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6384)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                319250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 330,291\n",
      "Trainable params: 330,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24903, 234, 1)\n",
      "Epoch 1/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.7451 - accuracy: 0.7629\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83454, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 3s 4ms/step - loss: 0.7451 - accuracy: 0.7629 - val_loss: 0.5461 - val_accuracy: 0.8345\n",
      "Epoch 2/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.8527\n",
      "Epoch 00002: val_accuracy improved from 0.83454 to 0.85730, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.4840 - accuracy: 0.8527 - val_loss: 0.4636 - val_accuracy: 0.8573\n",
      "Epoch 3/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8735\n",
      "Epoch 00003: val_accuracy improved from 0.85730 to 0.88078, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.4186 - accuracy: 0.8739 - val_loss: 0.4147 - val_accuracy: 0.8808\n",
      "Epoch 4/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.8912\n",
      "Epoch 00004: val_accuracy improved from 0.88078 to 0.88186, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.3734 - accuracy: 0.8914 - val_loss: 0.4093 - val_accuracy: 0.8819\n",
      "Epoch 5/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.9029\n",
      "Epoch 00005: val_accuracy improved from 0.88186 to 0.88584, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.3383 - accuracy: 0.9029 - val_loss: 0.3938 - val_accuracy: 0.8858\n",
      "Epoch 6/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.3145 - accuracy: 0.9120\n",
      "Epoch 00006: val_accuracy improved from 0.88584 to 0.90173, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.3146 - accuracy: 0.9117 - val_loss: 0.3598 - val_accuracy: 0.9017\n",
      "Epoch 7/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.9182\n",
      "Epoch 00007: val_accuracy improved from 0.90173 to 0.90210, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2953 - accuracy: 0.9183 - val_loss: 0.3491 - val_accuracy: 0.9021\n",
      "Epoch 8/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.2735 - accuracy: 0.9260\n",
      "Epoch 00008: val_accuracy improved from 0.90210 to 0.90607, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2742 - accuracy: 0.9256 - val_loss: 0.3411 - val_accuracy: 0.9061\n",
      "Epoch 9/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.9329\n",
      "Epoch 00009: val_accuracy did not improve from 0.90607\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2539 - accuracy: 0.9327 - val_loss: 0.3477 - val_accuracy: 0.9014\n",
      "Epoch 10/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.9367\n",
      "Epoch 00010: val_accuracy did not improve from 0.90607\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2444 - accuracy: 0.9363 - val_loss: 0.3717 - val_accuracy: 0.8963\n",
      "Epoch 11/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9381\n",
      "Epoch 00011: val_accuracy improved from 0.90607 to 0.91004, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2394 - accuracy: 0.9381 - val_loss: 0.3306 - val_accuracy: 0.9100\n",
      "Epoch 12/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.9413\n",
      "Epoch 00012: val_accuracy did not improve from 0.91004\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2268 - accuracy: 0.9413 - val_loss: 0.3443 - val_accuracy: 0.8988\n",
      "Epoch 13/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.2191 - accuracy: 0.9441\n",
      "Epoch 00013: val_accuracy improved from 0.91004 to 0.91113, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2189 - accuracy: 0.9440 - val_loss: 0.3245 - val_accuracy: 0.9111\n",
      "Epoch 14/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.2107 - accuracy: 0.9445\n",
      "Epoch 00014: val_accuracy did not improve from 0.91113\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2106 - accuracy: 0.9444 - val_loss: 0.3484 - val_accuracy: 0.9068\n",
      "Epoch 15/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9500\n",
      "Epoch 00015: val_accuracy improved from 0.91113 to 0.91691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1998 - accuracy: 0.9500 - val_loss: 0.3173 - val_accuracy: 0.9169\n",
      "Epoch 16/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9519\n",
      "Epoch 00016: val_accuracy improved from 0.91691 to 0.91727, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1931 - accuracy: 0.9520 - val_loss: 0.3209 - val_accuracy: 0.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.1907 - accuracy: 0.9522\n",
      "Epoch 00017: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1911 - accuracy: 0.9519 - val_loss: 0.3253 - val_accuracy: 0.9137\n",
      "Epoch 18/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9538\n",
      "Epoch 00018: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1878 - accuracy: 0.9537 - val_loss: 0.3188 - val_accuracy: 0.9147\n",
      "Epoch 19/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.1770 - accuracy: 0.9585\n",
      "Epoch 00019: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1771 - accuracy: 0.9586 - val_loss: 0.3226 - val_accuracy: 0.9144\n",
      "Epoch 20/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9574\n",
      "Epoch 00020: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1758 - accuracy: 0.9574 - val_loss: 0.3345 - val_accuracy: 0.9133\n",
      "Epoch 21/150\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9581\n",
      "Epoch 00021: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1722 - accuracy: 0.9581 - val_loss: 0.3198 - val_accuracy: 0.9173\n",
      "Epoch 22/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9607\n",
      "Epoch 00022: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1638 - accuracy: 0.9607 - val_loss: 0.3188 - val_accuracy: 0.9129\n",
      "Epoch 23/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.1634 - accuracy: 0.9623\n",
      "Epoch 00023: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1633 - accuracy: 0.9623 - val_loss: 0.3310 - val_accuracy: 0.9072\n",
      "Epoch 24/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9635\n",
      "Epoch 00024: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1567 - accuracy: 0.9633 - val_loss: 0.3258 - val_accuracy: 0.9162\n",
      "Epoch 25/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9649\n",
      "Epoch 00025: val_accuracy improved from 0.91727 to 0.91799, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1532 - accuracy: 0.9649 - val_loss: 0.3205 - val_accuracy: 0.9180\n",
      "Epoch 26/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.9642\n",
      "Epoch 00026: val_accuracy did not improve from 0.91799\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1530 - accuracy: 0.9642 - val_loss: 0.3095 - val_accuracy: 0.9176\n",
      "Epoch 27/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9658\n",
      "Epoch 00027: val_accuracy improved from 0.91799 to 0.91835, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1484 - accuracy: 0.9658 - val_loss: 0.3104 - val_accuracy: 0.9184\n",
      "Epoch 28/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9652\n",
      "Epoch 00028: val_accuracy did not improve from 0.91835\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1488 - accuracy: 0.9653 - val_loss: 0.3141 - val_accuracy: 0.9184\n",
      "Epoch 29/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9666\n",
      "Epoch 00029: val_accuracy improved from 0.91835 to 0.91908, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1450 - accuracy: 0.9666 - val_loss: 0.3155 - val_accuracy: 0.9191\n",
      "Epoch 30/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.9702\n",
      "Epoch 00030: val_accuracy improved from 0.91908 to 0.92269, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1384 - accuracy: 0.9704 - val_loss: 0.3193 - val_accuracy: 0.9227\n",
      "Epoch 31/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.9718\n",
      "Epoch 00031: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1321 - accuracy: 0.9717 - val_loss: 0.3174 - val_accuracy: 0.9184\n",
      "Epoch 32/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.9704\n",
      "Epoch 00032: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1366 - accuracy: 0.9704 - val_loss: 0.3094 - val_accuracy: 0.9212\n",
      "Epoch 33/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.9702\n",
      "Epoch 00033: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1374 - accuracy: 0.9702 - val_loss: 0.3297 - val_accuracy: 0.9151\n",
      "Epoch 34/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1354 - accuracy: 0.9694\n",
      "Epoch 00034: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1351 - accuracy: 0.9694 - val_loss: 0.3163 - val_accuracy: 0.9223\n",
      "Epoch 35/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.9729\n",
      "Epoch 00035: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1333 - accuracy: 0.9729 - val_loss: 0.3291 - val_accuracy: 0.9198\n",
      "Epoch 36/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9728\n",
      "Epoch 00036: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1244 - accuracy: 0.9729 - val_loss: 0.3122 - val_accuracy: 0.9212\n",
      "Epoch 37/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9723\n",
      "Epoch 00037: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9723 - val_loss: 0.3265 - val_accuracy: 0.9187\n",
      "Epoch 38/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9738\n",
      "Epoch 00038: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1255 - accuracy: 0.9739 - val_loss: 0.3177 - val_accuracy: 0.9187\n",
      "Epoch 39/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9720\n",
      "Epoch 00039: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1263 - accuracy: 0.9718 - val_loss: 0.3233 - val_accuracy: 0.9173\n",
      "Epoch 40/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9741\n",
      "Epoch 00040: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1234 - accuracy: 0.9741 - val_loss: 0.3251 - val_accuracy: 0.9155\n",
      "Epoch 41/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9752\n",
      "Epoch 00041: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1187 - accuracy: 0.9752 - val_loss: 0.3222 - val_accuracy: 0.9151\n",
      "Epoch 42/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9741\n",
      "Epoch 00042: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1183 - accuracy: 0.9742 - val_loss: 0.3248 - val_accuracy: 0.9187\n",
      "Epoch 43/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9752\n",
      "Epoch 00043: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1182 - accuracy: 0.9752 - val_loss: 0.3175 - val_accuracy: 0.9216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 0.9759\n",
      "Epoch 00044: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1150 - accuracy: 0.9759 - val_loss: 0.3336 - val_accuracy: 0.9176\n",
      "Epoch 45/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9752\n",
      "Epoch 00045: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1144 - accuracy: 0.9753 - val_loss: 0.3349 - val_accuracy: 0.9187\n",
      "Epoch 46/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9763\n",
      "Epoch 00046: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1122 - accuracy: 0.9763 - val_loss: 0.3265 - val_accuracy: 0.9205\n",
      "Epoch 47/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.1122 - accuracy: 0.9766\n",
      "Epoch 00047: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1124 - accuracy: 0.9765 - val_loss: 0.3303 - val_accuracy: 0.9202\n",
      "Epoch 48/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.9777\n",
      "Epoch 00048: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9777 - val_loss: 0.3361 - val_accuracy: 0.9180\n",
      "Epoch 49/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9770\n",
      "Epoch 00049: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1095 - accuracy: 0.9773 - val_loss: 0.3370 - val_accuracy: 0.9180\n",
      "Epoch 50/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.1104 - accuracy: 0.9761\n",
      "Epoch 00050: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1104 - accuracy: 0.9761 - val_loss: 0.3259 - val_accuracy: 0.9191\n",
      "Epoch 51/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9785\n",
      "Epoch 00051: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1071 - accuracy: 0.9785 - val_loss: 0.3273 - val_accuracy: 0.9212\n",
      "Epoch 52/150\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9805\n",
      "Epoch 00052: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1043 - accuracy: 0.9805 - val_loss: 0.3356 - val_accuracy: 0.9205\n",
      "Epoch 53/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.1016 - accuracy: 0.9803\n",
      "Epoch 00053: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1016 - accuracy: 0.9804 - val_loss: 0.3252 - val_accuracy: 0.9223\n",
      "Epoch 54/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1058 - accuracy: 0.9786\n",
      "Epoch 00054: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1058 - accuracy: 0.9786 - val_loss: 0.3251 - val_accuracy: 0.9216\n",
      "Epoch 55/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9782\n",
      "Epoch 00055: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1034 - accuracy: 0.9782 - val_loss: 0.3269 - val_accuracy: 0.9187\n",
      "Epoch 56/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.1054 - accuracy: 0.9784\n",
      "Epoch 00056: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1052 - accuracy: 0.9785 - val_loss: 0.3256 - val_accuracy: 0.9223\n",
      "Epoch 57/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9798\n",
      "Epoch 00057: val_accuracy improved from 0.92269 to 0.92630, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0996 - accuracy: 0.9798 - val_loss: 0.3281 - val_accuracy: 0.9263\n",
      "Epoch 58/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9803\n",
      "Epoch 00058: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0985 - accuracy: 0.9803 - val_loss: 0.3463 - val_accuracy: 0.9216\n",
      "Epoch 59/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9810\n",
      "Epoch 00059: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0979 - accuracy: 0.9809 - val_loss: 0.3517 - val_accuracy: 0.9212\n",
      "Epoch 60/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9822\n",
      "Epoch 00060: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0960 - accuracy: 0.9822 - val_loss: 0.3377 - val_accuracy: 0.9187\n",
      "Epoch 61/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9804\n",
      "Epoch 00061: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0961 - accuracy: 0.9804 - val_loss: 0.3251 - val_accuracy: 0.9249\n",
      "Epoch 62/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0962 - accuracy: 0.9810\n",
      "Epoch 00062: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0963 - accuracy: 0.9808 - val_loss: 0.3305 - val_accuracy: 0.9205\n",
      "Epoch 63/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9817\n",
      "Epoch 00063: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0964 - accuracy: 0.9816 - val_loss: 0.3173 - val_accuracy: 0.9249\n",
      "Epoch 64/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9820\n",
      "Epoch 00064: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0937 - accuracy: 0.9819 - val_loss: 0.3209 - val_accuracy: 0.9212\n",
      "Epoch 65/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9817\n",
      "Epoch 00065: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0926 - accuracy: 0.9816 - val_loss: 0.3308 - val_accuracy: 0.9234\n",
      "Epoch 66/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9816\n",
      "Epoch 00066: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0944 - accuracy: 0.9816 - val_loss: 0.3301 - val_accuracy: 0.9259\n",
      "Epoch 67/150\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9826\n",
      "Epoch 00067: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0897 - accuracy: 0.9826 - val_loss: 0.3267 - val_accuracy: 0.9252\n",
      "Epoch 68/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0902 - accuracy: 0.9820\n",
      "Epoch 00068: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0903 - accuracy: 0.9819 - val_loss: 0.3289 - val_accuracy: 0.9194\n",
      "Epoch 69/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9836\n",
      "Epoch 00069: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0882 - accuracy: 0.9835 - val_loss: 0.3286 - val_accuracy: 0.9241\n",
      "Epoch 70/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9838\n",
      "Epoch 00070: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0866 - accuracy: 0.9838 - val_loss: 0.3278 - val_accuracy: 0.9227\n",
      "Epoch 71/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9842\n",
      "Epoch 00071: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0878 - accuracy: 0.9842 - val_loss: 0.3215 - val_accuracy: 0.9249\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772/779 [============================>.] - ETA: 0s - loss: 0.0928 - accuracy: 0.9803\n",
      "Epoch 00072: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0929 - accuracy: 0.9804 - val_loss: 0.3222 - val_accuracy: 0.9223\n",
      "Epoch 73/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9837\n",
      "Epoch 00073: val_accuracy improved from 0.92630 to 0.92811, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9837 - val_loss: 0.3267 - val_accuracy: 0.9281\n",
      "Epoch 74/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9837\n",
      "Epoch 00074: val_accuracy did not improve from 0.92811\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9837 - val_loss: 0.3295 - val_accuracy: 0.9241\n",
      "Epoch 75/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0850 - accuracy: 0.9833\n",
      "Epoch 00075: val_accuracy did not improve from 0.92811\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0849 - accuracy: 0.9833 - val_loss: 0.3385 - val_accuracy: 0.9216\n",
      "Epoch 76/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9834\n",
      "Epoch 00076: val_accuracy improved from 0.92811 to 0.92883, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0864 - accuracy: 0.9835 - val_loss: 0.3246 - val_accuracy: 0.9288\n",
      "Epoch 77/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9846\n",
      "Epoch 00077: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0835 - accuracy: 0.9847 - val_loss: 0.3240 - val_accuracy: 0.9267\n",
      "Epoch 78/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9840\n",
      "Epoch 00078: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0833 - accuracy: 0.9840 - val_loss: 0.3267 - val_accuracy: 0.9270\n",
      "Epoch 79/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9838\n",
      "Epoch 00079: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0857 - accuracy: 0.9839 - val_loss: 0.3376 - val_accuracy: 0.9198\n",
      "Epoch 80/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0821 - accuracy: 0.9850\n",
      "Epoch 00080: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0819 - accuracy: 0.9851 - val_loss: 0.3310 - val_accuracy: 0.9234\n",
      "Epoch 81/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9839\n",
      "Epoch 00081: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0834 - accuracy: 0.9839 - val_loss: 0.3350 - val_accuracy: 0.9230\n",
      "Epoch 82/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9856\n",
      "Epoch 00082: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9857 - val_loss: 0.3286 - val_accuracy: 0.9259\n",
      "Epoch 83/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9839\n",
      "Epoch 00083: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0844 - accuracy: 0.9840 - val_loss: 0.3312 - val_accuracy: 0.9212\n",
      "Epoch 84/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9854\n",
      "Epoch 00084: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.9852 - val_loss: 0.3273 - val_accuracy: 0.9256\n",
      "Epoch 85/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9868\n",
      "Epoch 00085: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9869 - val_loss: 0.3524 - val_accuracy: 0.9173\n",
      "Epoch 86/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9849\n",
      "Epoch 00086: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9848 - val_loss: 0.3217 - val_accuracy: 0.9263\n",
      "Epoch 87/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9858\n",
      "Epoch 00087: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0794 - accuracy: 0.9860 - val_loss: 0.3293 - val_accuracy: 0.9223\n",
      "Epoch 88/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9864\n",
      "Epoch 00088: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9863 - val_loss: 0.3351 - val_accuracy: 0.9234\n",
      "Epoch 89/150\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9867\n",
      "Epoch 00089: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9866 - val_loss: 0.3222 - val_accuracy: 0.9270\n",
      "Epoch 90/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9858\n",
      "Epoch 00090: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0771 - accuracy: 0.9857 - val_loss: 0.3586 - val_accuracy: 0.9198\n",
      "Epoch 91/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9860\n",
      "Epoch 00091: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0774 - accuracy: 0.9860 - val_loss: 0.3336 - val_accuracy: 0.9212\n",
      "Epoch 92/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9852\n",
      "Epoch 00092: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9846 - val_loss: 0.3289 - val_accuracy: 0.9241\n",
      "Epoch 93/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9866\n",
      "Epoch 00093: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0755 - accuracy: 0.9865 - val_loss: 0.3398 - val_accuracy: 0.9238\n",
      "Epoch 94/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9858\n",
      "Epoch 00094: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0770 - accuracy: 0.9858 - val_loss: 0.3471 - val_accuracy: 0.9205\n",
      "Epoch 95/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9853\n",
      "Epoch 00095: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9853 - val_loss: 0.3316 - val_accuracy: 0.9270\n",
      "Epoch 96/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9853\n",
      "Epoch 00096: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0786 - accuracy: 0.9853 - val_loss: 0.3402 - val_accuracy: 0.9216\n",
      "Epoch 97/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9850\n",
      "Epoch 00097: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9850 - val_loss: 0.3369 - val_accuracy: 0.9220\n",
      "Epoch 98/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9847\n",
      "Epoch 00098: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0780 - accuracy: 0.9847 - val_loss: 0.3318 - val_accuracy: 0.9209\n",
      "Epoch 99/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9876\n",
      "Epoch 00099: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9876 - val_loss: 0.3387 - val_accuracy: 0.9227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9868\n",
      "Epoch 00100: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0736 - accuracy: 0.9869 - val_loss: 0.3297 - val_accuracy: 0.9252\n",
      "Epoch 101/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9871\n",
      "Epoch 00101: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9872 - val_loss: 0.3508 - val_accuracy: 0.9198\n",
      "Epoch 102/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9861\n",
      "Epoch 00102: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0764 - accuracy: 0.9861 - val_loss: 0.3261 - val_accuracy: 0.9238\n",
      "Epoch 103/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9876\n",
      "Epoch 00103: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0706 - accuracy: 0.9876 - val_loss: 0.3426 - val_accuracy: 0.9252\n",
      "Epoch 104/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9877\n",
      "Epoch 00104: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9877 - val_loss: 0.3359 - val_accuracy: 0.9259\n",
      "Epoch 105/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9893\n",
      "Epoch 00105: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9892 - val_loss: 0.3387 - val_accuracy: 0.9238\n",
      "Epoch 106/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9877\n",
      "Epoch 00106: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0702 - accuracy: 0.9877 - val_loss: 0.3467 - val_accuracy: 0.9216\n",
      "Epoch 107/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9890\n",
      "Epoch 00107: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 3s 3ms/step - loss: 0.0676 - accuracy: 0.9890 - val_loss: 0.3397 - val_accuracy: 0.9270\n",
      "Epoch 108/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9865\n",
      "Epoch 00108: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9865 - val_loss: 0.3315 - val_accuracy: 0.9223\n",
      "Epoch 109/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9867\n",
      "Epoch 00109: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0708 - accuracy: 0.9867 - val_loss: 0.3473 - val_accuracy: 0.9241\n",
      "Epoch 110/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9866\n",
      "Epoch 00110: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0710 - accuracy: 0.9865 - val_loss: 0.3348 - val_accuracy: 0.9234\n",
      "Epoch 111/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9881\n",
      "Epoch 00111: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9880 - val_loss: 0.3556 - val_accuracy: 0.9129\n",
      "Epoch 112/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9859\n",
      "Epoch 00112: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9859 - val_loss: 0.3519 - val_accuracy: 0.9216\n",
      "Epoch 113/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9864\n",
      "Epoch 00113: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0720 - accuracy: 0.9863 - val_loss: 0.3357 - val_accuracy: 0.9241\n",
      "Epoch 114/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9872\n",
      "Epoch 00114: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0686 - accuracy: 0.9871 - val_loss: 0.3409 - val_accuracy: 0.9227\n",
      "Epoch 115/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9871\n",
      "Epoch 00115: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0699 - accuracy: 0.9871 - val_loss: 0.3352 - val_accuracy: 0.9270\n",
      "Epoch 116/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0630 - accuracy: 0.9896\n",
      "Epoch 00116: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0630 - accuracy: 0.9896 - val_loss: 0.3446 - val_accuracy: 0.9274\n",
      "Epoch 117/150\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.0686 - accuracy: 0.9875\n",
      "Epoch 00117: val_accuracy improved from 0.92883 to 0.92991, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0686 - accuracy: 0.9874 - val_loss: 0.3293 - val_accuracy: 0.9299\n",
      "Epoch 118/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9893\n",
      "Epoch 00118: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9894 - val_loss: 0.3371 - val_accuracy: 0.9238\n",
      "Epoch 119/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9869\n",
      "Epoch 00119: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9867 - val_loss: 0.3295 - val_accuracy: 0.9259\n",
      "Epoch 120/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9890\n",
      "Epoch 00120: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9888 - val_loss: 0.3468 - val_accuracy: 0.9238\n",
      "Epoch 121/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9887\n",
      "Epoch 00121: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9888 - val_loss: 0.3468 - val_accuracy: 0.9230\n",
      "Epoch 122/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9888\n",
      "Epoch 00122: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9888 - val_loss: 0.3286 - val_accuracy: 0.9245\n",
      "Epoch 123/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0648 - accuracy: 0.9890\n",
      "Epoch 00123: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9889 - val_loss: 0.3354 - val_accuracy: 0.9216\n",
      "Epoch 124/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9883\n",
      "Epoch 00124: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0674 - accuracy: 0.9882 - val_loss: 0.3593 - val_accuracy: 0.9205\n",
      "Epoch 125/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9883\n",
      "Epoch 00125: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9882 - val_loss: 0.3351 - val_accuracy: 0.9249\n",
      "Epoch 126/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9876\n",
      "Epoch 00126: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9877 - val_loss: 0.3348 - val_accuracy: 0.9198\n",
      "Epoch 127/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9881\n",
      "Epoch 00127: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0654 - accuracy: 0.9881 - val_loss: 0.3443 - val_accuracy: 0.9212\n",
      "Epoch 128/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761/779 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9886\n",
      "Epoch 00128: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9887 - val_loss: 0.3489 - val_accuracy: 0.9263\n",
      "Epoch 129/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9873\n",
      "Epoch 00129: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9873 - val_loss: 0.3472 - val_accuracy: 0.9252\n",
      "Epoch 130/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9869\n",
      "Epoch 00130: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0683 - accuracy: 0.9869 - val_loss: 0.3349 - val_accuracy: 0.9292\n",
      "Epoch 131/150\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9891\n",
      "Epoch 00131: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9891 - val_loss: 0.3414 - val_accuracy: 0.9256\n",
      "Epoch 132/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9888\n",
      "Epoch 00132: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0624 - accuracy: 0.9889 - val_loss: 0.3560 - val_accuracy: 0.9245\n",
      "Epoch 133/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9870\n",
      "Epoch 00133: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0650 - accuracy: 0.9870 - val_loss: 0.3454 - val_accuracy: 0.9216\n",
      "Epoch 134/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9887\n",
      "Epoch 00134: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0646 - accuracy: 0.9887 - val_loss: 0.3560 - val_accuracy: 0.9227\n",
      "Epoch 135/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9875\n",
      "Epoch 00135: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0667 - accuracy: 0.9877 - val_loss: 0.3414 - val_accuracy: 0.9245\n",
      "Epoch 136/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9890\n",
      "Epoch 00136: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9890 - val_loss: 0.3324 - val_accuracy: 0.9234\n",
      "Epoch 137/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9898\n",
      "Epoch 00137: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0605 - accuracy: 0.9898 - val_loss: 0.3334 - val_accuracy: 0.9216\n",
      "Epoch 138/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9894\n",
      "Epoch 00138: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9892 - val_loss: 0.3230 - val_accuracy: 0.9252\n",
      "Epoch 139/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0612 - accuracy: 0.9890\n",
      "Epoch 00139: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9890 - val_loss: 0.3175 - val_accuracy: 0.9277\n",
      "Epoch 140/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0601 - accuracy: 0.9897\n",
      "Epoch 00140: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0601 - accuracy: 0.9896 - val_loss: 0.3351 - val_accuracy: 0.9238\n",
      "Epoch 141/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9877\n",
      "Epoch 00141: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9874 - val_loss: 0.3207 - val_accuracy: 0.9299\n",
      "Epoch 142/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0588 - accuracy: 0.9903\n",
      "Epoch 00142: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0591 - accuracy: 0.9902 - val_loss: 0.3325 - val_accuracy: 0.9281\n",
      "Epoch 143/150\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9895\n",
      "Epoch 00143: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0600 - accuracy: 0.9895 - val_loss: 0.3247 - val_accuracy: 0.9238\n",
      "Epoch 144/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9894\n",
      "Epoch 00144: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9895 - val_loss: 0.3524 - val_accuracy: 0.9263\n",
      "Epoch 145/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 0.9904\n",
      "Epoch 00145: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0567 - accuracy: 0.9906 - val_loss: 0.3470 - val_accuracy: 0.9205\n",
      "Epoch 146/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9905\n",
      "Epoch 00146: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0585 - accuracy: 0.9904 - val_loss: 0.3271 - val_accuracy: 0.9252\n",
      "Epoch 147/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9905\n",
      "Epoch 00147: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0577 - accuracy: 0.9904 - val_loss: 0.3577 - val_accuracy: 0.9212\n",
      "Epoch 148/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9892\n",
      "Epoch 00148: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9893 - val_loss: 0.3363 - val_accuracy: 0.9227\n",
      "Epoch 149/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0611 - accuracy: 0.9897\n",
      "Epoch 00149: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0610 - accuracy: 0.9898 - val_loss: 0.3512 - val_accuracy: 0.9209\n",
      "Epoch 150/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9896\n",
      "Epoch 00150: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9897 - val_loss: 0.3488 - val_accuracy: 0.9249\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:46<00:00, 263.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.66      0.70      0.68       700\n",
      "        car_horn       0.82      0.63      0.71       196\n",
      "children_playing       0.69      0.70      0.70       700\n",
      "        dog_bark       0.77      0.86      0.81       700\n",
      "           siren       0.72      0.59      0.65       539\n",
      "\n",
      "        accuracy                           0.71      2835\n",
      "       macro avg       0.73      0.70      0.71      2835\n",
      "    weighted avg       0.72      0.71      0.71      2835\n",
      "\n",
      "Validation fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27776, 375)\n",
      "X_val_norm shape.....:(2730, 375)\n",
      "\n",
      "Sum of elements: 0.9800451063769955\n",
      "Number of elements summed: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 234)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               176250    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 289,985\n",
      "Trainable params: 289,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24998, 234)\n",
      "Epoch 1/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.8196 - accuracy: 0.6999\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82793, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.8151 - accuracy: 0.7012 - val_loss: 0.5143 - val_accuracy: 0.8279\n",
      "Epoch 2/350\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.4584 - accuracy: 0.8368\n",
      "Epoch 00002: val_accuracy improved from 0.82793 to 0.86285, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4584 - accuracy: 0.8368 - val_loss: 0.4007 - val_accuracy: 0.8629\n",
      "Epoch 3/350\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.3532 - accuracy: 0.8733\n",
      "Epoch 00003: val_accuracy improved from 0.86285 to 0.88049, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.3527 - accuracy: 0.8733 - val_loss: 0.3464 - val_accuracy: 0.8805\n",
      "Epoch 4/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.8983\n",
      "Epoch 00004: val_accuracy improved from 0.88049 to 0.88949, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2870 - accuracy: 0.8982 - val_loss: 0.3139 - val_accuracy: 0.8895\n",
      "Epoch 5/350\n",
      "758/782 [============================>.] - ETA: 0s - loss: 0.2377 - accuracy: 0.9179\n",
      "Epoch 00005: val_accuracy improved from 0.88949 to 0.89921, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2361 - accuracy: 0.9185 - val_loss: 0.2915 - val_accuracy: 0.8992\n",
      "Epoch 6/350\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.1992 - accuracy: 0.9311\n",
      "Epoch 00006: val_accuracy improved from 0.89921 to 0.90929, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1993 - accuracy: 0.9310 - val_loss: 0.2690 - val_accuracy: 0.9093\n",
      "Epoch 7/350\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1674 - accuracy: 0.9422\n",
      "Epoch 00007: val_accuracy improved from 0.90929 to 0.91757, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1678 - accuracy: 0.9420 - val_loss: 0.2505 - val_accuracy: 0.9176\n",
      "Epoch 8/350\n",
      "758/782 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9522\n",
      "Epoch 00008: val_accuracy improved from 0.91757 to 0.92405, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1420 - accuracy: 0.9520 - val_loss: 0.2412 - val_accuracy: 0.9240\n",
      "Epoch 9/350\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9593\n",
      "Epoch 00009: val_accuracy improved from 0.92405 to 0.92729, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1209 - accuracy: 0.9597 - val_loss: 0.2295 - val_accuracy: 0.9273\n",
      "Epoch 10/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9672\n",
      "Epoch 00010: val_accuracy improved from 0.92729 to 0.93017, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1007 - accuracy: 0.9672 - val_loss: 0.2228 - val_accuracy: 0.9302\n",
      "Epoch 11/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9710\n",
      "Epoch 00011: val_accuracy did not improve from 0.93017\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0862 - accuracy: 0.9713 - val_loss: 0.2201 - val_accuracy: 0.9294\n",
      "Epoch 12/350\n",
      "761/782 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9759\n",
      "Epoch 00012: val_accuracy improved from 0.93017 to 0.93125, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0726 - accuracy: 0.9760 - val_loss: 0.2222 - val_accuracy: 0.9312\n",
      "Epoch 13/350\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9801\n",
      "Epoch 00013: val_accuracy improved from 0.93125 to 0.93521, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0619 - accuracy: 0.9802 - val_loss: 0.2199 - val_accuracy: 0.9352\n",
      "Epoch 14/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0541 - accuracy: 0.9830\n",
      "Epoch 00014: val_accuracy did not improve from 0.93521\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0539 - accuracy: 0.9830 - val_loss: 0.2109 - val_accuracy: 0.9352\n",
      "Epoch 15/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0438 - accuracy: 0.9870\n",
      "Epoch 00015: val_accuracy did not improve from 0.93521\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0438 - accuracy: 0.9870 - val_loss: 0.2181 - val_accuracy: 0.9352\n",
      "Epoch 16/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0382 - accuracy: 0.9886\n",
      "Epoch 00016: val_accuracy improved from 0.93521 to 0.93988, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.9887 - val_loss: 0.2157 - val_accuracy: 0.9399\n",
      "Epoch 17/350\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9900\n",
      "Epoch 00017: val_accuracy did not improve from 0.93988\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9900 - val_loss: 0.2441 - val_accuracy: 0.9356\n",
      "Epoch 18/350\n",
      "761/782 [============================>.] - ETA: 0s - loss: 0.0287 - accuracy: 0.9920\n",
      "Epoch 00018: val_accuracy improved from 0.93988 to 0.94096, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0290 - accuracy: 0.9918 - val_loss: 0.2222 - val_accuracy: 0.9410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9933\n",
      "Epoch 00019: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0262 - accuracy: 0.9932 - val_loss: 0.2243 - val_accuracy: 0.9392\n",
      "Epoch 20/350\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9947\n",
      "Epoch 00020: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9947 - val_loss: 0.2322 - val_accuracy: 0.9399\n",
      "Epoch 21/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9953\n",
      "Epoch 00021: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9953 - val_loss: 0.2381 - val_accuracy: 0.9381\n",
      "Epoch 22/350\n",
      "758/782 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9966\n",
      "Epoch 00022: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9965 - val_loss: 0.2438 - val_accuracy: 0.9381\n",
      "Epoch 23/350\n",
      "757/782 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9964\n",
      "Epoch 00023: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0152 - accuracy: 0.9963 - val_loss: 0.2502 - val_accuracy: 0.9406\n",
      "Epoch 24/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9969\n",
      "Epoch 00024: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.2423 - val_accuracy: 0.9406\n",
      "Epoch 25/350\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9972\n",
      "Epoch 00025: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9972 - val_loss: 0.2587 - val_accuracy: 0.9384\n",
      "Epoch 26/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9977\n",
      "Epoch 00026: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9977 - val_loss: 0.2527 - val_accuracy: 0.9399\n",
      "Epoch 27/350\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9975\n",
      "Epoch 00027: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0102 - accuracy: 0.9974 - val_loss: 0.2613 - val_accuracy: 0.9384\n",
      "Epoch 28/350\n",
      "755/782 [===========================>..] - ETA: 0s - loss: 0.0093 - accuracy: 0.9978\n",
      "Epoch 00028: val_accuracy improved from 0.94096 to 0.94204, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.2639 - val_accuracy: 0.9420\n",
      "Epoch 29/350\n",
      "757/782 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9975\n",
      "Epoch 00029: val_accuracy did not improve from 0.94204\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.2602 - val_accuracy: 0.9381\n",
      "Epoch 30/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9982\n",
      "Epoch 00030: val_accuracy did not improve from 0.94204\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9982 - val_loss: 0.2593 - val_accuracy: 0.9420\n",
      "Epoch 31/350\n",
      "749/782 [===========================>..] - ETA: 0s - loss: 0.0066 - accuracy: 0.9989\n",
      "Epoch 00031: val_accuracy did not improve from 0.94204\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.2673 - val_accuracy: 0.9399\n",
      "Epoch 32/350\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9987\n",
      "Epoch 00032: val_accuracy did not improve from 0.94204\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 0.2717 - val_accuracy: 0.9413\n",
      "Epoch 33/350\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9985\n",
      "Epoch 00033: val_accuracy improved from 0.94204 to 0.94384, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.2661 - val_accuracy: 0.9438\n",
      "Epoch 34/350\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9991\n",
      "Epoch 00034: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.2696 - val_accuracy: 0.9428\n",
      "Epoch 35/350\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9994\n",
      "Epoch 00035: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.2740 - val_accuracy: 0.9413\n",
      "Epoch 36/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9989\n",
      "Epoch 00036: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.2830 - val_accuracy: 0.9424\n",
      "Epoch 37/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9989\n",
      "Epoch 00037: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.2885 - val_accuracy: 0.9438\n",
      "Epoch 38/350\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9985\n",
      "Epoch 00038: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.2874 - val_accuracy: 0.9431\n",
      "Epoch 39/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9989\n",
      "Epoch 00039: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.2854 - val_accuracy: 0.9435\n",
      "Epoch 40/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.2864 - val_accuracy: 0.9435\n",
      "Epoch 41/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9993\n",
      "Epoch 00041: val_accuracy improved from 0.94384 to 0.94600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.2828 - val_accuracy: 0.9460\n",
      "Epoch 42/350\n",
      "750/782 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9992\n",
      "Epoch 00042: val_accuracy did not improve from 0.94600\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.2800 - val_accuracy: 0.9460\n",
      "Epoch 43/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00043: val_accuracy improved from 0.94600 to 0.94708, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2766 - val_accuracy: 0.9471\n",
      "Epoch 44/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9986\n",
      "Epoch 00044: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.2722 - val_accuracy: 0.9428\n",
      "Epoch 45/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 00045: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.2765 - val_accuracy: 0.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 00046: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.2767 - val_accuracy: 0.9442\n",
      "Epoch 47/350\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 00047: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.2806 - val_accuracy: 0.9428\n",
      "Epoch 48/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 00048: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.2830 - val_accuracy: 0.9388\n",
      "Epoch 49/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9996\n",
      "Epoch 00049: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.2807 - val_accuracy: 0.9438\n",
      "Epoch 50/350\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 00050: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2829 - val_accuracy: 0.9431\n",
      "Epoch 51/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9998\n",
      "Epoch 00051: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.2931 - val_accuracy: 0.9399\n",
      "Epoch 52/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00052: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2920 - val_accuracy: 0.9413\n",
      "Epoch 53/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9992\n",
      "Epoch 00053: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.2849 - val_accuracy: 0.9442\n",
      "Epoch 54/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 00054: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.2929 - val_accuracy: 0.9417\n",
      "Epoch 55/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9996\n",
      "Epoch 00055: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9996 - val_loss: 0.2963 - val_accuracy: 0.9413\n",
      "Epoch 56/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00056: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.2996 - val_accuracy: 0.9424\n",
      "Epoch 57/350\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 00057: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.2954 - val_accuracy: 0.9438\n",
      "Epoch 58/350\n",
      "762/782 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 00058: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2927 - val_accuracy: 0.9431\n",
      "Epoch 59/350\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00059: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.3090 - val_accuracy: 0.9413\n",
      "Epoch 60/350\n",
      "760/782 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9998\n",
      "Epoch 00060: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.3023 - val_accuracy: 0.9428\n",
      "Epoch 61/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.3038 - val_accuracy: 0.9435\n",
      "Epoch 62/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9999\n",
      "Epoch 00062: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.3089 - val_accuracy: 0.9431\n",
      "Epoch 63/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00063: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3072 - val_accuracy: 0.9438\n",
      "Epoch 64/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00064: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.3053 - val_accuracy: 0.9431\n",
      "Epoch 65/350\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9997\n",
      "Epoch 00065: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.3054 - val_accuracy: 0.9428\n",
      "Epoch 66/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 00066: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.2998 - val_accuracy: 0.9438\n",
      "Epoch 67/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 00067: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.3094 - val_accuracy: 0.9431\n",
      "Epoch 68/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00068: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9410\n",
      "Epoch 69/350\n",
      "760/782 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00069: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.3116 - val_accuracy: 0.9420\n",
      "Epoch 70/350\n",
      "760/782 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00070: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.3063 - val_accuracy: 0.9446\n",
      "Epoch 71/350\n",
      "752/782 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.3034 - val_accuracy: 0.9438\n",
      "Epoch 72/350\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00072: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.3078 - val_accuracy: 0.9428\n",
      "Epoch 73/350\n",
      "755/782 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00073: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2959 - val_accuracy: 0.9449\n",
      "Epoch 74/350\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00074: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3023 - val_accuracy: 0.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00075: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3026 - val_accuracy: 0.9442\n",
      "Epoch 76/350\n",
      "751/782 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00076: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3043 - val_accuracy: 0.9460\n",
      "Epoch 77/350\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00077: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3079 - val_accuracy: 0.9449\n",
      "Epoch 78/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3076 - val_accuracy: 0.9464\n",
      "Epoch 79/350\n",
      "754/782 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00079: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3100 - val_accuracy: 0.9453\n",
      "Epoch 80/350\n",
      "749/782 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00080: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3149 - val_accuracy: 0.9431\n",
      "Epoch 81/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00081: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3056 - val_accuracy: 0.9446\n",
      "Epoch 82/350\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 00082: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9460\n",
      "Epoch 83/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 9.9313e-04 - accuracy: 0.9999\n",
      "Epoch 00083: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3084 - val_accuracy: 0.9467\n",
      "Epoch 84/350\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3121 - val_accuracy: 0.9449\n",
      "Epoch 85/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00085: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.3182 - val_accuracy: 0.9460\n",
      "Epoch 86/350\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00086: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3138 - val_accuracy: 0.9446\n",
      "Epoch 87/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00087: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.3097 - val_accuracy: 0.9456\n",
      "Epoch 88/350\n",
      "756/782 [============================>.] - ETA: 0s - loss: 9.1036e-04 - accuracy: 0.9999\n",
      "Epoch 00088: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3158 - val_accuracy: 0.9446\n",
      "Epoch 89/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy improved from 0.94708 to 0.94744, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3185 - val_accuracy: 0.9474\n",
      "Epoch 90/350\n",
      "780/782 [============================>.] - ETA: 0s - loss: 9.7362e-04 - accuracy: 0.9998\n",
      "Epoch 00090: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.7295e-04 - accuracy: 0.9998 - val_loss: 0.3172 - val_accuracy: 0.9424\n",
      "Epoch 91/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 9.9463e-04 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.8843e-04 - accuracy: 0.9999 - val_loss: 0.3180 - val_accuracy: 0.9449\n",
      "Epoch 92/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 9.6414e-04 - accuracy: 0.9998\n",
      "Epoch 00092: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.5039e-04 - accuracy: 0.9998 - val_loss: 0.3209 - val_accuracy: 0.9431\n",
      "Epoch 93/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3179 - val_accuracy: 0.9435\n",
      "Epoch 94/350\n",
      "761/782 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00094: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.3228 - val_accuracy: 0.9460\n",
      "Epoch 95/350\n",
      "782/782 [==============================] - ETA: 0s - loss: 8.4511e-04 - accuracy: 0.9998\n",
      "Epoch 00095: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.4511e-04 - accuracy: 0.9998 - val_loss: 0.3243 - val_accuracy: 0.9453\n",
      "Epoch 96/350\n",
      "755/782 [===========================>..] - ETA: 0s - loss: 7.9958e-04 - accuracy: 1.0000\n",
      "Epoch 00096: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 7.9332e-04 - accuracy: 1.0000 - val_loss: 0.3244 - val_accuracy: 0.9442\n",
      "Epoch 97/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 9.0558e-04 - accuracy: 0.9999\n",
      "Epoch 00097: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.9790e-04 - accuracy: 0.9999 - val_loss: 0.3304 - val_accuracy: 0.9456\n",
      "Epoch 98/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3348 - val_accuracy: 0.9417\n",
      "Epoch 99/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 8.3844e-04 - accuracy: 1.0000\n",
      "Epoch 00099: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.3534e-04 - accuracy: 1.0000 - val_loss: 0.3339 - val_accuracy: 0.9417\n",
      "Epoch 100/350\n",
      "749/782 [===========================>..] - ETA: 0s - loss: 9.4830e-04 - accuracy: 0.9997\n",
      "Epoch 00100: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.2471e-04 - accuracy: 0.9998 - val_loss: 0.3277 - val_accuracy: 0.9431\n",
      "Epoch 101/350\n",
      "757/782 [============================>.] - ETA: 0s - loss: 8.0448e-04 - accuracy: 0.9999\n",
      "Epoch 00101: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.1871e-04 - accuracy: 0.9999 - val_loss: 0.3240 - val_accuracy: 0.9428\n",
      "Epoch 102/350\n",
      "757/782 [============================>.] - ETA: 0s - loss: 7.4965e-04 - accuracy: 0.9999\n",
      "Epoch 00102: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 7.5028e-04 - accuracy: 0.9999 - val_loss: 0.3189 - val_accuracy: 0.9456\n",
      "Epoch 103/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/782 [============================>.] - ETA: 0s - loss: 8.4075e-04 - accuracy: 0.9999\n",
      "Epoch 00103: val_accuracy improved from 0.94744 to 0.94780, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.4054e-04 - accuracy: 0.9999 - val_loss: 0.3140 - val_accuracy: 0.9478\n",
      "Epoch 104/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 8.9225e-04 - accuracy: 0.9998\n",
      "Epoch 00104: val_accuracy did not improve from 0.94780\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.7777e-04 - accuracy: 0.9998 - val_loss: 0.3231 - val_accuracy: 0.9464\n",
      "Epoch 105/350\n",
      "782/782 [==============================] - ETA: 0s - loss: 8.8921e-04 - accuracy: 0.9998\n",
      "Epoch 00105: val_accuracy did not improve from 0.94780\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.8921e-04 - accuracy: 0.9998 - val_loss: 0.3215 - val_accuracy: 0.9474\n",
      "Epoch 106/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 8.0071e-04 - accuracy: 0.9999\n",
      "Epoch 00106: val_accuracy did not improve from 0.94780\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.1913e-04 - accuracy: 0.9998 - val_loss: 0.3313 - val_accuracy: 0.9438\n",
      "Epoch 107/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 7.2959e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy improved from 0.94780 to 0.94960, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 7.2164e-04 - accuracy: 1.0000 - val_loss: 0.3270 - val_accuracy: 0.9496\n",
      "Epoch 108/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 6.2285e-04 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.94960\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 6.2799e-04 - accuracy: 1.0000 - val_loss: 0.3272 - val_accuracy: 0.9467\n",
      "Epoch 109/350\n",
      "781/782 [============================>.] - ETA: 0s - loss: 6.8589e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy did not improve from 0.94960\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 6.8573e-04 - accuracy: 0.9999 - val_loss: 0.3318 - val_accuracy: 0.9464\n",
      "Epoch 110/350\n",
      "749/782 [===========================>..] - ETA: 0s - loss: 9.5882e-04 - accuracy: 0.9998\n",
      "Epoch 00110: val_accuracy did not improve from 0.94960\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.2968e-04 - accuracy: 0.9998 - val_loss: 0.3320 - val_accuracy: 0.9474\n",
      "Epoch 111/350\n",
      "770/782 [============================>.] - ETA: 0s - loss: 6.7374e-04 - accuracy: 0.9999\n",
      "Epoch 00111: val_accuracy did not improve from 0.94960\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 6.8303e-04 - accuracy: 0.9999 - val_loss: 0.3309 - val_accuracy: 0.9456\n",
      "Epoch 112/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 9.5219e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.94960\n",
      "Restoring model weights from the end of the best epoch.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.4500e-04 - accuracy: 0.9998 - val_loss: 0.3289 - val_accuracy: 0.9471\n",
      "Epoch 00112: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:28<02:28, 148.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.72      0.80      0.76       560\n",
      "        car_horn       0.81      0.90      0.85       210\n",
      "children_playing       0.81      0.72      0.76       700\n",
      "        dog_bark       0.80      0.79      0.79       700\n",
      "           siren       0.80      0.81      0.81       560\n",
      "\n",
      "        accuracy                           0.78      2730\n",
      "       macro avg       0.79      0.80      0.79      2730\n",
      "    weighted avg       0.79      0.78      0.78      2730\n",
      "\n",
      "Model: \"Model_CNN_1D_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 228, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 228, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 228, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6384)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                319250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 330,291\n",
      "Trainable params: 330,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24998, 234, 1)\n",
      "Epoch 1/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.7778 - accuracy: 0.7519\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83585, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7778 - accuracy: 0.7519 - val_loss: 0.5536 - val_accuracy: 0.8359\n",
      "Epoch 2/150\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.5030 - accuracy: 0.8480\n",
      "Epoch 00002: val_accuracy improved from 0.83585 to 0.86537, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5028 - accuracy: 0.8481 - val_loss: 0.4692 - val_accuracy: 0.8654\n",
      "Epoch 3/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.4177 - accuracy: 0.8761\n",
      "Epoch 00003: val_accuracy improved from 0.86537 to 0.87545, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.4180 - accuracy: 0.8761 - val_loss: 0.4344 - val_accuracy: 0.8754\n",
      "Epoch 4/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8923\n",
      "Epoch 00004: val_accuracy improved from 0.87545 to 0.88373, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3664 - accuracy: 0.8924 - val_loss: 0.4085 - val_accuracy: 0.8837\n",
      "Epoch 5/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.9043\n",
      "Epoch 00005: val_accuracy improved from 0.88373 to 0.88805, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3335 - accuracy: 0.9040 - val_loss: 0.3947 - val_accuracy: 0.8880\n",
      "Epoch 6/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.9176\n",
      "Epoch 00006: val_accuracy improved from 0.88805 to 0.89237, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3029 - accuracy: 0.9176 - val_loss: 0.3861 - val_accuracy: 0.8924\n",
      "Epoch 7/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9208\n",
      "Epoch 00007: val_accuracy improved from 0.89237 to 0.89813, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2860 - accuracy: 0.9208 - val_loss: 0.3730 - val_accuracy: 0.8981\n",
      "Epoch 8/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.2675 - accuracy: 0.9273\n",
      "Epoch 00008: val_accuracy did not improve from 0.89813\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2674 - accuracy: 0.9273 - val_loss: 0.3731 - val_accuracy: 0.8934\n",
      "Epoch 9/150\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9307\n",
      "Epoch 00009: val_accuracy improved from 0.89813 to 0.90029, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2577 - accuracy: 0.9306 - val_loss: 0.3649 - val_accuracy: 0.9003\n",
      "Epoch 10/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9357\n",
      "Epoch 00010: val_accuracy did not improve from 0.90029\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2437 - accuracy: 0.9357 - val_loss: 0.3599 - val_accuracy: 0.8985\n",
      "Epoch 11/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.2297 - accuracy: 0.9404\n",
      "Epoch 00011: val_accuracy improved from 0.90029 to 0.90677, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2304 - accuracy: 0.9401 - val_loss: 0.3438 - val_accuracy: 0.9068\n",
      "Epoch 12/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.9424\n",
      "Epoch 00012: val_accuracy did not improve from 0.90677\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2210 - accuracy: 0.9423 - val_loss: 0.3650 - val_accuracy: 0.9032\n",
      "Epoch 13/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9477\n",
      "Epoch 00013: val_accuracy improved from 0.90677 to 0.90713, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2082 - accuracy: 0.9476 - val_loss: 0.3570 - val_accuracy: 0.9071\n",
      "Epoch 14/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9497\n",
      "Epoch 00014: val_accuracy improved from 0.90713 to 0.90929, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2024 - accuracy: 0.9495 - val_loss: 0.3535 - val_accuracy: 0.9093\n",
      "Epoch 15/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1955 - accuracy: 0.9521\n",
      "Epoch 00015: val_accuracy did not improve from 0.90929\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1961 - accuracy: 0.9516 - val_loss: 0.3732 - val_accuracy: 0.9046\n",
      "Epoch 16/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9542\n",
      "Epoch 00016: val_accuracy did not improve from 0.90929\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1876 - accuracy: 0.9542 - val_loss: 0.3584 - val_accuracy: 0.9068\n",
      "Epoch 17/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/782 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9560\n",
      "Epoch 00017: val_accuracy did not improve from 0.90929\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1814 - accuracy: 0.9562 - val_loss: 0.3627 - val_accuracy: 0.9082\n",
      "Epoch 18/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.9587\n",
      "Epoch 00018: val_accuracy improved from 0.90929 to 0.91001, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1758 - accuracy: 0.9587 - val_loss: 0.3582 - val_accuracy: 0.9100\n",
      "Epoch 19/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.9598\n",
      "Epoch 00019: val_accuracy did not improve from 0.91001\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1698 - accuracy: 0.9596 - val_loss: 0.3617 - val_accuracy: 0.9082\n",
      "Epoch 20/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.1687 - accuracy: 0.9599\n",
      "Epoch 00020: val_accuracy did not improve from 0.91001\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1687 - accuracy: 0.9600 - val_loss: 0.3652 - val_accuracy: 0.9071\n",
      "Epoch 21/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9609\n",
      "Epoch 00021: val_accuracy did not improve from 0.91001\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1613 - accuracy: 0.9610 - val_loss: 0.3624 - val_accuracy: 0.9100\n",
      "Epoch 22/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.1628 - accuracy: 0.9624\n",
      "Epoch 00022: val_accuracy improved from 0.91001 to 0.91181, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1634 - accuracy: 0.9620 - val_loss: 0.3578 - val_accuracy: 0.9118\n",
      "Epoch 23/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9633\n",
      "Epoch 00023: val_accuracy did not improve from 0.91181\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1567 - accuracy: 0.9634 - val_loss: 0.3510 - val_accuracy: 0.9078\n",
      "Epoch 24/150\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.1518 - accuracy: 0.9665\n",
      "Epoch 00024: val_accuracy improved from 0.91181 to 0.91505, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1519 - accuracy: 0.9664 - val_loss: 0.3580 - val_accuracy: 0.9150\n",
      "Epoch 25/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.1508 - accuracy: 0.9653\n",
      "Epoch 00025: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1508 - accuracy: 0.9654 - val_loss: 0.3633 - val_accuracy: 0.9089\n",
      "Epoch 26/150\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.1493 - accuracy: 0.9655\n",
      "Epoch 00026: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1503 - accuracy: 0.9653 - val_loss: 0.3539 - val_accuracy: 0.9136\n",
      "Epoch 27/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.9679\n",
      "Epoch 00027: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1428 - accuracy: 0.9677 - val_loss: 0.3642 - val_accuracy: 0.9111\n",
      "Epoch 28/150\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9677\n",
      "Epoch 00028: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1447 - accuracy: 0.9678 - val_loss: 0.3653 - val_accuracy: 0.9107\n",
      "Epoch 29/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9690\n",
      "Epoch 00029: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1408 - accuracy: 0.9692 - val_loss: 0.3604 - val_accuracy: 0.9114\n",
      "Epoch 30/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9698\n",
      "Epoch 00030: val_accuracy improved from 0.91505 to 0.91541, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1345 - accuracy: 0.9698 - val_loss: 0.3663 - val_accuracy: 0.9154\n",
      "Epoch 31/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9703\n",
      "Epoch 00031: val_accuracy did not improve from 0.91541\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1338 - accuracy: 0.9703 - val_loss: 0.3620 - val_accuracy: 0.9147\n",
      "Epoch 32/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9725\n",
      "Epoch 00032: val_accuracy did not improve from 0.91541\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1279 - accuracy: 0.9725 - val_loss: 0.3592 - val_accuracy: 0.9147\n",
      "Epoch 33/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9719\n",
      "Epoch 00033: val_accuracy did not improve from 0.91541\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1296 - accuracy: 0.9717 - val_loss: 0.4030 - val_accuracy: 0.9078\n",
      "Epoch 34/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9720\n",
      "Epoch 00034: val_accuracy improved from 0.91541 to 0.91865, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1288 - accuracy: 0.9722 - val_loss: 0.3558 - val_accuracy: 0.9186\n",
      "Epoch 35/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9738\n",
      "Epoch 00035: val_accuracy did not improve from 0.91865\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1227 - accuracy: 0.9736 - val_loss: 0.3713 - val_accuracy: 0.9125\n",
      "Epoch 36/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9746\n",
      "Epoch 00036: val_accuracy did not improve from 0.91865\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1234 - accuracy: 0.9746 - val_loss: 0.3680 - val_accuracy: 0.9172\n",
      "Epoch 37/150\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9745\n",
      "Epoch 00037: val_accuracy did not improve from 0.91865\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1213 - accuracy: 0.9746 - val_loss: 0.3628 - val_accuracy: 0.9136\n",
      "Epoch 38/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9765\n",
      "Epoch 00038: val_accuracy did not improve from 0.91865\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1150 - accuracy: 0.9764 - val_loss: 0.3671 - val_accuracy: 0.9150\n",
      "Epoch 39/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9760\n",
      "Epoch 00039: val_accuracy improved from 0.91865 to 0.92117, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1179 - accuracy: 0.9758 - val_loss: 0.3611 - val_accuracy: 0.9212\n",
      "Epoch 40/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9766\n",
      "Epoch 00040: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1140 - accuracy: 0.9766 - val_loss: 0.3808 - val_accuracy: 0.9172\n",
      "Epoch 41/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9764\n",
      "Epoch 00041: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1139 - accuracy: 0.9762 - val_loss: 0.3648 - val_accuracy: 0.9179\n",
      "Epoch 42/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9753\n",
      "Epoch 00042: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1154 - accuracy: 0.9753 - val_loss: 0.3573 - val_accuracy: 0.9154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9743\n",
      "Epoch 00043: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1171 - accuracy: 0.9742 - val_loss: 0.3618 - val_accuracy: 0.9201\n",
      "Epoch 44/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 0.9767\n",
      "Epoch 00044: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1143 - accuracy: 0.9763 - val_loss: 0.3623 - val_accuracy: 0.9140\n",
      "Epoch 45/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9763\n",
      "Epoch 00045: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1127 - accuracy: 0.9761 - val_loss: 0.3751 - val_accuracy: 0.9176\n",
      "Epoch 46/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.9795\n",
      "Epoch 00046: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1042 - accuracy: 0.9795 - val_loss: 0.3638 - val_accuracy: 0.9212\n",
      "Epoch 47/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9798\n",
      "Epoch 00047: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1047 - accuracy: 0.9796 - val_loss: 0.3778 - val_accuracy: 0.9176\n",
      "Epoch 48/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9780\n",
      "Epoch 00048: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1059 - accuracy: 0.9779 - val_loss: 0.3599 - val_accuracy: 0.9183\n",
      "Epoch 49/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9792\n",
      "Epoch 00049: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1051 - accuracy: 0.9792 - val_loss: 0.3665 - val_accuracy: 0.9165\n",
      "Epoch 50/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.1008 - accuracy: 0.9799\n",
      "Epoch 00050: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1009 - accuracy: 0.9799 - val_loss: 0.3597 - val_accuracy: 0.9190\n",
      "Epoch 51/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9797\n",
      "Epoch 00051: val_accuracy improved from 0.92117 to 0.92261, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1024 - accuracy: 0.9799 - val_loss: 0.3730 - val_accuracy: 0.9226\n",
      "Epoch 52/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9799\n",
      "Epoch 00052: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9800 - val_loss: 0.3814 - val_accuracy: 0.9176\n",
      "Epoch 53/150\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9800\n",
      "Epoch 00053: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1010 - accuracy: 0.9800 - val_loss: 0.3837 - val_accuracy: 0.9168\n",
      "Epoch 54/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9827\n",
      "Epoch 00054: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0958 - accuracy: 0.9825 - val_loss: 0.3690 - val_accuracy: 0.9197\n",
      "Epoch 55/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9797\n",
      "Epoch 00055: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0983 - accuracy: 0.9797 - val_loss: 0.3948 - val_accuracy: 0.9129\n",
      "Epoch 56/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9817\n",
      "Epoch 00056: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0955 - accuracy: 0.9817 - val_loss: 0.3912 - val_accuracy: 0.9194\n",
      "Epoch 57/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0973 - accuracy: 0.9815\n",
      "Epoch 00057: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0977 - accuracy: 0.9814 - val_loss: 0.3960 - val_accuracy: 0.9147\n",
      "Epoch 58/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9815\n",
      "Epoch 00058: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0944 - accuracy: 0.9815 - val_loss: 0.3726 - val_accuracy: 0.9204\n",
      "Epoch 59/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.9830\n",
      "Epoch 00059: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0906 - accuracy: 0.9830 - val_loss: 0.3822 - val_accuracy: 0.9219\n",
      "Epoch 60/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9825\n",
      "Epoch 00060: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0920 - accuracy: 0.9823 - val_loss: 0.3891 - val_accuracy: 0.9208\n",
      "Epoch 61/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9822\n",
      "Epoch 00061: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0946 - accuracy: 0.9821 - val_loss: 0.3966 - val_accuracy: 0.9165\n",
      "Epoch 62/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9824\n",
      "Epoch 00062: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0910 - accuracy: 0.9825 - val_loss: 0.3829 - val_accuracy: 0.9186\n",
      "Epoch 63/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9827\n",
      "Epoch 00063: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.9827 - val_loss: 0.3858 - val_accuracy: 0.9176\n",
      "Epoch 64/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9822\n",
      "Epoch 00064: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0916 - accuracy: 0.9821 - val_loss: 0.3882 - val_accuracy: 0.9179\n",
      "Epoch 65/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0910 - accuracy: 0.9816\n",
      "Epoch 00065: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0907 - accuracy: 0.9817 - val_loss: 0.3673 - val_accuracy: 0.9172\n",
      "Epoch 66/150\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9818\n",
      "Epoch 00066: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0911 - accuracy: 0.9818 - val_loss: 0.3875 - val_accuracy: 0.9172\n",
      "Epoch 67/150\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9822\n",
      "Epoch 00067: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9822 - val_loss: 0.3815 - val_accuracy: 0.9190\n",
      "Epoch 68/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9833\n",
      "Epoch 00068: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0880 - accuracy: 0.9833 - val_loss: 0.3954 - val_accuracy: 0.9158\n",
      "Epoch 69/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0874 - accuracy: 0.9837\n",
      "Epoch 00069: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0878 - accuracy: 0.9835 - val_loss: 0.3816 - val_accuracy: 0.9143\n",
      "Epoch 70/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9836\n",
      "Epoch 00070: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0856 - accuracy: 0.9834 - val_loss: 0.3786 - val_accuracy: 0.9186\n",
      "Epoch 71/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/782 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9839\n",
      "Epoch 00071: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9838 - val_loss: 0.3845 - val_accuracy: 0.9208\n",
      "Epoch 72/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9857\n",
      "Epoch 00072: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0808 - accuracy: 0.9858 - val_loss: 0.3944 - val_accuracy: 0.9150\n",
      "Epoch 73/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9833\n",
      "Epoch 00073: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9833 - val_loss: 0.3737 - val_accuracy: 0.9197\n",
      "Epoch 74/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0814 - accuracy: 0.9848\n",
      "Epoch 00074: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0814 - accuracy: 0.9848 - val_loss: 0.3961 - val_accuracy: 0.9168\n",
      "Epoch 75/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9852\n",
      "Epoch 00075: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9852 - val_loss: 0.4033 - val_accuracy: 0.9168\n",
      "Epoch 76/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9845\n",
      "Epoch 00076: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0813 - accuracy: 0.9844 - val_loss: 0.3922 - val_accuracy: 0.9194\n",
      "Epoch 77/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9848\n",
      "Epoch 00077: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0824 - accuracy: 0.9847 - val_loss: 0.4006 - val_accuracy: 0.9190\n",
      "Epoch 78/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9848\n",
      "Epoch 00078: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0818 - accuracy: 0.9847 - val_loss: 0.3924 - val_accuracy: 0.9212\n",
      "Epoch 79/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9842\n",
      "Epoch 00079: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0827 - accuracy: 0.9842 - val_loss: 0.3989 - val_accuracy: 0.9197\n",
      "Epoch 80/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9865\n",
      "Epoch 00080: val_accuracy improved from 0.92261 to 0.92585, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0761 - accuracy: 0.9864 - val_loss: 0.3798 - val_accuracy: 0.9258\n",
      "Epoch 81/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9863\n",
      "Epoch 00081: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0776 - accuracy: 0.9863 - val_loss: 0.3742 - val_accuracy: 0.9222\n",
      "Epoch 82/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9846\n",
      "Epoch 00082: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0797 - accuracy: 0.9844 - val_loss: 0.4086 - val_accuracy: 0.9204\n",
      "Epoch 83/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9863\n",
      "Epoch 00083: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0773 - accuracy: 0.9865 - val_loss: 0.4015 - val_accuracy: 0.9197\n",
      "Epoch 84/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9859\n",
      "Epoch 00084: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0779 - accuracy: 0.9859 - val_loss: 0.3834 - val_accuracy: 0.9237\n",
      "Epoch 85/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0788 - accuracy: 0.9846\n",
      "Epoch 00085: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0788 - accuracy: 0.9846 - val_loss: 0.3980 - val_accuracy: 0.9222\n",
      "Epoch 86/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9864\n",
      "Epoch 00086: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9864 - val_loss: 0.4013 - val_accuracy: 0.9201\n",
      "Epoch 87/150\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9852\n",
      "Epoch 00087: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0779 - accuracy: 0.9852 - val_loss: 0.4028 - val_accuracy: 0.9208\n",
      "Epoch 88/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9853\n",
      "Epoch 00088: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0777 - accuracy: 0.9854 - val_loss: 0.4014 - val_accuracy: 0.9208\n",
      "Epoch 89/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9852\n",
      "Epoch 00089: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9853 - val_loss: 0.3977 - val_accuracy: 0.9233\n",
      "Epoch 90/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9864\n",
      "Epoch 00090: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0756 - accuracy: 0.9864 - val_loss: 0.3834 - val_accuracy: 0.9212\n",
      "Epoch 91/150\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9850\n",
      "Epoch 00091: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9849 - val_loss: 0.3826 - val_accuracy: 0.9215\n",
      "Epoch 92/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9848\n",
      "Epoch 00092: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0782 - accuracy: 0.9848 - val_loss: 0.4132 - val_accuracy: 0.9212\n",
      "Epoch 93/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9852\n",
      "Epoch 00093: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9851 - val_loss: 0.4130 - val_accuracy: 0.9176\n",
      "Epoch 94/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9878\n",
      "Epoch 00094: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9878 - val_loss: 0.3988 - val_accuracy: 0.9226\n",
      "Epoch 95/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9864\n",
      "Epoch 00095: val_accuracy improved from 0.92585 to 0.92801, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0747 - accuracy: 0.9866 - val_loss: 0.3939 - val_accuracy: 0.9280\n",
      "Epoch 96/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9878\n",
      "Epoch 00096: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9878 - val_loss: 0.4159 - val_accuracy: 0.9197\n",
      "Epoch 97/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9880\n",
      "Epoch 00097: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0711 - accuracy: 0.9879 - val_loss: 0.3976 - val_accuracy: 0.9233\n",
      "Epoch 98/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9877\n",
      "Epoch 00098: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9878 - val_loss: 0.4077 - val_accuracy: 0.9208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9872\n",
      "Epoch 00099: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0712 - accuracy: 0.9872 - val_loss: 0.3966 - val_accuracy: 0.9215\n",
      "Epoch 100/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9874\n",
      "Epoch 00100: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0722 - accuracy: 0.9874 - val_loss: 0.3938 - val_accuracy: 0.9244\n",
      "Epoch 101/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9872\n",
      "Epoch 00101: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0699 - accuracy: 0.9873 - val_loss: 0.3949 - val_accuracy: 0.9237\n",
      "Epoch 102/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9882\n",
      "Epoch 00102: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0703 - accuracy: 0.9882 - val_loss: 0.3984 - val_accuracy: 0.9233\n",
      "Epoch 103/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9885\n",
      "Epoch 00103: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9884 - val_loss: 0.4079 - val_accuracy: 0.9204\n",
      "Epoch 104/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9871\n",
      "Epoch 00104: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0722 - accuracy: 0.9869 - val_loss: 0.3903 - val_accuracy: 0.9255\n",
      "Epoch 105/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9882\n",
      "Epoch 00105: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9884 - val_loss: 0.3866 - val_accuracy: 0.9226\n",
      "Epoch 106/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9862\n",
      "Epoch 00106: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0705 - accuracy: 0.9862 - val_loss: 0.4204 - val_accuracy: 0.9240\n",
      "Epoch 107/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9880\n",
      "Epoch 00107: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0680 - accuracy: 0.9881 - val_loss: 0.4149 - val_accuracy: 0.9183\n",
      "Epoch 108/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9867\n",
      "Epoch 00108: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0691 - accuracy: 0.9867 - val_loss: 0.4040 - val_accuracy: 0.9222\n",
      "Epoch 109/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9878\n",
      "Epoch 00109: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0688 - accuracy: 0.9878 - val_loss: 0.4018 - val_accuracy: 0.9251\n",
      "Epoch 110/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9900\n",
      "Epoch 00110: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0622 - accuracy: 0.9900 - val_loss: 0.3975 - val_accuracy: 0.9255\n",
      "Epoch 111/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9881\n",
      "Epoch 00111: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9879 - val_loss: 0.4106 - val_accuracy: 0.9212\n",
      "Epoch 112/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9876\n",
      "Epoch 00112: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0705 - accuracy: 0.9876 - val_loss: 0.4118 - val_accuracy: 0.9208\n",
      "Epoch 113/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9901\n",
      "Epoch 00113: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0616 - accuracy: 0.9901 - val_loss: 0.4072 - val_accuracy: 0.9212\n",
      "Epoch 114/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9879\n",
      "Epoch 00114: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0671 - accuracy: 0.9880 - val_loss: 0.4100 - val_accuracy: 0.9226\n",
      "Epoch 115/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9885\n",
      "Epoch 00115: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0653 - accuracy: 0.9885 - val_loss: 0.4085 - val_accuracy: 0.9190\n",
      "Epoch 116/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9891\n",
      "Epoch 00116: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9891 - val_loss: 0.4128 - val_accuracy: 0.9237\n",
      "Epoch 117/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0648 - accuracy: 0.9890\n",
      "Epoch 00117: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9889 - val_loss: 0.4002 - val_accuracy: 0.9215\n",
      "Epoch 118/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9893\n",
      "Epoch 00118: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9892 - val_loss: 0.4057 - val_accuracy: 0.9248\n",
      "Epoch 119/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0601 - accuracy: 0.9908\n",
      "Epoch 00119: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0601 - accuracy: 0.9908 - val_loss: 0.4049 - val_accuracy: 0.9273\n",
      "Epoch 120/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9890\n",
      "Epoch 00120: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0645 - accuracy: 0.9890 - val_loss: 0.4340 - val_accuracy: 0.9215\n",
      "Epoch 121/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0648 - accuracy: 0.9876\n",
      "Epoch 00121: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9877 - val_loss: 0.3876 - val_accuracy: 0.9244\n",
      "Epoch 122/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9875\n",
      "Epoch 00122: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9875 - val_loss: 0.4273 - val_accuracy: 0.9172\n",
      "Epoch 123/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.9886\n",
      "Epoch 00123: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9885 - val_loss: 0.4201 - val_accuracy: 0.9219\n",
      "Epoch 124/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9877\n",
      "Epoch 00124: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0669 - accuracy: 0.9876 - val_loss: 0.3927 - val_accuracy: 0.9237\n",
      "Epoch 125/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9893\n",
      "Epoch 00125: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0646 - accuracy: 0.9893 - val_loss: 0.4079 - val_accuracy: 0.9230\n",
      "Epoch 126/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9888\n",
      "Epoch 00126: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0645 - accuracy: 0.9888 - val_loss: 0.4064 - val_accuracy: 0.9226\n",
      "Epoch 127/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9879\n",
      "Epoch 00127: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9879 - val_loss: 0.4128 - val_accuracy: 0.9230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9898\n",
      "Epoch 00128: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9898 - val_loss: 0.3984 - val_accuracy: 0.9222\n",
      "Epoch 129/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9894\n",
      "Epoch 00129: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0598 - accuracy: 0.9895 - val_loss: 0.4147 - val_accuracy: 0.9219\n",
      "Epoch 130/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9881\n",
      "Epoch 00130: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0641 - accuracy: 0.9882 - val_loss: 0.4096 - val_accuracy: 0.9255\n",
      "Epoch 131/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9898\n",
      "Epoch 00131: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9899 - val_loss: 0.4041 - val_accuracy: 0.9197\n",
      "Epoch 132/150\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9888\n",
      "Epoch 00132: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0627 - accuracy: 0.9889 - val_loss: 0.4106 - val_accuracy: 0.9215\n",
      "Epoch 133/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9888\n",
      "Epoch 00133: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9888 - val_loss: 0.3990 - val_accuracy: 0.9248\n",
      "Epoch 134/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9887\n",
      "Epoch 00134: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9888 - val_loss: 0.4096 - val_accuracy: 0.9251\n",
      "Epoch 135/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9895\n",
      "Epoch 00135: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9894 - val_loss: 0.3852 - val_accuracy: 0.9248\n",
      "Epoch 136/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9896\n",
      "Epoch 00136: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9896 - val_loss: 0.4229 - val_accuracy: 0.9226\n",
      "Epoch 137/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0601 - accuracy: 0.9892\n",
      "Epoch 00137: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0600 - accuracy: 0.9893 - val_loss: 0.4139 - val_accuracy: 0.9237\n",
      "Epoch 138/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9900\n",
      "Epoch 00138: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0599 - accuracy: 0.9900 - val_loss: 0.4165 - val_accuracy: 0.9172\n",
      "Epoch 139/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0594 - accuracy: 0.9898\n",
      "Epoch 00139: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0595 - accuracy: 0.9899 - val_loss: 0.4046 - val_accuracy: 0.9230\n",
      "Epoch 140/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0632 - accuracy: 0.9897\n",
      "Epoch 00140: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0630 - accuracy: 0.9897 - val_loss: 0.4158 - val_accuracy: 0.9194\n",
      "Epoch 141/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9893\n",
      "Epoch 00141: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0586 - accuracy: 0.9892 - val_loss: 0.4045 - val_accuracy: 0.9215\n",
      "Epoch 142/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0540 - accuracy: 0.9912\n",
      "Epoch 00142: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0541 - accuracy: 0.9912 - val_loss: 0.4144 - val_accuracy: 0.9266\n",
      "Epoch 143/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9891\n",
      "Epoch 00143: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9892 - val_loss: 0.4072 - val_accuracy: 0.9201\n",
      "Epoch 144/150\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0611 - accuracy: 0.9888\n",
      "Epoch 00144: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0610 - accuracy: 0.9889 - val_loss: 0.4094 - val_accuracy: 0.9226\n",
      "Epoch 145/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0562 - accuracy: 0.9907\n",
      "Epoch 00145: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0562 - accuracy: 0.9907 - val_loss: 0.4090 - val_accuracy: 0.9226\n",
      "Epoch 146/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9902\n",
      "Epoch 00146: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0576 - accuracy: 0.9903 - val_loss: 0.4000 - val_accuracy: 0.9244\n",
      "Epoch 147/150\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0566 - accuracy: 0.9908\n",
      "Epoch 00147: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0564 - accuracy: 0.9908 - val_loss: 0.4086 - val_accuracy: 0.9222\n",
      "Epoch 148/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9900\n",
      "Epoch 00148: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0577 - accuracy: 0.9899 - val_loss: 0.3998 - val_accuracy: 0.9240\n",
      "Epoch 149/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9893\n",
      "Epoch 00149: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0571 - accuracy: 0.9892 - val_loss: 0.4202 - val_accuracy: 0.9208\n",
      "Epoch 150/150\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9894\n",
      "Epoch 00150: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0582 - accuracy: 0.9894 - val_loss: 0.4002 - val_accuracy: 0.9183\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:06<00:00, 243.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.69      0.78      0.73       560\n",
      "        car_horn       0.91      0.91      0.91       210\n",
      "children_playing       0.73      0.67      0.70       700\n",
      "        dog_bark       0.77      0.74      0.76       700\n",
      "           siren       0.79      0.80      0.79       560\n",
      "\n",
      "        accuracy                           0.76      2730\n",
      "       macro avg       0.78      0.78      0.78      2730\n",
      "    weighted avg       0.76      0.76      0.76      2730\n",
      "\n",
      "Validation fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27734, 375)\n",
      "X_val_norm shape.....:(2772, 375)\n",
      "\n",
      "Sum of elements: 0.980110730570213\n",
      "Number of elements summed: 235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 235)               55460     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 235)               55460     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 235)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               177000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 291,675\n",
      "Trainable params: 291,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24960, 235)\n",
      "Epoch 1/350\n",
      "  1/780 [..............................] - ETA: 0s - loss: 1.7406 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.8157 - accuracy: 0.7033\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82120, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.8080 - accuracy: 0.7060 - val_loss: 0.4974 - val_accuracy: 0.8212\n",
      "Epoch 2/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.4643 - accuracy: 0.8351\n",
      "Epoch 00002: val_accuracy improved from 0.82120 to 0.86662, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.4637 - accuracy: 0.8354 - val_loss: 0.3835 - val_accuracy: 0.8666\n",
      "Epoch 3/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8684\n",
      "Epoch 00003: val_accuracy improved from 0.86662 to 0.88536, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.3643 - accuracy: 0.8685 - val_loss: 0.3209 - val_accuracy: 0.8854\n",
      "Epoch 4/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8963\n",
      "Epoch 00004: val_accuracy improved from 0.88536 to 0.90195, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.2924 - accuracy: 0.8963 - val_loss: 0.2891 - val_accuracy: 0.9019\n",
      "Epoch 5/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.2450 - accuracy: 0.9121\n",
      "Epoch 00005: val_accuracy improved from 0.90195 to 0.91060, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.2449 - accuracy: 0.9121 - val_loss: 0.2630 - val_accuracy: 0.9106\n",
      "Epoch 6/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.2068 - accuracy: 0.9283\n",
      "Epoch 00006: val_accuracy improved from 0.91060 to 0.91781, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.2066 - accuracy: 0.9285 - val_loss: 0.2431 - val_accuracy: 0.9178\n",
      "Epoch 7/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9385\n",
      "Epoch 00007: val_accuracy improved from 0.91781 to 0.92430, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.9387 - val_loss: 0.2182 - val_accuracy: 0.9243\n",
      "Epoch 8/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9488\n",
      "Epoch 00008: val_accuracy improved from 0.92430 to 0.92610, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.1485 - accuracy: 0.9490 - val_loss: 0.2081 - val_accuracy: 0.9261\n",
      "Epoch 9/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9574\n",
      "Epoch 00009: val_accuracy improved from 0.92610 to 0.93223, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.1265 - accuracy: 0.9574 - val_loss: 0.2004 - val_accuracy: 0.9322\n",
      "Epoch 10/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9641\n",
      "Epoch 00010: val_accuracy improved from 0.93223 to 0.93439, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.9641 - val_loss: 0.1914 - val_accuracy: 0.9344\n",
      "Epoch 11/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 0.0906 - accuracy: 0.9711\n",
      "Epoch 00011: val_accuracy improved from 0.93439 to 0.93511, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0908 - accuracy: 0.9708 - val_loss: 0.1891 - val_accuracy: 0.9351\n",
      "Epoch 12/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9760\n",
      "Epoch 00012: val_accuracy improved from 0.93511 to 0.94304, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0754 - accuracy: 0.9758 - val_loss: 0.1826 - val_accuracy: 0.9430\n",
      "Epoch 13/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0672 - accuracy: 0.9786\n",
      "Epoch 00013: val_accuracy improved from 0.94304 to 0.94557, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0669 - accuracy: 0.9785 - val_loss: 0.1765 - val_accuracy: 0.9456\n",
      "Epoch 14/350\n",
      "742/780 [===========================>..] - ETA: 0s - loss: 0.0578 - accuracy: 0.9826\n",
      "Epoch 00014: val_accuracy did not improve from 0.94557\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0573 - accuracy: 0.9828 - val_loss: 0.1763 - val_accuracy: 0.9456\n",
      "Epoch 15/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 0.0484 - accuracy: 0.9848\n",
      "Epoch 00015: val_accuracy improved from 0.94557 to 0.94845, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0482 - accuracy: 0.9848 - val_loss: 0.1830 - val_accuracy: 0.9484\n",
      "Epoch 16/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0419 - accuracy: 0.9879\n",
      "Epoch 00016: val_accuracy improved from 0.94845 to 0.95061, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0418 - accuracy: 0.9879 - val_loss: 0.1802 - val_accuracy: 0.9506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0369 - accuracy: 0.9890\n",
      "Epoch 00017: val_accuracy did not improve from 0.95061\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0368 - accuracy: 0.9890 - val_loss: 0.1827 - val_accuracy: 0.9466\n",
      "Epoch 18/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9904\n",
      "Epoch 00018: val_accuracy improved from 0.95061 to 0.95169, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0316 - accuracy: 0.9905 - val_loss: 0.1748 - val_accuracy: 0.9517\n",
      "Epoch 19/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9926\n",
      "Epoch 00019: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0264 - accuracy: 0.9925 - val_loss: 0.1908 - val_accuracy: 0.9470\n",
      "Epoch 20/350\n",
      "742/780 [===========================>..] - ETA: 0s - loss: 0.0242 - accuracy: 0.9933\n",
      "Epoch 00020: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0243 - accuracy: 0.9933 - val_loss: 0.1868 - val_accuracy: 0.9510\n",
      "Epoch 21/350\n",
      "740/780 [===========================>..] - ETA: 0s - loss: 0.0205 - accuracy: 0.9946\n",
      "Epoch 00021: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0202 - accuracy: 0.9947 - val_loss: 0.1861 - val_accuracy: 0.9495\n",
      "Epoch 22/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9958\n",
      "Epoch 00022: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0179 - accuracy: 0.9958 - val_loss: 0.1948 - val_accuracy: 0.9488\n",
      "Epoch 23/350\n",
      "744/780 [===========================>..] - ETA: 0s - loss: 0.0162 - accuracy: 0.9957\n",
      "Epoch 00023: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0159 - accuracy: 0.9958 - val_loss: 0.1963 - val_accuracy: 0.9513\n",
      "Epoch 24/350\n",
      "747/780 [===========================>..] - ETA: 0s - loss: 0.0165 - accuracy: 0.9959\n",
      "Epoch 00024: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0164 - accuracy: 0.9959 - val_loss: 0.2006 - val_accuracy: 0.9503\n",
      "Epoch 25/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0141 - accuracy: 0.9959\n",
      "Epoch 00025: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0142 - accuracy: 0.9958 - val_loss: 0.2049 - val_accuracy: 0.9517\n",
      "Epoch 26/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9968\n",
      "Epoch 00026: val_accuracy improved from 0.95169 to 0.95278, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0130 - accuracy: 0.9968 - val_loss: 0.2026 - val_accuracy: 0.9528\n",
      "Epoch 27/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9977\n",
      "Epoch 00027: val_accuracy did not improve from 0.95278\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.2070 - val_accuracy: 0.9528\n",
      "Epoch 28/350\n",
      "745/780 [===========================>..] - ETA: 0s - loss: 0.0112 - accuracy: 0.9971\n",
      "Epoch 00028: val_accuracy did not improve from 0.95278\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.2125 - val_accuracy: 0.9481\n",
      "Epoch 29/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 0.0099 - accuracy: 0.9980\n",
      "Epoch 00029: val_accuracy did not improve from 0.95278\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0097 - accuracy: 0.9981 - val_loss: 0.2139 - val_accuracy: 0.9524\n",
      "Epoch 30/350\n",
      "751/780 [===========================>..] - ETA: 0s - loss: 0.0080 - accuracy: 0.9980\n",
      "Epoch 00030: val_accuracy did not improve from 0.95278\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0080 - accuracy: 0.9981 - val_loss: 0.2184 - val_accuracy: 0.9499\n",
      "Epoch 31/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9986\n",
      "Epoch 00031: val_accuracy improved from 0.95278 to 0.95494, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0078 - accuracy: 0.9986 - val_loss: 0.2210 - val_accuracy: 0.9549\n",
      "Epoch 32/350\n",
      "743/780 [===========================>..] - ETA: 0s - loss: 0.0079 - accuracy: 0.9983\n",
      "Epoch 00032: val_accuracy did not improve from 0.95494\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0080 - accuracy: 0.9982 - val_loss: 0.2130 - val_accuracy: 0.9503\n",
      "Epoch 33/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9984\n",
      "Epoch 00033: val_accuracy did not improve from 0.95494\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0081 - accuracy: 0.9984 - val_loss: 0.2121 - val_accuracy: 0.9542\n",
      "Epoch 34/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9987\n",
      "Epoch 00034: val_accuracy did not improve from 0.95494\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.2210 - val_accuracy: 0.9521\n",
      "Epoch 35/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9987\n",
      "Epoch 00035: val_accuracy did not improve from 0.95494\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.2156 - val_accuracy: 0.9549\n",
      "Epoch 36/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9985\n",
      "Epoch 00036: val_accuracy improved from 0.95494 to 0.95710, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.2269 - val_accuracy: 0.9571\n",
      "Epoch 37/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9986\n",
      "Epoch 00037: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0061 - accuracy: 0.9986 - val_loss: 0.2205 - val_accuracy: 0.9546\n",
      "Epoch 38/350\n",
      "740/780 [===========================>..] - ETA: 0s - loss: 0.0055 - accuracy: 0.9986\n",
      "Epoch 00038: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0055 - accuracy: 0.9987 - val_loss: 0.2186 - val_accuracy: 0.9560\n",
      "Epoch 39/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9991\n",
      "Epoch 00039: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.2243 - val_accuracy: 0.9564\n",
      "Epoch 40/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.2230 - val_accuracy: 0.9564\n",
      "Epoch 41/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 00041: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.2304 - val_accuracy: 0.9531\n",
      "Epoch 42/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9992\n",
      "Epoch 00042: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.2265 - val_accuracy: 0.9531\n",
      "Epoch 43/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9992\n",
      "Epoch 00043: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.2240 - val_accuracy: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00044: val_accuracy improved from 0.95710 to 0.95818, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2284 - val_accuracy: 0.9582\n",
      "Epoch 45/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9992\n",
      "Epoch 00045: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.2261 - val_accuracy: 0.9567\n",
      "Epoch 46/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9996\n",
      "Epoch 00046: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.2292 - val_accuracy: 0.9560\n",
      "Epoch 47/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00047: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2279 - val_accuracy: 0.9542\n",
      "Epoch 48/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 00048: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.2360 - val_accuracy: 0.9571\n",
      "Epoch 49/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00049: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2359 - val_accuracy: 0.9557\n",
      "Epoch 50/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9997\n",
      "Epoch 00050: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2354 - val_accuracy: 0.9560\n",
      "Epoch 51/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00051: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2382 - val_accuracy: 0.9549\n",
      "Epoch 52/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 00052: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.2380 - val_accuracy: 0.9575\n",
      "Epoch 53/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9996\n",
      "Epoch 00053: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2432 - val_accuracy: 0.9528\n",
      "Epoch 54/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 00054: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.2407 - val_accuracy: 0.9575\n",
      "Epoch 55/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2411 - val_accuracy: 0.9553\n",
      "Epoch 56/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9992\n",
      "Epoch 00056: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.2393 - val_accuracy: 0.9553\n",
      "Epoch 57/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 00057: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.2485 - val_accuracy: 0.9535\n",
      "Epoch 58/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 00058: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2447 - val_accuracy: 0.9542\n",
      "Epoch 59/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0019 - accuracy: 0.9999\n",
      "Epoch 00059: val_accuracy improved from 0.95818 to 0.95854, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.2395 - val_accuracy: 0.9585\n",
      "Epoch 60/350\n",
      "741/780 [===========================>..] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00060: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2440 - val_accuracy: 0.9567\n",
      "Epoch 61/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2465 - val_accuracy: 0.9567\n",
      "Epoch 62/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 00062: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2517 - val_accuracy: 0.9571\n",
      "Epoch 63/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00063: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2522 - val_accuracy: 0.9546\n",
      "Epoch 64/350\n",
      "744/780 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00064: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2530 - val_accuracy: 0.9531\n",
      "Epoch 65/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00065: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.2518 - val_accuracy: 0.9539\n",
      "Epoch 66/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00066: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2496 - val_accuracy: 0.9557\n",
      "Epoch 67/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00067: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2496 - val_accuracy: 0.9560\n",
      "Epoch 68/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00068: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2522 - val_accuracy: 0.9542\n",
      "Epoch 69/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00069: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2601 - val_accuracy: 0.9546\n",
      "Epoch 70/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 00070: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.2611 - val_accuracy: 0.9546\n",
      "Epoch 71/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2564 - val_accuracy: 0.9560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/350\n",
      "737/780 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00072: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2576 - val_accuracy: 0.9571\n",
      "Epoch 73/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 00073: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2617 - val_accuracy: 0.9546\n",
      "Epoch 74/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 00074: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.2616 - val_accuracy: 0.9571\n",
      "Epoch 75/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 00075: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2683 - val_accuracy: 0.9557\n",
      "Epoch 76/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00076: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2672 - val_accuracy: 0.9542\n",
      "Epoch 77/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00077: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2638 - val_accuracy: 0.9564\n",
      "Epoch 78/350\n",
      "743/780 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00078: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2636 - val_accuracy: 0.9557\n",
      "Epoch 79/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 00079: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2641 - val_accuracy: 0.9546\n",
      "Epoch 80/350\n",
      "736/780 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00080: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2633 - val_accuracy: 0.9571\n",
      "Epoch 81/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00081: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2637 - val_accuracy: 0.9582\n",
      "Epoch 82/350\n",
      "744/780 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00082: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2641 - val_accuracy: 0.9560\n",
      "Epoch 83/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00083: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2599 - val_accuracy: 0.9571\n",
      "Epoch 84/350\n",
      "742/780 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2672 - val_accuracy: 0.9571\n",
      "Epoch 85/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00085: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.2713 - val_accuracy: 0.9557\n",
      "Epoch 86/350\n",
      "738/780 [===========================>..] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00086: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2711 - val_accuracy: 0.9567\n",
      "Epoch 87/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00087: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2714 - val_accuracy: 0.9575\n",
      "Epoch 88/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2697 - val_accuracy: 0.9578\n",
      "Epoch 89/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00089: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2701 - val_accuracy: 0.9571\n",
      "Epoch 90/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 9.7086e-04 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.8090e-04 - accuracy: 1.0000 - val_loss: 0.2746 - val_accuracy: 0.9531\n",
      "Epoch 91/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 8.2040e-04 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.1128e-04 - accuracy: 0.9999 - val_loss: 0.2723 - val_accuracy: 0.9571\n",
      "Epoch 92/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00092: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2733 - val_accuracy: 0.9560\n",
      "Epoch 93/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2735 - val_accuracy: 0.9560\n",
      "Epoch 94/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00094: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2744 - val_accuracy: 0.9564\n",
      "Epoch 95/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 7.5294e-04 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.4811e-04 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 0.9560\n",
      "Epoch 96/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 9.9652e-04 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.9652e-04 - accuracy: 0.9998 - val_loss: 0.2741 - val_accuracy: 0.9560\n",
      "Epoch 97/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00097: val_accuracy improved from 0.95854 to 0.95926, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.9643e-04 - accuracy: 0.9998 - val_loss: 0.2675 - val_accuracy: 0.9593\n",
      "Epoch 98/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 6.2664e-04 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.2481e-04 - accuracy: 1.0000 - val_loss: 0.2702 - val_accuracy: 0.9585\n",
      "Epoch 99/350\n",
      "738/780 [===========================>..] - ETA: 0s - loss: 9.1450e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.3431e-04 - accuracy: 0.9998 - val_loss: 0.2690 - val_accuracy: 0.9571\n",
      "Epoch 100/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775/780 [============================>.] - ETA: 0s - loss: 8.4710e-04 - accuracy: 0.9999\n",
      "Epoch 00100: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.4408e-04 - accuracy: 0.9999 - val_loss: 0.2695 - val_accuracy: 0.9582\n",
      "Epoch 101/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 8.8281e-04 - accuracy: 0.9997\n",
      "Epoch 00101: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.7738e-04 - accuracy: 0.9997 - val_loss: 0.2680 - val_accuracy: 0.9578\n",
      "Epoch 102/350\n",
      "737/780 [===========================>..] - ETA: 0s - loss: 7.1866e-04 - accuracy: 0.9999\n",
      "Epoch 00102: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.2609e-04 - accuracy: 0.9999 - val_loss: 0.2659 - val_accuracy: 0.9582\n",
      "Epoch 103/350\n",
      "738/780 [===========================>..] - ETA: 0s - loss: 8.8963e-04 - accuracy: 0.9999\n",
      "Epoch 00103: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.9809e-04 - accuracy: 0.9999 - val_loss: 0.2692 - val_accuracy: 0.9585\n",
      "Epoch 104/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 8.1042e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy improved from 0.95926 to 0.95963, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.1042e-04 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9596\n",
      "Epoch 105/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 7.6933e-04 - accuracy: 0.9999\n",
      "Epoch 00105: val_accuracy did not improve from 0.95963\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.1357e-04 - accuracy: 0.9998 - val_loss: 0.2706 - val_accuracy: 0.9596\n",
      "Epoch 106/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 6.8645e-04 - accuracy: 0.9999\n",
      "Epoch 00106: val_accuracy did not improve from 0.95963\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.0037e-04 - accuracy: 0.9998 - val_loss: 0.2755 - val_accuracy: 0.9582\n",
      "Epoch 107/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 6.9545e-04 - accuracy: 0.9999\n",
      "Epoch 00107: val_accuracy improved from 0.95963 to 0.96071, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.9941e-04 - accuracy: 0.9999 - val_loss: 0.2700 - val_accuracy: 0.9607\n",
      "Epoch 108/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 8.0139e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.9019e-04 - accuracy: 0.9999 - val_loss: 0.2686 - val_accuracy: 0.9593\n",
      "Epoch 109/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 8.7919e-04 - accuracy: 0.9998\n",
      "Epoch 00109: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.7049e-04 - accuracy: 0.9998 - val_loss: 0.2666 - val_accuracy: 0.9593\n",
      "Epoch 110/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 9.1229e-04 - accuracy: 0.9998\n",
      "Epoch 00110: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.0902e-04 - accuracy: 0.9998 - val_loss: 0.2695 - val_accuracy: 0.9589\n",
      "Epoch 111/350\n",
      "737/780 [===========================>..] - ETA: 0s - loss: 6.8044e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.8279e-04 - accuracy: 1.0000 - val_loss: 0.2720 - val_accuracy: 0.9560\n",
      "Epoch 112/350\n",
      "745/780 [===========================>..] - ETA: 0s - loss: 7.4403e-04 - accuracy: 0.9999\n",
      "Epoch 00112: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.2691e-04 - accuracy: 0.9999 - val_loss: 0.2715 - val_accuracy: 0.9578\n",
      "Epoch 113/350\n",
      "761/780 [============================>.] - ETA: 0s - loss: 7.7192e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.6203e-04 - accuracy: 0.9999 - val_loss: 0.2714 - val_accuracy: 0.9560\n",
      "Epoch 114/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 6.6453e-04 - accuracy: 0.9999\n",
      "Epoch 00114: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.7686e-04 - accuracy: 0.9999 - val_loss: 0.2804 - val_accuracy: 0.9560\n",
      "Epoch 115/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 7.8668e-04 - accuracy: 0.9999\n",
      "Epoch 00115: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.8570e-04 - accuracy: 0.9999 - val_loss: 0.2733 - val_accuracy: 0.9585\n",
      "Epoch 116/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 8.1687e-04 - accuracy: 0.9998\n",
      "Epoch 00116: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.1441e-04 - accuracy: 0.9998 - val_loss: 0.2756 - val_accuracy: 0.9585\n",
      "Epoch 117/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 7.5736e-04 - accuracy: 0.9998\n",
      "Epoch 00117: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.5672e-04 - accuracy: 0.9998 - val_loss: 0.2738 - val_accuracy: 0.9582\n",
      "Epoch 118/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 7.4303e-04 - accuracy: 0.9999\n",
      "Epoch 00118: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.3474e-04 - accuracy: 0.9999 - val_loss: 0.2705 - val_accuracy: 0.9567\n",
      "Epoch 119/350\n",
      "738/780 [===========================>..] - ETA: 0s - loss: 9.3083e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.0055e-04 - accuracy: 0.9999 - val_loss: 0.2699 - val_accuracy: 0.9582\n",
      "Epoch 120/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 5.9976e-04 - accuracy: 0.9999\n",
      "Epoch 00120: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.8556e-04 - accuracy: 0.9999 - val_loss: 0.2678 - val_accuracy: 0.9600\n",
      "Epoch 121/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 6.6961e-04 - accuracy: 0.9999\n",
      "Epoch 00121: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.6577e-04 - accuracy: 0.9999 - val_loss: 0.2712 - val_accuracy: 0.9593\n",
      "Epoch 122/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 4.6168e-04 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.5767e-04 - accuracy: 1.0000 - val_loss: 0.2707 - val_accuracy: 0.9585\n",
      "Epoch 123/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 3.8904e-04 - accuracy: 1.0000\n",
      "Epoch 00123: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 3.8667e-04 - accuracy: 1.0000 - val_loss: 0.2711 - val_accuracy: 0.9603\n",
      "Epoch 124/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 6.7056e-04 - accuracy: 0.9998\n",
      "Epoch 00124: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.6430e-04 - accuracy: 0.9998 - val_loss: 0.2761 - val_accuracy: 0.9582\n",
      "Epoch 125/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 4.8554e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.8554e-04 - accuracy: 0.9999 - val_loss: 0.2745 - val_accuracy: 0.9585\n",
      "Epoch 126/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 4.9617e-04 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.9558e-04 - accuracy: 1.0000 - val_loss: 0.2755 - val_accuracy: 0.9571\n",
      "Epoch 127/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/780 [============================>.] - ETA: 0s - loss: 5.3943e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.3879e-04 - accuracy: 0.9999 - val_loss: 0.2754 - val_accuracy: 0.9571\n",
      "Epoch 128/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 6.2164e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.1073e-04 - accuracy: 0.9999 - val_loss: 0.2760 - val_accuracy: 0.9582\n",
      "Epoch 129/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 5.6246e-04 - accuracy: 0.9999\n",
      "Epoch 00129: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.7001e-04 - accuracy: 0.9999 - val_loss: 0.2788 - val_accuracy: 0.9578\n",
      "Epoch 130/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 4.1367e-04 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.0864e-04 - accuracy: 1.0000 - val_loss: 0.2779 - val_accuracy: 0.9578\n",
      "Epoch 131/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 4.7257e-04 - accuracy: 0.9999\n",
      "Epoch 00131: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.7257e-04 - accuracy: 0.9999 - val_loss: 0.2766 - val_accuracy: 0.9585\n",
      "Epoch 132/350\n",
      "740/780 [===========================>..] - ETA: 0s - loss: 7.0389e-04 - accuracy: 0.9999\n",
      "Epoch 00132: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.8302e-04 - accuracy: 0.9999 - val_loss: 0.2702 - val_accuracy: 0.9596\n",
      "Epoch 133/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 6.2908e-04 - accuracy: 0.9999\n",
      "Epoch 00133: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.2908e-04 - accuracy: 0.9999 - val_loss: 0.2723 - val_accuracy: 0.9585\n",
      "Epoch 134/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 7.1124e-04 - accuracy: 0.9998\n",
      "Epoch 00134: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.0847e-04 - accuracy: 0.9998 - val_loss: 0.2748 - val_accuracy: 0.9582\n",
      "Epoch 135/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 5.7990e-04 - accuracy: 1.0000\n",
      "Epoch 00135: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.7822e-04 - accuracy: 1.0000 - val_loss: 0.2760 - val_accuracy: 0.9585\n",
      "Epoch 136/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 4.9082e-04 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.8893e-04 - accuracy: 1.0000 - val_loss: 0.2750 - val_accuracy: 0.9593\n",
      "Epoch 137/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 4.7750e-04 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.7529e-04 - accuracy: 1.0000 - val_loss: 0.2783 - val_accuracy: 0.9567\n",
      "Epoch 138/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 7.9248e-04 - accuracy: 0.9999\n",
      "Epoch 00138: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.9306e-04 - accuracy: 0.9999 - val_loss: 0.2830 - val_accuracy: 0.9582\n",
      "Epoch 139/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 6.9190e-04 - accuracy: 0.9999\n",
      "Epoch 00139: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.9101e-04 - accuracy: 0.9999 - val_loss: 0.2831 - val_accuracy: 0.9578\n",
      "Epoch 140/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 6.2824e-04 - accuracy: 0.9999\n",
      "Epoch 00140: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.2610e-04 - accuracy: 0.9999 - val_loss: 0.2810 - val_accuracy: 0.9582\n",
      "Epoch 141/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 5.3081e-04 - accuracy: 0.9999\n",
      "Epoch 00141: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.3735e-04 - accuracy: 0.9999 - val_loss: 0.2800 - val_accuracy: 0.9582\n",
      "Epoch 142/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 3.7069e-04 - accuracy: 1.0000\n",
      "Epoch 00142: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 3.7262e-04 - accuracy: 1.0000 - val_loss: 0.2803 - val_accuracy: 0.9582\n",
      "Epoch 143/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 6.5873e-04 - accuracy: 0.9999\n",
      "Epoch 00143: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.5523e-04 - accuracy: 0.9999 - val_loss: 0.2802 - val_accuracy: 0.9578\n",
      "Epoch 144/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 4.9294e-04 - accuracy: 0.9999\n",
      "Epoch 00144: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.9341e-04 - accuracy: 0.9999 - val_loss: 0.2870 - val_accuracy: 0.9557\n",
      "Epoch 145/350\n",
      "761/780 [============================>.] - ETA: 0s - loss: 6.0297e-04 - accuracy: 0.9999\n",
      "Epoch 00145: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.0392e-04 - accuracy: 0.9999 - val_loss: 0.2844 - val_accuracy: 0.9571\n",
      "Epoch 146/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 4.6447e-04 - accuracy: 1.0000\n",
      "Epoch 00146: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.6763e-04 - accuracy: 1.0000 - val_loss: 0.2857 - val_accuracy: 0.9575\n",
      "Epoch 147/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 7.4619e-04 - accuracy: 0.9998\n",
      "Epoch 00147: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.8075e-04 - accuracy: 0.9998 - val_loss: 0.2866 - val_accuracy: 0.9557\n",
      "Epoch 148/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 4.4744e-04 - accuracy: 1.0000\n",
      "Epoch 00148: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.1457e-04 - accuracy: 1.0000 - val_loss: 0.2853 - val_accuracy: 0.9564\n",
      "Epoch 149/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 5.7549e-04 - accuracy: 0.9998\n",
      "Epoch 00149: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.6505e-04 - accuracy: 0.9998 - val_loss: 0.2835 - val_accuracy: 0.9578\n",
      "Epoch 150/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 3.9451e-04 - accuracy: 1.0000\n",
      "Epoch 00150: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 3.9107e-04 - accuracy: 1.0000 - val_loss: 0.2838 - val_accuracy: 0.9585\n",
      "Epoch 151/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 5.1882e-04 - accuracy: 0.9999\n",
      "Epoch 00151: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.1515e-04 - accuracy: 0.9999 - val_loss: 0.2904 - val_accuracy: 0.9578\n",
      "Epoch 152/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 3.7115e-04 - accuracy: 1.0000\n",
      "Epoch 00152: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 3.7115e-04 - accuracy: 1.0000 - val_loss: 0.2903 - val_accuracy: 0.9575\n",
      "Epoch 153/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 4.3738e-04 - accuracy: 0.9999\n",
      "Epoch 00153: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.3617e-04 - accuracy: 0.9999 - val_loss: 0.2898 - val_accuracy: 0.9607\n",
      "Epoch 154/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 7.0683e-04 - accuracy: 0.9998\n",
      "Epoch 00154: val_accuracy did not improve from 0.96071\n",
      "Restoring model weights from the end of the best epoch.\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.9783e-04 - accuracy: 0.9998 - val_loss: 0.2916 - val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00154: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:41<02:41, 161.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.85      0.85      0.85       574\n",
      "        car_horn       0.82      0.91      0.86       224\n",
      "children_playing       0.82      0.77      0.80       700\n",
      "        dog_bark       0.79      0.74      0.76       700\n",
      "           siren       0.87      0.97      0.92       574\n",
      "\n",
      "        accuracy                           0.83      2772\n",
      "       macro avg       0.83      0.85      0.84      2772\n",
      "    weighted avg       0.83      0.83      0.83      2772\n",
      "\n",
      "Model: \"Model_CNN_1D_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 229, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 229, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 229, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6384)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                319250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 330,291\n",
      "Trainable params: 330,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24960, 235, 1)\n",
      "Epoch 1/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.7479 - accuracy: 0.7617\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84643, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.7438 - accuracy: 0.7634 - val_loss: 0.5203 - val_accuracy: 0.8464\n",
      "Epoch 2/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.8524\n",
      "Epoch 00002: val_accuracy improved from 0.84643 to 0.86373, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.4890 - accuracy: 0.8524 - val_loss: 0.4505 - val_accuracy: 0.8637\n",
      "Epoch 3/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.4245 - accuracy: 0.8744\n",
      "Epoch 00003: val_accuracy improved from 0.86373 to 0.88104, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.4242 - accuracy: 0.8743 - val_loss: 0.4096 - val_accuracy: 0.8810\n",
      "Epoch 4/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8898\n",
      "Epoch 00004: val_accuracy improved from 0.88104 to 0.89402, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3751 - accuracy: 0.8897 - val_loss: 0.3780 - val_accuracy: 0.8940\n",
      "Epoch 5/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.9048\n",
      "Epoch 00005: val_accuracy improved from 0.89402 to 0.89690, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3377 - accuracy: 0.9046 - val_loss: 0.3662 - val_accuracy: 0.8969\n",
      "Epoch 6/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.9124\n",
      "Epoch 00006: val_accuracy improved from 0.89690 to 0.90375, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3129 - accuracy: 0.9124 - val_loss: 0.3476 - val_accuracy: 0.9037\n",
      "Epoch 7/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9196\n",
      "Epoch 00007: val_accuracy did not improve from 0.90375\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2918 - accuracy: 0.9192 - val_loss: 0.3445 - val_accuracy: 0.9034\n",
      "Epoch 8/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.2727 - accuracy: 0.9251\n",
      "Epoch 00008: val_accuracy did not improve from 0.90375\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2730 - accuracy: 0.9252 - val_loss: 0.3371 - val_accuracy: 0.9027\n",
      "Epoch 9/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.9297\n",
      "Epoch 00009: val_accuracy improved from 0.90375 to 0.91132, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2621 - accuracy: 0.9297 - val_loss: 0.3250 - val_accuracy: 0.9113\n",
      "Epoch 10/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.2466 - accuracy: 0.9345\n",
      "Epoch 00010: val_accuracy did not improve from 0.91132\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2464 - accuracy: 0.9345 - val_loss: 0.3280 - val_accuracy: 0.9084\n",
      "Epoch 11/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.9402\n",
      "Epoch 00011: val_accuracy did not improve from 0.91132\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2344 - accuracy: 0.9402 - val_loss: 0.3298 - val_accuracy: 0.9041\n",
      "Epoch 12/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.2249 - accuracy: 0.9417\n",
      "Epoch 00012: val_accuracy did not improve from 0.91132\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2247 - accuracy: 0.9417 - val_loss: 0.3564 - val_accuracy: 0.9016\n",
      "Epoch 13/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.2155 - accuracy: 0.9444\n",
      "Epoch 00013: val_accuracy improved from 0.91132 to 0.91456, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2159 - accuracy: 0.9444 - val_loss: 0.3232 - val_accuracy: 0.9146\n",
      "Epoch 14/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9477\n",
      "Epoch 00014: val_accuracy did not improve from 0.91456\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2104 - accuracy: 0.9476 - val_loss: 0.3373 - val_accuracy: 0.9081\n",
      "Epoch 15/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9503\n",
      "Epoch 00015: val_accuracy did not improve from 0.91456\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2009 - accuracy: 0.9503 - val_loss: 0.3113 - val_accuracy: 0.9066\n",
      "Epoch 16/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9510\n",
      "Epoch 00016: val_accuracy did not improve from 0.91456\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1987 - accuracy: 0.9507 - val_loss: 0.3230 - val_accuracy: 0.9102\n",
      "Epoch 17/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1896 - accuracy: 0.9514\n",
      "Epoch 00017: val_accuracy improved from 0.91456 to 0.91673, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1899 - accuracy: 0.9512 - val_loss: 0.3125 - val_accuracy: 0.9167\n",
      "Epoch 18/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761/780 [============================>.] - ETA: 0s - loss: 0.1841 - accuracy: 0.9543\n",
      "Epoch 00018: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1839 - accuracy: 0.9545 - val_loss: 0.3187 - val_accuracy: 0.9128\n",
      "Epoch 19/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9559\n",
      "Epoch 00019: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1786 - accuracy: 0.9559 - val_loss: 0.3239 - val_accuracy: 0.9092\n",
      "Epoch 20/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1739 - accuracy: 0.9589\n",
      "Epoch 00020: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1742 - accuracy: 0.9588 - val_loss: 0.3255 - val_accuracy: 0.9149\n",
      "Epoch 21/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.9584\n",
      "Epoch 00021: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1709 - accuracy: 0.9582 - val_loss: 0.3322 - val_accuracy: 0.9099\n",
      "Epoch 22/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1672 - accuracy: 0.9593\n",
      "Epoch 00022: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1666 - accuracy: 0.9595 - val_loss: 0.3284 - val_accuracy: 0.9124\n",
      "Epoch 23/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1626 - accuracy: 0.9621\n",
      "Epoch 00023: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1625 - accuracy: 0.9622 - val_loss: 0.3381 - val_accuracy: 0.9095\n",
      "Epoch 24/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9627\n",
      "Epoch 00024: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1597 - accuracy: 0.9626 - val_loss: 0.3141 - val_accuracy: 0.9156\n",
      "Epoch 25/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9625\n",
      "Epoch 00025: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1566 - accuracy: 0.9625 - val_loss: 0.3339 - val_accuracy: 0.9110\n",
      "Epoch 26/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.9641\n",
      "Epoch 00026: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1530 - accuracy: 0.9641 - val_loss: 0.3307 - val_accuracy: 0.9167\n",
      "Epoch 27/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9659\n",
      "Epoch 00027: val_accuracy improved from 0.91673 to 0.91889, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1482 - accuracy: 0.9661 - val_loss: 0.3305 - val_accuracy: 0.9189\n",
      "Epoch 28/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1493 - accuracy: 0.9658\n",
      "Epoch 00028: val_accuracy did not improve from 0.91889\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1492 - accuracy: 0.9659 - val_loss: 0.3307 - val_accuracy: 0.9124\n",
      "Epoch 29/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.1484 - accuracy: 0.9650\n",
      "Epoch 00029: val_accuracy did not improve from 0.91889\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1491 - accuracy: 0.9649 - val_loss: 0.3383 - val_accuracy: 0.9142\n",
      "Epoch 30/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.9679\n",
      "Epoch 00030: val_accuracy improved from 0.91889 to 0.92033, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1398 - accuracy: 0.9682 - val_loss: 0.3324 - val_accuracy: 0.9203\n",
      "Epoch 31/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1366 - accuracy: 0.9680\n",
      "Epoch 00031: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1363 - accuracy: 0.9681 - val_loss: 0.3310 - val_accuracy: 0.9164\n",
      "Epoch 32/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.9685\n",
      "Epoch 00032: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1386 - accuracy: 0.9686 - val_loss: 0.3479 - val_accuracy: 0.9135\n",
      "Epoch 33/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.9703\n",
      "Epoch 00033: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1336 - accuracy: 0.9703 - val_loss: 0.3433 - val_accuracy: 0.9203\n",
      "Epoch 34/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9712\n",
      "Epoch 00034: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1311 - accuracy: 0.9713 - val_loss: 0.3382 - val_accuracy: 0.9117\n",
      "Epoch 35/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9705\n",
      "Epoch 00035: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1306 - accuracy: 0.9705 - val_loss: 0.3325 - val_accuracy: 0.9156\n",
      "Epoch 36/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9710\n",
      "Epoch 00036: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1291 - accuracy: 0.9712 - val_loss: 0.3317 - val_accuracy: 0.9178\n",
      "Epoch 37/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9713\n",
      "Epoch 00037: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1263 - accuracy: 0.9711 - val_loss: 0.3289 - val_accuracy: 0.9193\n",
      "Epoch 38/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9736\n",
      "Epoch 00038: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1245 - accuracy: 0.9736 - val_loss: 0.3413 - val_accuracy: 0.9138\n",
      "Epoch 39/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9721\n",
      "Epoch 00039: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1241 - accuracy: 0.9720 - val_loss: 0.3373 - val_accuracy: 0.9110\n",
      "Epoch 40/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9760\n",
      "Epoch 00040: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1167 - accuracy: 0.9761 - val_loss: 0.3326 - val_accuracy: 0.9167\n",
      "Epoch 41/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9756\n",
      "Epoch 00041: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1173 - accuracy: 0.9756 - val_loss: 0.3501 - val_accuracy: 0.9149\n",
      "Epoch 42/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9748\n",
      "Epoch 00042: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1206 - accuracy: 0.9746 - val_loss: 0.3352 - val_accuracy: 0.9160\n",
      "Epoch 43/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9746\n",
      "Epoch 00043: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1179 - accuracy: 0.9746 - val_loss: 0.3340 - val_accuracy: 0.9135\n",
      "Epoch 44/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9734\n",
      "Epoch 00044: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1230 - accuracy: 0.9735 - val_loss: 0.3333 - val_accuracy: 0.9117\n",
      "Epoch 45/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1142 - accuracy: 0.9754\n",
      "Epoch 00045: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1144 - accuracy: 0.9752 - val_loss: 0.3350 - val_accuracy: 0.9174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9756\n",
      "Epoch 00046: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1141 - accuracy: 0.9755 - val_loss: 0.3279 - val_accuracy: 0.9167\n",
      "Epoch 47/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9774\n",
      "Epoch 00047: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1117 - accuracy: 0.9775 - val_loss: 0.3415 - val_accuracy: 0.9146\n",
      "Epoch 48/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9774\n",
      "Epoch 00048: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1096 - accuracy: 0.9773 - val_loss: 0.3411 - val_accuracy: 0.9135\n",
      "Epoch 49/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1078 - accuracy: 0.9769\n",
      "Epoch 00049: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1080 - accuracy: 0.9769 - val_loss: 0.3386 - val_accuracy: 0.9153\n",
      "Epoch 50/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9773\n",
      "Epoch 00050: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1107 - accuracy: 0.9771 - val_loss: 0.3388 - val_accuracy: 0.9142\n",
      "Epoch 51/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1083 - accuracy: 0.9777\n",
      "Epoch 00051: val_accuracy improved from 0.92033 to 0.92394, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1083 - accuracy: 0.9776 - val_loss: 0.3187 - val_accuracy: 0.9239\n",
      "Epoch 52/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9775\n",
      "Epoch 00052: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1067 - accuracy: 0.9774 - val_loss: 0.3260 - val_accuracy: 0.9203\n",
      "Epoch 53/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9768\n",
      "Epoch 00053: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9767 - val_loss: 0.3314 - val_accuracy: 0.9182\n",
      "Epoch 54/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.1028 - accuracy: 0.9797\n",
      "Epoch 00054: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1027 - accuracy: 0.9798 - val_loss: 0.3380 - val_accuracy: 0.9189\n",
      "Epoch 55/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9786\n",
      "Epoch 00055: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1042 - accuracy: 0.9786 - val_loss: 0.3351 - val_accuracy: 0.9239\n",
      "Epoch 56/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1045 - accuracy: 0.9773\n",
      "Epoch 00056: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1048 - accuracy: 0.9774 - val_loss: 0.3502 - val_accuracy: 0.9182\n",
      "Epoch 57/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9795\n",
      "Epoch 00057: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1030 - accuracy: 0.9795 - val_loss: 0.3471 - val_accuracy: 0.9156\n",
      "Epoch 58/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9802\n",
      "Epoch 00058: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9803 - val_loss: 0.3360 - val_accuracy: 0.9203\n",
      "Epoch 59/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9802\n",
      "Epoch 00059: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0979 - accuracy: 0.9802 - val_loss: 0.3309 - val_accuracy: 0.9221\n",
      "Epoch 60/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9803\n",
      "Epoch 00060: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0996 - accuracy: 0.9803 - val_loss: 0.3431 - val_accuracy: 0.9189\n",
      "Epoch 61/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1002 - accuracy: 0.9786\n",
      "Epoch 00061: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1002 - accuracy: 0.9786 - val_loss: 0.3531 - val_accuracy: 0.9171\n",
      "Epoch 62/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9797\n",
      "Epoch 00062: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0999 - accuracy: 0.9797 - val_loss: 0.3432 - val_accuracy: 0.9189\n",
      "Epoch 63/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0992 - accuracy: 0.9785\n",
      "Epoch 00063: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0987 - accuracy: 0.9787 - val_loss: 0.3456 - val_accuracy: 0.9167\n",
      "Epoch 64/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9809\n",
      "Epoch 00064: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0958 - accuracy: 0.9808 - val_loss: 0.3622 - val_accuracy: 0.9171\n",
      "Epoch 65/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9810\n",
      "Epoch 00065: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0938 - accuracy: 0.9810 - val_loss: 0.3464 - val_accuracy: 0.9207\n",
      "Epoch 66/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9813\n",
      "Epoch 00066: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0955 - accuracy: 0.9810 - val_loss: 0.3394 - val_accuracy: 0.9167\n",
      "Epoch 67/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9813\n",
      "Epoch 00067: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0943 - accuracy: 0.9812 - val_loss: 0.3432 - val_accuracy: 0.9189\n",
      "Epoch 68/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0936 - accuracy: 0.9819\n",
      "Epoch 00068: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0935 - accuracy: 0.9819 - val_loss: 0.3531 - val_accuracy: 0.9225\n",
      "Epoch 69/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9817\n",
      "Epoch 00069: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0938 - accuracy: 0.9816 - val_loss: 0.3528 - val_accuracy: 0.9211\n",
      "Epoch 70/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9822\n",
      "Epoch 00070: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0918 - accuracy: 0.9822 - val_loss: 0.3639 - val_accuracy: 0.9185\n",
      "Epoch 71/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9810\n",
      "Epoch 00071: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0962 - accuracy: 0.9809 - val_loss: 0.3670 - val_accuracy: 0.9153\n",
      "Epoch 72/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9814\n",
      "Epoch 00072: val_accuracy improved from 0.92394 to 0.92574, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0915 - accuracy: 0.9812 - val_loss: 0.3361 - val_accuracy: 0.9257\n",
      "Epoch 73/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9803\n",
      "Epoch 00073: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0941 - accuracy: 0.9803 - val_loss: 0.3422 - val_accuracy: 0.9221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9825\n",
      "Epoch 00074: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0874 - accuracy: 0.9826 - val_loss: 0.3417 - val_accuracy: 0.9200\n",
      "Epoch 75/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0872 - accuracy: 0.9830\n",
      "Epoch 00075: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0869 - accuracy: 0.9832 - val_loss: 0.3430 - val_accuracy: 0.9214\n",
      "Epoch 76/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9846\n",
      "Epoch 00076: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0840 - accuracy: 0.9845 - val_loss: 0.3440 - val_accuracy: 0.9203\n",
      "Epoch 77/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9833\n",
      "Epoch 00077: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0869 - accuracy: 0.9832 - val_loss: 0.3359 - val_accuracy: 0.9247\n",
      "Epoch 78/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0885 - accuracy: 0.9828\n",
      "Epoch 00078: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0885 - accuracy: 0.9828 - val_loss: 0.3391 - val_accuracy: 0.9200\n",
      "Epoch 79/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9837\n",
      "Epoch 00079: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0853 - accuracy: 0.9836 - val_loss: 0.3372 - val_accuracy: 0.9211\n",
      "Epoch 80/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9836\n",
      "Epoch 00080: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0840 - accuracy: 0.9836 - val_loss: 0.3440 - val_accuracy: 0.9211\n",
      "Epoch 81/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9846\n",
      "Epoch 00081: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0833 - accuracy: 0.9845 - val_loss: 0.3513 - val_accuracy: 0.9193\n",
      "Epoch 82/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9840\n",
      "Epoch 00082: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0846 - accuracy: 0.9841 - val_loss: 0.3526 - val_accuracy: 0.9232\n",
      "Epoch 83/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9840\n",
      "Epoch 00083: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0863 - accuracy: 0.9841 - val_loss: 0.3428 - val_accuracy: 0.9254\n",
      "Epoch 84/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0837 - accuracy: 0.9837\n",
      "Epoch 00084: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0837 - accuracy: 0.9837 - val_loss: 0.3389 - val_accuracy: 0.9257\n",
      "Epoch 85/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0835 - accuracy: 0.9849\n",
      "Epoch 00085: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0833 - accuracy: 0.9850 - val_loss: 0.3463 - val_accuracy: 0.9232\n",
      "Epoch 86/150\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9839\n",
      "Epoch 00086: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0834 - accuracy: 0.9837 - val_loss: 0.3394 - val_accuracy: 0.9232\n",
      "Epoch 87/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9869\n",
      "Epoch 00087: val_accuracy improved from 0.92574 to 0.92826, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0770 - accuracy: 0.9869 - val_loss: 0.3397 - val_accuracy: 0.9283\n",
      "Epoch 88/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9835\n",
      "Epoch 00088: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0813 - accuracy: 0.9835 - val_loss: 0.3455 - val_accuracy: 0.9236\n",
      "Epoch 89/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9836\n",
      "Epoch 00089: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0830 - accuracy: 0.9835 - val_loss: 0.3399 - val_accuracy: 0.9218\n",
      "Epoch 90/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9840\n",
      "Epoch 00090: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0821 - accuracy: 0.9839 - val_loss: 0.3365 - val_accuracy: 0.9229\n",
      "Epoch 91/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0825 - accuracy: 0.9836\n",
      "Epoch 00091: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9836 - val_loss: 0.3495 - val_accuracy: 0.9229\n",
      "Epoch 92/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9848\n",
      "Epoch 00092: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0784 - accuracy: 0.9849 - val_loss: 0.3383 - val_accuracy: 0.9247\n",
      "Epoch 93/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9845\n",
      "Epoch 00093: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0789 - accuracy: 0.9845 - val_loss: 0.3287 - val_accuracy: 0.9265\n",
      "Epoch 94/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9836\n",
      "Epoch 00094: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9837 - val_loss: 0.3364 - val_accuracy: 0.9272\n",
      "Epoch 95/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9853\n",
      "Epoch 00095: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0779 - accuracy: 0.9853 - val_loss: 0.3397 - val_accuracy: 0.9221\n",
      "Epoch 96/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9865\n",
      "Epoch 00096: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0739 - accuracy: 0.9867 - val_loss: 0.3454 - val_accuracy: 0.9200\n",
      "Epoch 97/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9843\n",
      "Epoch 00097: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9842 - val_loss: 0.3452 - val_accuracy: 0.9239\n",
      "Epoch 98/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9855\n",
      "Epoch 00098: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0773 - accuracy: 0.9855 - val_loss: 0.3358 - val_accuracy: 0.9221\n",
      "Epoch 99/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9849\n",
      "Epoch 00099: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9851 - val_loss: 0.3351 - val_accuracy: 0.9243\n",
      "Epoch 100/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9857\n",
      "Epoch 00100: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9857 - val_loss: 0.3414 - val_accuracy: 0.9196\n",
      "Epoch 101/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9859\n",
      "Epoch 00101: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9861 - val_loss: 0.3355 - val_accuracy: 0.9257\n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770/780 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9868\n",
      "Epoch 00102: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0737 - accuracy: 0.9867 - val_loss: 0.3312 - val_accuracy: 0.9239\n",
      "Epoch 103/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9860\n",
      "Epoch 00103: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9859 - val_loss: 0.3374 - val_accuracy: 0.9268\n",
      "Epoch 104/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9849\n",
      "Epoch 00104: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0756 - accuracy: 0.9848 - val_loss: 0.3408 - val_accuracy: 0.9200\n",
      "Epoch 105/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9851\n",
      "Epoch 00105: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9851 - val_loss: 0.3493 - val_accuracy: 0.9247\n",
      "Epoch 106/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9869\n",
      "Epoch 00106: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0727 - accuracy: 0.9870 - val_loss: 0.3365 - val_accuracy: 0.9232\n",
      "Epoch 107/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9867\n",
      "Epoch 00107: val_accuracy improved from 0.92826 to 0.92898, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9865 - val_loss: 0.3439 - val_accuracy: 0.9290\n",
      "Epoch 108/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9858\n",
      "Epoch 00108: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0743 - accuracy: 0.9857 - val_loss: 0.3363 - val_accuracy: 0.9250\n",
      "Epoch 109/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9855\n",
      "Epoch 00109: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0750 - accuracy: 0.9855 - val_loss: 0.3366 - val_accuracy: 0.9247\n",
      "Epoch 110/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9868\n",
      "Epoch 00110: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9868 - val_loss: 0.3565 - val_accuracy: 0.9196\n",
      "Epoch 111/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9871\n",
      "Epoch 00111: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9868 - val_loss: 0.3566 - val_accuracy: 0.9247\n",
      "Epoch 112/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9877\n",
      "Epoch 00112: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0701 - accuracy: 0.9876 - val_loss: 0.3534 - val_accuracy: 0.9193\n",
      "Epoch 113/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9865\n",
      "Epoch 00113: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9865 - val_loss: 0.3373 - val_accuracy: 0.9247\n",
      "Epoch 114/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9858\n",
      "Epoch 00114: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0720 - accuracy: 0.9858 - val_loss: 0.3538 - val_accuracy: 0.9243\n",
      "Epoch 115/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9857\n",
      "Epoch 00115: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0743 - accuracy: 0.9857 - val_loss: 0.3462 - val_accuracy: 0.9250\n",
      "Epoch 116/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9858\n",
      "Epoch 00116: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0723 - accuracy: 0.9858 - val_loss: 0.3492 - val_accuracy: 0.9214\n",
      "Epoch 117/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9869\n",
      "Epoch 00117: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0708 - accuracy: 0.9869 - val_loss: 0.3460 - val_accuracy: 0.9247\n",
      "Epoch 118/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9861\n",
      "Epoch 00118: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0731 - accuracy: 0.9861 - val_loss: 0.3304 - val_accuracy: 0.9275\n",
      "Epoch 119/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0686 - accuracy: 0.9875\n",
      "Epoch 00119: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0686 - accuracy: 0.9875 - val_loss: 0.3520 - val_accuracy: 0.9268\n",
      "Epoch 120/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9857\n",
      "Epoch 00120: val_accuracy improved from 0.92898 to 0.93079, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0716 - accuracy: 0.9857 - val_loss: 0.3343 - val_accuracy: 0.9308\n",
      "Epoch 121/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 0.9876\n",
      "Epoch 00121: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0694 - accuracy: 0.9876 - val_loss: 0.3536 - val_accuracy: 0.9211\n",
      "Epoch 122/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9879\n",
      "Epoch 00122: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0669 - accuracy: 0.9879 - val_loss: 0.3439 - val_accuracy: 0.9290\n",
      "Epoch 123/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9867\n",
      "Epoch 00123: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0706 - accuracy: 0.9867 - val_loss: 0.3648 - val_accuracy: 0.9236\n",
      "Epoch 124/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9885\n",
      "Epoch 00124: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9884 - val_loss: 0.3433 - val_accuracy: 0.9257\n",
      "Epoch 125/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9878\n",
      "Epoch 00125: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9876 - val_loss: 0.3518 - val_accuracy: 0.9275\n",
      "Epoch 126/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9879\n",
      "Epoch 00126: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9879 - val_loss: 0.3355 - val_accuracy: 0.9293\n",
      "Epoch 127/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9887\n",
      "Epoch 00127: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9885 - val_loss: 0.3558 - val_accuracy: 0.9297\n",
      "Epoch 128/150\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0696 - accuracy: 0.9871\n",
      "Epoch 00128: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0698 - accuracy: 0.9871 - val_loss: 0.3357 - val_accuracy: 0.9297\n",
      "Epoch 129/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9872\n",
      "Epoch 00129: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9872 - val_loss: 0.3611 - val_accuracy: 0.9261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9891\n",
      "Epoch 00130: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0643 - accuracy: 0.9890 - val_loss: 0.3548 - val_accuracy: 0.9257\n",
      "Epoch 131/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9877\n",
      "Epoch 00131: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9877 - val_loss: 0.3478 - val_accuracy: 0.9297\n",
      "Epoch 132/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9876\n",
      "Epoch 00132: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9877 - val_loss: 0.3436 - val_accuracy: 0.9265\n",
      "Epoch 133/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9879\n",
      "Epoch 00133: val_accuracy improved from 0.93079 to 0.93187, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9877 - val_loss: 0.3394 - val_accuracy: 0.9319\n",
      "Epoch 134/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.9883\n",
      "Epoch 00134: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0653 - accuracy: 0.9882 - val_loss: 0.3462 - val_accuracy: 0.9243\n",
      "Epoch 135/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.9871\n",
      "Epoch 00135: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0690 - accuracy: 0.9871 - val_loss: 0.3407 - val_accuracy: 0.9290\n",
      "Epoch 136/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9880\n",
      "Epoch 00136: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0665 - accuracy: 0.9880 - val_loss: 0.3567 - val_accuracy: 0.9283\n",
      "Epoch 137/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9880\n",
      "Epoch 00137: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0647 - accuracy: 0.9880 - val_loss: 0.3468 - val_accuracy: 0.9243\n",
      "Epoch 138/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9888\n",
      "Epoch 00138: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0637 - accuracy: 0.9888 - val_loss: 0.3423 - val_accuracy: 0.9272\n",
      "Epoch 139/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9890\n",
      "Epoch 00139: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9889 - val_loss: 0.3566 - val_accuracy: 0.9286\n",
      "Epoch 140/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9885\n",
      "Epoch 00140: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0649 - accuracy: 0.9885 - val_loss: 0.3432 - val_accuracy: 0.9261\n",
      "Epoch 141/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9872\n",
      "Epoch 00141: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0668 - accuracy: 0.9869 - val_loss: 0.3366 - val_accuracy: 0.9279\n",
      "Epoch 142/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9868\n",
      "Epoch 00142: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0673 - accuracy: 0.9867 - val_loss: 0.3558 - val_accuracy: 0.9286\n",
      "Epoch 143/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9897\n",
      "Epoch 00143: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9897 - val_loss: 0.3498 - val_accuracy: 0.9239\n",
      "Epoch 144/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9888\n",
      "Epoch 00144: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0629 - accuracy: 0.9887 - val_loss: 0.3367 - val_accuracy: 0.9290\n",
      "Epoch 145/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9889\n",
      "Epoch 00145: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9889 - val_loss: 0.3468 - val_accuracy: 0.9283\n",
      "Epoch 146/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9884\n",
      "Epoch 00146: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0640 - accuracy: 0.9883 - val_loss: 0.3428 - val_accuracy: 0.9265\n",
      "Epoch 147/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9891\n",
      "Epoch 00147: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9891 - val_loss: 0.3478 - val_accuracy: 0.9290\n",
      "Epoch 148/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9893\n",
      "Epoch 00148: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0599 - accuracy: 0.9893 - val_loss: 0.3575 - val_accuracy: 0.9272\n",
      "Epoch 149/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0603 - accuracy: 0.9892\n",
      "Epoch 00149: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0602 - accuracy: 0.9893 - val_loss: 0.3510 - val_accuracy: 0.9283\n",
      "Epoch 150/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0612 - accuracy: 0.9890\n",
      "Epoch 00150: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0611 - accuracy: 0.9890 - val_loss: 0.3570 - val_accuracy: 0.9254\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [07:59<00:00, 239.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.77      0.82      0.79       574\n",
      "        car_horn       0.83      0.89      0.86       224\n",
      "children_playing       0.78      0.75      0.76       700\n",
      "        dog_bark       0.76      0.67      0.71       700\n",
      "           siren       0.87      0.96      0.91       574\n",
      "\n",
      "        accuracy                           0.80      2772\n",
      "       macro avg       0.80      0.82      0.81      2772\n",
      "    weighted avg       0.80      0.80      0.80      2772\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Option for scalerOpt is either \"normalization\" or \"standardization\"\n",
    "\n",
    "metrics_set, models_set, batch_name = model_classifiers(classifiers, \n",
    "                                                        DB_from_pkl, \n",
    "                                                        scalerOpt = 'standardization',\n",
    "                                                        use_PCA = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a578bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>Accuracy(Val)</th>\n",
       "      <th>...</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Conf_M</th>\n",
       "      <th>Process_time</th>\n",
       "      <th>Class_report(Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>[[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>[[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]</td>\n",
       "      <td>265.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANN</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>[[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>[[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>[[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]</td>\n",
       "      <td>109.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>[[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]</td>\n",
       "      <td>703.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>[[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>[[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]</td>\n",
       "      <td>359.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>[[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>[[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]</td>\n",
       "      <td>812.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>[[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]</td>\n",
       "      <td>140.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>[[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ANN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>[[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>[[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>[[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>[[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]</td>\n",
       "      <td>437.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ANN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>[[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>[[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]</td>\n",
       "      <td>328.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>[[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>[[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]</td>\n",
       "      <td>546.875</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model Fold  Accuracy(Train)  Accuracy(Val)  ...  Recall(Val)                                                                                                               Conf_M  Process_time  \\\n",
       "0      ANN    1         1.000000       0.802990  ...     0.802990       [[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]        62.500   \n",
       "1   CNN_1D    1         0.999758       0.798007  ...     0.798007       [[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]       265.625   \n",
       "2      ANN   10         1.000000       0.763382  ...     0.763382    [[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]        62.500   \n",
       "3   CNN_1D   10         0.999275       0.747358  ...     0.747358     [[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]       343.750   \n",
       "4      ANN    2         1.000000       0.741592  ...     0.741592   [[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]       109.375   \n",
       "5   CNN_1D    2         0.999677       0.731269  ...     0.731269    [[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]       703.125   \n",
       "6      ANN    3         1.000000       0.823355  ...     0.823355         [[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]        78.125   \n",
       "7   CNN_1D    3         0.999713       0.808239  ...     0.808239        [[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]       359.375   \n",
       "8      ANN    4         1.000000       0.761993  ...     0.761993  [[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]        78.125   \n",
       "9   CNN_1D    4         0.999875       0.755102  ...     0.755102  [[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]       812.500   \n",
       "10     ANN    5         1.000000       0.774420  ...     0.774420     [[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]       140.625   \n",
       "11  CNN_1D    5         0.999878       0.748779  ...     0.748779    [[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]       343.750   \n",
       "12     ANN    6         1.000000       0.772500  ...     0.772500       [[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]        62.500   \n",
       "13  CNN_1D    6         0.999799       0.747500  ...     0.747500     [[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]        93.750   \n",
       "14     ANN    7         1.000000       0.738977  ...     0.738977     [[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]        62.500   \n",
       "15  CNN_1D    7         0.999880       0.714638  ...     0.714638   [[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]       437.500   \n",
       "16     ANN    8         1.000000       0.784982  ...     0.784982       [[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]        62.500   \n",
       "17  CNN_1D    8         0.999600       0.756410  ...     0.756410         [[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]       328.125   \n",
       "18     ANN    9         1.000000       0.831890  ...     0.831890         [[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]        93.750   \n",
       "19  CNN_1D    9         0.999679       0.798341  ...     0.798341       [[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]       546.875   \n",
       "\n",
       "                                                                                                          Class_report(Val)  \n",
       "0                     precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...  \n",
       "1                     precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...  \n",
       "2                     precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...  \n",
       "3                     precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...  \n",
       "4                     precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...  \n",
       "5                     precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...  \n",
       "6                     precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...  \n",
       "7                     precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...  \n",
       "8                     precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...  \n",
       "9                     precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...  \n",
       "10                    precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...  \n",
       "11                    precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...  \n",
       "12                    precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...  \n",
       "13                    precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...  \n",
       "14                    precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...  \n",
       "15                    precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...  \n",
       "16                    precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...  \n",
       "17                    precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...  \n",
       "18                    precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...  \n",
       "19                    precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...  \n",
       "\n",
       "[20 rows x 13 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e29d1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Conf_M</th>\n",
       "      <th>Process_time</th>\n",
       "      <th>Class_report(Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>ANN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>[[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>ANN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>[[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]</td>\n",
       "      <td>109.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>ANN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>[[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ANN</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>[[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>ANN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>[[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>ANN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>[[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]</td>\n",
       "      <td>140.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>ANN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>[[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>ANN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>[[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>ANN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>[[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>ANN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>[[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>[[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]</td>\n",
       "      <td>437.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>[[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]</td>\n",
       "      <td>703.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>[[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>[[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>[[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>[[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]</td>\n",
       "      <td>812.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>[[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]</td>\n",
       "      <td>328.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>[[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]</td>\n",
       "      <td>265.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>[[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]</td>\n",
       "      <td>546.875</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>[[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]</td>\n",
       "      <td>359.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   Model Fold  Accuracy(Train)  ...  Recall(Val)                                                                                                               Conf_M  Process_time  \\\n",
       "0      14     ANN    7         1.000000  ...     0.738977     [[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]        62.500   \n",
       "1       4     ANN    2         1.000000  ...     0.741592   [[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]       109.375   \n",
       "2       8     ANN    4         1.000000  ...     0.761993  [[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]        78.125   \n",
       "3       2     ANN   10         1.000000  ...     0.763382    [[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]        62.500   \n",
       "4      12     ANN    6         1.000000  ...     0.772500       [[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]        62.500   \n",
       "5      10     ANN    5         1.000000  ...     0.774420     [[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]       140.625   \n",
       "6      16     ANN    8         1.000000  ...     0.784982       [[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]        62.500   \n",
       "7       0     ANN    1         1.000000  ...     0.802990       [[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]        62.500   \n",
       "8       6     ANN    3         1.000000  ...     0.823355         [[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]        78.125   \n",
       "9      18     ANN    9         1.000000  ...     0.831890         [[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]        93.750   \n",
       "10     15  CNN_1D    7         0.999880  ...     0.714638   [[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]       437.500   \n",
       "11      5  CNN_1D    2         0.999677  ...     0.731269    [[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]       703.125   \n",
       "12      3  CNN_1D   10         0.999275  ...     0.747358     [[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]       343.750   \n",
       "13     13  CNN_1D    6         0.999799  ...     0.747500     [[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]        93.750   \n",
       "14     11  CNN_1D    5         0.999878  ...     0.748779    [[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]       343.750   \n",
       "15      9  CNN_1D    4         0.999875  ...     0.755102  [[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]       812.500   \n",
       "16     17  CNN_1D    8         0.999600  ...     0.756410         [[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]       328.125   \n",
       "17      1  CNN_1D    1         0.999758  ...     0.798007       [[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]       265.625   \n",
       "18     19  CNN_1D    9         0.999679  ...     0.798341       [[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]       546.875   \n",
       "19      7  CNN_1D    3         0.999713  ...     0.808239        [[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]       359.375   \n",
       "\n",
       "                                                                                                          Class_report(Val)  \n",
       "0                     precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...  \n",
       "1                     precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...  \n",
       "2                     precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...  \n",
       "3                     precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...  \n",
       "4                     precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...  \n",
       "5                     precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...  \n",
       "6                     precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...  \n",
       "7                     precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...  \n",
       "8                     precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...  \n",
       "9                     precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...  \n",
       "10                    precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...  \n",
       "11                    precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...  \n",
       "12                    precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...  \n",
       "13                    precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...  \n",
       "14                    precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...  \n",
       "15                    precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...  \n",
       "16                    precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...  \n",
       "17                    precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...  \n",
       "18                    precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...  \n",
       "19                    precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...  \n",
       "\n",
       "[20 rows x 14 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by Model and Accuracy test. Reset the index.\n",
    "\n",
    "metrics_set = metrics_set.sort_values(['Model', 'Accuracy(Val)'], ascending = [True, True]).reset_index()\n",
    "metrics_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c5af9afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3b3b9_row0_col1 {\n",
       "  background-color: #cee0f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row1_col1 {\n",
       "  background-color: #cadef0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row2_col1 {\n",
       "  background-color: #92c4de;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row3_col1 {\n",
       "  background-color: #8dc1dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row4_col1 {\n",
       "  background-color: #6dafd7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row5_col1 {\n",
       "  background-color: #68acd5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row6_col1 {\n",
       "  background-color: #4a98c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row7_col1 {\n",
       "  background-color: #2070b4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row8_col1 {\n",
       "  background-color: #084387;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row9_col1 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row10_col1 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row11_col1 {\n",
       "  background-color: #dbe9f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row12_col1, #T_3b3b9_row13_col1 {\n",
       "  background-color: #bdd7ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row14_col1 {\n",
       "  background-color: #b9d6ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row15_col1 {\n",
       "  background-color: #a8cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row16_col1 {\n",
       "  background-color: #a4cce3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row17_col1, #T_3b3b9_row18_col1 {\n",
       "  background-color: #2b7bba;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row19_col1 {\n",
       "  background-color: #1764ab;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3b3b9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3b3b9_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_3b3b9_level0_col1\" class=\"col_heading level0 col1\" >Accuracy(Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3b3b9_row0_col0\" class=\"data row0 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row0_col1\" class=\"data row0 col1\" >0.738977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3b3b9_row1_col0\" class=\"data row1 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row1_col1\" class=\"data row1 col1\" >0.741592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3b3b9_row2_col0\" class=\"data row2 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row2_col1\" class=\"data row2 col1\" >0.761993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3b3b9_row3_col0\" class=\"data row3 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row3_col1\" class=\"data row3 col1\" >0.763382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3b3b9_row4_col0\" class=\"data row4 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row4_col1\" class=\"data row4 col1\" >0.772500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3b3b9_row5_col0\" class=\"data row5 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row5_col1\" class=\"data row5 col1\" >0.774420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3b3b9_row6_col0\" class=\"data row6 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row6_col1\" class=\"data row6 col1\" >0.784982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3b3b9_row7_col0\" class=\"data row7 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row7_col1\" class=\"data row7 col1\" >0.802990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_3b3b9_row8_col0\" class=\"data row8 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row8_col1\" class=\"data row8 col1\" >0.823355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_3b3b9_row9_col0\" class=\"data row9 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row9_col1\" class=\"data row9 col1\" >0.831890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_3b3b9_row10_col0\" class=\"data row10 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row10_col1\" class=\"data row10 col1\" >0.714638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_3b3b9_row11_col0\" class=\"data row11 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row11_col1\" class=\"data row11 col1\" >0.731269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_3b3b9_row12_col0\" class=\"data row12 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row12_col1\" class=\"data row12 col1\" >0.747358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_3b3b9_row13_col0\" class=\"data row13 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row13_col1\" class=\"data row13 col1\" >0.747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_3b3b9_row14_col0\" class=\"data row14 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row14_col1\" class=\"data row14 col1\" >0.748779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_3b3b9_row15_col0\" class=\"data row15 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row15_col1\" class=\"data row15 col1\" >0.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_3b3b9_row16_col0\" class=\"data row16 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row16_col1\" class=\"data row16 col1\" >0.756410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_3b3b9_row17_col0\" class=\"data row17 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row17_col1\" class=\"data row17 col1\" >0.798007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_3b3b9_row18_col0\" class=\"data row18 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row18_col1\" class=\"data row18 col1\" >0.798341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_3b3b9_row19_col0\" class=\"data row19 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row19_col1\" class=\"data row19 col1\" >0.808239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26783abd250>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_set[['Model', 'Accuracy(Val)']].style.background_gradient(cmap = cmap_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "74b6fa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "ANN       0.831890\n",
       "CNN_1D    0.808239\n",
       "Name: Accuracy(Val), dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_accuracy = metrics_set.groupby('Model')['Accuracy(Val)'].max()\n",
    "highest_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a40226fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary of each classifier and its data explanation\n",
    "\n",
    "unique_models = []\n",
    "results       = {}\n",
    "\n",
    "for c in classifiers:\n",
    "    unique_models.append(c)\n",
    "\n",
    "for model in unique_models:\n",
    "    result = metrics_set[metrics_set['Model'] == model].describe().round(4)\n",
    "    results[model] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1b6a5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model...: ANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>Accuracy(Val)</th>\n",
       "      <th>F1(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Precision(Val)</th>\n",
       "      <th>Recall(Train)</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Process_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7827</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>81.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.0553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>26.3523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>62.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7702</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7623</td>\n",
       "      <td>62.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7735</td>\n",
       "      <td>70.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>89.8438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8319</td>\n",
       "      <td>140.6250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Accuracy(Train)  Accuracy(Val)  F1(Train)  ...  Precision(Val)  Recall(Train)  Recall(Val)  Process_time\n",
       "count  10.0000             10.0        10.0000       10.0  ...         10.0000           10.0      10.0000       10.0000\n",
       "mean    9.0000              1.0         0.7796        1.0  ...          0.7827            1.0       0.7796       81.2500\n",
       "std     6.0553              0.0         0.0316        0.0  ...          0.0313            0.0       0.0316       26.3523\n",
       "min     0.0000              1.0         0.7390        1.0  ...          0.7372            1.0       0.7390       62.5000\n",
       "25%     4.5000              1.0         0.7623        1.0  ...          0.7702            1.0       0.7623       62.5000\n",
       "50%     9.0000              1.0         0.7735        1.0  ...          0.7811            1.0       0.7735       70.3125\n",
       "75%    13.5000              1.0         0.7985        1.0  ...          0.8008            1.0       0.7985       89.8438\n",
       "max    18.0000              1.0         0.8319        1.0  ...          0.8301            1.0       0.8319      140.6250\n",
       "\n",
       "[8 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model...: CNN_1D\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>Accuracy(Val)</th>\n",
       "      <th>F1(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Precision(Val)</th>\n",
       "      <th>Recall(Train)</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Process_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7606</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7668</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7606</td>\n",
       "      <td>423.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.0553</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>212.0054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.7146</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7161</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.7146</td>\n",
       "      <td>93.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.5000</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7526</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>332.0312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7519</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7651</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7519</td>\n",
       "      <td>351.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.5000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.7876</td>\n",
       "      <td>519.5312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.0000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>812.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Accuracy(Train)  Accuracy(Val)  F1(Train)  ...  Precision(Val)  Recall(Train)  Recall(Val)  Process_time\n",
       "count  10.0000          10.0000        10.0000    10.0000  ...         10.0000        10.0000      10.0000       10.0000\n",
       "mean   10.0000           0.9997         0.7606     0.9997  ...          0.7668         0.9997       0.7606      423.4375\n",
       "std     6.0553           0.0002         0.0309     0.0002  ...          0.0312         0.0002       0.0309      212.0054\n",
       "min     1.0000           0.9993         0.7146     0.9993  ...          0.7161         0.9993       0.7146       93.7500\n",
       "25%     5.5000           0.9997         0.7474     0.9997  ...          0.7526         0.9997       0.7474      332.0312\n",
       "50%    10.0000           0.9997         0.7519     0.9997  ...          0.7651         0.9997       0.7519      351.5625\n",
       "75%    14.5000           0.9999         0.7876     0.9999  ...          0.7892         0.9999       0.7876      519.5312\n",
       "max    19.0000           0.9999         0.8082     0.9999  ...          0.8126         0.9999       0.8082      812.5000\n",
       "\n",
       "[8 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in results.keys():\n",
    "    print(f'Model...: {model}')\n",
    "    display(results[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "47b38499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Precision(Val)</th>\n",
       "      <th>Recall(Train)</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Process_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>ANN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>ANN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>109.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>ANN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771040</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>78.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ANN</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769877</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>ANN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>ANN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787695</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>140.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>ANN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>ANN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>ANN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>78.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>ANN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>93.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716088</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>437.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729723</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>703.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761811</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>343.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750839</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>93.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768301</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>343.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770036</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>812.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757799</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>328.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805613</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>265.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795552</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>546.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812591</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>359.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   Model Fold  Accuracy(Train)  ...  Precision(Val)  Recall(Train)  Recall(Val)  Process_time\n",
       "0      14     ANN    7         1.000000  ...        0.739424       1.000000     0.738977        62.500\n",
       "1       4     ANN    2         1.000000  ...        0.737229       1.000000     0.741592       109.375\n",
       "2       8     ANN    4         1.000000  ...        0.771040       1.000000     0.761993        78.125\n",
       "3       2     ANN   10         1.000000  ...        0.769877       1.000000     0.763382        62.500\n",
       "4      12     ANN    6         1.000000  ...        0.775436       1.000000     0.772500        62.500\n",
       "5      10     ANN    5         1.000000  ...        0.787695       1.000000     0.774420       140.625\n",
       "6      16     ANN    8         1.000000  ...        0.786730       1.000000     0.784982        62.500\n",
       "7       0     ANN    1         1.000000  ...        0.805109       1.000000     0.802990        62.500\n",
       "8       6     ANN    3         1.000000  ...        0.824088       1.000000     0.823355        78.125\n",
       "9      18     ANN    9         1.000000  ...        0.830138       1.000000     0.831890        93.750\n",
       "10     15  CNN_1D    7         0.999880  ...        0.716088       0.999880     0.714638       437.500\n",
       "11      5  CNN_1D    2         0.999677  ...        0.729723       0.999677     0.731269       703.125\n",
       "12      3  CNN_1D   10         0.999275  ...        0.761811       0.999275     0.747358       343.750\n",
       "13     13  CNN_1D    6         0.999799  ...        0.750839       0.999799     0.747500        93.750\n",
       "14     11  CNN_1D    5         0.999878  ...        0.768301       0.999878     0.748779       343.750\n",
       "15      9  CNN_1D    4         0.999875  ...        0.770036       0.999875     0.755102       812.500\n",
       "16     17  CNN_1D    8         0.999600  ...        0.757799       0.999600     0.756410       328.125\n",
       "17      1  CNN_1D    1         0.999758  ...        0.805613       0.999758     0.798007       265.625\n",
       "18     19  CNN_1D    9         0.999679  ...        0.795552       0.999679     0.798341       546.875\n",
       "19      7  CNN_1D    3         0.999713  ...        0.812591       0.999713     0.808239       359.375\n",
       "\n",
       "[20 rows x 12 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_set_no_cm = metrics_set.drop(['Conf_M', 'Class_report(Val)'], axis=1)\n",
    "metrics_set_no_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2ea27fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US8K_AV_metrics_set_NN_std_PCA_windowed.pkl\n",
      "US8K_AV_metrics_set_NN_std_PCA_windowed_no_cm.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_set_name       = nom_dataset + '_metrics_set_NN' + batch_name +  model_surname + '.pkl'\n",
    "metrics_set_name_no_cm = nom_dataset + '_metrics_set_NN' + batch_name +  model_surname + '_no_cm.csv'\n",
    "\n",
    "print(metrics_set_name)\n",
    "print(metrics_set_name_no_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3d17b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes de results to a PKL and CSV file\n",
    "\n",
    "with open(os.path.join(path_models, metrics_set_name), 'wb') as file:\n",
    "    pickle.dump(metrics_set, file)\n",
    "    \n",
    "metrics_set_no_cm.to_csv(os.path.join(path_models, metrics_set_name_no_cm), sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "edb90a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Conf_M</th>\n",
       "      <th>Process_time</th>\n",
       "      <th>Class_report(Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>ANN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>[[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>ANN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>[[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]</td>\n",
       "      <td>109.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>ANN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>[[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ANN</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>[[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>ANN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>[[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>ANN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>[[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]</td>\n",
       "      <td>140.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>ANN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>[[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>ANN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>[[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>ANN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>[[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>ANN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>[[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>[[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]</td>\n",
       "      <td>437.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>[[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]</td>\n",
       "      <td>703.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>[[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>[[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>[[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>[[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]</td>\n",
       "      <td>812.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>[[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]</td>\n",
       "      <td>328.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>[[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]</td>\n",
       "      <td>265.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>[[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]</td>\n",
       "      <td>546.875</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>[[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]</td>\n",
       "      <td>359.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   Model Fold  Accuracy(Train)  ...  Recall(Val)                                                                                                               Conf_M  Process_time  \\\n",
       "0      14     ANN    7         1.000000  ...     0.738977     [[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]        62.500   \n",
       "1       4     ANN    2         1.000000  ...     0.741592   [[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]       109.375   \n",
       "2       8     ANN    4         1.000000  ...     0.761993  [[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]        78.125   \n",
       "3       2     ANN   10         1.000000  ...     0.763382    [[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]        62.500   \n",
       "4      12     ANN    6         1.000000  ...     0.772500       [[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]        62.500   \n",
       "5      10     ANN    5         1.000000  ...     0.774420     [[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]       140.625   \n",
       "6      16     ANN    8         1.000000  ...     0.784982       [[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]        62.500   \n",
       "7       0     ANN    1         1.000000  ...     0.802990       [[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]        62.500   \n",
       "8       6     ANN    3         1.000000  ...     0.823355         [[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]        78.125   \n",
       "9      18     ANN    9         1.000000  ...     0.831890         [[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]        93.750   \n",
       "10     15  CNN_1D    7         0.999880  ...     0.714638   [[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]       437.500   \n",
       "11      5  CNN_1D    2         0.999677  ...     0.731269    [[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]       703.125   \n",
       "12      3  CNN_1D   10         0.999275  ...     0.747358     [[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]       343.750   \n",
       "13     13  CNN_1D    6         0.999799  ...     0.747500     [[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]        93.750   \n",
       "14     11  CNN_1D    5         0.999878  ...     0.748779    [[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]       343.750   \n",
       "15      9  CNN_1D    4         0.999875  ...     0.755102  [[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]       812.500   \n",
       "16     17  CNN_1D    8         0.999600  ...     0.756410         [[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]       328.125   \n",
       "17      1  CNN_1D    1         0.999758  ...     0.798007       [[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]       265.625   \n",
       "18     19  CNN_1D    9         0.999679  ...     0.798341       [[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]       546.875   \n",
       "19      7  CNN_1D    3         0.999713  ...     0.808239        [[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]       359.375   \n",
       "\n",
       "                                                                                                          Class_report(Val)  \n",
       "0                     precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...  \n",
       "1                     precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...  \n",
       "2                     precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...  \n",
       "3                     precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...  \n",
       "4                     precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...  \n",
       "5                     precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...  \n",
       "6                     precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...  \n",
       "7                     precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...  \n",
       "8                     precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...  \n",
       "9                     precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...  \n",
       "10                    precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...  \n",
       "11                    precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...  \n",
       "12                    precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...  \n",
       "13                    precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...  \n",
       "14                    precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...  \n",
       "15                    precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...  \n",
       "16                    precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...  \n",
       "17                    precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...  \n",
       "18                    precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...  \n",
       "19                    precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...  \n",
       "\n",
       "[20 rows x 14 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_set_from_pkl = pd.read_pickle(os.path.join(path_models, metrics_set_name))\n",
    "metrics_set_from_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "faea2f5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANN': {'Accuracy(Val)': 0.8318903318903319,\n",
       "  'Conf_M': array([[486,  25,  29,  14,  20],\n",
       "         [ 10, 204,   1,   7,   2],\n",
       "         [ 12,   7, 542, 107,  32],\n",
       "         [ 58,  13,  87, 515,  27],\n",
       "         [  5,   0,   4,   6, 559]], dtype=int64)},\n",
       " 'CNN_1D': {'Accuracy(Val)': 0.8082394783639597,\n",
       "  'Conf_M': array([[698,  41,  58,  20,  23],\n",
       "         [  5, 289,   0,   7,   0],\n",
       "         [ 46,   1, 551,  81,  21],\n",
       "         [ 45,   1,  92, 536,  26],\n",
       "         [125,   1,  36,  18, 653]], dtype=int64)}}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = metrics_set.groupby('Model')['Accuracy(Val)'].idxmax()\n",
    "conf_matrices = metrics_set.loc[idx, ['Model','Accuracy(Val)','Conf_M']]\n",
    "conf_matrices.set_index('Model', inplace=True)\n",
    "conf_matrices_dict = conf_matrices.to_dict('index')\n",
    "conf_matrices_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1603ff77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[486,  25,  29,  14,  20],\n",
       "       [ 10, 204,   1,   7,   2],\n",
       "       [ 12,   7, 542, 107,  32],\n",
       "       [ 58,  13,  87, 515,  27],\n",
       "       [  5,   0,   4,   6, 559]], dtype=int64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrices_dict['ANN']['Conf_M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4c32d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ANN\n",
      "0.8318903318903319\n",
      "[[486  25  29  14  20]\n",
      " [ 10 204   1   7   2]\n",
      " [ 12   7 542 107  32]\n",
      " [ 58  13  87 515  27]\n",
      " [  5   0   4   6 559]]\n",
      "2\n",
      "CNN_1D\n",
      "0.8082394783639597\n",
      "[[698  41  58  20  23]\n",
      " [  5 289   0   7   0]\n",
      " [ 46   1 551  81  21]\n",
      " [ 45   1  92 536  26]\n",
      " [125   1  36  18 653]]\n"
     ]
    }
   ],
   "source": [
    "for i, idx in zip(conf_matrices_dict.keys(), range(1, len(conf_matrices_dict) + 1)):\n",
    "    print(idx)\n",
    "    print(i)\n",
    "    print(conf_matrices_dict[i]['Accuracy(Val)'])\n",
    "    print(conf_matrices_dict[i]['Conf_M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e5e53550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix for the highest accuracy test classifiers\n",
    "\n",
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.suptitle(nom_dataset + model_surname + batch_name + ' - Confusion matrices of the best results for each classifier', fontsize = 16,  y=0.99)\n",
    "for i, idx in zip(conf_matrices_dict.keys(), range(1, len(conf_matrices_dict) + 1)):\n",
    "    title = 'Classifier '+ i + ' (Highest accuracy validation of the best models: ' + str(\"{:0.4f}\".format(conf_matrices_dict[i]['Accuracy(Val)'])) +')'\n",
    "    plt.subplot(1,2,idx)\n",
    "    plot_confusion_matrix(conf_matrices_dict[i]['Conf_M'],  \n",
    "                          nom_classes, \n",
    "                          title,\n",
    "                          cmap = None,                          \n",
    "                          normalize = False)\n",
    "\n",
    "plt.savefig(os.path.join(path_pic, picture_name))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "75f0df34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.suptitle(f'{nom_dataset} - Box plot each classifier (batch type: {model_surname + batch_name})', fontsize = 16,  y=0.97)\n",
    "box_plot = sns.boxplot(data=metrics_set, x=\"Model\", y=\"Accuracy(Val)\", showfliers = True)\n",
    "\n",
    "medians = list(metrics_set.groupby(['Model'])['Accuracy(Val)'].median())\n",
    "medians = [round(element, 2) for element in medians]\n",
    "\n",
    "vertical_offset = metrics_set['Accuracy(Val)'].median()*0.001  # offset from median for display\n",
    "\n",
    "for xtick in box_plot.get_xticks():\n",
    "    box_plot.text(xtick, medians[xtick] + vertical_offset, medians[xtick], \n",
    "            horizontalalignment='center',size='medium',color='w',weight='semibold')\n",
    "plt.savefig(os.path.join(path_pic, picture_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41968385",
   "metadata": {},
   "source": [
    "## Results ESC-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cece92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e275303",
   "metadata": {},
   "source": [
    "## Results BDLib2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69cc24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b29ab96b",
   "metadata": {},
   "source": [
    "## Results US8K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568845ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85e46c9c",
   "metadata": {},
   "source": [
    "## Results US8K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294ed2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb45a0d9",
   "metadata": {},
   "source": [
    "# End of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93509039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d9f7d3",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57de80",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abcdd9",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
