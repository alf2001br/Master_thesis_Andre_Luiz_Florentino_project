{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3faa6a4b",
   "metadata": {},
   "source": [
    "### Faculdade de Engenharia Industrial - FEI\n",
    "\n",
    "### Centro Universitário da Fundação Educacional Inaciana \"Padre Sabóia de Medeiros\" (FEI)\n",
    "\n",
    "\n",
    "*FEI's Stricto Sensu Graduate Program in Electrical Engineering*\n",
    "\n",
    "Concentration area: ARTIFICIAL INTELLIGENCE APPLIED TO AUTOMATION AND ROBOTICS\n",
    "\n",
    "Master's thesis student Andre Luiz Florentino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd16632",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d17ec4",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b237808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "------------------------------------------------------------------------------------------\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "<function is_built_with_cuda at 0x0000019A2FAEA0D0>\n",
      "/device:GPU:0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "pd = tf.config.experimental.list_physical_devices()\n",
    "for i in pd:\n",
    "    print(i)\n",
    "print('------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "\n",
    "print(tf.test.is_built_with_cuda)\n",
    "# <function is_built_with_cuda at 0x000001AA24AFEC10>\n",
    "\n",
    "print(tf.test.gpu_device_name())\n",
    "# /device:GPU:0\n",
    "\n",
    "#gvd = tf.config.get_visible_devices()\n",
    "for j in tf.config.get_visible_devices():\n",
    "    print(j)\n",
    "# PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
    "# PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices()\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de63c0",
   "metadata": {},
   "source": [
    "# Chapter 6: Neural networks\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4504379",
   "metadata": {},
   "source": [
    "## Importe modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f29048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import warnings\n",
    "import itertools\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import pandas     as pd\n",
    "import seaborn    as sns\n",
    "import numpy      as np\n",
    "\n",
    "from matplotlib  import pyplot  as plt\n",
    "from keras       import backend as K\n",
    "\n",
    "from tqdm                        import tqdm\n",
    "from collections                 import Counter\n",
    "\n",
    "from sklearn                     import metrics\n",
    "from sklearn.model_selection     import train_test_split\n",
    "from sklearn.metrics             import confusion_matrix, classification_report\n",
    "from sklearn.decomposition       import PCA\n",
    "\n",
    "from tensorflow                  import keras\n",
    "from tensorflow.keras.models     import Sequential, load_model\n",
    "from tensorflow.keras.layers     import Dense, Dropout, Conv1D, GlobalAveragePooling1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks             import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers          import l2\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "pd.set_option('display.max_columns', 9)\n",
    "pd.set_option('display.width', 300)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "cmap_cm   = plt.cm.Blues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0dbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# For the picture names\n",
    "pic_first_name = '06_Neural_network_'\n",
    "\n",
    "# For Librosa\n",
    "FRAME_SIZE  = 1024\n",
    "HOP_LENGTH  = 512\n",
    "SEED        = 1000\n",
    "SR          = 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53326eea",
   "metadata": {},
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097f8ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-) ESC-10\n",
      "2-) BDLib2\n",
      "3-) US8K\n",
      "4-) US8K_AV\n",
      "\n",
      "Select the dataset: 4\n"
     ]
    }
   ],
   "source": [
    "# Select the dataset\n",
    "\n",
    "opc = 0\n",
    "while str(opc) not in '1234':\n",
    "    print()\n",
    "    print(\"1-) ESC-10\")\n",
    "    print(\"2-) BDLib2\")\n",
    "    print(\"3-) US8K\")\n",
    "    print(\"4-) US8K_AV\")\n",
    "\n",
    "\n",
    "    opc = input(\"\\nSelect the dataset: \")\n",
    "    if opc.isdigit():\n",
    "        opc = int(opc)\n",
    "    else:\n",
    "        opc = 0\n",
    "\n",
    "if opc == 1:\n",
    "\n",
    "    path        = os.path.join(current_path, \"_dataset\", \"ESC-10\")\n",
    "    path_pic    = os.path.join(current_path, \"ESC-10_results\")\n",
    "    path_models = os.path.join(current_path, \"ESC-10_saved_models\")\n",
    "    \n",
    "   \n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'ESC-10' \n",
    "    csv_file    = 'ESC-10.csv'\n",
    "    fold        = '1'\n",
    "\n",
    "    pkl_features          = 'ESC-10_features_original.pkl'\n",
    "    pkl_aug_features      = 'ESC-10_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'ESC-10_features_augmented.pkl'\n",
    "\n",
    "    \n",
    "if opc == 2:\n",
    "    \n",
    "    path        = os.path.join(current_path, \"_dataset\", \"BDLib2\")\n",
    "    path_pic    = os.path.join(current_path, \"BDLib2_results\")\n",
    "    path_models = os.path.join(current_path, \"BDLib2_saved_models\")\n",
    "\n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'BDLib2' \n",
    "    csv_file    = 'BDLib2.csv'\n",
    "    fold        = 'fold-1'\n",
    "\n",
    "    pkl_features          = 'BDLib2_features_original.pkl'\n",
    "    pkl_aug_features      = 'BDLib2_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'BDLib2_features_augmented.pkl'\n",
    "\n",
    "    \n",
    "if opc == 3:\n",
    "    \n",
    "    path        = os.path.join(current_path, \"_dataset\", \"US8K\")\n",
    "    path_pic    = os.path.join(current_path, \"US8K_results\")\n",
    "    path_models = os.path.join(current_path, \"US8K_saved_models\")\n",
    "    \n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'US8K' \n",
    "    csv_file    = 'US8K.csv'\n",
    "    fold        = '1'\n",
    "    \n",
    "    pkl_features          = 'US8K_features_original.pkl'\n",
    "    pkl_aug_features      = 'US8K_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'US8K_features_windowed.pkl' # augmented and windowed makes no sense. Dataset is already quite large\n",
    "    \n",
    "\n",
    "if opc == 4:\n",
    "\n",
    "    path        = os.path.join(current_path, \"_dataset\", \"US8K_AV\")\n",
    "    path_pic    = os.path.join(current_path, \"US8K_AV_results\")\n",
    "    path_models = os.path.join(current_path, \"US8K_AV_saved_models\")\n",
    "\n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'US8K_AV' \n",
    "    csv_file    = 'US8K_AV.csv'\n",
    "    fold        = '1'\n",
    "\n",
    "    pkl_features          = 'US8K_AV_features_original.pkl'\n",
    "    pkl_aug_features      = 'US8K_AV_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'US8K_AV_features_windowed.pkl' # augmented and windowed makes no sense. Dataset is already quite large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c339e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_file_number(folder: str):\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) and f.startswith(pic_first_name)]\n",
    "    if not files:\n",
    "        return 1\n",
    "    else:\n",
    "        numbers = [int(f.split('.')[0].split('_')[-1]) for f in files]\n",
    "        return max(numbers) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9974f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MT_loadDataset import loadDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32aca6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes:\n",
      "--------------------\n",
      "Class_categorical\n",
      "dog_bark            1000\n",
      "children_playing    1000\n",
      "background          1000\n",
      "siren                929\n",
      "car_horn             429\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of unique files..........:  4358\n",
      "Total number of AUDIO files...........:  4358\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Folder_name</th>\n",
       "      <th>Class_OHEV</th>\n",
       "      <th>Class_categorical</th>\n",
       "      <th>...</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>...</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>...</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>...</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>...</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>fold5</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>...</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>159.522205</td>\n",
       "      <td>163.522205</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>181.142431</td>\n",
       "      <td>183.284976</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4355</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>242.691902</td>\n",
       "      <td>246.197885</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>253.209850</td>\n",
       "      <td>255.741948</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4357</th>\n",
       "      <td>7</td>\n",
       "      <td>fold7</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>...</td>\n",
       "      <td>99812</td>\n",
       "      <td>332.289233</td>\n",
       "      <td>334.821332</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4358 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fold Folder_name       Class_OHEV Class_categorical  ...    fsID       start         end  salience\n",
       "0        5       fold5  [0, 0, 0, 1, 0]          dog_bark  ...  100032    0.000000    0.317551         1\n",
       "1        5       fold5  [0, 0, 1, 0, 0]  children_playing  ...  100263   58.500000   62.500000         1\n",
       "2        5       fold5  [0, 0, 1, 0, 0]  children_playing  ...  100263   60.500000   64.500000         1\n",
       "3        5       fold5  [0, 0, 1, 0, 0]  children_playing  ...  100263   63.000000   67.000000         1\n",
       "4        5       fold5  [0, 0, 1, 0, 0]  children_playing  ...  100263   68.500000   72.500000         1\n",
       "...    ...         ...              ...               ...  ...     ...         ...         ...       ...\n",
       "4353     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  159.522205  163.522205         2\n",
       "4354     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  181.142431  183.284976         2\n",
       "4355     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  242.691902  246.197885         2\n",
       "4356     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  253.209850  255.741948         2\n",
       "4357     7       fold7  [0, 1, 0, 0, 0]          car_horn  ...   99812  332.289233  334.821332         2\n",
       "\n",
       "[4358 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadDataset = loadDataset(path)\n",
    "DB          = loadDataset.db_B\n",
    "\n",
    "print(\"\\nClasses:\\n--------------------\")\n",
    "print(DB[\"Class_categorical\"].value_counts())\n",
    "print(\"\\nTotal number of unique files..........: \", len(np.unique(DB[\"File_name\"])))\n",
    "print(\"Total number of AUDIO files...........: \", len(DB))\n",
    "DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7923e1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fold                   int64\n",
       "Folder_name           object\n",
       "Class_OHEV            object\n",
       "Class_categorical     object\n",
       "File_name             object\n",
       "Path                  object\n",
       "classID                int64\n",
       "fsID                   int64\n",
       "start                float64\n",
       "end                  float64\n",
       "salience               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72726b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1150.62x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analysis of the class balancing\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "gTitle = f'{nom_dataset} - Number of classes = ' + str(len(pd.Series(DB['Class_categorical']).unique()))\n",
    "g = sns.displot(DB,x='Class_categorical', hue='Class_categorical',height = 5, aspect = 2).set(title=gTitle)\n",
    "g.set_xticklabels(rotation=90)\n",
    "g.set_titles('Number of classes')\n",
    "\n",
    "# Retrieve the axes object from the plot\n",
    "axes = g.ax\n",
    "\n",
    "# Iterate over each bar in the plot\n",
    "for p in axes.patches:\n",
    "    # Get the coordinates of the bar\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    cord_x, cord_y = p.get_xy()\n",
    "    if height > 0:\n",
    "        axes.annotate(f'{height}', (cord_x + width/2, cord_y + height), ha='center')\n",
    "        \n",
    "g._legend.remove()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9727f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-) Features original\n",
      "2-) Features augmented\n",
      "3-) Features augmented and windowed (US8K is only windowed)\n",
      "\n",
      "Select the dataset: 3\n"
     ]
    }
   ],
   "source": [
    "# Read the pkl file with the augmented features extracted\n",
    "\n",
    "opc = 0\n",
    "while str(opc) not in '123':\n",
    "    print()\n",
    "    print(\"1-) Features original\")\n",
    "    print(\"2-) Features augmented\")\n",
    "    print(\"3-) Features augmented and windowed (US8K is only windowed)\")\n",
    "\n",
    "    opc = input(\"\\nSelect the dataset: \")\n",
    "    if opc.isdigit():\n",
    "        opc = int(opc)\n",
    "    else:\n",
    "        opc = 0\n",
    "\n",
    "if opc == 1:\n",
    "    DB_from_pkl   = pd.read_pickle(os.path.join(path_models, pkl_features))\n",
    "    model_surname = '_original'\n",
    "\n",
    "if opc == 2:\n",
    "    DB_from_pkl   = pd.read_pickle(os.path.join(path_models, pkl_aug_features))\n",
    "    model_surname = '_augmented'\n",
    "\n",
    "if opc == 3:\n",
    "    DB_from_pkl = pd.read_pickle(os.path.join(path_models, pkl_aug_wind_features))\n",
    "    model_surname = '_windowed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc2befe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Audio                  object\n",
       "Class_categorical      object\n",
       "Class_OHEV             object\n",
       "Fold                   object\n",
       "RMSE                  float64\n",
       "                       ...   \n",
       "TONNETZ_6             float64\n",
       "TONNETZ_std_6         float64\n",
       "TONNETZ_median_6      float64\n",
       "TONNETZ_skew_6        float64\n",
       "TONNETZ_kurtosis_6    float64\n",
       "Length: 379, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_from_pkl.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b9f36a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration of the dataset:  8.4608 h\n"
     ]
    }
   ],
   "source": [
    "total_duration = 0\n",
    "for audio in DB_from_pkl['Audio']:\n",
    "    total_duration = total_duration + librosa.get_duration(y=audio)\n",
    "print('Total duration of the dataset: ' , \"{:0.4f} h\".format(total_duration / 3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5a4f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Audio</th>\n",
       "      <th>Class_categorical</th>\n",
       "      <th>Class_OHEV</th>\n",
       "      <th>Fold</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.0034710653, -0.0050192624, -0.004654482, -0.0049833283, -0.0038681468, -0.0023575649, -0.00025486574, 0.00135406...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.046785</td>\n",
       "      <td>0.461191</td>\n",
       "      <td>-0.338882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.015428771, -0.0064468235, -0.002025701, -0.009768408, -0.020482529, -0.03246226, -0.046539657, -0.050950672, -0....</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026433</td>\n",
       "      <td>-0.030688</td>\n",
       "      <td>0.414979</td>\n",
       "      <td>-0.840459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.17165461, -0.1961453, -0.2095497, -0.116395764, 0.02499168, 0.15181583, 0.2456393, 0.24304995, 0.1697193, 0.0706...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023640</td>\n",
       "      <td>-0.045384</td>\n",
       "      <td>0.629947</td>\n",
       "      <td>0.096665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.004147315, -0.014180049, -0.016550057, -0.017083425, -0.010863152, -0.0018686495, 0.006234308, 0.00725661, 0.005...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>-0.025407</td>\n",
       "      <td>0.554764</td>\n",
       "      <td>-0.364067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.047271818, -0.08598116, -0.08329079, -0.15244874, -0.21367016, -0.26584676, -0.19677721, -0.17195633, -0.0841544...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>-0.036388</td>\n",
       "      <td>0.770805</td>\n",
       "      <td>0.232676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30501</th>\n",
       "      <td>[-0.024376377, -0.010054192, 0.0009410139, 0.005863713, -0.0005189497, -0.0021953415, 0.0013682423, -0.0043018945, -...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>-0.069738</td>\n",
       "      <td>0.175013</td>\n",
       "      <td>-0.974133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30502</th>\n",
       "      <td>[0.0066777063, 0.008175363, 0.006157635, 0.0026387093, 0.00059039844, 0.0030925209, 0.0033645788, 0.0056151543, 0.00...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>-0.018432</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.099409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30503</th>\n",
       "      <td>[-0.0020232266, 0.001032982, 0.0023606261, 0.0017964527, 0.0009908059, -0.0011728329, -0.003362489, -0.0054453667, -...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.028383</td>\n",
       "      <td>-1.071243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30504</th>\n",
       "      <td>[-0.0015619812, 0.0005301243, 0.0032789772, 0.002725502, 0.0023546133, -0.0019822496, -0.002666029, -0.004454969, -0...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027106</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>-0.121982</td>\n",
       "      <td>-0.606880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30505</th>\n",
       "      <td>[9.745802e-05, -0.003326854, -0.007318441, -0.0072634676, -0.0057516163, -0.0039380156, -0.0028245365, -0.000594039,...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.054895</td>\n",
       "      <td>-0.195825</td>\n",
       "      <td>-1.209213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30506 rows × 379 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         Audio Class_categorical       Class_OHEV Fold  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "0      [-0.0034710653, -0.0050192624, -0.004654482, -0.0049833283, -0.0038681468, -0.0023575649, -0.00025486574, 0.00135406...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.025644         -0.046785        0.461191           -0.338882\n",
       "1      [-0.015428771, -0.0064468235, -0.002025701, -0.009768408, -0.020482529, -0.03246226, -0.046539657, -0.050950672, -0....          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.026433         -0.030688        0.414979           -0.840459\n",
       "2      [-0.17165461, -0.1961453, -0.2095497, -0.116395764, 0.02499168, 0.15181583, 0.2456393, 0.24304995, 0.1697193, 0.0706...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.023640         -0.045384        0.629947            0.096665\n",
       "3      [-0.004147315, -0.014180049, -0.016550057, -0.017083425, -0.010863152, -0.0018686495, 0.006234308, 0.00725661, 0.005...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.024617         -0.025407        0.554764           -0.364067\n",
       "4      [-0.047271818, -0.08598116, -0.08329079, -0.15244874, -0.21367016, -0.26584676, -0.19677721, -0.17195633, -0.0841544...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.024934         -0.036388        0.770805            0.232676\n",
       "...                                                                                                                        ...               ...              ...  ...  ...            ...               ...             ...                 ...\n",
       "30501  [-0.024376377, -0.010054192, 0.0009410139, 0.005863713, -0.0005189497, -0.0021953415, 0.0013682423, -0.0043018945, -...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.033488         -0.069738        0.175013           -0.974133\n",
       "30502  [0.0066777063, 0.008175363, 0.006157635, 0.0026387093, 0.00059039844, 0.0030925209, 0.0033645788, 0.0056151543, 0.00...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.021007         -0.018432       -0.690702           -0.099409\n",
       "30503  [-0.0020232266, 0.001032982, 0.0023606261, 0.0017964527, 0.0009908059, -0.0011728329, -0.003362489, -0.0054453667, -...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.021124         -0.016391       -0.028383           -1.071243\n",
       "30504  [-0.0015619812, 0.0005301243, 0.0032789772, 0.002725502, 0.0023546133, -0.0019822496, -0.002666029, -0.004454969, -0...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.027106         -0.017513       -0.121982           -0.606880\n",
       "30505  [9.745802e-05, -0.003326854, -0.007318441, -0.0072634676, -0.0057516163, -0.0039380156, -0.0028245365, -0.000594039,...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.038524         -0.054895       -0.195825           -1.209213\n",
       "\n",
       "[30506 rows x 379 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_from_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d945e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'numpy.int32'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(DB_from_pkl['Fold'][0][0]))\n",
    "print(type(DB_from_pkl['Class_OHEV'][0][0]))\n",
    "print(type(DB_from_pkl['Class_OHEV'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad53881",
   "metadata": {},
   "source": [
    "## Input split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1b3e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate 1 fold for validation and create a DB for the training / testing according to the datasets specification\n",
    "\n",
    "DB_from_pkl_VAL = DB_from_pkl[DB_from_pkl['Fold'] == fold].copy()\n",
    "DB_from_pkl_TRN = DB_from_pkl[DB_from_pkl['Fold'] != fold].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93bc0a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3010\n",
      "27496\n",
      "Total:  30506 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(DB_from_pkl_VAL))\n",
    "print(len(DB_from_pkl_TRN))\n",
    "print('Total: ', len(DB_from_pkl_VAL) + len(DB_from_pkl_TRN),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28d4d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Audio</th>\n",
       "      <th>Class_categorical</th>\n",
       "      <th>Class_OHEV</th>\n",
       "      <th>Fold</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>[6.402111e-05, 8.269498e-05, 5.2123058e-05, 7.1509836e-05, 3.3138364e-05, -6.1828905e-07, -8.950657e-05, -9.0291964e...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.230194</td>\n",
       "      <td>-0.774566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>[0.0005136457, 0.00041881658, 0.00034897702, 0.00021603762, 0.0002278979, 0.00011100468, 0.00010083006, 0.0001630317...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>0.529034</td>\n",
       "      <td>0.970660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>[-5.3512427e-05, 2.2222208e-05, 2.7161423e-05, 0.00017825539, 0.00032240857, 0.00041231932, 0.0005614782, 0.00053010...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030452</td>\n",
       "      <td>-0.029692</td>\n",
       "      <td>0.291676</td>\n",
       "      <td>-0.927084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>[-9.721824e-05, -0.0002176053, -0.00031682133, -0.00042641407, -0.00044769727, -0.00042776082, -0.00044338158, -0.00...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>-0.005922</td>\n",
       "      <td>-0.290679</td>\n",
       "      <td>-1.082696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>[0.00023775743, 0.00023206181, 0.00023593163, 0.00017538742, 0.00011133426, 0.00021567091, 0.00011633049, 8.274122e-...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030058</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>1.003930</td>\n",
       "      <td>1.476461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30333</th>\n",
       "      <td>[-0.28165075, -0.39413118, -0.48126578, -0.54062337, -0.5627302, -0.5343282, -0.45225257, -0.33417547, -0.2137392, -...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>0.014569</td>\n",
       "      <td>0.348922</td>\n",
       "      <td>-0.837185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30334</th>\n",
       "      <td>[-0.74769396, -0.725761, -0.69069016, -0.64954436, -0.6015309, -0.53891814, -0.45484614, -0.35106456, -0.23903547, -...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>0.736680</td>\n",
       "      <td>0.175157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30335</th>\n",
       "      <td>[-0.0073982505, 0.00089106406, 0.0060087573, -0.0026300459, -0.028010733, -0.059386022, -0.07765661, -0.06947853, -0...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032426</td>\n",
       "      <td>0.030839</td>\n",
       "      <td>0.338930</td>\n",
       "      <td>-0.276007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30336</th>\n",
       "      <td>[-0.19841202, -0.19705483, -0.20790972, -0.22519125, -0.2374166, -0.23624307, -0.2194226, -0.18771836, -0.14198783, ...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053901</td>\n",
       "      <td>0.111005</td>\n",
       "      <td>-0.161630</td>\n",
       "      <td>-1.176320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30337</th>\n",
       "      <td>[0.028792929, -0.011599504, -0.03312391, -0.02478233, 0.00010168506, 0.017408311, 0.012290241, -0.011743451, -0.0403...</td>\n",
       "      <td>background</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035027</td>\n",
       "      <td>0.117277</td>\n",
       "      <td>0.230642</td>\n",
       "      <td>-0.647329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3010 rows × 379 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         Audio Class_categorical       Class_OHEV Fold  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "231    [6.402111e-05, 8.269498e-05, 5.2123058e-05, 7.1509836e-05, 3.3138364e-05, -6.1828905e-07, -8.950657e-05, -9.0291964e...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.019867          0.005833        0.230194           -0.774566\n",
       "232    [0.0005136457, 0.00041881658, 0.00034897702, 0.00021603762, 0.0002278979, 0.00011100468, 0.00010083006, 0.0001630317...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.018672          0.009202        0.529034            0.970660\n",
       "233    [-5.3512427e-05, 2.2222208e-05, 2.7161423e-05, 0.00017825539, 0.00032240857, 0.00041231932, 0.0005614782, 0.00053010...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.030452         -0.029692        0.291676           -0.927084\n",
       "234    [-9.721824e-05, -0.0002176053, -0.00031682133, -0.00042641407, -0.00044769727, -0.00042776082, -0.00044338158, -0.00...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.031560         -0.005922       -0.290679           -1.082696\n",
       "235    [0.00023775743, 0.00023206181, 0.00023593163, 0.00017538742, 0.00011133426, 0.00021567091, 0.00011633049, 8.274122e-...          dog_bark  [0, 0, 0, 1, 0]    1  ...       0.030058          0.002691        1.003930            1.476461\n",
       "...                                                                                                                        ...               ...              ...  ...  ...            ...               ...             ...                 ...\n",
       "30333  [-0.28165075, -0.39413118, -0.48126578, -0.54062337, -0.5627302, -0.5343282, -0.45225257, -0.33417547, -0.2137392, -...        background  [1, 0, 0, 0, 0]    1  ...       0.029579          0.014569        0.348922           -0.837185\n",
       "30334  [-0.74769396, -0.725761, -0.69069016, -0.64954436, -0.6015309, -0.53891814, -0.45484614, -0.35106456, -0.23903547, -...        background  [1, 0, 0, 0, 0]    1  ...       0.023069          0.014551        0.736680            0.175157\n",
       "30335  [-0.0073982505, 0.00089106406, 0.0060087573, -0.0026300459, -0.028010733, -0.059386022, -0.07765661, -0.06947853, -0...        background  [1, 0, 0, 0, 0]    1  ...       0.032426          0.030839        0.338930           -0.276007\n",
       "30336  [-0.19841202, -0.19705483, -0.20790972, -0.22519125, -0.2374166, -0.23624307, -0.2194226, -0.18771836, -0.14198783, ...        background  [1, 0, 0, 0, 0]    1  ...       0.053901          0.111005       -0.161630           -1.176320\n",
       "30337  [0.028792929, -0.011599504, -0.03312391, -0.02478233, 0.00010168506, 0.017408311, 0.012290241, -0.011743451, -0.0403...        background  [1, 0, 0, 0, 0]    1  ...       0.035027          0.117277        0.230642           -0.647329\n",
       "\n",
       "[3010 rows x 379 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_from_pkl_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1a0a434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Audio</th>\n",
       "      <th>Class_categorical</th>\n",
       "      <th>Class_OHEV</th>\n",
       "      <th>Fold</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.0034710653, -0.0050192624, -0.004654482, -0.0049833283, -0.0038681468, -0.0023575649, -0.00025486574, 0.00135406...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.046785</td>\n",
       "      <td>0.461191</td>\n",
       "      <td>-0.338882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.015428771, -0.0064468235, -0.002025701, -0.009768408, -0.020482529, -0.03246226, -0.046539657, -0.050950672, -0....</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026433</td>\n",
       "      <td>-0.030688</td>\n",
       "      <td>0.414979</td>\n",
       "      <td>-0.840459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.17165461, -0.1961453, -0.2095497, -0.116395764, 0.02499168, 0.15181583, 0.2456393, 0.24304995, 0.1697193, 0.0706...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023640</td>\n",
       "      <td>-0.045384</td>\n",
       "      <td>0.629947</td>\n",
       "      <td>0.096665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.004147315, -0.014180049, -0.016550057, -0.017083425, -0.010863152, -0.0018686495, 0.006234308, 0.00725661, 0.005...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>-0.025407</td>\n",
       "      <td>0.554764</td>\n",
       "      <td>-0.364067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.047271818, -0.08598116, -0.08329079, -0.15244874, -0.21367016, -0.26584676, -0.19677721, -0.17195633, -0.0841544...</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>-0.036388</td>\n",
       "      <td>0.770805</td>\n",
       "      <td>0.232676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30501</th>\n",
       "      <td>[-0.024376377, -0.010054192, 0.0009410139, 0.005863713, -0.0005189497, -0.0021953415, 0.0013682423, -0.0043018945, -...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>-0.069738</td>\n",
       "      <td>0.175013</td>\n",
       "      <td>-0.974133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30502</th>\n",
       "      <td>[0.0066777063, 0.008175363, 0.006157635, 0.0026387093, 0.00059039844, 0.0030925209, 0.0033645788, 0.0056151543, 0.00...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>-0.018432</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.099409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30503</th>\n",
       "      <td>[-0.0020232266, 0.001032982, 0.0023606261, 0.0017964527, 0.0009908059, -0.0011728329, -0.003362489, -0.0054453667, -...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.028383</td>\n",
       "      <td>-1.071243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30504</th>\n",
       "      <td>[-0.0015619812, 0.0005301243, 0.0032789772, 0.002725502, 0.0023546133, -0.0019822496, -0.002666029, -0.004454969, -0...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027106</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>-0.121982</td>\n",
       "      <td>-0.606880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30505</th>\n",
       "      <td>[9.745802e-05, -0.003326854, -0.007318441, -0.0072634676, -0.0057516163, -0.0039380156, -0.0028245365, -0.000594039,...</td>\n",
       "      <td>car_horn</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.054895</td>\n",
       "      <td>-0.195825</td>\n",
       "      <td>-1.209213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27496 rows × 379 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         Audio Class_categorical       Class_OHEV Fold  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "0      [-0.0034710653, -0.0050192624, -0.004654482, -0.0049833283, -0.0038681468, -0.0023575649, -0.00025486574, 0.00135406...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.025644         -0.046785        0.461191           -0.338882\n",
       "1      [-0.015428771, -0.0064468235, -0.002025701, -0.009768408, -0.020482529, -0.03246226, -0.046539657, -0.050950672, -0....          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.026433         -0.030688        0.414979           -0.840459\n",
       "2      [-0.17165461, -0.1961453, -0.2095497, -0.116395764, 0.02499168, 0.15181583, 0.2456393, 0.24304995, 0.1697193, 0.0706...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.023640         -0.045384        0.629947            0.096665\n",
       "3      [-0.004147315, -0.014180049, -0.016550057, -0.017083425, -0.010863152, -0.0018686495, 0.006234308, 0.00725661, 0.005...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.024617         -0.025407        0.554764           -0.364067\n",
       "4      [-0.047271818, -0.08598116, -0.08329079, -0.15244874, -0.21367016, -0.26584676, -0.19677721, -0.17195633, -0.0841544...          dog_bark  [0, 0, 0, 1, 0]    5  ...       0.024934         -0.036388        0.770805            0.232676\n",
       "...                                                                                                                        ...               ...              ...  ...  ...            ...               ...             ...                 ...\n",
       "30501  [-0.024376377, -0.010054192, 0.0009410139, 0.005863713, -0.0005189497, -0.0021953415, 0.0013682423, -0.0043018945, -...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.033488         -0.069738        0.175013           -0.974133\n",
       "30502  [0.0066777063, 0.008175363, 0.006157635, 0.0026387093, 0.00059039844, 0.0030925209, 0.0033645788, 0.0056151543, 0.00...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.021007         -0.018432       -0.690702           -0.099409\n",
       "30503  [-0.0020232266, 0.001032982, 0.0023606261, 0.0017964527, 0.0009908059, -0.0011728329, -0.003362489, -0.0054453667, -...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.021124         -0.016391       -0.028383           -1.071243\n",
       "30504  [-0.0015619812, 0.0005301243, 0.0032789772, 0.002725502, 0.0023546133, -0.0019822496, -0.002666029, -0.004454969, -0...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.027106         -0.017513       -0.121982           -0.606880\n",
       "30505  [9.745802e-05, -0.003326854, -0.007318441, -0.0072634676, -0.0057516163, -0.0039380156, -0.0028245365, -0.000594039,...          car_horn  [0, 1, 0, 0, 0]    7  ...       0.038524         -0.054895       -0.195825           -1.209213\n",
       "\n",
       "[27496 rows x 379 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_from_pkl_TRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e0c05bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio\n",
      "Class_categorical\n",
      "Class_OHEV\n",
      "Fold\n",
      "RMSE\n",
      "ZCR\n",
      "CENTROIDS\n",
      "BANDWIDTH\n",
      "ROLLOFF\n",
      "MEL_1\n",
      "MEL_2\n",
      "MEL_3\n",
      "MEL_4\n",
      "MEL_5\n",
      "MEL_6\n",
      "MEL_7\n",
      "MEL_8\n",
      "MEL_9\n",
      "MEL_10\n",
      "MEL_11\n",
      "MEL_12\n",
      "MEL_13\n",
      "MEL_14\n",
      "MEL_15\n",
      "MEL_16\n",
      "MEL_17\n",
      "MEL_18\n",
      "MEL_19\n",
      "MEL_20\n",
      "MEL_21\n",
      "MEL_22\n",
      "MEL_23\n",
      "MEL_24\n",
      "MEL_25\n",
      "MEL_26\n",
      "MEL_27\n",
      "MEL_28\n",
      "MEL_29\n",
      "MEL_30\n",
      "MEL_31\n",
      "MEL_32\n",
      "MEL_33\n",
      "MEL_34\n",
      "MEL_35\n",
      "MEL_36\n",
      "MEL_37\n",
      "MEL_38\n",
      "MEL_39\n",
      "MEL_40\n",
      "MEL_41\n",
      "MEL_42\n",
      "MEL_43\n",
      "MEL_44\n",
      "MEL_45\n",
      "MEL_46\n",
      "MEL_47\n",
      "MEL_48\n",
      "MEL_49\n",
      "MEL_50\n",
      "MEL_51\n",
      "MEL_52\n",
      "MEL_53\n",
      "MEL_54\n",
      "MEL_55\n",
      "MEL_56\n",
      "MEL_57\n",
      "MEL_58\n",
      "MEL_59\n",
      "MEL_60\n",
      "MEL_61\n",
      "MEL_62\n",
      "MEL_63\n",
      "MEL_64\n",
      "MEL_65\n",
      "MEL_66\n",
      "MEL_67\n",
      "MEL_68\n",
      "MEL_69\n",
      "MEL_70\n",
      "MEL_71\n",
      "MEL_72\n",
      "MEL_73\n",
      "MEL_74\n",
      "MEL_75\n",
      "MEL_76\n",
      "MEL_77\n",
      "MEL_78\n",
      "MEL_79\n",
      "MEL_80\n",
      "MEL_81\n",
      "MEL_82\n",
      "MEL_83\n",
      "MEL_84\n",
      "MEL_85\n",
      "MEL_86\n",
      "MEL_87\n",
      "MEL_88\n",
      "MEL_89\n",
      "MEL_90\n",
      "MEL_91\n",
      "MEL_92\n",
      "MEL_93\n",
      "MEL_94\n",
      "MEL_95\n",
      "MEL_96\n",
      "MEL_97\n",
      "MEL_98\n",
      "MEL_99\n",
      "MEL_100\n",
      "MEL_101\n",
      "MEL_102\n",
      "MEL_103\n",
      "MEL_104\n",
      "MEL_105\n",
      "MEL_106\n",
      "MEL_107\n",
      "MEL_108\n",
      "MEL_109\n",
      "MEL_110\n",
      "MEL_111\n",
      "MEL_112\n",
      "MEL_113\n",
      "MEL_114\n",
      "MEL_115\n",
      "MEL_116\n",
      "MEL_117\n",
      "MEL_118\n",
      "MEL_119\n",
      "MEL_120\n",
      "MEL_121\n",
      "MEL_122\n",
      "MEL_123\n",
      "MEL_124\n",
      "MEL_125\n",
      "MEL_126\n",
      "MEL_127\n",
      "MEL_128\n",
      "MFCC_1\n",
      "MFCC_std_1\n",
      "MFCC_median_1\n",
      "MFCC_skew_1\n",
      "MFCC_kurtosis_1\n",
      "MFCC_delta1_mean_1\n",
      "MFCC_delta1_std_1\n",
      "MFCC_delta2_mean_1\n",
      "MFCC_delta2_std_1\n",
      "MFCC_2\n",
      "MFCC_std_2\n",
      "MFCC_median_2\n",
      "MFCC_skew_2\n",
      "MFCC_kurtosis_2\n",
      "MFCC_delta1_mean_2\n",
      "MFCC_delta1_std_2\n",
      "MFCC_delta2_mean_2\n",
      "MFCC_delta2_std_2\n",
      "MFCC_3\n",
      "MFCC_std_3\n",
      "MFCC_median_3\n",
      "MFCC_skew_3\n",
      "MFCC_kurtosis_3\n",
      "MFCC_delta1_mean_3\n",
      "MFCC_delta1_std_3\n",
      "MFCC_delta2_mean_3\n",
      "MFCC_delta2_std_3\n",
      "MFCC_4\n",
      "MFCC_std_4\n",
      "MFCC_median_4\n",
      "MFCC_skew_4\n",
      "MFCC_kurtosis_4\n",
      "MFCC_delta1_mean_4\n",
      "MFCC_delta1_std_4\n",
      "MFCC_delta2_mean_4\n",
      "MFCC_delta2_std_4\n",
      "MFCC_5\n",
      "MFCC_std_5\n",
      "MFCC_median_5\n",
      "MFCC_skew_5\n",
      "MFCC_kurtosis_5\n",
      "MFCC_delta1_mean_5\n",
      "MFCC_delta1_std_5\n",
      "MFCC_delta2_mean_5\n",
      "MFCC_delta2_std_5\n",
      "MFCC_6\n",
      "MFCC_std_6\n",
      "MFCC_median_6\n",
      "MFCC_skew_6\n",
      "MFCC_kurtosis_6\n",
      "MFCC_delta1_mean_6\n",
      "MFCC_delta1_std_6\n",
      "MFCC_delta2_mean_6\n",
      "MFCC_delta2_std_6\n",
      "MFCC_7\n",
      "MFCC_std_7\n",
      "MFCC_median_7\n",
      "MFCC_skew_7\n",
      "MFCC_kurtosis_7\n",
      "MFCC_delta1_mean_7\n",
      "MFCC_delta1_std_7\n",
      "MFCC_delta2_mean_7\n",
      "MFCC_delta2_std_7\n",
      "MFCC_8\n",
      "MFCC_std_8\n",
      "MFCC_median_8\n",
      "MFCC_skew_8\n",
      "MFCC_kurtosis_8\n",
      "MFCC_delta1_mean_8\n",
      "MFCC_delta1_std_8\n",
      "MFCC_delta2_mean_8\n",
      "MFCC_delta2_std_8\n",
      "MFCC_9\n",
      "MFCC_std_9\n",
      "MFCC_median_9\n",
      "MFCC_skew_9\n",
      "MFCC_kurtosis_9\n",
      "MFCC_delta1_mean_9\n",
      "MFCC_delta1_std_9\n",
      "MFCC_delta2_mean_9\n",
      "MFCC_delta2_std_9\n",
      "MFCC_10\n",
      "MFCC_std_10\n",
      "MFCC_median_10\n",
      "MFCC_skew_10\n",
      "MFCC_kurtosis_10\n",
      "MFCC_delta1_mean_10\n",
      "MFCC_delta1_std_10\n",
      "MFCC_delta2_mean_10\n",
      "MFCC_delta2_std_10\n",
      "MFCC_11\n",
      "MFCC_std_11\n",
      "MFCC_median_11\n",
      "MFCC_skew_11\n",
      "MFCC_kurtosis_11\n",
      "MFCC_delta1_mean_11\n",
      "MFCC_delta1_std_11\n",
      "MFCC_delta2_mean_11\n",
      "MFCC_delta2_std_11\n",
      "MFCC_12\n",
      "MFCC_std_12\n",
      "MFCC_median_12\n",
      "MFCC_skew_12\n",
      "MFCC_kurtosis_12\n",
      "MFCC_delta1_mean_12\n",
      "MFCC_delta1_std_12\n",
      "MFCC_delta2_mean_12\n",
      "MFCC_delta2_std_12\n",
      "MFCC_13\n",
      "MFCC_std_13\n",
      "MFCC_median_13\n",
      "MFCC_skew_13\n",
      "MFCC_kurtosis_13\n",
      "MFCC_delta1_mean_13\n",
      "MFCC_delta1_std_13\n",
      "MFCC_delta2_mean_13\n",
      "MFCC_delta2_std_13\n",
      "CONSTRAST_1\n",
      "CONSTRAST_std_1\n",
      "CONSTRAST_median_1\n",
      "CONSTRAST_skew_1\n",
      "CONSTRAST_kurtosis_1\n",
      "CONSTRAST_2\n",
      "CONSTRAST_std_2\n",
      "CONSTRAST_median_2\n",
      "CONSTRAST_skew_2\n",
      "CONSTRAST_kurtosis_2\n",
      "CONSTRAST_3\n",
      "CONSTRAST_std_3\n",
      "CONSTRAST_median_3\n",
      "CONSTRAST_skew_3\n",
      "CONSTRAST_kurtosis_3\n",
      "CONSTRAST_4\n",
      "CONSTRAST_std_4\n",
      "CONSTRAST_median_4\n",
      "CONSTRAST_skew_4\n",
      "CONSTRAST_kurtosis_4\n",
      "CONSTRAST_5\n",
      "CONSTRAST_std_5\n",
      "CONSTRAST_median_5\n",
      "CONSTRAST_skew_5\n",
      "CONSTRAST_kurtosis_5\n",
      "CONSTRAST_6\n",
      "CONSTRAST_std_6\n",
      "CONSTRAST_median_6\n",
      "CONSTRAST_skew_6\n",
      "CONSTRAST_kurtosis_6\n",
      "CONSTRAST_7\n",
      "CONSTRAST_std_7\n",
      "CONSTRAST_median_7\n",
      "CONSTRAST_skew_7\n",
      "CONSTRAST_kurtosis_7\n",
      "CHROMA_1\n",
      "CHROMA_std_1\n",
      "CHROMA_median_1\n",
      "CHROMA_skew_1\n",
      "CHROMA_kurtosis_1\n",
      "CHROMA_2\n",
      "CHROMA_std_2\n",
      "CHROMA_median_2\n",
      "CHROMA_skew_2\n",
      "CHROMA_kurtosis_2\n",
      "CHROMA_3\n",
      "CHROMA_std_3\n",
      "CHROMA_median_3\n",
      "CHROMA_skew_3\n",
      "CHROMA_kurtosis_3\n",
      "CHROMA_4\n",
      "CHROMA_std_4\n",
      "CHROMA_median_4\n",
      "CHROMA_skew_4\n",
      "CHROMA_kurtosis_4\n",
      "CHROMA_5\n",
      "CHROMA_std_5\n",
      "CHROMA_median_5\n",
      "CHROMA_skew_5\n",
      "CHROMA_kurtosis_5\n",
      "CHROMA_6\n",
      "CHROMA_std_6\n",
      "CHROMA_median_6\n",
      "CHROMA_skew_6\n",
      "CHROMA_kurtosis_6\n",
      "CHROMA_7\n",
      "CHROMA_std_7\n",
      "CHROMA_median_7\n",
      "CHROMA_skew_7\n",
      "CHROMA_kurtosis_7\n",
      "CHROMA_8\n",
      "CHROMA_std_8\n",
      "CHROMA_median_8\n",
      "CHROMA_skew_8\n",
      "CHROMA_kurtosis_8\n",
      "CHROMA_9\n",
      "CHROMA_std_9\n",
      "CHROMA_median_9\n",
      "CHROMA_skew_9\n",
      "CHROMA_kurtosis_9\n",
      "CHROMA_10\n",
      "CHROMA_std_10\n",
      "CHROMA_median_10\n",
      "CHROMA_skew_10\n",
      "CHROMA_kurtosis_10\n",
      "CHROMA_11\n",
      "CHROMA_std_11\n",
      "CHROMA_median_11\n",
      "CHROMA_skew_11\n",
      "CHROMA_kurtosis_11\n",
      "CHROMA_12\n",
      "CHROMA_std_12\n",
      "CHROMA_median_12\n",
      "CHROMA_skew_12\n",
      "CHROMA_kurtosis_12\n",
      "TONNETZ_1\n",
      "TONNETZ_std_1\n",
      "TONNETZ_median_1\n",
      "TONNETZ_skew_1\n",
      "TONNETZ_kurtosis_1\n",
      "TONNETZ_2\n",
      "TONNETZ_std_2\n",
      "TONNETZ_median_2\n",
      "TONNETZ_skew_2\n",
      "TONNETZ_kurtosis_2\n",
      "TONNETZ_3\n",
      "TONNETZ_std_3\n",
      "TONNETZ_median_3\n",
      "TONNETZ_skew_3\n",
      "TONNETZ_kurtosis_3\n",
      "TONNETZ_4\n",
      "TONNETZ_std_4\n",
      "TONNETZ_median_4\n",
      "TONNETZ_skew_4\n",
      "TONNETZ_kurtosis_4\n",
      "TONNETZ_5\n",
      "TONNETZ_std_5\n",
      "TONNETZ_median_5\n",
      "TONNETZ_skew_5\n",
      "TONNETZ_kurtosis_5\n",
      "TONNETZ_6\n",
      "TONNETZ_std_6\n",
      "TONNETZ_median_6\n",
      "TONNETZ_skew_6\n",
      "TONNETZ_kurtosis_6\n"
     ]
    }
   ],
   "source": [
    "for i in DB_from_pkl_TRN.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c4b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating data and labels\n",
    "\n",
    "X      = DB_from_pkl_TRN.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "y      = np.array(DB_from_pkl_TRN.Class_categorical.to_list())\n",
    "y_OHEV = np.array(DB_from_pkl_TRN.Class_OHEV.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1e3f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the augmented dataset (only validation set)\n",
    "\n",
    "X_val      = DB_from_pkl_VAL.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "y_val      = np.array(DB_from_pkl_VAL.Class_categorical.to_list())\n",
    "y_OHEV_val = np.array(DB_from_pkl_VAL.Class_OHEV.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15e67353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.117183</td>\n",
       "      <td>0.129483</td>\n",
       "      <td>2069.471399</td>\n",
       "      <td>1734.789901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.046785</td>\n",
       "      <td>0.461191</td>\n",
       "      <td>-0.338882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111724</td>\n",
       "      <td>0.131570</td>\n",
       "      <td>2098.418718</td>\n",
       "      <td>1743.608984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026433</td>\n",
       "      <td>-0.030688</td>\n",
       "      <td>0.414979</td>\n",
       "      <td>-0.840459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.124646</td>\n",
       "      <td>0.131459</td>\n",
       "      <td>2096.616414</td>\n",
       "      <td>1747.412264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023640</td>\n",
       "      <td>-0.045384</td>\n",
       "      <td>0.629947</td>\n",
       "      <td>0.096665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110741</td>\n",
       "      <td>0.131658</td>\n",
       "      <td>2093.378630</td>\n",
       "      <td>1736.615179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>-0.025407</td>\n",
       "      <td>0.554764</td>\n",
       "      <td>-0.364067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125728</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>2105.951159</td>\n",
       "      <td>1750.491381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>-0.036388</td>\n",
       "      <td>0.770805</td>\n",
       "      <td>0.232676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30501</th>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.165439</td>\n",
       "      <td>2435.099768</td>\n",
       "      <td>2199.072455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>-0.069738</td>\n",
       "      <td>0.175013</td>\n",
       "      <td>-0.974133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30502</th>\n",
       "      <td>0.007954</td>\n",
       "      <td>0.148016</td>\n",
       "      <td>2363.629594</td>\n",
       "      <td>2317.859881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>-0.018432</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.099409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30503</th>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.154519</td>\n",
       "      <td>2389.715351</td>\n",
       "      <td>2316.671469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.028383</td>\n",
       "      <td>-1.071243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30504</th>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>2467.091641</td>\n",
       "      <td>2211.234749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027106</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>-0.121982</td>\n",
       "      <td>-0.606880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30505</th>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.188943</td>\n",
       "      <td>2540.724327</td>\n",
       "      <td>2134.914700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>-0.054895</td>\n",
       "      <td>-0.195825</td>\n",
       "      <td>-1.209213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27496 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           RMSE       ZCR    CENTROIDS    BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "0      0.117183  0.129483  2069.471399  1734.789901  ...       0.025644         -0.046785        0.461191           -0.338882\n",
       "1      0.111724  0.131570  2098.418718  1743.608984  ...       0.026433         -0.030688        0.414979           -0.840459\n",
       "2      0.124646  0.131459  2096.616414  1747.412264  ...       0.023640         -0.045384        0.629947            0.096665\n",
       "3      0.110741  0.131658  2093.378630  1736.615179  ...       0.024617         -0.025407        0.554764           -0.364067\n",
       "4      0.125728  0.131836  2105.951159  1750.491381  ...       0.024934         -0.036388        0.770805            0.232676\n",
       "...         ...       ...          ...          ...  ...            ...               ...             ...                 ...\n",
       "30501  0.012771  0.165439  2435.099768  2199.072455  ...       0.033488         -0.069738        0.175013           -0.974133\n",
       "30502  0.007954  0.148016  2363.629594  2317.859881  ...       0.021007         -0.018432       -0.690702           -0.099409\n",
       "30503  0.008383  0.154519  2389.715351  2316.671469  ...       0.021124         -0.016391       -0.028383           -1.071243\n",
       "30504  0.010459  0.173873  2467.091641  2211.234749  ...       0.027106         -0.017513       -0.121982           -0.606880\n",
       "30505  0.013639  0.188943  2540.724327  2134.914700  ...       0.038524         -0.054895       -0.195825           -1.209213\n",
       "\n",
       "[27496 rows x 375 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2d0edb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.061757</td>\n",
       "      <td>0.111211</td>\n",
       "      <td>1946.880604</td>\n",
       "      <td>1941.677599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028865</td>\n",
       "      <td>-0.001712</td>\n",
       "      <td>0.117679</td>\n",
       "      <td>-0.267797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.065669</td>\n",
       "      <td>0.075426</td>\n",
       "      <td>851.483462</td>\n",
       "      <td>537.300056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022403</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.575863</td>\n",
       "      <td>1.089295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>316.436258</td>\n",
       "      <td>478.380671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>-0.208415</td>\n",
       "      <td>-4.493470</td>\n",
       "      <td>-1.900563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1369.743048</td>\n",
       "      <td>1545.724198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016744</td>\n",
       "      <td>-0.018674</td>\n",
       "      <td>-0.229967</td>\n",
       "      <td>-0.896474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.039756</td>\n",
       "      <td>0.094150</td>\n",
       "      <td>1792.585279</td>\n",
       "      <td>1896.864013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022840</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>0.106490</td>\n",
       "      <td>-0.506396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.082771</td>\n",
       "      <td>0.130216</td>\n",
       "      <td>2276.412570</td>\n",
       "      <td>2283.346224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033357</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.450431</td>\n",
       "      <td>0.035052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.701893</td>\n",
       "      <td>0.640292</td>\n",
       "      <td>6674.189171</td>\n",
       "      <td>4340.169312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257286</td>\n",
       "      <td>0.314713</td>\n",
       "      <td>3.473303</td>\n",
       "      <td>21.586886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               RMSE           ZCR     CENTROIDS     BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "count  27496.000000  27496.000000  27496.000000  27496.000000  ...   27496.000000      27496.000000    27496.000000        27496.000000\n",
       "mean       0.061757      0.111211   1946.880604   1941.677599  ...       0.028865         -0.001712        0.117679           -0.267797\n",
       "std        0.065669      0.075426    851.483462    537.300056  ...       0.022403          0.035297        0.575863            1.089295\n",
       "min        0.000145      0.000000    316.436258    478.380671  ...       0.003497         -0.208415       -4.493470           -1.900563\n",
       "25%        0.018061      0.064742   1369.743048   1545.724198  ...       0.016744         -0.018674       -0.229967           -0.896474\n",
       "50%        0.039756      0.094150   1792.585279   1896.864013  ...       0.022840         -0.001946        0.106490           -0.506396\n",
       "75%        0.082771      0.130216   2276.412570   2283.346224  ...       0.033357          0.015029        0.450431            0.035052\n",
       "max        0.701893      0.640292   6674.189171   4340.169312  ...       0.257286          0.314713        3.473303           21.586886\n",
       "\n",
       "[8 rows x 375 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fd62f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dog_bark', 'dog_bark', 'dog_bark', ..., 'car_horn', 'car_horn',\n",
       "       'car_horn'], dtype='<U16')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e485221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_OHEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8fa7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_statistics = pd.DataFrame({'mean': X.mean(), 'std': X.std(), 'min': X.min(), 'max': X.max()})\n",
    "\n",
    "X_mean = X_statistics.values[:, 0]\n",
    "X_std  = X_statistics.values[:, 1]\n",
    "X_min  = X_statistics.values[:, 2]\n",
    "X_max  = X_statistics.values[:, 3]\n",
    "\n",
    "X_norm   =  (X.values - X_min) / (X_max - X_min)\n",
    "\n",
    "X_normDB = X.apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ec649f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08779795319423672\n"
     ]
    }
   ],
   "source": [
    "RMSE_lst = []\n",
    "for i in X_norm:\n",
    "    RMSE_lst.append([i][0][0])\n",
    "    \n",
    "print(np.mean(RMSE_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3d3be1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>27496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.087798</td>\n",
       "      <td>0.173688</td>\n",
       "      <td>0.256450</td>\n",
       "      <td>0.378917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099956</td>\n",
       "      <td>0.395129</td>\n",
       "      <td>0.578798</td>\n",
       "      <td>0.069517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.093580</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.133928</td>\n",
       "      <td>0.139132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088276</td>\n",
       "      <td>0.067473</td>\n",
       "      <td>0.072283</td>\n",
       "      <td>0.046378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.025530</td>\n",
       "      <td>0.101113</td>\n",
       "      <td>0.165673</td>\n",
       "      <td>0.276386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052194</td>\n",
       "      <td>0.362705</td>\n",
       "      <td>0.535161</td>\n",
       "      <td>0.042750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.056446</td>\n",
       "      <td>0.147041</td>\n",
       "      <td>0.232181</td>\n",
       "      <td>0.367313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076216</td>\n",
       "      <td>0.394680</td>\n",
       "      <td>0.577393</td>\n",
       "      <td>0.059358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.117743</td>\n",
       "      <td>0.203369</td>\n",
       "      <td>0.308281</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117657</td>\n",
       "      <td>0.427130</td>\n",
       "      <td>0.620565</td>\n",
       "      <td>0.082411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               RMSE           ZCR     CENTROIDS     BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "count  27496.000000  27496.000000  27496.000000  27496.000000  ...   27496.000000      27496.000000    27496.000000        27496.000000\n",
       "mean       0.087798      0.173688      0.256450      0.378917  ...       0.099956          0.395129        0.578798            0.069517\n",
       "std        0.093580      0.117800      0.133928      0.139132  ...       0.088276          0.067473        0.072283            0.046378\n",
       "min        0.000000      0.000000      0.000000      0.000000  ...       0.000000          0.000000        0.000000            0.000000\n",
       "25%        0.025530      0.101113      0.165673      0.276386  ...       0.052194          0.362705        0.535161            0.042750\n",
       "50%        0.056446      0.147041      0.232181      0.367313  ...       0.076216          0.394680        0.577393            0.059358\n",
       "75%        0.117743      0.203369      0.308281      0.467391  ...       0.117657          0.427130        0.620565            0.082411\n",
       "max        1.000000      1.000000      1.000000      1.000000  ...       1.000000          1.000000        1.000000            1.000000\n",
       "\n",
       "[8 rows x 375 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normDB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a069edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standard = (X.values - X_mean) / X_std\n",
    "\n",
    "X_standardDB = X.apply(lambda x: (x - x.mean()) / x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b42f5a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.269336465057902e-18\n",
      "0.9999818153714614\n"
     ]
    }
   ],
   "source": [
    "RMSE_lst    = []\n",
    "for i in X_standard:\n",
    "    RMSE_lst.append([i][0][0])\n",
    "    \n",
    "print(np.mean(RMSE_lst))\n",
    "print(np.std(RMSE_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "102f1c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>2.749600e+04</td>\n",
       "      <td>27496.000000</td>\n",
       "      <td>2.749600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-8.269336e-18</td>\n",
       "      <td>-5.788536e-17</td>\n",
       "      <td>1.488481e-16</td>\n",
       "      <td>-5.126989e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>5.168335e-17</td>\n",
       "      <td>-2.144859e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.033667e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.382175e-01</td>\n",
       "      <td>-1.474433e+00</td>\n",
       "      <td>-1.914828e+00</td>\n",
       "      <td>-2.723426e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.132315e+00</td>\n",
       "      <td>-5.856130e+00</td>\n",
       "      <td>-8.007378</td>\n",
       "      <td>-1.498921e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.654041e-01</td>\n",
       "      <td>-6.160890e-01</td>\n",
       "      <td>-6.778024e-01</td>\n",
       "      <td>-7.369316e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.410548e-01</td>\n",
       "      <td>-4.805513e-01</td>\n",
       "      <td>-0.603696</td>\n",
       "      <td>-5.771418e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.350271e-01</td>\n",
       "      <td>-2.262003e-01</td>\n",
       "      <td>-1.812077e-01</td>\n",
       "      <td>-8.340514e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.689247e-01</td>\n",
       "      <td>-6.645925e-03</td>\n",
       "      <td>-0.019429</td>\n",
       "      <td>-2.190398e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.199957e-01</td>\n",
       "      <td>2.519650e-01</td>\n",
       "      <td>3.870092e-01</td>\n",
       "      <td>6.358991e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.005251e-01</td>\n",
       "      <td>4.742897e-01</td>\n",
       "      <td>0.577832</td>\n",
       "      <td>2.780233e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.747881e+00</td>\n",
       "      <td>7.014547e+00</td>\n",
       "      <td>5.551850e+00</td>\n",
       "      <td>4.463971e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019584e+01</td>\n",
       "      <td>8.964689e+00</td>\n",
       "      <td>5.827128</td>\n",
       "      <td>2.006315e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               RMSE           ZCR     CENTROIDS     BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "count  2.749600e+04  2.749600e+04  2.749600e+04  2.749600e+04  ...   2.749600e+04      2.749600e+04    27496.000000        2.749600e+04\n",
       "mean  -8.269336e-18 -5.788536e-17  1.488481e-16 -5.126989e-16  ...   5.168335e-17     -2.144859e-17        0.000000       -1.033667e-17\n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  ...   1.000000e+00      1.000000e+00        1.000000        1.000000e+00\n",
       "min   -9.382175e-01 -1.474433e+00 -1.914828e+00 -2.723426e+00  ...  -1.132315e+00     -5.856130e+00       -8.007378       -1.498921e+00\n",
       "25%   -6.654041e-01 -6.160890e-01 -6.778024e-01 -7.369316e-01  ...  -5.410548e-01     -4.805513e-01       -0.603696       -5.771418e-01\n",
       "50%   -3.350271e-01 -2.262003e-01 -1.812077e-01 -8.340514e-02  ...  -2.689247e-01     -6.645925e-03       -0.019429       -2.190398e-01\n",
       "75%    3.199957e-01  2.519650e-01  3.870092e-01  6.358991e-01  ...   2.005251e-01      4.742897e-01        0.577832        2.780233e-01\n",
       "max    9.747881e+00  7.014547e+00  5.551850e+00  4.463971e+00  ...   1.019584e+01      8.964689e+00        5.827128        2.006315e+01\n",
       "\n",
       "[8 rows x 375 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_standardDB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae16f79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27496, 375)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81aa62e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27496, 375)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "832d29f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27496, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_OHEV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a638bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27496"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9017e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27496"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee2d7aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.031516</td>\n",
       "      <td>1152.318462</td>\n",
       "      <td>1898.286145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.230194</td>\n",
       "      <td>-0.774566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.035680</td>\n",
       "      <td>0.036821</td>\n",
       "      <td>1237.588550</td>\n",
       "      <td>1949.387843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>0.529034</td>\n",
       "      <td>0.970660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.036087</td>\n",
       "      <td>0.045854</td>\n",
       "      <td>1422.395537</td>\n",
       "      <td>2117.020765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030452</td>\n",
       "      <td>-0.029692</td>\n",
       "      <td>0.291676</td>\n",
       "      <td>-0.927084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.040949</td>\n",
       "      <td>1877.232022</td>\n",
       "      <td>2785.436746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>-0.005922</td>\n",
       "      <td>-0.290679</td>\n",
       "      <td>-1.082696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.076167</td>\n",
       "      <td>0.042880</td>\n",
       "      <td>1592.015997</td>\n",
       "      <td>2199.860601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030058</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>1.003930</td>\n",
       "      <td>1.476461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30333</th>\n",
       "      <td>0.197044</td>\n",
       "      <td>0.070557</td>\n",
       "      <td>1087.442478</td>\n",
       "      <td>886.308473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>0.014569</td>\n",
       "      <td>0.348922</td>\n",
       "      <td>-0.837185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30334</th>\n",
       "      <td>0.197267</td>\n",
       "      <td>0.073486</td>\n",
       "      <td>1114.525841</td>\n",
       "      <td>940.949276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>0.736680</td>\n",
       "      <td>0.175157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30335</th>\n",
       "      <td>0.196901</td>\n",
       "      <td>0.072865</td>\n",
       "      <td>1059.255494</td>\n",
       "      <td>857.858019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032426</td>\n",
       "      <td>0.030839</td>\n",
       "      <td>0.338930</td>\n",
       "      <td>-0.276007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30336</th>\n",
       "      <td>0.192756</td>\n",
       "      <td>0.066939</td>\n",
       "      <td>1058.530590</td>\n",
       "      <td>902.456226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053901</td>\n",
       "      <td>0.111005</td>\n",
       "      <td>-0.161630</td>\n",
       "      <td>-1.176320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30337</th>\n",
       "      <td>0.169005</td>\n",
       "      <td>0.068271</td>\n",
       "      <td>1053.554950</td>\n",
       "      <td>875.421447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035027</td>\n",
       "      <td>0.117277</td>\n",
       "      <td>0.230642</td>\n",
       "      <td>-0.647329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3010 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           RMSE       ZCR    CENTROIDS    BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "231    0.020142  0.031516  1152.318462  1898.286145  ...       0.019867          0.005833        0.230194           -0.774566\n",
       "232    0.035680  0.036821  1237.588550  1949.387843  ...       0.018672          0.009202        0.529034            0.970660\n",
       "233    0.036087  0.045854  1422.395537  2117.020765  ...       0.030452         -0.029692        0.291676           -0.927084\n",
       "234    0.000778  0.040949  1877.232022  2785.436746  ...       0.031560         -0.005922       -0.290679           -1.082696\n",
       "235    0.076167  0.042880  1592.015997  2199.860601  ...       0.030058          0.002691        1.003930            1.476461\n",
       "...         ...       ...          ...          ...  ...            ...               ...             ...                 ...\n",
       "30333  0.197044  0.070557  1087.442478   886.308473  ...       0.029579          0.014569        0.348922           -0.837185\n",
       "30334  0.197267  0.073486  1114.525841   940.949276  ...       0.023069          0.014551        0.736680            0.175157\n",
       "30335  0.196901  0.072865  1059.255494   857.858019  ...       0.032426          0.030839        0.338930           -0.276007\n",
       "30336  0.192756  0.066939  1058.530590   902.456226  ...       0.053901          0.111005       -0.161630           -1.176320\n",
       "30337  0.169005  0.068271  1053.554950   875.421447  ...       0.035027          0.117277        0.230642           -0.647329\n",
       "\n",
       "[3010 rows x 375 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e31ec6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ZCR</th>\n",
       "      <th>CENTROIDS</th>\n",
       "      <th>BANDWIDTH</th>\n",
       "      <th>...</th>\n",
       "      <th>TONNETZ_std_6</th>\n",
       "      <th>TONNETZ_median_6</th>\n",
       "      <th>TONNETZ_skew_6</th>\n",
       "      <th>TONNETZ_kurtosis_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "      <td>3010.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.065873</td>\n",
       "      <td>0.113265</td>\n",
       "      <td>2027.375554</td>\n",
       "      <td>1988.820959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036852</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>0.118041</td>\n",
       "      <td>-0.305626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.066159</td>\n",
       "      <td>0.081072</td>\n",
       "      <td>972.540017</td>\n",
       "      <td>531.004250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041177</td>\n",
       "      <td>0.044567</td>\n",
       "      <td>0.595448</td>\n",
       "      <td>1.049706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>406.477247</td>\n",
       "      <td>695.196438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>-0.178653</td>\n",
       "      <td>-2.506518</td>\n",
       "      <td>-1.733951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.015080</td>\n",
       "      <td>0.060275</td>\n",
       "      <td>1318.434027</td>\n",
       "      <td>1621.312707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>-0.019668</td>\n",
       "      <td>-0.247503</td>\n",
       "      <td>-0.945905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.040219</td>\n",
       "      <td>0.101485</td>\n",
       "      <td>1894.516205</td>\n",
       "      <td>1917.492054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023036</td>\n",
       "      <td>-0.001336</td>\n",
       "      <td>0.113718</td>\n",
       "      <td>-0.550888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.137557</td>\n",
       "      <td>2361.173895</td>\n",
       "      <td>2314.618141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036021</td>\n",
       "      <td>0.016743</td>\n",
       "      <td>0.504786</td>\n",
       "      <td>0.015309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.469498</td>\n",
       "      <td>0.661577</td>\n",
       "      <td>6970.949555</td>\n",
       "      <td>3658.796731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227059</td>\n",
       "      <td>0.199019</td>\n",
       "      <td>2.530284</td>\n",
       "      <td>12.081950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              RMSE          ZCR    CENTROIDS    BANDWIDTH  ...  TONNETZ_std_6  TONNETZ_median_6  TONNETZ_skew_6  TONNETZ_kurtosis_6\n",
       "count  3010.000000  3010.000000  3010.000000  3010.000000  ...    3010.000000       3010.000000     3010.000000         3010.000000\n",
       "mean      0.065873     0.113265  2027.375554  1988.820959  ...       0.036852         -0.001223        0.118041           -0.305626\n",
       "std       0.066159     0.081072   972.540017   531.004250  ...       0.041177          0.044567        0.595448            1.049706\n",
       "min       0.000083     0.005549   406.477247   695.196438  ...       0.003733         -0.178653       -2.506518           -1.733951\n",
       "25%       0.015080     0.060275  1318.434027  1621.312707  ...       0.016706         -0.019668       -0.247503           -0.945905\n",
       "50%       0.040219     0.101485  1894.516205  1917.492054  ...       0.023036         -0.001336        0.113718           -0.550888\n",
       "75%       0.097500     0.137557  2361.173895  2314.618141  ...       0.036021          0.016743        0.504786            0.015309\n",
       "max       0.469498     0.661577  6970.949555  3658.796731  ...       0.227059          0.199019        2.530284           12.081950\n",
       "\n",
       "[8 rows x 375 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94008013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_OHEV_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55830538",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_norm   =  (X_val.values - X_min) / (X_max - X_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e67b03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_standard = (X_val.values - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cb06ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3010, 375)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ebc21b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3010, 375)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d0bb619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3010, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_OHEV_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bd2a8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3010"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_OHEV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8087fae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(0, 0, 0, 1, 0): 700,\n",
       "         (1, 0, 0, 0, 0): 756,\n",
       "         (0, 0, 1, 0, 0): 700,\n",
       "         (0, 0, 0, 0, 1): 602,\n",
       "         (0, 1, 0, 0, 0): 252})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter_val = Counter(map(tuple, y_OHEV_val))\n",
    "Counter_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92b398cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_categorical       \n",
      "background         24455    [1, 0, 0, 0, 0]\n",
      "car_horn           25513    [0, 1, 0, 0, 0]\n",
      "children_playing   15386    [0, 0, 1, 0, 0]\n",
      "dog_bark           14901    [0, 0, 0, 1, 0]\n",
      "siren              14366    [0, 0, 0, 0, 1]\n",
      "Name: Class_OHEV, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'background': array([1, 0, 0, 0, 0]),\n",
       " 'car_horn': array([0, 1, 0, 0, 0]),\n",
       " 'children_playing': array([0, 0, 1, 0, 0]),\n",
       " 'dog_bark': array([0, 0, 0, 1, 0]),\n",
       " 'siren': array([0, 0, 0, 0, 1])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by the class and get one random sample of each class\n",
    "k = DB_from_pkl.groupby('Class_categorical')['Class_OHEV'].apply(lambda s: s.sample(1))\n",
    "print(k)\n",
    "\n",
    "# Convert the pandas series into a dataframe\n",
    "temp_k_df = k.reset_index()\n",
    "\n",
    "# Delete the index from the grouppby result\n",
    "del temp_k_df['level_1']\n",
    "\n",
    "# Set the \"Class\" as the dataframe index\n",
    "temp_k_df.set_index(\"Class_categorical\", inplace=True)\n",
    "\n",
    "# Convert the dataframe to a dictionary (Class: Class_encoder)\n",
    "encoder_dict = temp_k_df[\"Class_OHEV\"].to_dict()\n",
    "encoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28674adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['background', 'car_horn', 'children_playing', 'dog_bark', 'siren']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nom_classes = list(encoder_dict.keys())\n",
    "nom_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b53876a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of classes in the dataset\n",
    "\n",
    "num_classes = len(encoder_dict.keys())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0504cb8",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee250105",
   "metadata": {},
   "outputs": [],
   "source": [
    "del DB_from_pkl_VAL, DB_from_pkl_TRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e866b039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-) Normalization\n",
      "2-) Standardization\n",
      "\n",
      "Select the dataset: 1\n"
     ]
    }
   ],
   "source": [
    "# Separate 1 fold for validation and create a DB for the training / testing\n",
    "\n",
    "opc = 0\n",
    "while str(opc) not in '12':\n",
    "    print()\n",
    "    print(\"1-) Normalization\")\n",
    "    print(\"2-) Standardization\")\n",
    "\n",
    "    opc = input(\"\\nSelect the dataset: \")\n",
    "    if opc.isdigit():\n",
    "        opc = int(opc)\n",
    "    else:\n",
    "        opc = 0\n",
    "\n",
    "\n",
    "    DB_from_pkl_VAL = DB_from_pkl[DB_from_pkl['Fold'] == fold].copy()\n",
    "    DB_from_pkl_TRN = DB_from_pkl[DB_from_pkl['Fold'] != fold].copy()\n",
    "    \n",
    "    X      = DB_from_pkl_TRN.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "    y      = np.array(DB_from_pkl_TRN.Class_categorical.to_list())\n",
    "    y_OHEV = np.array(DB_from_pkl_TRN.Class_OHEV.to_list())\n",
    "\n",
    "    X_val      = DB_from_pkl_VAL.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "    y_val      = np.array(DB_from_pkl_VAL.Class_categorical.to_list())\n",
    "    y_OHEV_val = np.array(DB_from_pkl_VAL.Class_OHEV.to_list())\n",
    "\n",
    "    X_statistics = pd.DataFrame({'mean': X.mean(), 'std': X.std(), 'min': X.min(), 'max': X.max()})\n",
    "\n",
    "    X_mean   = X_statistics.values[:, 0]\n",
    "    X_std    = X_statistics.values[:, 1]\n",
    "    X_min    = X_statistics.values[:, 2]\n",
    "    X_max    = X_statistics.values[:, 3]\n",
    "    \n",
    "    # Normalization or standardization using values from the training set.\n",
    "    if opc == 1:\n",
    "        X_norm     = (X.values - X_min) / (X_max - X_min)\n",
    "        X_val_norm = (X_val.values - X_min) / (X_max - X_min)\n",
    "        norm_type  = '_norm'\n",
    "\n",
    "    if opc == 2:\n",
    "        X_norm     = (X.values - X_mean) / X_std\n",
    "        X_val_norm = (X_val.values - X_mean) / X_std\n",
    "        norm_type  = '_std'\n",
    "\n",
    "    # Retrieve the indexes used for training the classifiers\n",
    "    idx_trn = np.genfromtxt(os.path.join(path_models, '_idx_trn_' + nom_dataset + model_surname + '.csv'), delimiter=',', dtype = int)\n",
    "    idx_tst = np.genfromtxt(os.path.join(path_models, '_idx_tst_' + nom_dataset + model_surname + '.csv'), delimiter=',', dtype = int)\n",
    "\n",
    "    X_train      = X_norm[idx_trn]\n",
    "    X_test       = X_norm[idx_tst]\n",
    "    y_train      = y[idx_trn]\n",
    "    y_test       = y[idx_tst]\n",
    "    y_train_OHEV = y_OHEV[idx_trn]\n",
    "    y_test_OHEV  = y_OHEV[idx_tst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13d793b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Training set\n",
      "\n",
      "X_train.........: (24746, 375)\n",
      "y_train.........: (24746,)\n",
      "y_train_OHEV....: (24746, 5)\n",
      "\n",
      "==================================\n",
      "Testing set\n",
      "\n",
      "X_test..........: (2750, 375)\n",
      "y_test..........: (2750,)\n",
      "y_test_OHEV.....: (2750, 5)\n",
      "\n",
      "==================================\n",
      "Validation set\n",
      "\n",
      "X_val_norm......: (3010, 375)\n",
      "y_val...........: (3010,)\n",
      "y_OHEV_val......: (3010, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==================================\")\n",
    "print(\"Training set\\n\")\n",
    "\n",
    "print(f'X_train.........: {np.shape(X_train)}')\n",
    "print(f'y_train.........: {np.shape(y_train)}')\n",
    "print(f'y_train_OHEV....: {np.shape(y_train_OHEV)}')\n",
    "\n",
    "print(\"\\n==================================\")\n",
    "print(\"Testing set\\n\")\n",
    "\n",
    "print(f'X_test..........: {np.shape(X_test)}')\n",
    "print(f'y_test..........: {np.shape(y_test)}')\n",
    "print(f'y_test_OHEV.....: {np.shape(y_test_OHEV)}')\n",
    "\n",
    "print(\"\\n==================================\")\n",
    "print(\"Validation set\\n\")\n",
    "\n",
    "print(f'X_val_norm......: {np.shape(X_val_norm)}')\n",
    "print(f'y_val...........: {np.shape(y_val)}')\n",
    "print(f'y_OHEV_val......: {np.shape(y_OHEV_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73c4cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple confusion matrix\n",
    "\n",
    "def simple_conf_matrix(y_true, y_pred, nom_classes, clf, acc):\n",
    "    \n",
    "    picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "    conf_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    title = nom_dataset + model_surname + norm_type + ' - Classifier ' + clf + ' - Validation accuracy: '+ str(\"{:0.2f} %\".format(acc*100))\n",
    "\n",
    "    plt.figure(figsize = (10,10))\n",
    "    sns.heatmap(conf_matrix, \n",
    "                annot=True, \n",
    "                fmt='g', \n",
    "                cmap=cmap_cm, \n",
    "                annot_kws={\"size\": 8}, \n",
    "                xticklabels=nom_classes, \n",
    "                yticklabels=nom_classes)\n",
    "    plt.title(title, fontsize = 12)\n",
    "    plt.savefig(os.path.join(path_pic, picture_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "149e7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, labels, title, cmap, normalize):\n",
    "\n",
    "    if labels is not None:\n",
    "        tick_marks = np.arange(len(labels))\n",
    "        plt.xticks(tick_marks, labels, fontsize=10, rotation=45)\n",
    "        plt.yticks(tick_marks, labels, fontsize=10)\n",
    "   \n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 8)\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 8)\n",
    "\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title, fontsize=13)\n",
    "    plt.colorbar(shrink=1)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15571e04",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240cc9bf",
   "metadata": {},
   "source": [
    "- **Multilayer Perceptron** (MLP) is a type of Artificial Neural Network (ANN) used for supervised learning tasks, including classification, regression, and pattern recognition. It's a feedforward neural network that consists of multiple layers of nodes, including an input layer, one or more hidden layers, and an output layer. Each node, or neuron, in the network is connected to every node in the adjacent layers, and these connections have weights that are adjusted during training. MLP is capable of modeling complex relationships in data, making it suitable for tasks where the relationship between inputs and outputs is non-linear and intricate. It uses activation functions to introduce non-linearity into the network, allowing it to learn and approximate a wide variety of functions. One of the key advantages of MLP is its ability to learn from large and high-dimensional datasets. However, this advantage comes with the cost of increased complexity, making it more challenging to train and requiring careful tuning of hyperparameters like the number of hidden layers, the number of neurons in each layer, and the learning rate. Additionally, MLP is sensitive to feature scaling, and preprocessing techniques such as normalization are often applied to the input data to improve performance.\n",
    "***\n",
    "- **Convolutional Neural Networks** (CNNs) are a class of deep learning algorithms specifically designed for processing grid-like data, such as images and videos. CNNs are highly effective in tasks related to computer vision, including image recognition, object detection, and image segmentation. They are characterized by their ability to automatically and adaptively learn spatial hierarchies of features from input data. CNNs consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers apply convolution operations to the input data, enabling the network to automatically learn patterns and features from images, such as edges, textures, and more complex structures. The pooling layers downsample the spatial dimensions of the data, reducing computational complexity while retaining important features. Fully connected layers at the end of the network process the learned features and make predictions based on them. One of the significant advantages of CNNs is their ability to capture local patterns and spatial hierarchies of features. By using shared weights and biases in the convolutional layers, CNNs are capable of learning translation-invariant features, making them well-suited for tasks where the spatial arrangement of features in the input data is essential. Additionally, CNNs can automatically learn relevant features from raw pixel values, eliminating the need for manual feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72775254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of the dimensions of the input layer\n",
    "\n",
    "n_dim       = X_norm.shape[1]\n",
    "n_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "843264dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OHEV\n",
    "\n",
    "Counter_test = Counter(map(tuple, y_test_OHEV))\n",
    "Counter_train = Counter(map(tuple, y_train_OHEV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "020f71ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({(0, 0, 0, 0, 1): 5311,\n",
       "         (0, 0, 0, 1, 0): 5670,\n",
       "         (0, 1, 0, 0, 0): 2476,\n",
       "         (0, 0, 1, 0, 0): 5670,\n",
       "         (1, 0, 0, 0, 0): 5619})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training samples')\n",
    "Counter_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d8a4668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({(1, 0, 0, 0, 0): 625,\n",
       "         (0, 0, 0, 1, 0): 630,\n",
       "         (0, 1, 0, 0, 0): 275,\n",
       "         (0, 0, 1, 0, 0): 630,\n",
       "         (0, 0, 0, 0, 1): 590})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Testing samples')\n",
    "Counter_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4357c48",
   "metadata": {},
   "source": [
    "### ANN - Grid search for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3ffbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_model():\n",
    "    \n",
    "#    model = Sequential()\n",
    "#    model.add(Dense(n_dim, activation='relu', input_shape=(n_dim,)))\n",
    "#    model.add(Dropout(0.2))\n",
    "#    model.add(Dense(375, activation='relu'))\n",
    "#    model.add(Dropout(0.2))\n",
    "#    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#    model.compile(loss='MeanSquaredError', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b8805a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for the batch size and epochs\n",
    "\n",
    "#model = KerasClassifier(build_fn = create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "#batch_size  = [20, 40, 80, 160]\n",
    "#epochs      = [100, 250, 500]\n",
    "#param_grid  = dict(batch_size = batch_size, epochs = epochs)\n",
    "#grid        = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs=-1, cv=3)\n",
    "#grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#means  = grid_result.cv_results_['mean_test_score']\n",
    "#stds   = grid_result.cv_results_['std_test_score']\n",
    "#params = grid_result.cv_results_['params']\n",
    "\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7140571e",
   "metadata": {},
   "source": [
    "Results of the above GridSearch:\n",
    "\n",
    "Best: 0.857639 using {'batch_size': 80, 'epochs': 100}\n",
    "\n",
    "0.815972 (0.032200) with: {'batch_size': 20, 'epochs': 100}\n",
    "0.836806 (0.017705) with: {'batch_size': 20, 'epochs': 250}\n",
    "0.840278 (0.004910) with: {'batch_size': 20, 'epochs': 500}\n",
    "0.836806 (0.032200) with: {'batch_size': 40, 'epochs': 100}\n",
    "0.854167 (0.017010) with: {'batch_size': 40, 'epochs': 250}\n",
    "0.840278 (0.024552) with: {'batch_size': 40, 'epochs': 500}\n",
    "0.857639 (0.027340) with: {'batch_size': 80, 'epochs': 100}\n",
    "0.854167 (0.030666) with: {'batch_size': 80, 'epochs': 250}\n",
    "0.802083 (0.038976) with: {'batch_size': 80, 'epochs': 500}\n",
    "0.840278 (0.041955) with: {'batch_size': 160, 'epochs': 100}\n",
    "0.850694 (0.029869) with: {'batch_size': 160, 'epochs': 250}\n",
    "0.836806 (0.032200) with: {'batch_size': 160, 'epochs': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56f7bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "#K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68655d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, Adamax, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28d49a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hidden layers, neurons, activation, dropout_rate and optimizer\n",
    "\n",
    "#def tune_model(hidden_layers, neurons, activation, dropout_rate, optimizer='adam', learning_rate=0.001, momentum=0.9, nesterov=False, rho=0.9, epsilon=1e-07, centered=False, \n",
    "#                 initial_accumulator_value=0.1, amsgrad=False, beta_1=0.9, beta_2=0.999):\n",
    "    \n",
    "#    model = Sequential()\n",
    "#    model.add(Dense(units = neurons, activation = activation, input_shape = (n_dim,)))\n",
    "\n",
    "#    for i in range(hidden_layers):\n",
    "#        model.add(Dense(units = neurons, activation = activation))\n",
    "#        model.add(Dropout(dropout_rate))\n",
    "\n",
    "#    model.add(Dense(units = num_classes, activation = 'sigmoid'))\n",
    "    \n",
    "#    if optimizer == 'sgd':\n",
    "#        optimizer = SGD(lr=learning_rate, momentum=momentum, nesterov=nesterov)\n",
    "#    elif optimizer == 'rmsprop':\n",
    "#        optimizer = RMSprop(lr=learning_rate, rho=rho, epsilon=epsilon, centered=centered)\n",
    "#    elif optimizer == 'adam':\n",
    "#        optimizer = Adam(lr=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=amsgrad)\n",
    "#    elif optimizer == 'adagrad':\n",
    "#        optimizer = Adagrad(lr=learning_rate, initial_accumulator_value=initial_accumulator_value, epsilon=epsilon)\n",
    "#    elif optimizer == 'adadelta':\n",
    "#        optimizer = Adadelta(lr=learning_rate, rho=rho, epsilon=epsilon)\n",
    "#    elif optimizer == 'adamax':\n",
    "#        optimizer = Adamax(lr=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "#    elif optimizer == 'nadam':\n",
    "#        optimizer = Nadam(lr=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "        \n",
    "#    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "#    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3748cb9a",
   "metadata": {},
   "source": [
    "'learning_rate' represents the learning rate of the optimizer.\n",
    "'momentum' is the momentum factor for optimizers like SGD and RMSprop.\n",
    "'nesterov' is a boolean indicating whether to apply Nesterov momentum for SGD.\n",
    "'rho' is the decay factor for RMSprop.\n",
    "'epsilon' is a small constant for numerical stability.\n",
    "'centered' is a boolean indicating whether to compute centralized gradients for RMSprop.\n",
    "'initial_accumulator_value' is the starting value for accumulators in Adagrad.\n",
    "'amsgrad' is a boolean indicating whether to use the AMSGrad variant of Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a075bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_layers  = [1, 2, 3]\n",
    "#neurons        = [375, 500, 750, 1000]\n",
    "#activation     = ['relu', 'sigmoid']\n",
    "#dropout_rate   = [0.1, 0.2, 0.3]\n",
    "#optimizer      = ['sgd', 'rmsprop', 'adam', 'adagrad', 'adadelta', 'adamax', 'nadam']\n",
    "\n",
    "#learning_rate  = [0.001, 0.01, 0.1]\n",
    "\n",
    "#param_grid     = dict(hidden_layers = hidden_layers, \n",
    "#                      neurons       = neurons, \n",
    "#                      activation    = activation,\n",
    "#                     dropout_rate  = dropout_rate,\n",
    "#                      optimizer     = optimizer,\n",
    "#                      learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa178c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.autograph.set_verbosity(0)\n",
    "#tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "944f5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KerasClassifier(build_fn = tune_model, verbose=1, epochs = 100, batch_size = 80)\n",
    "#grid  = GridSearchCV(estimator = model, param_grid = param_grid, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d21c9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fc2f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize results\n",
    "#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#means  = grid_result.cv_results_['mean_test_score']\n",
    "#stds   = grid_result.cv_results_['std_test_score']\n",
    "#params = grid_result.cv_results_['params']\n",
    "\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f466e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN (Artificial Neural Network) or MLP (Multi layer Perceptron) using Tensorflow\n",
    "\n",
    "initializer = keras.initializers.Ones()\n",
    "\n",
    "def build_ANN_model(model_name: str, neurons: int):\n",
    "    \n",
    "    #optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=False)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001, \n",
    "                                      beta_1=0.5, \n",
    "                                      beta_2=0.999, \n",
    "                                      epsilon=1e-07, \n",
    "                                      amsgrad=True)\n",
    "    \n",
    "    \n",
    "    model = Sequential(name = model_name)\n",
    "    model.add(Dense(neurons, activation = 'relu', input_shape = (neurons,), name = 'Input'))\n",
    "\n",
    "    # First hiden layer with 375 neurons\n",
    "    model.add(Dense(neurons, activation ='relu', name = 'Hiden_1'))\n",
    "\n",
    "    # Dropout de 20%\n",
    "    model.add(Dropout(0.2, name = 'Dropout_1'))\n",
    "    \n",
    "    # Second hiden layer with 750 neurons (Kolmogorov's theorem)\n",
    "    model.add(Dense(n_dim * 2, activation ='relu', name = 'Hiden_2'))\n",
    "\n",
    "    # Dropout de 20%\n",
    "    model.add(Dropout(0.2, name = 'Dropout_2'))\n",
    "\n",
    "    # Final classification layer, with 1 neuron for each output class. Softmax divides the probability of each class.\n",
    "    model.add(Dense(num_classes, activation='softmax', name = 'Output'))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "383a342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=150, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "if not os.path.exists(path_models):\n",
    "    os.makedirs(path_models)\n",
    "\n",
    "filepath       = os.path.join(path_models, 'Model_ANN_weights_0_best' + norm_type + model_surname + '.hdf5')\n",
    "checkpoint     = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e40f2486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ANN_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 375)               141000    \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 375)               141000    \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               282000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 567,755\n",
      "Trainable params: 567,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ANN = build_ANN_model('ANN_1', neurons = n_dim)\n",
    "model_ANN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "649f2c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_ANN, to_file= os.path.join(path_models, 'Model_ANN' + norm_type + model_surname + '.png'), show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7006273",
   "metadata": {},
   "source": [
    "### Understanding the column \"Param\":\n",
    "\n",
    "- 141,000 parameters is the result of 375 neurons with 375 features + 375  bias values\n",
    "- 141,000 parameters is the result of 375 neurons with 375 features + 375  bias values\n",
    "- 282,000 parameters is the result of 750 neurons with 375 features + 750 bias values\n",
    "- 3,755   parameters is the result of 750 neurons with 5 features  + 5  bias values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2f2fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Training set\n",
      "\n",
      "X_train.........: (24746, 375)\n",
      "y_train.........: (24746,)\n",
      "y_train_OHEV....: (24746, 5)\n",
      "\n",
      "==================================\n",
      "Testing set\n",
      "\n",
      "X_test..........: (2750, 375)\n",
      "y_test..........: (2750,)\n",
      "y_test_OHEV.....: (2750, 5)\n",
      "\n",
      "==================================\n",
      "Validation set\n",
      "\n",
      "X_val_norm......: (3010, 375)\n",
      "y_val...........: (3010,)\n",
      "y_OHEV_val......: (3010, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==================================\")\n",
    "print(\"Training set\\n\")\n",
    "\n",
    "print(f'X_train.........: {np.shape(X_train)}')\n",
    "print(f'y_train.........: {np.shape(y_train)}')\n",
    "print(f'y_train_OHEV....: {np.shape(y_train_OHEV)}')\n",
    "\n",
    "print(\"\\n==================================\")\n",
    "print(\"Testing set\\n\")\n",
    "\n",
    "print(f'X_test..........: {np.shape(X_test)}')\n",
    "print(f'y_test..........: {np.shape(y_test)}')\n",
    "print(f'y_test_OHEV.....: {np.shape(y_test_OHEV)}')\n",
    "\n",
    "print(\"\\n==================================\")\n",
    "print(\"Validation set\\n\")\n",
    "\n",
    "print(f'X_val_norm......: {np.shape(X_val_norm)}')\n",
    "print(f'y_val...........: {np.shape(y_val)}')\n",
    "print(f'y_OHEV_val......: {np.shape(y_OHEV_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d16c3f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_OHEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41fe4f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "  1/774 [..............................] - ETA: 0s - loss: 1.6136 - accuracy: 0.2188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0070s). Check your callbacks.\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.9132 - accuracy: 0.6547\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.76655, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.9088 - accuracy: 0.6566 - val_loss: 0.6312 - val_accuracy: 0.7665\n",
      "Epoch 2/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.7843\n",
      "Epoch 00002: val_accuracy improved from 0.76655 to 0.79491, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.5938 - accuracy: 0.7844 - val_loss: 0.5518 - val_accuracy: 0.7949\n",
      "Epoch 3/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.5136 - accuracy: 0.8139\n",
      "Epoch 00003: val_accuracy improved from 0.79491 to 0.80836, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.5131 - accuracy: 0.8138 - val_loss: 0.4947 - val_accuracy: 0.8084\n",
      "Epoch 4/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.4611 - accuracy: 0.8331\n",
      "Epoch 00004: val_accuracy improved from 0.80836 to 0.83600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.4614 - accuracy: 0.8331 - val_loss: 0.4447 - val_accuracy: 0.8360\n",
      "Epoch 5/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.4233 - accuracy: 0.8462\n",
      "Epoch 00005: val_accuracy improved from 0.83600 to 0.85418, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.4232 - accuracy: 0.8463 - val_loss: 0.3989 - val_accuracy: 0.8542\n",
      "Epoch 6/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8592\n",
      "Epoch 00006: val_accuracy did not improve from 0.85418\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.3892 - accuracy: 0.8590 - val_loss: 0.4100 - val_accuracy: 0.8480\n",
      "Epoch 7/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.3668 - accuracy: 0.8659\n",
      "Epoch 00007: val_accuracy did not improve from 0.85418\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.3656 - accuracy: 0.8665 - val_loss: 0.5175 - val_accuracy: 0.8011\n",
      "Epoch 8/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.8760\n",
      "Epoch 00008: val_accuracy did not improve from 0.85418\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.3431 - accuracy: 0.8761 - val_loss: 0.4153 - val_accuracy: 0.8422\n",
      "Epoch 9/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8838\n",
      "Epoch 00009: val_accuracy improved from 0.85418 to 0.88145, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.3236 - accuracy: 0.8839 - val_loss: 0.3413 - val_accuracy: 0.8815\n",
      "Epoch 10/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.3066 - accuracy: 0.8895\n",
      "Epoch 00010: val_accuracy did not improve from 0.88145\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8895 - val_loss: 0.3557 - val_accuracy: 0.8640\n",
      "Epoch 11/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8961\n",
      "Epoch 00011: val_accuracy improved from 0.88145 to 0.88364, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2898 - accuracy: 0.8963 - val_loss: 0.3262 - val_accuracy: 0.8836\n",
      "Epoch 12/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.2740 - accuracy: 0.9033\n",
      "Epoch 00012: val_accuracy did not improve from 0.88364\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2739 - accuracy: 0.9034 - val_loss: 0.4193 - val_accuracy: 0.8436\n",
      "Epoch 13/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.2616 - accuracy: 0.9089\n",
      "Epoch 00013: val_accuracy improved from 0.88364 to 0.89600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2611 - accuracy: 0.9088 - val_loss: 0.2914 - val_accuracy: 0.8960\n",
      "Epoch 14/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.2500 - accuracy: 0.9114\n",
      "Epoch 00014: val_accuracy improved from 0.89600 to 0.90073, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2489 - accuracy: 0.9121 - val_loss: 0.2774 - val_accuracy: 0.9007\n",
      "Epoch 15/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.2371 - accuracy: 0.9163\n",
      "Epoch 00015: val_accuracy improved from 0.90073 to 0.90509, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2376 - accuracy: 0.9161 - val_loss: 0.2776 - val_accuracy: 0.9051\n",
      "Epoch 16/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.2262 - accuracy: 0.9206\n",
      "Epoch 00016: val_accuracy improved from 0.90509 to 0.90582, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.2266 - accuracy: 0.9204 - val_loss: 0.2677 - val_accuracy: 0.9058\n",
      "Epoch 17/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.2150 - accuracy: 0.9253\n",
      "Epoch 00017: val_accuracy improved from 0.90582 to 0.90764, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2154 - accuracy: 0.9248 - val_loss: 0.2579 - val_accuracy: 0.9076\n",
      "Epoch 18/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.2046 - accuracy: 0.9281\n",
      "Epoch 00018: val_accuracy did not improve from 0.90764\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2042 - accuracy: 0.9282 - val_loss: 0.2663 - val_accuracy: 0.9000\n",
      "Epoch 19/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.2001 - accuracy: 0.9291\n",
      "Epoch 00019: val_accuracy did not improve from 0.90764\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.2002 - accuracy: 0.9291 - val_loss: 0.2725 - val_accuracy: 0.9055\n",
      "Epoch 20/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.9367\n",
      "Epoch 00020: val_accuracy did not improve from 0.90764\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.9369 - val_loss: 0.2626 - val_accuracy: 0.9076\n",
      "Epoch 21/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9361\n",
      "Epoch 00021: val_accuracy did not improve from 0.90764\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1840 - accuracy: 0.9359 - val_loss: 0.2705 - val_accuracy: 0.8978\n",
      "Epoch 22/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.1725 - accuracy: 0.9401\n",
      "Epoch 00022: val_accuracy did not improve from 0.90764\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1718 - accuracy: 0.9403 - val_loss: 0.2927 - val_accuracy: 0.8978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9408\n",
      "Epoch 00023: val_accuracy improved from 0.90764 to 0.92509, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1667 - accuracy: 0.9408 - val_loss: 0.2112 - val_accuracy: 0.9251\n",
      "Epoch 24/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1593 - accuracy: 0.9442\n",
      "Epoch 00024: val_accuracy improved from 0.92509 to 0.92836, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1581 - accuracy: 0.9447 - val_loss: 0.2157 - val_accuracy: 0.9284\n",
      "Epoch 25/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9470\n",
      "Epoch 00025: val_accuracy improved from 0.92836 to 0.93200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1547 - accuracy: 0.9470 - val_loss: 0.1993 - val_accuracy: 0.9320\n",
      "Epoch 26/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.1462 - accuracy: 0.9471\n",
      "Epoch 00026: val_accuracy improved from 0.93200 to 0.93382, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1449 - accuracy: 0.9475 - val_loss: 0.2009 - val_accuracy: 0.9338\n",
      "Epoch 27/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.1383 - accuracy: 0.9505\n",
      "Epoch 00027: val_accuracy did not improve from 0.93382\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1389 - accuracy: 0.9505 - val_loss: 0.2475 - val_accuracy: 0.9113\n",
      "Epoch 28/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.9530\n",
      "Epoch 00028: val_accuracy did not improve from 0.93382\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1344 - accuracy: 0.9533 - val_loss: 0.2167 - val_accuracy: 0.9305\n",
      "Epoch 29/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.1303 - accuracy: 0.9537\n",
      "Epoch 00029: val_accuracy improved from 0.93382 to 0.93964, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1305 - accuracy: 0.9537 - val_loss: 0.1945 - val_accuracy: 0.9396\n",
      "Epoch 30/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9581\n",
      "Epoch 00030: val_accuracy did not improve from 0.93964\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1230 - accuracy: 0.9581 - val_loss: 0.2690 - val_accuracy: 0.9062\n",
      "Epoch 31/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.1200 - accuracy: 0.9601\n",
      "Epoch 00031: val_accuracy did not improve from 0.93964\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1203 - accuracy: 0.9595 - val_loss: 0.2009 - val_accuracy: 0.9382\n",
      "Epoch 32/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9604\n",
      "Epoch 00032: val_accuracy did not improve from 0.93964\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1143 - accuracy: 0.9602 - val_loss: 0.1986 - val_accuracy: 0.9291\n",
      "Epoch 33/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1119 - accuracy: 0.9610\n",
      "Epoch 00033: val_accuracy did not improve from 0.93964\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1118 - accuracy: 0.9611 - val_loss: 0.1911 - val_accuracy: 0.9349\n",
      "Epoch 34/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.1075 - accuracy: 0.9625\n",
      "Epoch 00034: val_accuracy improved from 0.93964 to 0.94073, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1077 - accuracy: 0.9624 - val_loss: 0.1776 - val_accuracy: 0.9407\n",
      "Epoch 35/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1019 - accuracy: 0.9662\n",
      "Epoch 00035: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1021 - accuracy: 0.9660 - val_loss: 0.2359 - val_accuracy: 0.9164\n",
      "Epoch 36/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9653\n",
      "Epoch 00036: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1009 - accuracy: 0.9653 - val_loss: 0.2325 - val_accuracy: 0.9229\n",
      "Epoch 37/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0952 - accuracy: 0.9670\n",
      "Epoch 00037: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0952 - accuracy: 0.9671 - val_loss: 0.1791 - val_accuracy: 0.9345\n",
      "Epoch 38/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0899 - accuracy: 0.9690\n",
      "Epoch 00038: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0900 - accuracy: 0.9686 - val_loss: 0.1711 - val_accuracy: 0.9400\n",
      "Epoch 39/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0857 - accuracy: 0.9703\n",
      "Epoch 00039: val_accuracy did not improve from 0.94073\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0858 - accuracy: 0.9702 - val_loss: 0.1720 - val_accuracy: 0.9396\n",
      "Epoch 40/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9710\n",
      "Epoch 00040: val_accuracy improved from 0.94073 to 0.94582, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0852 - accuracy: 0.9710 - val_loss: 0.1690 - val_accuracy: 0.9458\n",
      "Epoch 41/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9730\n",
      "Epoch 00041: val_accuracy did not improve from 0.94582\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9728 - val_loss: 0.1907 - val_accuracy: 0.9360\n",
      "Epoch 42/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9725\n",
      "Epoch 00042: val_accuracy did not improve from 0.94582\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.9725 - val_loss: 0.1772 - val_accuracy: 0.9415\n",
      "Epoch 43/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0748 - accuracy: 0.9748\n",
      "Epoch 00043: val_accuracy did not improve from 0.94582\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0761 - accuracy: 0.9743 - val_loss: 0.1699 - val_accuracy: 0.9447\n",
      "Epoch 44/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9744\n",
      "Epoch 00044: val_accuracy improved from 0.94582 to 0.94691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9744 - val_loss: 0.1587 - val_accuracy: 0.9469\n",
      "Epoch 45/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 0.0688 - accuracy: 0.9765\n",
      "Epoch 00045: val_accuracy improved from 0.94691 to 0.95164, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0689 - accuracy: 0.9766 - val_loss: 0.1435 - val_accuracy: 0.9516\n",
      "Epoch 46/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9774\n",
      "Epoch 00046: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0671 - accuracy: 0.9774 - val_loss: 0.1744 - val_accuracy: 0.9411\n",
      "Epoch 47/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761/774 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9785\n",
      "Epoch 00047: val_accuracy improved from 0.95164 to 0.95636, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0632 - accuracy: 0.9786 - val_loss: 0.1462 - val_accuracy: 0.9564\n",
      "Epoch 48/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9796\n",
      "Epoch 00048: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0625 - accuracy: 0.9796 - val_loss: 0.1695 - val_accuracy: 0.9433\n",
      "Epoch 49/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9789\n",
      "Epoch 00049: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0618 - accuracy: 0.9789 - val_loss: 0.1411 - val_accuracy: 0.9516\n",
      "Epoch 50/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9805\n",
      "Epoch 00050: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0598 - accuracy: 0.9805 - val_loss: 0.1543 - val_accuracy: 0.9487\n",
      "Epoch 51/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9818\n",
      "Epoch 00051: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0574 - accuracy: 0.9817 - val_loss: 0.1678 - val_accuracy: 0.9465\n",
      "Epoch 52/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0537 - accuracy: 0.9817\n",
      "Epoch 00052: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0538 - accuracy: 0.9817 - val_loss: 0.2566 - val_accuracy: 0.9124\n",
      "Epoch 53/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0548 - accuracy: 0.9806\n",
      "Epoch 00053: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0546 - accuracy: 0.9807 - val_loss: 0.1568 - val_accuracy: 0.9545\n",
      "Epoch 54/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0537 - accuracy: 0.9814\n",
      "Epoch 00054: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0537 - accuracy: 0.9814 - val_loss: 0.1550 - val_accuracy: 0.9509\n",
      "Epoch 55/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0495 - accuracy: 0.9842\n",
      "Epoch 00055: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0497 - accuracy: 0.9842 - val_loss: 0.1659 - val_accuracy: 0.9451\n",
      "Epoch 56/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0476 - accuracy: 0.9838\n",
      "Epoch 00056: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0477 - accuracy: 0.9838 - val_loss: 0.1634 - val_accuracy: 0.9487\n",
      "Epoch 57/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0501 - accuracy: 0.9824\n",
      "Epoch 00057: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0500 - accuracy: 0.9824 - val_loss: 0.1699 - val_accuracy: 0.9455\n",
      "Epoch 58/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9845\n",
      "Epoch 00058: val_accuracy did not improve from 0.95636\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0464 - accuracy: 0.9845 - val_loss: 0.1615 - val_accuracy: 0.9527\n",
      "Epoch 59/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0423 - accuracy: 0.9865\n",
      "Epoch 00059: val_accuracy improved from 0.95636 to 0.95818, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9865 - val_loss: 0.1260 - val_accuracy: 0.9582\n",
      "Epoch 60/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0410 - accuracy: 0.9859\n",
      "Epoch 00060: val_accuracy did not improve from 0.95818\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0410 - accuracy: 0.9859 - val_loss: 0.1612 - val_accuracy: 0.9516\n",
      "Epoch 61/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0401 - accuracy: 0.9866\n",
      "Epoch 00061: val_accuracy did not improve from 0.95818\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0400 - accuracy: 0.9867 - val_loss: 0.1477 - val_accuracy: 0.9545\n",
      "Epoch 62/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 0.9871\n",
      "Epoch 00062: val_accuracy did not improve from 0.95818\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0397 - accuracy: 0.9871 - val_loss: 0.1432 - val_accuracy: 0.9538\n",
      "Epoch 63/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0389 - accuracy: 0.9873\n",
      "Epoch 00063: val_accuracy improved from 0.95818 to 0.95964, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.9873 - val_loss: 0.1345 - val_accuracy: 0.9596\n",
      "Epoch 64/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0359 - accuracy: 0.9885\n",
      "Epoch 00064: val_accuracy did not improve from 0.95964\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0365 - accuracy: 0.9882 - val_loss: 0.1398 - val_accuracy: 0.9571\n",
      "Epoch 65/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0344 - accuracy: 0.9890\n",
      "Epoch 00065: val_accuracy improved from 0.95964 to 0.96145, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0342 - accuracy: 0.9891 - val_loss: 0.1340 - val_accuracy: 0.9615\n",
      "Epoch 66/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9879\n",
      "Epoch 00066: val_accuracy did not improve from 0.96145\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9878 - val_loss: 0.1648 - val_accuracy: 0.9458\n",
      "Epoch 67/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 0.0325 - accuracy: 0.9900\n",
      "Epoch 00067: val_accuracy did not improve from 0.96145\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9899 - val_loss: 0.1399 - val_accuracy: 0.9567\n",
      "Epoch 68/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0314 - accuracy: 0.9900\n",
      "Epoch 00068: val_accuracy did not improve from 0.96145\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 0.1268 - val_accuracy: 0.9596\n",
      "Epoch 69/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0294 - accuracy: 0.9911\n",
      "Epoch 00069: val_accuracy did not improve from 0.96145\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0297 - accuracy: 0.9909 - val_loss: 0.1587 - val_accuracy: 0.9571\n",
      "Epoch 70/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0292 - accuracy: 0.9907\n",
      "Epoch 00070: val_accuracy did not improve from 0.96145\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0293 - accuracy: 0.9907 - val_loss: 0.1571 - val_accuracy: 0.9469\n",
      "Epoch 71/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0318 - accuracy: 0.9896\n",
      "Epoch 00071: val_accuracy did not improve from 0.96145\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9894 - val_loss: 0.1391 - val_accuracy: 0.9560\n",
      "Epoch 72/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0309 - accuracy: 0.9893\n",
      "Epoch 00072: val_accuracy improved from 0.96145 to 0.96218, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 0.1289 - val_accuracy: 0.9622\n",
      "Epoch 73/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/774 [============================>.] - ETA: 0s - loss: 0.0277 - accuracy: 0.9910\n",
      "Epoch 00073: val_accuracy did not improve from 0.96218\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0277 - accuracy: 0.9910 - val_loss: 0.1558 - val_accuracy: 0.9509\n",
      "Epoch 74/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0253 - accuracy: 0.9922\n",
      "Epoch 00074: val_accuracy did not improve from 0.96218\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.1275 - val_accuracy: 0.9611\n",
      "Epoch 75/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0259 - accuracy: 0.9916\n",
      "Epoch 00075: val_accuracy did not improve from 0.96218\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0258 - accuracy: 0.9917 - val_loss: 0.1227 - val_accuracy: 0.9585\n",
      "Epoch 76/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0230 - accuracy: 0.9929\n",
      "Epoch 00076: val_accuracy improved from 0.96218 to 0.96545, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9929 - val_loss: 0.1196 - val_accuracy: 0.9655\n",
      "Epoch 77/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0277 - accuracy: 0.9910\n",
      "Epoch 00077: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0277 - accuracy: 0.9909 - val_loss: 0.1930 - val_accuracy: 0.9425\n",
      "Epoch 78/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9911\n",
      "Epoch 00078: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0258 - accuracy: 0.9913 - val_loss: 0.1478 - val_accuracy: 0.9567\n",
      "Epoch 79/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9931\n",
      "Epoch 00079: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9928 - val_loss: 0.1660 - val_accuracy: 0.9509\n",
      "Epoch 80/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9929\n",
      "Epoch 00080: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0224 - accuracy: 0.9928 - val_loss: 0.1202 - val_accuracy: 0.9651\n",
      "Epoch 81/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.9922\n",
      "Epoch 00081: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0243 - accuracy: 0.9922 - val_loss: 0.1554 - val_accuracy: 0.9524\n",
      "Epoch 82/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0217 - accuracy: 0.9937\n",
      "Epoch 00082: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0218 - accuracy: 0.9937 - val_loss: 0.1243 - val_accuracy: 0.9644\n",
      "Epoch 83/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9938\n",
      "Epoch 00083: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.1399 - val_accuracy: 0.9582\n",
      "Epoch 84/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0234 - accuracy: 0.9926\n",
      "Epoch 00084: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.1436 - val_accuracy: 0.9625\n",
      "Epoch 85/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9921\n",
      "Epoch 00085: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0231 - accuracy: 0.9920 - val_loss: 0.1905 - val_accuracy: 0.9447\n",
      "Epoch 86/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9940\n",
      "Epoch 00086: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9940 - val_loss: 0.1305 - val_accuracy: 0.9596\n",
      "Epoch 87/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9954\n",
      "Epoch 00087: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1350 - val_accuracy: 0.9611\n",
      "Epoch 88/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0205 - accuracy: 0.9931\n",
      "Epoch 00088: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9933 - val_loss: 0.1417 - val_accuracy: 0.9622\n",
      "Epoch 89/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0168 - accuracy: 0.9947\n",
      "Epoch 00089: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9947 - val_loss: 0.1425 - val_accuracy: 0.9618\n",
      "Epoch 90/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9949\n",
      "Epoch 00090: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0164 - accuracy: 0.9948 - val_loss: 0.1550 - val_accuracy: 0.9582\n",
      "Epoch 91/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9959\n",
      "Epoch 00091: val_accuracy did not improve from 0.96545\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9958 - val_loss: 0.1589 - val_accuracy: 0.9567\n",
      "Epoch 92/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0147 - accuracy: 0.9958\n",
      "Epoch 00092: val_accuracy improved from 0.96545 to 0.96691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0147 - accuracy: 0.9958 - val_loss: 0.1230 - val_accuracy: 0.9669\n",
      "Epoch 93/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9951\n",
      "Epoch 00093: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0167 - accuracy: 0.9951 - val_loss: 0.1485 - val_accuracy: 0.9589\n",
      "Epoch 94/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9948\n",
      "Epoch 00094: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 0.1692 - val_accuracy: 0.9505\n",
      "Epoch 95/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9947\n",
      "Epoch 00095: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.3020 - val_accuracy: 0.9247\n",
      "Epoch 96/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0158 - accuracy: 0.9949\n",
      "Epoch 00096: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 0.1274 - val_accuracy: 0.9629\n",
      "Epoch 97/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.9949\n",
      "Epoch 00097: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.1335 - val_accuracy: 0.9640\n",
      "Epoch 98/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9970\n",
      "Epoch 00098: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9970 - val_loss: 0.1578 - val_accuracy: 0.9538\n",
      "Epoch 99/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9977\n",
      "Epoch 00099: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0100 - accuracy: 0.9977 - val_loss: 0.1564 - val_accuracy: 0.9578\n",
      "Epoch 100/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9940\n",
      "Epoch 00100: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.1390 - val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9949\n",
      "Epoch 00101: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9949 - val_loss: 0.1460 - val_accuracy: 0.9596\n",
      "Epoch 102/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9955\n",
      "Epoch 00102: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.1382 - val_accuracy: 0.9596\n",
      "Epoch 103/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9965\n",
      "Epoch 00103: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.1372 - val_accuracy: 0.9644\n",
      "Epoch 104/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9955\n",
      "Epoch 00104: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.1373 - val_accuracy: 0.9629\n",
      "Epoch 105/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9967\n",
      "Epoch 00105: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9968 - val_loss: 0.1325 - val_accuracy: 0.9662\n",
      "Epoch 106/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9978\n",
      "Epoch 00106: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0094 - accuracy: 0.9977 - val_loss: 0.1328 - val_accuracy: 0.9669\n",
      "Epoch 107/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9949\n",
      "Epoch 00107: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9949 - val_loss: 0.1404 - val_accuracy: 0.9625\n",
      "Epoch 108/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9971\n",
      "Epoch 00108: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.1464 - val_accuracy: 0.9578\n",
      "Epoch 109/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0106 - accuracy: 0.9969\n",
      "Epoch 00109: val_accuracy did not improve from 0.96691\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.1429 - val_accuracy: 0.9589\n",
      "Epoch 110/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9974\n",
      "Epoch 00110: val_accuracy improved from 0.96691 to 0.96727, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.1346 - val_accuracy: 0.9673\n",
      "Epoch 111/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9957\n",
      "Epoch 00111: val_accuracy did not improve from 0.96727\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0142 - accuracy: 0.9957 - val_loss: 0.1556 - val_accuracy: 0.9604\n",
      "Epoch 112/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9962\n",
      "Epoch 00112: val_accuracy improved from 0.96727 to 0.96800, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9962 - val_loss: 0.1143 - val_accuracy: 0.9680\n",
      "Epoch 113/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9976\n",
      "Epoch 00113: val_accuracy did not improve from 0.96800\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.1472 - val_accuracy: 0.9604\n",
      "Epoch 114/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0100 - accuracy: 0.9972\n",
      "Epoch 00114: val_accuracy did not improve from 0.96800\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.1310 - val_accuracy: 0.9662\n",
      "Epoch 115/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9982\n",
      "Epoch 00115: val_accuracy improved from 0.96800 to 0.96836, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.1278 - val_accuracy: 0.9684\n",
      "Epoch 116/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9969\n",
      "Epoch 00116: val_accuracy did not improve from 0.96836\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.1430 - val_accuracy: 0.9658\n",
      "Epoch 117/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9988\n",
      "Epoch 00117: val_accuracy did not improve from 0.96836\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.1524 - val_accuracy: 0.9629\n",
      "Epoch 118/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9982\n",
      "Epoch 00118: val_accuracy did not improve from 0.96836\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.1487 - val_accuracy: 0.9629\n",
      "Epoch 119/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9970\n",
      "Epoch 00119: val_accuracy did not improve from 0.96836\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.1354 - val_accuracy: 0.9673\n",
      "Epoch 120/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0113 - accuracy: 0.9969\n",
      "Epoch 00120: val_accuracy improved from 0.96836 to 0.96945, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 0.1273 - val_accuracy: 0.9695\n",
      "Epoch 121/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9985\n",
      "Epoch 00121: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.1379 - val_accuracy: 0.9640\n",
      "Epoch 122/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9990\n",
      "Epoch 00122: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.1420 - val_accuracy: 0.9629\n",
      "Epoch 123/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9970\n",
      "Epoch 00123: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0085 - accuracy: 0.9970 - val_loss: 0.1500 - val_accuracy: 0.9615\n",
      "Epoch 124/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 00124: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.1476 - val_accuracy: 0.9618\n",
      "Epoch 125/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9985\n",
      "Epoch 00125: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.1446 - val_accuracy: 0.9676\n",
      "Epoch 126/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9973\n",
      "Epoch 00126: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.1464 - val_accuracy: 0.9596\n",
      "Epoch 127/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9971\n",
      "Epoch 00127: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0106 - accuracy: 0.9971 - val_loss: 0.1349 - val_accuracy: 0.9651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9964\n",
      "Epoch 00128: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.1318 - val_accuracy: 0.9676\n",
      "Epoch 129/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9985\n",
      "Epoch 00129: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.1486 - val_accuracy: 0.9633\n",
      "Epoch 130/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9984\n",
      "Epoch 00130: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.1539 - val_accuracy: 0.9636\n",
      "Epoch 131/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9978\n",
      "Epoch 00131: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0078 - accuracy: 0.9978 - val_loss: 0.1482 - val_accuracy: 0.9622\n",
      "Epoch 132/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9983\n",
      "Epoch 00132: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0068 - accuracy: 0.9983 - val_loss: 0.1403 - val_accuracy: 0.9662\n",
      "Epoch 133/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9964\n",
      "Epoch 00133: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9964 - val_loss: 0.1382 - val_accuracy: 0.9680\n",
      "Epoch 134/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9968\n",
      "Epoch 00134: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 0.2021 - val_accuracy: 0.9476\n",
      "Epoch 135/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9956\n",
      "Epoch 00135: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0143 - accuracy: 0.9956 - val_loss: 0.2104 - val_accuracy: 0.9516\n",
      "Epoch 136/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9975\n",
      "Epoch 00136: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.1329 - val_accuracy: 0.9673\n",
      "Epoch 137/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9986\n",
      "Epoch 00137: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.1329 - val_accuracy: 0.9676\n",
      "Epoch 138/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9988\n",
      "Epoch 00138: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.1938 - val_accuracy: 0.9535\n",
      "Epoch 139/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9985\n",
      "Epoch 00139: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.1458 - val_accuracy: 0.9640\n",
      "Epoch 140/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 00140: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.1470 - val_accuracy: 0.9684\n",
      "Epoch 141/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9998\n",
      "Epoch 00141: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.1320 - val_accuracy: 0.9684\n",
      "Epoch 142/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9964\n",
      "Epoch 00142: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9965 - val_loss: 0.1481 - val_accuracy: 0.9604\n",
      "Epoch 143/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9984\n",
      "Epoch 00143: val_accuracy did not improve from 0.96945\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.1444 - val_accuracy: 0.9669\n",
      "Epoch 144/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9988\n",
      "Epoch 00144: val_accuracy improved from 0.96945 to 0.97055, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.1326 - val_accuracy: 0.9705\n",
      "Epoch 145/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9985\n",
      "Epoch 00145: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.1728 - val_accuracy: 0.9604\n",
      "Epoch 146/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n",
      "Epoch 00146: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.1455 - val_accuracy: 0.9655\n",
      "Epoch 147/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 00147: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.1430 - val_accuracy: 0.9655\n",
      "Epoch 148/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 00148: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9978 - val_loss: 0.1298 - val_accuracy: 0.9669\n",
      "Epoch 149/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9993\n",
      "Epoch 00149: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.1427 - val_accuracy: 0.9669\n",
      "Epoch 150/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9988\n",
      "Epoch 00150: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.1468 - val_accuracy: 0.9684\n",
      "Epoch 151/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 00151: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.1457 - val_accuracy: 0.9655\n",
      "Epoch 152/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00152: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.1460 - val_accuracy: 0.9676\n",
      "Epoch 153/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 00153: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.1621 - val_accuracy: 0.9593\n",
      "Epoch 154/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0063 - accuracy: 0.9983\n",
      "Epoch 00154: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.1364 - val_accuracy: 0.9687\n",
      "Epoch 155/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 00155: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.1523 - val_accuracy: 0.9665\n",
      "Epoch 156/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9987\n",
      "Epoch 00156: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.1465 - val_accuracy: 0.9658\n",
      "Epoch 157/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9989\n",
      "Epoch 00157: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.1649 - val_accuracy: 0.9636\n",
      "Epoch 158/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 00158: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.1986 - val_accuracy: 0.9480\n",
      "Epoch 159/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9984\n",
      "Epoch 00159: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.1660 - val_accuracy: 0.9622\n",
      "Epoch 160/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9972\n",
      "Epoch 00160: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.1427 - val_accuracy: 0.9680\n",
      "Epoch 161/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9976\n",
      "Epoch 00161: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0069 - accuracy: 0.9975 - val_loss: 0.1470 - val_accuracy: 0.9625\n",
      "Epoch 162/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 00162: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.1422 - val_accuracy: 0.9665\n",
      "Epoch 163/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0048 - accuracy: 0.9987\n",
      "Epoch 00163: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.1387 - val_accuracy: 0.9665\n",
      "Epoch 164/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9990\n",
      "Epoch 00164: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.1376 - val_accuracy: 0.9705\n",
      "Epoch 165/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9986\n",
      "Epoch 00165: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.1363 - val_accuracy: 0.9676\n",
      "Epoch 166/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 00166: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.1522 - val_accuracy: 0.9691\n",
      "Epoch 167/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00167: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.1632 - val_accuracy: 0.9618\n",
      "Epoch 168/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00168: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.1349 - val_accuracy: 0.9687\n",
      "Epoch 169/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 00169: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.1561 - val_accuracy: 0.9636\n",
      "Epoch 170/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9997\n",
      "Epoch 00170: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.1417 - val_accuracy: 0.9695\n",
      "Epoch 171/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00171: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.1344 - val_accuracy: 0.9684\n",
      "Epoch 172/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9979\n",
      "Epoch 00172: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.1790 - val_accuracy: 0.9571\n",
      "Epoch 173/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9992\n",
      "Epoch 00173: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.1385 - val_accuracy: 0.9680\n",
      "Epoch 174/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9999\n",
      "Epoch 00174: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.1505 - val_accuracy: 0.9695\n",
      "Epoch 175/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 00175: val_accuracy did not improve from 0.97055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.1467 - val_accuracy: 0.9673\n",
      "Epoch 176/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9989\n",
      "Epoch 00176: val_accuracy improved from 0.97055 to 0.97091, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.1400 - val_accuracy: 0.9709\n",
      "Epoch 177/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 00177: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.1520 - val_accuracy: 0.9644\n",
      "Epoch 178/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0055 - accuracy: 0.9983\n",
      "Epoch 00178: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.1673 - val_accuracy: 0.9585\n",
      "Epoch 179/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 00179: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.2509 - val_accuracy: 0.9491\n",
      "Epoch 180/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0053 - accuracy: 0.9983\n",
      "Epoch 00180: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.1495 - val_accuracy: 0.9680\n",
      "Epoch 181/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9950\n",
      "Epoch 00181: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 0.1275 - val_accuracy: 0.9709\n",
      "Epoch 182/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 00182: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.2174 - val_accuracy: 0.9538\n",
      "Epoch 183/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 00183: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.1332 - val_accuracy: 0.9673\n",
      "Epoch 184/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761/774 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9979\n",
      "Epoch 00184: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.1608 - val_accuracy: 0.9673\n",
      "Epoch 185/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 00185: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.1439 - val_accuracy: 0.9698\n",
      "Epoch 186/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 00186: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.1309 - val_accuracy: 0.9680\n",
      "Epoch 187/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00187: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1866 - val_accuracy: 0.9636\n",
      "Epoch 188/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9982\n",
      "Epoch 00188: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.1401 - val_accuracy: 0.9665\n",
      "Epoch 189/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9996\n",
      "Epoch 00189: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.1367 - val_accuracy: 0.9676\n",
      "Epoch 190/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00190: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2346 - val_accuracy: 0.9422\n",
      "Epoch 191/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 00191: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.1476 - val_accuracy: 0.9691\n",
      "Epoch 192/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9987\n",
      "Epoch 00192: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.1400 - val_accuracy: 0.9669\n",
      "Epoch 193/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9993\n",
      "Epoch 00193: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.1480 - val_accuracy: 0.9680\n",
      "Epoch 194/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00194: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.1482 - val_accuracy: 0.9687\n",
      "Epoch 195/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00195: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.1811 - val_accuracy: 0.9644\n",
      "Epoch 196/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9985\n",
      "Epoch 00196: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.2091 - val_accuracy: 0.9545\n",
      "Epoch 197/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9990\n",
      "Epoch 00197: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.1420 - val_accuracy: 0.9709\n",
      "Epoch 198/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 00198: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.2432 - val_accuracy: 0.9502\n",
      "Epoch 199/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9985\n",
      "Epoch 00199: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.1777 - val_accuracy: 0.9629\n",
      "Epoch 200/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0170 - accuracy: 0.9944\n",
      "Epoch 00200: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.1422 - val_accuracy: 0.9673\n",
      "Epoch 201/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00201: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.1398 - val_accuracy: 0.9695\n",
      "Epoch 202/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 00202: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.1348 - val_accuracy: 0.9705\n",
      "Epoch 203/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00203: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.1423 - val_accuracy: 0.9684\n",
      "Epoch 204/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9980\n",
      "Epoch 00204: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.1610 - val_accuracy: 0.9669\n",
      "Epoch 205/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9982\n",
      "Epoch 00205: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.1711 - val_accuracy: 0.9633\n",
      "Epoch 206/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9956\n",
      "Epoch 00206: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0127 - accuracy: 0.9956 - val_loss: 0.1314 - val_accuracy: 0.9662\n",
      "Epoch 207/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 0.9996\n",
      "Epoch 00207: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.1496 - val_accuracy: 0.9695\n",
      "Epoch 208/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00208: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.1936 - val_accuracy: 0.9545\n",
      "Epoch 209/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 00209: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.1337 - val_accuracy: 0.9709\n",
      "Epoch 210/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9986\n",
      "Epoch 00210: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.1571 - val_accuracy: 0.9629\n",
      "Epoch 211/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9968\n",
      "Epoch 00211: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9968 - val_loss: 0.1380 - val_accuracy: 0.9709\n",
      "Epoch 212/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9987\n",
      "Epoch 00212: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.1439 - val_accuracy: 0.9698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00213: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.1462 - val_accuracy: 0.9665\n",
      "Epoch 214/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00214: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.1386 - val_accuracy: 0.9662\n",
      "Epoch 215/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00215: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.1411 - val_accuracy: 0.9691\n",
      "Epoch 216/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9980\n",
      "Epoch 00216: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.1440 - val_accuracy: 0.9662\n",
      "Epoch 217/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 00217: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.1459 - val_accuracy: 0.9662\n",
      "Epoch 218/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 9.4877e-04 - accuracy: 0.9999\n",
      "Epoch 00218: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.4385e-04 - accuracy: 0.9999 - val_loss: 0.1400 - val_accuracy: 0.9698\n",
      "Epoch 219/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00219: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2415 - val_accuracy: 0.9531\n",
      "Epoch 220/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00220: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.1520 - val_accuracy: 0.9687\n",
      "Epoch 221/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 7.8049e-04 - accuracy: 1.0000\n",
      "Epoch 00221: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.0016e-04 - accuracy: 1.0000 - val_loss: 0.1658 - val_accuracy: 0.9651\n",
      "Epoch 222/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\n",
      "Epoch 00222: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.1502 - val_accuracy: 0.9658\n",
      "Epoch 223/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00223: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.1481 - val_accuracy: 0.9709\n",
      "Epoch 224/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00224: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.1354 - val_accuracy: 0.9702\n",
      "Epoch 225/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 9.9439e-04 - accuracy: 0.9998\n",
      "Epoch 00225: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.8016e-04 - accuracy: 0.9998 - val_loss: 0.1485 - val_accuracy: 0.9691\n",
      "Epoch 226/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\n",
      "Epoch 00226: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.1801 - val_accuracy: 0.9615\n",
      "Epoch 227/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00227: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.1534 - val_accuracy: 0.9665\n",
      "Epoch 228/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00228: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.1518 - val_accuracy: 0.9673\n",
      "Epoch 229/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 00229: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.1476 - val_accuracy: 0.9680\n",
      "Epoch 230/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00230: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.1546 - val_accuracy: 0.9636\n",
      "Epoch 231/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 00231: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.2175 - val_accuracy: 0.9520\n",
      "Epoch 232/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9983\n",
      "Epoch 00232: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.1552 - val_accuracy: 0.9662\n",
      "Epoch 233/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00233: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.1508 - val_accuracy: 0.9651\n",
      "Epoch 234/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00234: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.1495 - val_accuracy: 0.9673\n",
      "Epoch 235/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00235: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.1784 - val_accuracy: 0.9658\n",
      "Epoch 236/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 00236: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.1664 - val_accuracy: 0.9622\n",
      "Epoch 237/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9982\n",
      "Epoch 00237: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.1515 - val_accuracy: 0.9684\n",
      "Epoch 238/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 00238: val_accuracy did not improve from 0.97091\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.1418 - val_accuracy: 0.9691\n",
      "Epoch 239/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00239: val_accuracy improved from 0.97091 to 0.97127, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.1397 - val_accuracy: 0.9713\n",
      "Epoch 240/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00240: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.1495 - val_accuracy: 0.9665\n",
      "Epoch 241/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743/774 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00241: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.1522 - val_accuracy: 0.9665\n",
      "Epoch 242/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9974\n",
      "Epoch 00242: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.1643 - val_accuracy: 0.9633\n",
      "Epoch 243/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 00243: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.1396 - val_accuracy: 0.9676\n",
      "Epoch 244/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 7.8444e-04 - accuracy: 1.0000\n",
      "Epoch 00244: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.8424e-04 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9695\n",
      "Epoch 245/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 8.3398e-04 - accuracy: 0.9999\n",
      "Epoch 00245: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.3398e-04 - accuracy: 0.9999 - val_loss: 0.1566 - val_accuracy: 0.9673\n",
      "Epoch 246/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0033 - accuracy: 0.9990\n",
      "Epoch 00246: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.1424 - val_accuracy: 0.9673\n",
      "Epoch 247/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9993\n",
      "Epoch 00247: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.1528 - val_accuracy: 0.9680\n",
      "Epoch 248/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9967\n",
      "Epoch 00248: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0095 - accuracy: 0.9967 - val_loss: 0.1413 - val_accuracy: 0.9658\n",
      "Epoch 249/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00249: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.1468 - val_accuracy: 0.9676\n",
      "Epoch 250/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9990\n",
      "Epoch 00250: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.1434 - val_accuracy: 0.9691\n",
      "Epoch 251/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 00251: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.1420 - val_accuracy: 0.9687\n",
      "Epoch 252/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 7.2335e-04 - accuracy: 1.0000\n",
      "Epoch 00252: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.1808e-04 - accuracy: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.9713\n",
      "Epoch 253/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 5.7560e-04 - accuracy: 1.0000\n",
      "Epoch 00253: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.6977e-04 - accuracy: 1.0000 - val_loss: 0.1527 - val_accuracy: 0.9662\n",
      "Epoch 254/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 4.6938e-04 - accuracy: 1.0000\n",
      "Epoch 00254: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.6898e-04 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9687\n",
      "Epoch 255/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 7.6737e-04 - accuracy: 0.9999\n",
      "Epoch 00255: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.6244e-04 - accuracy: 0.9999 - val_loss: 0.1525 - val_accuracy: 0.9698\n",
      "Epoch 256/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 00256: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.1808 - val_accuracy: 0.9596\n",
      "Epoch 257/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 00257: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.1558 - val_accuracy: 0.9655\n",
      "Epoch 258/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 8.6772e-04 - accuracy: 0.9999\n",
      "Epoch 00258: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.6187e-04 - accuracy: 0.9999 - val_loss: 0.1500 - val_accuracy: 0.9680\n",
      "Epoch 259/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 7.0070e-04 - accuracy: 1.0000\n",
      "Epoch 00259: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.0298e-04 - accuracy: 1.0000 - val_loss: 0.1488 - val_accuracy: 0.9676\n",
      "Epoch 260/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 5.0030e-04 - accuracy: 1.0000\n",
      "Epoch 00260: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.9437e-04 - accuracy: 1.0000 - val_loss: 0.1464 - val_accuracy: 0.9669\n",
      "Epoch 261/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 5.2160e-04 - accuracy: 1.0000\n",
      "Epoch 00261: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.2154e-04 - accuracy: 1.0000 - val_loss: 0.1404 - val_accuracy: 0.9702\n",
      "Epoch 262/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0111 - accuracy: 0.9964\n",
      "Epoch 00262: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9962 - val_loss: 0.1726 - val_accuracy: 0.9611\n",
      "Epoch 263/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9988\n",
      "Epoch 00263: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.2055 - val_accuracy: 0.9629\n",
      "Epoch 264/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 00264: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.1402 - val_accuracy: 0.9662\n",
      "Epoch 265/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00265: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.1484 - val_accuracy: 0.9680\n",
      "Epoch 266/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 7.9964e-04 - accuracy: 1.0000\n",
      "Epoch 00266: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.9016e-04 - accuracy: 1.0000 - val_loss: 0.1439 - val_accuracy: 0.9702\n",
      "Epoch 267/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00267: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.1843 - val_accuracy: 0.9578\n",
      "Epoch 268/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 7.6080e-04 - accuracy: 0.9999\n",
      "Epoch 00268: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.5639e-04 - accuracy: 0.9999 - val_loss: 0.1632 - val_accuracy: 0.9680\n",
      "Epoch 269/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994    \n",
      "Epoch 00269: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.1854 - val_accuracy: 0.9647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9990\n",
      "Epoch 00270: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.1497 - val_accuracy: 0.9676\n",
      "Epoch 271/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9970\n",
      "Epoch 00271: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.1450 - val_accuracy: 0.9702\n",
      "Epoch 272/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 00272: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.1571 - val_accuracy: 0.9684\n",
      "Epoch 273/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n",
      "Epoch 00273: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.3014 - val_accuracy: 0.9476\n",
      "Epoch 274/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9977\n",
      "Epoch 00274: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.1372 - val_accuracy: 0.9669\n",
      "Epoch 275/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998  \n",
      "Epoch 00275: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.1453 - val_accuracy: 0.9687\n",
      "Epoch 276/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 00276: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.1655 - val_accuracy: 0.9658\n",
      "Epoch 277/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00277: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.1495 - val_accuracy: 0.9673\n",
      "Epoch 278/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00278: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.1891 - val_accuracy: 0.9633\n",
      "Epoch 279/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9971\n",
      "Epoch 00279: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 0.1665 - val_accuracy: 0.9662\n",
      "Epoch 280/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 00280: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.1523 - val_accuracy: 0.9644\n",
      "Epoch 281/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9986\n",
      "Epoch 00281: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.1471 - val_accuracy: 0.9644\n",
      "Epoch 282/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 7.5339e-04 - accuracy: 1.0000\n",
      "Epoch 00282: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.5610e-04 - accuracy: 1.0000 - val_loss: 0.1455 - val_accuracy: 0.9669\n",
      "Epoch 283/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 8.5617e-04 - accuracy: 0.9999\n",
      "Epoch 00283: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.4776e-04 - accuracy: 0.9999 - val_loss: 0.1439 - val_accuracy: 0.9680\n",
      "Epoch 284/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9989\n",
      "Epoch 00284: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.1476 - val_accuracy: 0.9691\n",
      "Epoch 285/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 6.3111e-04 - accuracy: 0.9999\n",
      "Epoch 00285: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.2144e-04 - accuracy: 0.9999 - val_loss: 0.1506 - val_accuracy: 0.9709\n",
      "Epoch 286/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 6.2957e-04 - accuracy: 1.0000\n",
      "Epoch 00286: val_accuracy did not improve from 0.97127\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.1973e-04 - accuracy: 1.0000 - val_loss: 0.1542 - val_accuracy: 0.9698\n",
      "Epoch 287/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 8.9784e-04 - accuracy: 0.9998\n",
      "Epoch 00287: val_accuracy improved from 0.97127 to 0.97236, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.9979e-04 - accuracy: 0.9998 - val_loss: 0.1468 - val_accuracy: 0.9724\n",
      "Epoch 288/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9985\n",
      "Epoch 00288: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.1668 - val_accuracy: 0.9633\n",
      "Epoch 289/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9993\n",
      "Epoch 00289: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.1379 - val_accuracy: 0.9709\n",
      "Epoch 290/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 7.6439e-04 - accuracy: 1.0000\n",
      "Epoch 00290: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.5868e-04 - accuracy: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.9687\n",
      "Epoch 291/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 3.8565e-04 - accuracy: 1.0000\n",
      "Epoch 00291: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8414e-04 - accuracy: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.9687\n",
      "Epoch 292/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 3.9176e-04 - accuracy: 1.0000\n",
      "Epoch 00292: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.9159e-04 - accuracy: 1.0000 - val_loss: 0.1419 - val_accuracy: 0.9695\n",
      "Epoch 293/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 3.7419e-04 - accuracy: 1.0000\n",
      "Epoch 00293: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8768e-04 - accuracy: 1.0000 - val_loss: 0.1479 - val_accuracy: 0.9702\n",
      "Epoch 294/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9985\n",
      "Epoch 00294: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.1720 - val_accuracy: 0.9593\n",
      "Epoch 295/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9986\n",
      "Epoch 00295: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.1450 - val_accuracy: 0.9676\n",
      "Epoch 296/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 6.6019e-04 - accuracy: 0.9999\n",
      "Epoch 00296: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.5450e-04 - accuracy: 0.9999 - val_loss: 0.1474 - val_accuracy: 0.9644\n",
      "Epoch 297/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 4.4693e-04 - accuracy: 1.0000\n",
      "Epoch 00297: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.4297e-04 - accuracy: 1.0000 - val_loss: 0.1467 - val_accuracy: 0.9680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 3.8158e-04 - accuracy: 1.0000\n",
      "Epoch 00298: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8247e-04 - accuracy: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.9673\n",
      "Epoch 299/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 3.8697e-04 - accuracy: 1.0000\n",
      "Epoch 00299: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.9352e-04 - accuracy: 1.0000 - val_loss: 0.1500 - val_accuracy: 0.9684\n",
      "Epoch 300/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9994\n",
      "Epoch 00300: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.1544 - val_accuracy: 0.9680\n",
      "Epoch 301/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 7.1833e-04 - accuracy: 0.9998\n",
      "Epoch 00301: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.1805e-04 - accuracy: 0.9998 - val_loss: 0.1457 - val_accuracy: 0.9691\n",
      "Epoch 302/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 5.4610e-04 - accuracy: 0.9999\n",
      "Epoch 00302: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.4539e-04 - accuracy: 0.9999 - val_loss: 0.1547 - val_accuracy: 0.9702\n",
      "Epoch 303/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 5.0119e-04 - accuracy: 0.9999\n",
      "Epoch 00303: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.9421e-04 - accuracy: 0.9999 - val_loss: 0.1511 - val_accuracy: 0.9676\n",
      "Epoch 304/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 00304: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.1816 - val_accuracy: 0.9658\n",
      "Epoch 305/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00305: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.1569 - val_accuracy: 0.9676\n",
      "Epoch 306/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 5.3696e-04 - accuracy: 1.0000\n",
      "Epoch 00306: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.4453e-04 - accuracy: 1.0000 - val_loss: 0.1532 - val_accuracy: 0.9702\n",
      "Epoch 307/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 4.3509e-04 - accuracy: 1.0000\n",
      "Epoch 00307: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.3413e-04 - accuracy: 1.0000 - val_loss: 0.1485 - val_accuracy: 0.9695\n",
      "Epoch 308/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 5.6853e-04 - accuracy: 0.9999\n",
      "Epoch 00308: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.7558e-04 - accuracy: 0.9999 - val_loss: 0.1571 - val_accuracy: 0.9680\n",
      "Epoch 309/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 3.8985e-04 - accuracy: 1.0000\n",
      "Epoch 00309: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8682e-04 - accuracy: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.9713\n",
      "Epoch 310/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 3.9449e-04 - accuracy: 1.0000\n",
      "Epoch 00310: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.9459e-04 - accuracy: 1.0000 - val_loss: 0.1478 - val_accuracy: 0.9684\n",
      "Epoch 311/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 3.9157e-04 - accuracy: 1.0000\n",
      "Epoch 00311: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8853e-04 - accuracy: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.9680\n",
      "Epoch 312/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 3.2202e-04 - accuracy: 1.0000\n",
      "Epoch 00312: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.2202e-04 - accuracy: 1.0000 - val_loss: 0.1475 - val_accuracy: 0.9709\n",
      "Epoch 313/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 3.8112e-04 - accuracy: 1.0000\n",
      "Epoch 00313: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8014e-04 - accuracy: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.9669\n",
      "Epoch 314/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 5.4879e-04 - accuracy: 0.9999\n",
      "Epoch 00314: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5027e-04 - accuracy: 0.9999 - val_loss: 0.1421 - val_accuracy: 0.9709\n",
      "Epoch 315/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998  \n",
      "Epoch 00315: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.1588 - val_accuracy: 0.9684\n",
      "Epoch 316/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 8.9528e-04 - accuracy: 0.9998\n",
      "Epoch 00316: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.9003e-04 - accuracy: 0.9998 - val_loss: 0.1474 - val_accuracy: 0.9687\n",
      "Epoch 317/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 4.7187e-04 - accuracy: 1.0000\n",
      "Epoch 00317: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.6635e-04 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9705\n",
      "Epoch 318/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 7.7983e-04 - accuracy: 0.9998\n",
      "Epoch 00318: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.7544e-04 - accuracy: 0.9998 - val_loss: 0.1544 - val_accuracy: 0.9702\n",
      "Epoch 319/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9995\n",
      "Epoch 00319: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.1467 - val_accuracy: 0.9691\n",
      "Epoch 320/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 2.9452e-04 - accuracy: 1.0000\n",
      "Epoch 00320: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.9846e-04 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9687\n",
      "Epoch 321/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 3.1728e-04 - accuracy: 1.0000\n",
      "Epoch 00321: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.2041e-04 - accuracy: 1.0000 - val_loss: 0.1416 - val_accuracy: 0.9716\n",
      "Epoch 322/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9968\n",
      "Epoch 00322: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0092 - accuracy: 0.9968 - val_loss: 0.1436 - val_accuracy: 0.9684\n",
      "Epoch 323/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 00323: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.1437 - val_accuracy: 0.9705\n",
      "Epoch 324/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 5.6849e-04 - accuracy: 0.9999\n",
      "Epoch 00324: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.6922e-04 - accuracy: 0.9999 - val_loss: 0.1462 - val_accuracy: 0.9702\n",
      "Epoch 325/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 5.9263e-04 - accuracy: 0.9999\n",
      "Epoch 00325: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.0858e-04 - accuracy: 0.9999 - val_loss: 0.1515 - val_accuracy: 0.9680\n",
      "Epoch 326/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/774 [============================>.] - ETA: 0s - loss: 4.4729e-04 - accuracy: 1.0000\n",
      "Epoch 00326: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.4563e-04 - accuracy: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.9691\n",
      "Epoch 327/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 3.0776e-04 - accuracy: 1.0000\n",
      "Epoch 00327: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.0634e-04 - accuracy: 1.0000 - val_loss: 0.1521 - val_accuracy: 0.9676\n",
      "Epoch 328/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 4.3650e-04 - accuracy: 1.0000\n",
      "Epoch 00328: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.3412e-04 - accuracy: 1.0000 - val_loss: 0.1461 - val_accuracy: 0.9702\n",
      "Epoch 329/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 3.2983e-04 - accuracy: 1.0000\n",
      "Epoch 00329: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.3003e-04 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9684\n",
      "Epoch 330/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9993\n",
      "Epoch 00330: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.1561 - val_accuracy: 0.9676\n",
      "Epoch 331/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 7.0954e-04 - accuracy: 0.9999\n",
      "Epoch 00331: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.9950e-04 - accuracy: 0.9999 - val_loss: 0.1453 - val_accuracy: 0.9724\n",
      "Epoch 332/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 6.4927e-04 - accuracy: 0.9999\n",
      "Epoch 00332: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.4901e-04 - accuracy: 0.9999 - val_loss: 0.1563 - val_accuracy: 0.9687\n",
      "Epoch 333/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 8.1975e-04 - accuracy: 0.9998\n",
      "Epoch 00333: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.1724e-04 - accuracy: 0.9998 - val_loss: 0.1464 - val_accuracy: 0.9716\n",
      "Epoch 334/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 8.1663e-04 - accuracy: 0.9998\n",
      "Epoch 00334: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.0618e-04 - accuracy: 0.9998 - val_loss: 0.1486 - val_accuracy: 0.9709\n",
      "Epoch 335/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9981\n",
      "Epoch 00335: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.1540 - val_accuracy: 0.9658\n",
      "Epoch 336/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9978\n",
      "Epoch 00336: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.1545 - val_accuracy: 0.9636\n",
      "Epoch 337/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9983\n",
      "Epoch 00337: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.1893 - val_accuracy: 0.9622\n",
      "Epoch 338/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 00338: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.1515 - val_accuracy: 0.9673\n",
      "Epoch 339/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 9.5110e-04 - accuracy: 0.9999\n",
      "Epoch 00339: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.3811e-04 - accuracy: 0.9999 - val_loss: 0.1481 - val_accuracy: 0.9698\n",
      "Epoch 340/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 3.8287e-04 - accuracy: 1.0000\n",
      "Epoch 00340: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.8612e-04 - accuracy: 1.0000 - val_loss: 0.1434 - val_accuracy: 0.9702\n",
      "Epoch 341/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 3.4098e-04 - accuracy: 1.0000\n",
      "Epoch 00341: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.3535e-04 - accuracy: 1.0000 - val_loss: 0.1505 - val_accuracy: 0.9698\n",
      "Epoch 342/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 4.1760e-04 - accuracy: 1.0000\n",
      "Epoch 00342: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.2066e-04 - accuracy: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.9662\n",
      "Epoch 343/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 3.1574e-04 - accuracy: 1.0000\n",
      "Epoch 00343: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.1053e-04 - accuracy: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.9705\n",
      "Epoch 344/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 2.5162e-04 - accuracy: 1.0000\n",
      "Epoch 00344: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 2.5532e-04 - accuracy: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.9687\n",
      "Epoch 345/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 4.7217e-04 - accuracy: 0.9999\n",
      "Epoch 00345: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7185e-04 - accuracy: 0.9999 - val_loss: 0.1586 - val_accuracy: 0.9676\n",
      "Epoch 346/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 4.1413e-04 - accuracy: 0.9999\n",
      "Epoch 00346: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.0913e-04 - accuracy: 0.9999 - val_loss: 0.1428 - val_accuracy: 0.9702\n",
      "Epoch 347/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 3.5667e-04 - accuracy: 1.0000\n",
      "Epoch 00347: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.5025e-04 - accuracy: 0.9999 - val_loss: 0.1567 - val_accuracy: 0.9655\n",
      "Epoch 348/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 3.2537e-04 - accuracy: 1.0000\n",
      "Epoch 00348: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.2605e-04 - accuracy: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.9676\n",
      "Epoch 349/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00349: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.1573 - val_accuracy: 0.9702\n",
      "Epoch 350/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 3.9536e-04 - accuracy: 1.0000\n",
      "Epoch 00350: val_accuracy did not improve from 0.97236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 3.9216e-04 - accuracy: 1.0000 - val_loss: 0.1450 - val_accuracy: 0.9702\n"
     ]
    }
   ],
   "source": [
    "batch_size_ANN = 32\n",
    "epochs_ANN     = 350\n",
    "\n",
    "history_ANN    = model_ANN.fit(X_train, y_train_OHEV,\n",
    "                               batch_size      = batch_size_ANN,\n",
    "                               epochs          = epochs_ANN,\n",
    "                               verbose         = 1,\n",
    "                               validation_data = (X_test, y_test_OHEV),\n",
    "                               callbacks       = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "337ca30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1449861079454422\n",
      "Test accuracy: 0.9701818227767944\n"
     ]
    }
   ],
   "source": [
    "score_ANN = model_ANN.evaluate(X_test, y_test_OHEV, verbose=0, batch_size = 32)\n",
    "print('Test loss:', score_ANN[0])\n",
    "print('Test accuracy:', score_ANN[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84b5d2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9701818227767944"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ANN[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "864b45b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "fig.suptitle(nom_dataset + norm_type + model_surname + ' - ANN - Training / Testing loss and accuracy', fontsize = 18)\n",
    "ax[0].plot(history_ANN.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history_ANN.history['val_loss'], color='r', label=\"Testing loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True, fontsize = 14)\n",
    "ax[0].tick_params(axis='x', labelsize=14)\n",
    "ax[0].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "ax[1].plot(history_ANN.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history_ANN.history['val_accuracy'], color='r',label=\"Testing accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True, fontsize = 14)\n",
    "ax[1].tick_params(axis='x', labelsize=14)\n",
    "ax[1].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(os.path.join(path_pic, picture_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1723f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file (not the best model though)\n",
    "\n",
    "#model_ANN.save(path_models + \"Model_ANN.h5\")\n",
    "#print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4c8a5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ANN = np.argmax(model_ANN.predict(X_val_norm),axis=1)\n",
    "y_pred_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c71e5f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc = np.argmax(y_OHEV_val, axis=1)\n",
    "y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b650a54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.86      0.82      0.84       756\n",
      "        car_horn       0.86      0.94      0.90       252\n",
      "children_playing       0.77      0.81      0.79       700\n",
      "        dog_bark       0.77      0.85      0.81       700\n",
      "           siren       0.88      0.75      0.81       602\n",
      "\n",
      "        accuracy                           0.82      3010\n",
      "       macro avg       0.83      0.83      0.83      3010\n",
      "    weighted avg       0.82      0.82      0.82      3010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_set_ANN = classification_report(y_test_enc, y_pred_ANN, target_names=nom_classes)\n",
    "print(metrics_set_ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07704de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ANN_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 375)               141000    \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 375)               141000    \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               282000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 567,755\n",
      "Trainable params: 567,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the highest accuracy\n",
    "\n",
    "model_ANN_saved = load_model(os.path.join(path_models, 'Model_ANN_weights_0_best' + norm_type + model_surname + '.hdf5'))\n",
    "model_ANN_saved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "690dfa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 3ms/step - loss: 2.5394 - accuracy: 0.8219\n",
      "Test loss: 2.539357900619507\n",
      "Test accuracy: 0.8219268918037415\n"
     ]
    }
   ],
   "source": [
    "score_ANN_saved = model_ANN_saved.evaluate(X_val_norm, y_OHEV_val, verbose=1, batch_size = 32)\n",
    "print('Test loss:', score_ANN_saved[0])\n",
    "print('Test accuracy:', score_ANN_saved[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "62882b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ANN_saved = np.argmax(model_ANN_saved.predict(X_val_norm),axis=1)\n",
    "y_pred_ANN_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eceba4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.88      0.80      0.84       756\n",
      "        car_horn       0.85      0.94      0.89       252\n",
      "children_playing       0.75      0.85      0.80       700\n",
      "        dog_bark       0.79      0.84      0.81       700\n",
      "           siren       0.89      0.74      0.81       602\n",
      "\n",
      "        accuracy                           0.82      3010\n",
      "       macro avg       0.83      0.84      0.83      3010\n",
      "    weighted avg       0.83      0.82      0.82      3010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_set_ANN_saved = classification_report(y_test_enc, y_pred_ANN_saved, target_names=nom_classes)\n",
    "print(metrics_set_ANN_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ade4c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple confusion matrix\n",
    "\n",
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(y_test_enc, y_pred_ANN_saved)\n",
    "title = nom_dataset + norm_type + model_surname + ' - Classifier ANN (best model) - Highest accuracy test: '+ str(\"{:0.2f}%\".format(score_ANN_saved[1]*100))\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True, \n",
    "            fmt='g', \n",
    "            cmap=cmap_cm, \n",
    "            annot_kws={\"size\": 8}, \n",
    "            xticklabels=nom_classes, \n",
    "            yticklabels=nom_classes)\n",
    "plt.title(title, fontsize = 12)\n",
    "plt.savefig(os.path.join(path_pic, picture_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca7b9cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x2667af52fa0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2667ad0c8b0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x265c3f8d430>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x265cc999d00>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x2667ac07e80>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x265cc9998e0>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ANN_saved.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f108fca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00617652,  0.0074141 , -0.03259105, ..., -0.06934977,\n",
      "        -0.01211499,  0.01012983],\n",
      "       [-0.03593363, -0.02493946, -0.10158896, ...,  0.0765382 ,\n",
      "         0.1033669 ,  0.06684168],\n",
      "       [-0.06668829,  0.0580395 ,  0.06656573, ..., -0.08797868,\n",
      "        -0.02697253, -0.01941396],\n",
      "       ...,\n",
      "       [-0.0320451 , -0.00680774, -0.0255002 , ..., -0.02849849,\n",
      "         0.00094115,  0.10453606],\n",
      "       [-0.02970457, -0.00199844, -0.03100302, ..., -0.00411687,\n",
      "         0.03040912, -0.05984844],\n",
      "       [ 0.0093646 , -0.0233783 , -0.06762841, ..., -0.06590807,\n",
      "         0.07432549,  0.06687903]], dtype=float32), array([-2.28268933e-02, -5.85257914e-03,  2.55066925e-03,  5.00838272e-03,\n",
      "        9.50783119e-03,  1.35360453e-02, -2.30955202e-02, -1.17225489e-02,\n",
      "        1.52969155e-02,  1.35888197e-02,  9.24894819e-04, -1.77176762e-02,\n",
      "       -2.55466998e-02, -5.91116445e-03,  7.46766711e-03, -6.15090504e-03,\n",
      "       -6.68980600e-03, -2.92973183e-02, -7.04758521e-03, -1.94932881e-03,\n",
      "        1.71450339e-02,  2.23674793e-02,  1.75733697e-02, -8.64203647e-03,\n",
      "        6.16869750e-03, -2.03479710e-03, -1.45341596e-02, -4.48291935e-03,\n",
      "        1.81114133e-02, -1.81338505e-03, -1.21754035e-02,  9.60743800e-03,\n",
      "       -2.33934894e-02,  1.92698576e-02, -2.05864646e-02, -4.91585757e-04,\n",
      "       -1.32417437e-02,  7.51712592e-03, -1.57306213e-02, -7.98397418e-03,\n",
      "       -8.77936743e-03,  9.85896215e-03, -1.64980348e-02,  2.33837939e-03,\n",
      "       -8.86889640e-03,  2.16185432e-02, -1.38692223e-02,  8.44045077e-03,\n",
      "       -2.26987582e-02, -1.85198002e-02, -1.57864410e-02,  6.97769178e-03,\n",
      "        4.75226808e-03, -4.79091238e-03,  1.68738719e-02, -1.08679831e-02,\n",
      "       -7.71794468e-03,  8.64897203e-03,  1.39864972e-02, -3.78206815e-03,\n",
      "        2.16686055e-02,  2.03909259e-03,  2.18608038e-04, -2.56566238e-03,\n",
      "       -3.44142802e-02, -1.45007465e-02, -7.94594362e-03,  8.86696298e-03,\n",
      "       -2.03132234e-03,  1.23478901e-02, -1.04277069e-02,  9.46204411e-04,\n",
      "       -1.40569666e-02, -1.48063982e-02,  2.87772692e-03, -1.62974931e-02,\n",
      "        2.17258511e-03,  2.69429348e-02,  7.00229406e-03, -4.74966038e-03,\n",
      "        7.55302096e-03,  9.03267879e-03,  1.74777210e-02, -1.15096048e-02,\n",
      "       -3.90363438e-03, -1.62986405e-02, -1.87004115e-02,  2.59788241e-03,\n",
      "       -1.91522893e-02,  1.19230151e-02,  2.22035265e-03, -1.22226030e-02,\n",
      "       -1.54339857e-02, -2.53888182e-02,  9.95597616e-03,  1.14773994e-03,\n",
      "        7.63661694e-04, -7.68949324e-03, -1.76492508e-03,  2.25856807e-02,\n",
      "        2.34998734e-04,  1.31902780e-05, -4.67061810e-03,  1.40648317e-02,\n",
      "        5.60385128e-03, -1.55773191e-02, -8.90182145e-03, -1.43361129e-02,\n",
      "        3.50839971e-03, -2.48034056e-02, -2.29387134e-02, -9.29482747e-03,\n",
      "        1.58763528e-02, -1.31805837e-02,  1.96628403e-02, -1.07515361e-02,\n",
      "       -5.41983964e-03, -1.00434003e-02,  3.22584361e-02, -2.74030808e-02,\n",
      "        7.59096863e-03, -1.44436397e-03,  1.37800979e-03,  1.31462766e-02,\n",
      "       -1.13576222e-02,  7.01252883e-03, -1.21939892e-03,  1.66387502e-02,\n",
      "       -3.61562450e-03, -1.08133545e-02,  1.89183075e-02, -1.64619349e-02,\n",
      "        5.51668787e-03, -3.12011950e-02, -1.04935812e-02,  2.33904831e-02,\n",
      "        6.12662127e-03,  1.47076603e-02, -1.64868124e-02,  3.77162057e-03,\n",
      "       -3.40509266e-02, -4.38840687e-03, -2.24018488e-02,  8.60654004e-03,\n",
      "       -1.52099142e-02, -1.08309807e-02,  1.47877512e-02,  2.04226747e-03,\n",
      "        1.30829876e-02,  8.13384354e-03, -1.23880543e-02,  1.69094652e-02,\n",
      "        6.22540293e-03,  2.28799637e-02, -1.24318358e-02,  7.99957011e-03,\n",
      "        1.13858012e-02,  9.99668892e-03, -1.00898277e-02,  5.16233500e-03,\n",
      "        3.93441645e-03,  8.53633136e-03,  7.35582225e-03, -2.72924709e-03,\n",
      "       -1.42175341e-02,  2.81699526e-04, -1.15008838e-02, -8.08673538e-03,\n",
      "       -1.32646468e-02, -2.86102071e-02,  3.25148203e-03,  7.72477873e-03,\n",
      "       -7.16876751e-03, -1.38910841e-02, -2.33342294e-02,  5.45070041e-03,\n",
      "       -2.28976440e-02, -7.41127413e-03, -4.04866412e-03, -3.12161190e-03,\n",
      "        1.40594300e-02, -2.37291045e-02,  1.05981785e-03,  3.26745072e-03,\n",
      "       -4.29008901e-03,  2.80691590e-03, -8.63034744e-03, -1.41734409e-03,\n",
      "       -1.74058191e-02, -1.47618195e-02,  2.24441383e-02, -2.83185393e-02,\n",
      "       -5.28854365e-03, -3.76880094e-02,  3.35457958e-02,  7.95031432e-03,\n",
      "       -3.15991440e-03, -2.10074093e-02,  6.43641362e-03, -8.55795760e-03,\n",
      "        1.36946328e-02,  4.09393664e-03,  1.37949977e-02, -1.24741578e-02,\n",
      "       -1.07939839e-02,  9.22083948e-03, -4.43468429e-03,  9.46350116e-03,\n",
      "        3.22718220e-03,  2.88175400e-02,  2.11876798e-02, -1.23161520e-03,\n",
      "       -1.90569628e-02,  1.48947979e-03, -2.23671794e-02,  1.21662708e-03,\n",
      "        2.11749622e-03,  1.71383061e-02, -2.14133295e-03, -2.53730286e-02,\n",
      "        2.20515556e-03,  1.20407436e-02, -1.42226201e-02, -5.73080964e-03,\n",
      "        1.16196126e-02,  1.02740694e-02,  2.67860759e-02, -5.45964809e-03,\n",
      "        5.46512427e-03, -6.94378186e-03, -1.20427343e-04,  7.70627661e-03,\n",
      "        4.07283287e-03,  6.21364219e-03, -7.62895495e-03, -2.16190238e-02,\n",
      "        4.22170432e-03, -2.01694425e-02,  8.27047229e-03, -2.98842117e-02,\n",
      "        7.61758656e-06,  1.76219102e-02,  1.33732939e-02, -1.30704856e-02,\n",
      "        1.17555808e-03, -5.20797540e-03, -1.73522215e-02, -4.12088297e-02,\n",
      "       -8.98503605e-03, -1.53760221e-02,  3.57188378e-03, -1.60700676e-03,\n",
      "       -9.74354148e-03, -1.41052464e-02, -2.19850428e-03, -1.47672817e-02,\n",
      "       -8.51284340e-03, -3.31403613e-02, -1.34532461e-02,  1.18327122e-02,\n",
      "       -1.47016556e-03,  4.42652404e-03, -1.45993084e-02,  3.01889703e-02,\n",
      "        1.01549923e-02, -4.73004859e-03, -6.43518381e-03,  2.13673376e-02,\n",
      "       -3.56793124e-03,  1.74514037e-02, -3.22174653e-02,  3.35633568e-03,\n",
      "        2.02408358e-02, -3.22093116e-03, -3.00025921e-02,  6.72062254e-03,\n",
      "        1.40002994e-02,  1.24933142e-02,  2.07280088e-03, -2.58876551e-02,\n",
      "        2.98176724e-02,  6.52495259e-03, -2.17055138e-02, -1.98227819e-02,\n",
      "       -7.23863440e-03, -1.62258539e-02,  9.98727884e-03, -2.05513537e-02,\n",
      "       -3.86232627e-03,  5.43096708e-03, -1.10858222e-02, -2.80801561e-02,\n",
      "       -2.78397743e-02,  6.41353894e-03,  3.22338706e-03, -3.49337538e-03,\n",
      "        2.73336889e-03, -4.78291325e-03,  1.03021469e-02,  7.62612838e-03,\n",
      "        3.13204131e-03, -3.87810008e-03, -7.96043966e-03, -2.46676113e-02,\n",
      "       -8.69626366e-03,  4.08634823e-03, -2.27845330e-02,  1.07731381e-02,\n",
      "       -1.92330219e-02, -3.38848941e-02,  1.17693832e-02,  7.98902940e-03,\n",
      "       -1.50189335e-02,  9.70975868e-03,  1.21068247e-02,  6.57001568e-04,\n",
      "        4.09414200e-03, -3.88017995e-03, -3.32713104e-03, -1.41844274e-02,\n",
      "       -1.22299865e-02, -3.43145570e-03, -2.11272202e-03,  9.96117108e-03,\n",
      "       -8.19498301e-03, -1.11388741e-02,  1.17251892e-02, -3.04401815e-02,\n",
      "       -6.57775672e-03,  1.54855689e-02, -3.64130177e-03, -1.14101116e-02,\n",
      "        2.11344585e-02, -5.80165675e-03, -1.39395539e-02, -2.40783785e-02,\n",
      "        6.36191526e-03,  5.25757717e-03,  2.32667569e-02, -9.47212707e-03,\n",
      "       -2.06790492e-03, -3.14208455e-02,  1.09539544e-02, -1.70275997e-02,\n",
      "        3.96507094e-03,  8.08050297e-03,  5.04740328e-03,  2.71598483e-03,\n",
      "       -1.44701703e-02, -1.40110664e-02, -1.58759989e-02, -8.95635132e-03,\n",
      "       -1.48921283e-02, -4.67709405e-03,  1.16881328e-02, -1.00357579e-02,\n",
      "        5.53893903e-03,  1.23343337e-02, -2.15167496e-02,  1.94871146e-02,\n",
      "        1.45304147e-02,  3.38823535e-02,  1.63907446e-02,  1.26248477e-02,\n",
      "        6.64276676e-03, -9.06180590e-03, -1.62998736e-02,  1.24503728e-02,\n",
      "        1.40564665e-02, -9.13127139e-03, -1.34859039e-02, -1.85982380e-02,\n",
      "       -1.85761391e-03,  7.32009718e-03,  1.86118914e-03], dtype=float32)]\n",
      "[array([[ 0.021144  ,  0.0270822 , -0.03237258, ...,  0.03728148,\n",
      "        -0.00142959, -0.08435362],\n",
      "       [-0.0299642 ,  0.02903894, -0.03666255, ...,  0.07572623,\n",
      "         0.026222  ,  0.08280992],\n",
      "       [ 0.02628736,  0.07408725, -0.01260204, ...,  0.05316445,\n",
      "        -0.00094219, -0.02453404],\n",
      "       ...,\n",
      "       [ 0.04366888, -0.02644758,  0.11814125, ..., -0.04778031,\n",
      "        -0.01923196, -0.02676605],\n",
      "       [-0.07927885,  0.06618554, -0.05820944, ..., -0.09138871,\n",
      "        -0.06012042,  0.01990888],\n",
      "       [ 0.02383763, -0.03943063, -0.00036387, ...,  0.01450938,\n",
      "         0.06335565, -0.0940685 ]], dtype=float32), array([ 7.89309200e-03, -1.40022598e-02,  1.17403520e-02,  3.21331061e-03,\n",
      "       -1.13011571e-02,  5.68192778e-03,  1.65441521e-02,  3.75855668e-03,\n",
      "       -1.16423275e-02,  7.05443276e-03, -3.48308799e-03, -1.37785403e-02,\n",
      "       -1.42509360e-02, -1.48534188e-02, -1.86673203e-03,  1.64631419e-02,\n",
      "       -8.40573994e-05, -2.46666139e-03,  1.44814122e-02, -1.35431625e-03,\n",
      "        1.18327187e-02,  5.85902994e-03,  1.70960289e-03, -6.94701169e-03,\n",
      "        4.41516237e-03, -4.80433041e-03,  2.36206083e-03, -7.95563776e-03,\n",
      "        2.37006359e-02, -1.64471623e-02, -1.92889583e-03,  7.23135006e-03,\n",
      "        1.26525089e-02,  1.68092102e-02, -2.07124394e-03,  5.35735488e-03,\n",
      "       -5.46275172e-03, -5.24038309e-03,  7.60498596e-03, -1.19773094e-02,\n",
      "       -1.41607150e-02, -2.16783723e-03,  2.77907850e-04,  1.56846531e-02,\n",
      "        1.66901313e-02,  3.67426663e-03, -1.48228789e-02, -2.67858291e-03,\n",
      "        8.48066714e-03, -8.63469299e-03,  7.31010176e-03,  1.24918725e-02,\n",
      "       -6.35323441e-03,  9.30061098e-03, -2.43743270e-04, -9.46633052e-03,\n",
      "        1.63658205e-02, -1.04640927e-02,  2.00790912e-02, -7.75106344e-03,\n",
      "        1.56150302e-02,  1.35008460e-02,  2.38666423e-02, -2.97756959e-03,\n",
      "       -6.56994432e-03, -6.14331570e-03,  2.16032518e-03, -8.88533890e-03,\n",
      "       -7.72389490e-03, -4.73969383e-03,  2.33778916e-03, -9.38050263e-03,\n",
      "       -4.82393755e-03,  1.33166267e-02,  7.17735523e-03, -8.08468368e-03,\n",
      "       -8.67081340e-03,  2.00111177e-02,  8.43252894e-03, -5.00622811e-03,\n",
      "        1.06544578e-02,  1.00547271e-02, -4.19574324e-03,  6.03030762e-03,\n",
      "        6.06002379e-03, -1.15550095e-02,  8.01776815e-03,  9.52505227e-03,\n",
      "        4.16857144e-03,  7.49039138e-03,  1.83868024e-03, -2.83185067e-03,\n",
      "        1.47726210e-02,  5.87709108e-03, -9.41152778e-03,  2.11885553e-02,\n",
      "       -8.50009732e-03, -2.58753239e-03,  3.67930625e-03,  8.51850212e-03,\n",
      "        9.67371464e-03,  8.59086681e-03, -5.79833193e-03, -8.96675792e-03,\n",
      "        1.27376821e-02, -7.04163406e-03,  3.68882320e-04,  1.10469665e-02,\n",
      "        5.57410903e-03, -7.46402191e-03,  1.00057805e-02,  2.97109643e-03,\n",
      "        1.19388234e-02,  9.77329258e-03, -1.03570493e-02, -1.08843297e-03,\n",
      "        4.22967644e-03,  1.63318180e-02,  8.14221450e-04,  1.98352896e-02,\n",
      "       -2.35121566e-04, -5.67321759e-03,  4.98384936e-03,  2.06378242e-03,\n",
      "        1.67652089e-02,  1.12975175e-02,  2.68767960e-03,  9.42333695e-03,\n",
      "       -2.10912316e-03, -2.36537587e-03,  1.04839234e-02,  8.00974946e-03,\n",
      "        1.85751580e-02,  2.07507014e-02, -7.88532291e-03,  1.08645605e-02,\n",
      "       -4.72442945e-03,  6.19271444e-03, -4.68121096e-03, -6.38506003e-03,\n",
      "       -6.73994888e-03,  1.32794250e-02,  1.04423696e-02,  8.88399035e-03,\n",
      "       -1.73192937e-03,  5.75228885e-04, -2.88046547e-03, -4.82039154e-03,\n",
      "       -3.57688870e-03,  1.55141447e-02,  3.84304649e-03,  3.98443267e-03,\n",
      "       -1.28240662e-03, -1.39945233e-02,  1.02927741e-02, -1.71426013e-02,\n",
      "       -6.65408373e-03, -1.75015954e-03, -7.43700308e-04,  1.06238890e-02,\n",
      "        3.48064373e-03, -1.20891584e-02,  1.80728436e-02,  4.82550636e-03,\n",
      "        1.25022633e-02, -1.42066851e-02,  6.68810122e-03, -1.29292579e-02,\n",
      "        1.92711279e-02,  1.91173423e-02,  9.38301813e-03, -1.05811143e-03,\n",
      "        5.44928433e-03, -2.30163964e-03, -1.49481383e-03, -1.54952724e-02,\n",
      "       -5.71985962e-03,  2.45765485e-02,  1.44597711e-02, -2.91771314e-04,\n",
      "        1.54378153e-02, -6.35130284e-03,  1.03218695e-02,  1.48740003e-03,\n",
      "       -8.38134717e-03,  5.20578120e-03,  1.11642331e-02,  9.83380713e-03,\n",
      "        8.31695565e-04,  8.07384867e-03, -3.33379884e-03,  5.30926837e-03,\n",
      "        8.13123863e-03, -7.28566898e-04, -1.43632793e-03,  1.54142305e-02,\n",
      "       -8.87828041e-03, -4.20744065e-03,  1.04712462e-02,  1.47204334e-02,\n",
      "       -1.02196609e-04, -1.13398544e-02,  4.41482896e-03,  1.44963553e-02,\n",
      "        1.46189937e-03,  2.49284301e-02,  4.19212360e-04,  1.53707862e-02,\n",
      "        2.07142904e-02,  2.44727987e-03, -5.21352654e-03,  4.38417448e-03,\n",
      "       -5.06644696e-03,  4.92337998e-03,  1.92551985e-02,  1.23633463e-02,\n",
      "       -1.05750589e-02,  4.37867362e-03,  1.54794538e-02,  1.01454994e-02,\n",
      "       -1.09619349e-02, -7.11897854e-03,  9.92340781e-03, -1.41089933e-03,\n",
      "        7.04727089e-03, -1.15669332e-02,  3.00460192e-03,  2.03858223e-03,\n",
      "        6.43136725e-03,  1.06127849e-02, -1.57226771e-02,  7.92099908e-03,\n",
      "        7.45553942e-03,  1.87142137e-02, -1.11780604e-02, -4.66506882e-03,\n",
      "        1.33708864e-02,  2.32434482e-03, -1.24464761e-02, -2.54599145e-03,\n",
      "       -9.17050615e-03, -1.69484454e-04,  1.58894081e-02,  1.44680114e-02,\n",
      "        4.35373606e-03,  6.51151233e-04,  7.73173105e-03,  6.19007554e-03,\n",
      "        2.55814265e-03,  1.40557457e-02,  2.97387782e-03,  1.81882307e-02,\n",
      "        2.13193870e-03,  1.06999502e-02,  9.18102544e-03,  1.15261618e-02,\n",
      "       -1.35983806e-03,  1.78869453e-03,  1.18392031e-03,  7.30953412e-03,\n",
      "       -2.12591309e-02, -1.80892542e-03,  1.07310228e-02,  1.70600973e-02,\n",
      "        1.73648447e-02, -1.70884887e-03,  3.19551909e-03,  4.93190018e-03,\n",
      "        9.31285229e-03,  7.68507458e-03,  1.02293706e-02, -5.15451189e-03,\n",
      "        1.84458401e-02, -4.65948891e-04,  1.41237816e-02, -5.49185649e-03,\n",
      "        1.09694749e-02,  2.54045124e-03, -1.32330391e-03, -9.54024121e-03,\n",
      "        7.34565267e-03,  1.71113219e-02, -8.89308006e-03,  1.25604570e-02,\n",
      "        6.80330582e-03,  4.10257373e-03,  1.53143685e-02, -8.17944482e-03,\n",
      "        2.00860240e-02,  1.38868263e-03,  2.32904870e-02,  1.46188242e-02,\n",
      "        3.03739030e-03, -2.91061727e-03, -7.69352959e-03, -1.88308884e-03,\n",
      "        1.19770747e-02,  2.31894981e-02,  1.23117873e-02, -4.46435716e-03,\n",
      "        6.17648661e-03, -7.51737447e-04,  1.04076667e-02,  1.50430566e-02,\n",
      "       -5.10146469e-03,  1.21253571e-02,  9.24020598e-04,  3.62291606e-03,\n",
      "        4.38280264e-03, -2.93877930e-03, -7.93533027e-03,  1.15909185e-02,\n",
      "       -4.23541339e-03, -2.13190890e-03, -9.17355809e-03,  1.45661728e-02,\n",
      "        1.10636307e-02,  2.26117228e-03, -5.77834761e-03, -5.06884744e-03,\n",
      "       -1.04447724e-02,  2.91805179e-03,  3.78410029e-03, -2.32625240e-03,\n",
      "       -4.26576938e-03,  1.31077319e-03, -7.29736360e-03,  9.63173865e-04,\n",
      "       -5.58386184e-03,  9.68614686e-03, -2.77969381e-03,  1.35085648e-02,\n",
      "        7.62436946e-04, -1.05307214e-02, -1.72560997e-02, -1.32712023e-02,\n",
      "        1.12606054e-02,  1.79198105e-02, -1.19763715e-02,  1.74658373e-02,\n",
      "        6.54150080e-03, -9.54269152e-03, -8.19744915e-03,  5.86203858e-03,\n",
      "        1.27495162e-03,  5.17947739e-03,  1.15920408e-02,  1.32647520e-02,\n",
      "        1.25736173e-03,  1.02378996e-02,  5.30246599e-03, -4.06065537e-03,\n",
      "        1.27001256e-02,  4.90234094e-03,  9.39501263e-03,  2.01955065e-03,\n",
      "        4.51208238e-04,  1.98721723e-03,  1.50184156e-02, -1.47743579e-02,\n",
      "       -1.19836451e-02,  6.10336335e-03, -2.28808052e-03,  1.75215658e-02,\n",
      "       -3.90488142e-03, -7.64405355e-03, -1.39492918e-02,  2.59047607e-03,\n",
      "        1.06465220e-02, -2.39930372e-03, -6.96906121e-03, -1.06507877e-03,\n",
      "        1.27475299e-02, -8.86341091e-03,  3.19602521e-04], dtype=float32)]\n",
      "[]\n",
      "[array([[ 0.01829996, -0.00352219, -0.00555539, ...,  0.01426842,\n",
      "         0.09006909,  0.02076388],\n",
      "       [ 0.07783809, -0.00263364, -0.07539979, ...,  0.03192178,\n",
      "        -0.00695989,  0.01888257],\n",
      "       [-0.09425826,  0.05104906,  0.0446612 , ...,  0.02073849,\n",
      "         0.01288175, -0.01094793],\n",
      "       ...,\n",
      "       [-0.02612575,  0.0043136 , -0.02611316, ..., -0.03569854,\n",
      "         0.01294735,  0.03362192],\n",
      "       [ 0.06642744,  0.03078956, -0.03781603, ...,  0.05843351,\n",
      "         0.04952642,  0.03112462],\n",
      "       [ 0.06055261, -0.0329772 , -0.03511037, ..., -0.06415141,\n",
      "         0.01443851,  0.02527911]], dtype=float32), array([-1.18764758e-03,  2.01201029e-02, -9.31669772e-03,  3.74821899e-03,\n",
      "        6.56139292e-03,  1.26112690e-02,  1.70369409e-02,  3.81847203e-04,\n",
      "        3.67214298e-03,  2.11981181e-02,  1.77662689e-02, -6.06924482e-03,\n",
      "        1.50098447e-02,  5.24753658e-03,  8.50061513e-03,  1.24626830e-02,\n",
      "        2.35683136e-02,  1.27257053e-02,  3.15185473e-03, -9.48293041e-03,\n",
      "        7.73580093e-03,  1.99318342e-02,  8.30825791e-03,  1.30771855e-02,\n",
      "        2.37006471e-02,  7.03307241e-03,  1.97811611e-02,  5.39942970e-03,\n",
      "        2.05947403e-02,  6.81589078e-03, -1.46981352e-03,  1.03442771e-02,\n",
      "        1.61429420e-02,  2.87283066e-04,  2.61019934e-02, -5.44213504e-03,\n",
      "       -1.88369697e-04,  1.31627806e-02,  1.34740304e-02,  2.15058774e-02,\n",
      "        2.12772079e-02,  1.92164071e-02, -4.66342177e-03, -7.82154221e-03,\n",
      "        1.04338555e-02,  8.24140967e-04,  5.43264672e-03, -3.92977241e-03,\n",
      "       -6.26099715e-03,  1.41994040e-02, -2.51792558e-03,  1.68040954e-02,\n",
      "        9.88563709e-03,  1.11354450e-02,  7.98110943e-03,  6.32982003e-03,\n",
      "        5.32127777e-03,  3.37154255e-03,  1.31125310e-02,  1.71715822e-02,\n",
      "       -1.78316885e-04,  8.81582219e-03,  1.51619175e-02,  1.52891893e-02,\n",
      "        4.02465556e-03,  1.18794283e-02,  1.04094883e-02,  1.86382886e-02,\n",
      "        2.20194533e-02, -7.08513241e-03,  3.32919811e-03,  1.61618851e-02,\n",
      "        2.31141895e-02,  1.65557358e-02,  1.88679877e-03,  2.67198645e-02,\n",
      "        2.25237627e-02,  1.73790995e-02,  2.83505451e-02, -5.74923935e-04,\n",
      "        6.93356665e-03,  1.06453635e-02,  2.13254690e-02,  1.69106089e-02,\n",
      "        3.12743746e-02,  1.24231577e-02,  2.94455774e-02,  1.47855291e-02,\n",
      "        6.09025266e-03,  1.58552937e-02, -2.54009035e-03, -7.33037433e-03,\n",
      "        3.15674511e-03,  7.91361276e-03,  9.46199335e-03,  1.78751815e-02,\n",
      "        1.27166091e-02,  1.20587181e-02,  7.49096833e-03,  1.72635913e-02,\n",
      "        2.39531137e-02,  1.02165807e-02,  9.40359384e-03,  4.23378916e-03,\n",
      "       -1.28864334e-03,  1.98404975e-02,  1.73921194e-02,  4.48315963e-03,\n",
      "        2.10350063e-02,  5.71935624e-03,  1.52660832e-02,  1.61333438e-02,\n",
      "        1.28717264e-02,  2.80265417e-02,  4.96004196e-03,  2.58940738e-02,\n",
      "        1.23525942e-02,  2.53823120e-02,  4.98117320e-03, -7.06269452e-03,\n",
      "        2.08503678e-02,  1.34698264e-02,  1.76391192e-02,  3.02045289e-02,\n",
      "        1.85705759e-02,  2.35115625e-02, -3.64048476e-03,  1.96897499e-02,\n",
      "        4.89235297e-03,  1.88497566e-02, -4.29859385e-03,  2.56008357e-02,\n",
      "        2.39162128e-02,  8.51054210e-03,  2.20166240e-02, -1.63918932e-03,\n",
      "        8.08632374e-03,  3.95803479e-03,  5.53293293e-03, -9.72544402e-03,\n",
      "        1.06572108e-02,  1.48270959e-02,  1.56214433e-02,  2.30858400e-02,\n",
      "       -3.81261180e-03,  1.14853932e-02,  1.74107519e-03, -3.98262497e-03,\n",
      "       -1.64498307e-03,  1.93194728e-02,  1.21990107e-02,  1.23719648e-02,\n",
      "        2.30420809e-02,  1.63953546e-02, -1.42874592e-03, -3.53357755e-03,\n",
      "        1.16400365e-02,  1.28931105e-02, -7.94940349e-03,  1.45580154e-02,\n",
      "        2.11905688e-02,  2.22650394e-02,  1.46247735e-02,  2.81842258e-02,\n",
      "        3.66409984e-03,  2.17889324e-02,  2.75286613e-03,  4.26566927e-03,\n",
      "        5.32706734e-03,  3.87213100e-03,  2.02490017e-02,  2.25177547e-03,\n",
      "        2.51653828e-02, -3.11994180e-03,  1.34105096e-02,  2.50009466e-02,\n",
      "        6.64775912e-03,  1.66329741e-02,  1.23854810e-02,  2.36218832e-02,\n",
      "        8.07192549e-03,  7.98005797e-03,  2.42964234e-02,  1.66995786e-02,\n",
      "        1.57635976e-02,  1.38254249e-02,  1.82447396e-02,  1.76963601e-02,\n",
      "        8.11089482e-03,  7.57979811e-04, -5.66844875e-03,  1.43974479e-02,\n",
      "       -9.55720060e-03, -1.26922748e-03,  2.01750901e-02,  1.00511638e-02,\n",
      "        2.38205027e-02,  1.06307175e-02,  1.76120251e-02,  1.26231788e-03,\n",
      "        1.30329989e-02,  2.10126210e-02,  3.05980742e-02,  3.27216228e-03,\n",
      "        1.61290616e-02,  8.10394622e-03,  2.62498315e-02,  1.44470120e-02,\n",
      "        1.75902247e-02,  2.35428140e-02,  5.64812543e-03,  4.73982701e-03,\n",
      "        3.26799639e-02,  1.05903717e-02,  5.80258435e-03,  2.41914913e-02,\n",
      "        2.63425987e-02,  3.11653875e-02,  3.81506688e-04,  7.44728930e-03,\n",
      "        5.78831974e-03,  8.42719991e-03,  4.80008341e-04,  1.91225652e-02,\n",
      "        1.42916208e-02,  1.15907090e-02,  4.78171743e-03, -3.47583625e-03,\n",
      "        1.51380114e-02,  2.85106171e-02,  2.53322050e-02,  4.07999288e-03,\n",
      "        2.73714066e-02,  1.35565223e-02,  7.61411339e-03,  2.04974916e-02,\n",
      "        6.52449112e-03, -4.39646654e-03,  1.85016729e-02,  3.47984135e-02,\n",
      "        1.56466849e-02,  1.37274023e-02,  1.97412781e-02,  1.11967726e-02,\n",
      "       -3.15260841e-03,  1.74058564e-02,  1.94490096e-03,  1.66360531e-02,\n",
      "        2.55496055e-02,  9.52351652e-03,  1.49887586e-02,  2.48134024e-02,\n",
      "        2.26474535e-02,  1.62555221e-02,  2.87750401e-02,  1.00307250e-02,\n",
      "        1.11816293e-02,  3.52730253e-03,  1.71463657e-02,  1.08149024e-02,\n",
      "        2.25434210e-02,  5.78023540e-03,  1.40525037e-02,  1.22535750e-02,\n",
      "        4.68072109e-03,  2.37841718e-03,  1.90574955e-02,  1.67434067e-02,\n",
      "        1.59068648e-02,  4.71991394e-03,  2.41815802e-02,  1.08395582e-02,\n",
      "        1.50122622e-03,  1.09650688e-02,  1.41848987e-02,  2.24554259e-02,\n",
      "        1.43333497e-02,  7.08581787e-03,  2.80876420e-02,  1.88797880e-02,\n",
      "        1.90907996e-02,  1.34947775e-02,  4.48970124e-03,  1.66956484e-02,\n",
      "        1.74763761e-02,  3.45662721e-02,  2.37578731e-02,  2.29796786e-02,\n",
      "       -3.93765979e-03,  2.50505656e-02,  2.22837757e-02, -4.68133949e-03,\n",
      "        1.70438662e-02,  2.07587034e-02,  1.50805907e-02,  1.82517618e-02,\n",
      "        1.74386352e-02,  9.56386328e-03,  2.21007280e-02,  1.82567276e-02,\n",
      "        1.16545660e-02,  1.51076000e-02,  4.38230997e-03,  2.13329997e-02,\n",
      "        1.69646014e-02,  2.81735342e-02,  6.54041208e-03,  1.47546008e-02,\n",
      "       -1.37830561e-03,  2.67536584e-02,  1.50663368e-02, -1.20302651e-03,\n",
      "        1.51636191e-02,  5.38266264e-04,  4.25254507e-03,  2.07080860e-02,\n",
      "        1.86729785e-02,  7.41935614e-03,  3.68210953e-03,  2.97957119e-02,\n",
      "        2.73778345e-02,  2.70437449e-03,  7.35607650e-03, -7.26165576e-03,\n",
      "        2.08090749e-02,  6.90238317e-03,  9.91251040e-03,  2.61625685e-02,\n",
      "        1.22569054e-02, -4.03495133e-03,  1.18397456e-02,  2.43327841e-02,\n",
      "        5.28923748e-03, -4.04971419e-03,  4.17963834e-03,  2.03017704e-02,\n",
      "        1.33257611e-02,  1.10427830e-02,  1.30556822e-02,  1.47443134e-02,\n",
      "       -1.01726479e-03, -5.78693626e-03,  7.86889810e-03,  1.72415785e-02,\n",
      "        2.61558294e-02,  6.49295794e-03,  1.76622514e-02,  2.34393645e-02,\n",
      "        4.44218926e-02,  2.00898256e-02,  1.04824053e-02,  2.31316667e-02,\n",
      "        1.64935309e-02,  8.18449352e-03,  2.09093392e-02,  1.82948653e-02,\n",
      "        1.25396000e-02,  1.12371910e-02,  1.96730085e-02,  1.92409419e-02,\n",
      "       -3.37887369e-03,  1.10786362e-02,  1.13592707e-02,  2.26800609e-02,\n",
      "        1.45339519e-02,  1.97602659e-02,  8.56669527e-03,  3.00914068e-02,\n",
      "        6.96224067e-03,  1.60746425e-02,  1.16108749e-02,  1.61604267e-02,\n",
      "        2.02767905e-02,  1.97154582e-02,  1.17745204e-02,  5.91270719e-03,\n",
      "        1.42524531e-03,  1.96134560e-02, -6.19803555e-03,  1.54720778e-02,\n",
      "        4.22670087e-03,  2.63787843e-02,  1.95293929e-02,  1.10521913e-02,\n",
      "        1.70790665e-02,  9.01731197e-04,  1.83163639e-02,  1.69998687e-02,\n",
      "        2.09332015e-02,  3.14312652e-02,  1.65824238e-02,  2.56769732e-02,\n",
      "        2.36808974e-02,  1.74310934e-02,  3.08326874e-02,  2.15751585e-02,\n",
      "        3.88562772e-03,  2.89289206e-02,  2.81839464e-02,  1.42468056e-02,\n",
      "        1.33123454e-02,  3.36009101e-03,  1.83523446e-02,  1.90678816e-02,\n",
      "        1.57684414e-03,  2.53277961e-02,  2.22120471e-02,  2.53229532e-02,\n",
      "        2.20582057e-02,  2.09423378e-02,  2.29420848e-02,  6.79569505e-03,\n",
      "        1.62308626e-02,  3.75474524e-03,  9.55627300e-03, -3.54236993e-03,\n",
      "        6.24532485e-03,  1.40353097e-02,  1.04195706e-03,  1.27939302e-02,\n",
      "        1.44929131e-02,  1.82609037e-02,  1.88640449e-02,  2.01301612e-02,\n",
      "        7.79250683e-03,  1.24290737e-03,  2.58573648e-02,  4.23790328e-03,\n",
      "        1.39331082e-02,  7.24378368e-03,  9.12443362e-03,  1.48676708e-02,\n",
      "        6.14139764e-03,  1.73633720e-03,  2.35216990e-02,  7.73048028e-03,\n",
      "        9.46685765e-03,  2.86616888e-02, -9.46418149e-04,  4.53112228e-03,\n",
      "        3.01843528e-02,  2.23303605e-02,  1.83530431e-02,  1.64311547e-02,\n",
      "       -1.74200209e-03, -8.45739897e-03, -3.67076416e-03,  2.48888731e-02,\n",
      "        1.43014574e-02,  5.42331673e-03,  1.26564279e-02,  1.39450720e-02,\n",
      "        1.73015557e-02, -5.74378204e-03,  1.35171283e-02,  2.52402201e-02,\n",
      "        8.05765484e-03,  1.75500882e-03,  2.49436032e-03,  3.41590382e-02,\n",
      "        1.36065511e-02, -6.46882411e-03,  3.36382585e-03,  1.25794287e-03,\n",
      "        1.33646997e-02,  1.77969802e-02,  8.82529828e-04,  1.28342072e-02,\n",
      "        2.33996566e-02,  2.01380774e-02,  1.44121423e-02,  2.02488201e-03,\n",
      "        9.68243275e-03,  1.64381601e-02, -8.47269723e-04,  1.89592727e-02,\n",
      "        2.73755193e-03,  1.74058916e-03, -9.33983756e-05, -1.44366769e-03,\n",
      "        2.73044780e-02,  2.68871933e-02,  4.53630928e-03,  1.30586512e-02,\n",
      "        1.82093885e-02, -3.70580214e-03,  1.21944193e-02,  1.86235812e-02,\n",
      "        1.90278776e-02,  2.10419390e-03,  3.83110680e-02,  2.47317627e-02,\n",
      "        1.61365680e-02,  2.15491746e-02,  7.08336663e-03,  1.52411014e-02,\n",
      "        1.20878946e-02,  3.63138169e-02,  2.95275380e-03,  1.10416589e-02,\n",
      "        1.46681545e-02,  8.01603310e-03,  2.89593521e-03,  3.44317243e-03,\n",
      "        3.84997553e-03, -6.89852424e-03,  4.17512283e-03,  5.94559917e-03,\n",
      "        2.27501597e-02,  2.65809633e-02,  7.67007517e-03, -9.02090222e-03,\n",
      "        1.14292861e-03,  2.73110252e-03,  1.68622993e-02,  2.29429337e-04,\n",
      "        1.71757229e-02, -6.13210863e-03,  7.79184792e-03,  9.70395841e-03,\n",
      "        1.56224957e-02,  1.22342231e-02,  8.22104607e-03,  6.45118766e-03,\n",
      "        1.61897279e-02,  2.71096341e-02,  1.41909886e-02,  8.24029371e-03,\n",
      "        2.23714299e-02, -4.54792957e-04,  1.38669368e-02,  3.31148994e-03,\n",
      "       -8.64709169e-03,  2.09755432e-02,  1.98955517e-02,  2.36926209e-02,\n",
      "        2.32519349e-03, -1.03631592e-03,  7.57707609e-03,  5.17963106e-03,\n",
      "        2.16792934e-02,  2.29344750e-03,  2.24430598e-02,  1.01004615e-02,\n",
      "        7.81761669e-03,  1.15599493e-02,  1.08112290e-03, -4.48285695e-03,\n",
      "        1.69684738e-02,  1.45988697e-02,  1.42355729e-02,  1.79956108e-02,\n",
      "        1.88460127e-02,  1.56671721e-02,  2.20043119e-02,  9.63607430e-03,\n",
      "        8.18825327e-03,  9.87336412e-03,  8.73227604e-03, -1.25347963e-02,\n",
      "        6.16450189e-03, -2.58935324e-04,  1.39446901e-02,  2.27234587e-02,\n",
      "        5.56024956e-03,  1.94802657e-02,  1.14675499e-02,  8.20897054e-03,\n",
      "        1.70399658e-02,  2.22118292e-02,  9.68761928e-03,  9.27146990e-03,\n",
      "        1.61956344e-02,  7.68945878e-03,  8.47267359e-03,  2.27554198e-02,\n",
      "        1.95404552e-02,  2.92708050e-03,  2.27158610e-02,  1.09201036e-02,\n",
      "        1.86403822e-02, -8.95838998e-03,  7.66861485e-03,  1.84521135e-02,\n",
      "        1.47399371e-02,  2.28355713e-02,  1.61872935e-02,  2.19607335e-02,\n",
      "        4.97866189e-03,  2.51761042e-02,  2.12353878e-02,  2.05433592e-02,\n",
      "        1.88988708e-02,  7.06916163e-03,  6.47606980e-03,  2.90825162e-02,\n",
      "        1.69150867e-02,  1.39820185e-02,  1.47667639e-02,  2.46477015e-02,\n",
      "        2.39420333e-03,  1.91734191e-02,  3.16736624e-02,  1.79478712e-02,\n",
      "        3.04389969e-02, -1.76091015e-03,  2.44312156e-02,  2.55738143e-02,\n",
      "        1.93239842e-02, -2.65724608e-03,  1.19456882e-02,  4.44562081e-03,\n",
      "        1.61857321e-03,  1.75221637e-02,  1.40943816e-02,  1.39601445e-02,\n",
      "       -1.59214716e-03,  9.64421500e-03, -4.92330361e-03,  1.97985750e-02,\n",
      "       -2.44239927e-03,  7.10469298e-03,  1.88350631e-03,  1.27387913e-02,\n",
      "        4.59106034e-03, -4.74336836e-03,  2.43856516e-02,  2.46043969e-02,\n",
      "        1.68017298e-02,  1.47266779e-02,  1.02450391e-02,  2.58982163e-02,\n",
      "        2.29387060e-02,  3.73145845e-03,  1.49541972e-02,  8.58619250e-03,\n",
      "        1.04478579e-02,  2.23619118e-02,  1.96576137e-02,  2.90341508e-02,\n",
      "        2.62643788e-02,  8.95988196e-03,  7.43771251e-03,  2.50539668e-02,\n",
      "       -3.68010296e-05,  2.41094735e-02,  1.04611693e-02,  2.58641262e-02,\n",
      "        1.86960381e-02,  8.79034027e-03,  2.03861156e-03,  7.35323131e-03,\n",
      "        2.97073498e-02,  1.37865040e-02, -3.30719049e-04,  2.41604969e-02,\n",
      "        2.78679449e-02,  3.54393795e-02,  1.97142158e-02,  5.36191557e-03,\n",
      "        8.91472958e-03,  8.10846034e-03,  1.78322215e-02,  3.64251016e-03,\n",
      "        1.84568111e-02, -8.61724094e-03,  2.39043646e-02,  1.11378795e-02,\n",
      "        1.05569065e-02, -4.81725158e-03,  2.37899687e-04,  6.39770972e-03,\n",
      "        5.57356514e-03,  1.62999928e-02,  7.07575865e-03,  1.96489673e-02,\n",
      "        1.85046140e-02,  4.37245658e-03,  2.53706034e-02,  1.58724654e-02,\n",
      "        1.22075304e-02,  4.65694722e-03,  1.49284732e-02, -1.70078054e-02,\n",
      "        2.78641805e-02,  3.02344393e-02,  4.16123308e-03,  4.38513671e-04,\n",
      "        2.45290715e-02,  2.13190150e-02,  2.67345887e-02,  7.39493361e-03,\n",
      "        1.41082508e-02, -3.16927442e-03,  1.41387694e-02,  1.09835435e-03,\n",
      "       -4.42864932e-03,  9.70644236e-04,  6.33002492e-03,  1.40989227e-02,\n",
      "        1.05618378e-02,  1.02032516e-02,  1.36432853e-02,  1.99588556e-02,\n",
      "        1.61565877e-02,  2.58471947e-02,  1.91759448e-02,  1.60134304e-02,\n",
      "        1.56088602e-02,  1.55314533e-02,  1.94750894e-02,  5.24486415e-03,\n",
      "        8.23909324e-03,  6.51686778e-03,  1.47254225e-02,  1.02757132e-02,\n",
      "        2.23987773e-02,  5.88360662e-03,  8.78158305e-03, -8.84814747e-03,\n",
      "       -7.36045837e-03,  2.16339692e-03,  4.54540178e-03,  8.67243297e-03,\n",
      "        8.68941285e-03,  1.94512233e-02,  1.64292287e-02,  2.67616697e-02,\n",
      "        1.43896844e-02,  2.79943682e-02, -2.72688153e-03,  1.47656752e-02,\n",
      "        9.72132199e-03,  1.60072241e-02,  1.14462851e-02,  4.33157757e-03,\n",
      "        2.15409547e-02,  1.62655972e-02,  3.79037904e-03,  1.44971171e-02,\n",
      "       -5.55649493e-03,  1.07303206e-02,  1.80406887e-02,  1.52331367e-02,\n",
      "        1.49698779e-02,  2.21336447e-03,  2.27089692e-02,  2.03485098e-02,\n",
      "        2.97378027e-03,  3.39813740e-03], dtype=float32)]\n",
      "[]\n",
      "[array([[-0.07100847, -0.0506745 , -0.06806362,  0.05929948, -0.01883668],\n",
      "       [ 0.05627139, -0.02874283,  0.12244166,  0.01283117, -0.05049294],\n",
      "       [ 0.05047762,  0.02167685, -0.06092309, -0.0199836 , -0.02571777],\n",
      "       ...,\n",
      "       [ 0.06295196, -0.11193097,  0.06000063, -0.00230839,  0.04514375],\n",
      "       [-0.09497841, -0.04332707, -0.1279283 ,  0.10552206,  0.00430932],\n",
      "       [-0.10098404,  0.04395951, -0.0702332 ,  0.11660624, -0.04534686]],\n",
      "      dtype=float32), array([ 0.00611983, -0.00645709,  0.01358221, -0.01235961, -0.0048225 ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model_ANN_saved.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73c9686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00713627, -0.02420996,  0.03483262, -0.0268757 , -0.00466874],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model_ANN_saved.get_layer('Output').get_weights()\n",
    "weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b430548",
   "metadata": {},
   "source": [
    "### CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4deabf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN (Convolutional Neural Network) using Tensorflow\n",
    "\n",
    "def build_CNN_1D_model(model_name: str, neurons: int):\n",
    "    \n",
    "    model = Sequential(name = model_name)\n",
    "\n",
    "    # 1st conv layer\n",
    "    model.add(Conv1D(28, 7, activation = 'relu', input_shape = (neurons, 1), name = 'Conv1D_1'))\n",
    "    #model.add(MaxPooling1D(3, name = 'MaxPool1D_1'))\n",
    "\n",
    "    # 2nd conv layer\n",
    "    model.add(Conv1D(34, 5, activation = 'relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.01), padding='same', name = 'Conv1D_2'))\n",
    "    #model.add(MaxPooling1D(2, name = 'MaxPool1D_2'))\n",
    "    \n",
    "    # 3nd conv layer \n",
    "    model.add(Conv1D(56, 3, activation = 'relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.01), padding='same', name = 'Conv1D_3'))\n",
    "    model.add(MaxPooling1D(2, name = 'MaxPool1D_3'))\n",
    "    model.add(Dropout(0.2, name = 'Dropout_1'))\n",
    "    \n",
    "    # 4nd conv layer + dropout 20%\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, name = 'Dense'))\n",
    "\n",
    "    # Final classification layer, with 1 neuron for each output class. Softmax divides the probability of each class.\n",
    "    model.add(Dense(num_classes, activation = 'softmax', name = 'Output'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adamax', metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e20c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=50, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "if not os.path.exists(path_models):\n",
    "    os.makedirs(path_models)\n",
    "    \n",
    "filepath       = os.path.join(path_models, 'Model_CNN_1D_weights_0_best' + norm_type + model_surname + '.hdf5')\n",
    "checkpoint     = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, monitor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9fe8a297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_1D\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 369, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 369, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 369, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 184, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 184, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10304)             0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                515250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 526,291\n",
      "Trainable params: 526,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CNN_1D = build_CNN_1D_model('CNN_1D', neurons = n_dim)\n",
    "model_CNN_1D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "10a28354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_ANN, to_file= os.path.join(path_models, 'Model_CNN_1D' + norm_type + model_surname + '.png'), show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372210cd",
   "metadata": {},
   "source": [
    "### Understanding the column \"Param\":\n",
    "\n",
    "1. For `Conv1D` layer:\n",
    "   - The number of parameters for a `Conv1D` layer is calculated as `(kernel_size * input_channels + 1) * output_channels`, where `kernel_size` is the size of the convolutional kernel, `input_channels` is the number of input channels (1 in this case), and `output_channels` is the number of output channels.\n",
    "\n",
    "2. For `Dense` layer:\n",
    "   - The number of parameters for a `Dense` layer is calculated as `(input_units + 1) * output_units`, where `input_units` is the number of input units and `output_units` is the number of output units.\n",
    "   \n",
    "3. In the calculation of parameters for a convolutional layer, the term \"channels\" refers to the number of filters used in that layer.\n",
    "\n",
    "- 224   parameters is the result of 28 filters * (7 kernels * 1 filter + 1)\n",
    "- 4,794 parameters is the result of 34 filter * (5 kernels * 28 filters + 1)\n",
    "- 5,768  parameters is the result of 56 filters * (3 kernels * 34 filters + 1)\n",
    "- 515,250  parameters is the result of 50 neurons with 10,304 features + 50 bias values\n",
    "- 255    parameters is the result of 5 neurons with 50 features + 5 bias values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bcda9a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.7970 - accuracy: 0.7278\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.77200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.7970 - accuracy: 0.7278 - val_loss: 0.6699 - val_accuracy: 0.7720\n",
      "Epoch 2/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.5573 - accuracy: 0.8201\n",
      "Epoch 00002: val_accuracy improved from 0.77200 to 0.82436, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.5572 - accuracy: 0.8201 - val_loss: 0.5363 - val_accuracy: 0.8244\n",
      "Epoch 3/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.8454\n",
      "Epoch 00003: val_accuracy improved from 0.82436 to 0.85273, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.4837 - accuracy: 0.8456 - val_loss: 0.4605 - val_accuracy: 0.8527\n",
      "Epoch 4/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.4407 - accuracy: 0.8595\n",
      "Epoch 00004: val_accuracy improved from 0.85273 to 0.86400, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.4407 - accuracy: 0.8595 - val_loss: 0.4233 - val_accuracy: 0.8640\n",
      "Epoch 5/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.4126 - accuracy: 0.8694\n",
      "Epoch 00005: val_accuracy improved from 0.86400 to 0.86945, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.4131 - accuracy: 0.8693 - val_loss: 0.4156 - val_accuracy: 0.8695\n",
      "Epoch 6/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.3890 - accuracy: 0.8781\n",
      "Epoch 00006: val_accuracy did not improve from 0.86945\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3898 - accuracy: 0.8780 - val_loss: 0.4266 - val_accuracy: 0.8596\n",
      "Epoch 7/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.3726 - accuracy: 0.8851\n",
      "Epoch 00007: val_accuracy improved from 0.86945 to 0.87600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3736 - accuracy: 0.8847 - val_loss: 0.3890 - val_accuracy: 0.8760\n",
      "Epoch 8/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.8894\n",
      "Epoch 00008: val_accuracy improved from 0.87600 to 0.87709, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3554 - accuracy: 0.8892 - val_loss: 0.3846 - val_accuracy: 0.8771\n",
      "Epoch 9/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.8955\n",
      "Epoch 00009: val_accuracy improved from 0.87709 to 0.88036, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3418 - accuracy: 0.8954 - val_loss: 0.3770 - val_accuracy: 0.8804\n",
      "Epoch 10/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.3303 - accuracy: 0.9001\n",
      "Epoch 00010: val_accuracy did not improve from 0.88036\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3305 - accuracy: 0.8999 - val_loss: 0.3855 - val_accuracy: 0.8731\n",
      "Epoch 11/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.9037\n",
      "Epoch 00011: val_accuracy improved from 0.88036 to 0.88473, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3241 - accuracy: 0.9036 - val_loss: 0.3572 - val_accuracy: 0.8847\n",
      "Epoch 12/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.9029\n",
      "Epoch 00012: val_accuracy improved from 0.88473 to 0.89055, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3155 - accuracy: 0.9032 - val_loss: 0.3444 - val_accuracy: 0.8905\n",
      "Epoch 13/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.9060\n",
      "Epoch 00013: val_accuracy improved from 0.89055 to 0.89200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.3089 - accuracy: 0.9060 - val_loss: 0.3456 - val_accuracy: 0.8920\n",
      "Epoch 14/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.9100\n",
      "Epoch 00014: val_accuracy improved from 0.89200 to 0.89382, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2990 - accuracy: 0.9101 - val_loss: 0.3509 - val_accuracy: 0.8938\n",
      "Epoch 15/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.9123\n",
      "Epoch 00015: val_accuracy did not improve from 0.89382\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2939 - accuracy: 0.9123 - val_loss: 0.3375 - val_accuracy: 0.8935\n",
      "Epoch 16/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9128\n",
      "Epoch 00016: val_accuracy improved from 0.89382 to 0.89891, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2873 - accuracy: 0.9128 - val_loss: 0.3340 - val_accuracy: 0.8989\n",
      "Epoch 17/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.2751 - accuracy: 0.9178\n",
      "Epoch 00017: val_accuracy did not improve from 0.89891\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2747 - accuracy: 0.9178 - val_loss: 0.3507 - val_accuracy: 0.8865\n",
      "Epoch 18/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.2718 - accuracy: 0.9185\n",
      "Epoch 00018: val_accuracy improved from 0.89891 to 0.90109, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2719 - accuracy: 0.9184 - val_loss: 0.3197 - val_accuracy: 0.9011\n",
      "Epoch 19/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.9204\n",
      "Epoch 00019: val_accuracy improved from 0.90109 to 0.90873, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2667 - accuracy: 0.9206 - val_loss: 0.2999 - val_accuracy: 0.9087\n",
      "Epoch 20/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.9219\n",
      "Epoch 00020: val_accuracy did not improve from 0.90873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2602 - accuracy: 0.9219 - val_loss: 0.3130 - val_accuracy: 0.9015\n",
      "Epoch 21/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.2571 - accuracy: 0.9222\n",
      "Epoch 00021: val_accuracy did not improve from 0.90873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2576 - accuracy: 0.9221 - val_loss: 0.3040 - val_accuracy: 0.9025\n",
      "Epoch 22/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/774 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.9251\n",
      "Epoch 00022: val_accuracy did not improve from 0.90873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2528 - accuracy: 0.9250 - val_loss: 0.3024 - val_accuracy: 0.9058\n",
      "Epoch 23/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.2543 - accuracy: 0.9241\n",
      "Epoch 00023: val_accuracy did not improve from 0.90873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2545 - accuracy: 0.9239 - val_loss: 0.3023 - val_accuracy: 0.9047\n",
      "Epoch 24/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.9269\n",
      "Epoch 00024: val_accuracy did not improve from 0.90873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2467 - accuracy: 0.9269 - val_loss: 0.2954 - val_accuracy: 0.9084\n",
      "Epoch 25/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.2440 - accuracy: 0.9268\n",
      "Epoch 00025: val_accuracy did not improve from 0.90873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2446 - accuracy: 0.9265 - val_loss: 0.2945 - val_accuracy: 0.9084\n",
      "Epoch 26/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.9298\n",
      "Epoch 00026: val_accuracy did not improve from 0.90873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2389 - accuracy: 0.9298 - val_loss: 0.2952 - val_accuracy: 0.9080\n",
      "Epoch 27/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.9304\n",
      "Epoch 00027: val_accuracy improved from 0.90873 to 0.90909, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2344 - accuracy: 0.9305 - val_loss: 0.3018 - val_accuracy: 0.9091\n",
      "Epoch 28/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.2308 - accuracy: 0.9302\n",
      "Epoch 00028: val_accuracy did not improve from 0.90909\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2309 - accuracy: 0.9302 - val_loss: 0.3146 - val_accuracy: 0.9040\n",
      "Epoch 29/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.2295 - accuracy: 0.9329\n",
      "Epoch 00029: val_accuracy improved from 0.90909 to 0.91527, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2293 - accuracy: 0.9330 - val_loss: 0.2779 - val_accuracy: 0.9153\n",
      "Epoch 30/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9331\n",
      "Epoch 00030: val_accuracy did not improve from 0.91527\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2278 - accuracy: 0.9331 - val_loss: 0.2869 - val_accuracy: 0.9138\n",
      "Epoch 31/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.2218 - accuracy: 0.9340\n",
      "Epoch 00031: val_accuracy did not improve from 0.91527\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2223 - accuracy: 0.9337 - val_loss: 0.2862 - val_accuracy: 0.9113\n",
      "Epoch 32/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9358\n",
      "Epoch 00032: val_accuracy did not improve from 0.91527\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2201 - accuracy: 0.9359 - val_loss: 0.2833 - val_accuracy: 0.9124\n",
      "Epoch 33/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2170 - accuracy: 0.9361\n",
      "Epoch 00033: val_accuracy did not improve from 0.91527\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2168 - accuracy: 0.9362 - val_loss: 0.2873 - val_accuracy: 0.9102\n",
      "Epoch 34/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.2132 - accuracy: 0.9365\n",
      "Epoch 00034: val_accuracy improved from 0.91527 to 0.91636, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2124 - accuracy: 0.9367 - val_loss: 0.2817 - val_accuracy: 0.9164\n",
      "Epoch 35/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.2112 - accuracy: 0.9364\n",
      "Epoch 00035: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2111 - accuracy: 0.9364 - val_loss: 0.2772 - val_accuracy: 0.9153\n",
      "Epoch 36/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.9405\n",
      "Epoch 00036: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2083 - accuracy: 0.9406 - val_loss: 0.2773 - val_accuracy: 0.9149\n",
      "Epoch 37/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9384\n",
      "Epoch 00037: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2055 - accuracy: 0.9387 - val_loss: 0.2784 - val_accuracy: 0.9153\n",
      "Epoch 38/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.2028 - accuracy: 0.9398\n",
      "Epoch 00038: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2027 - accuracy: 0.9399 - val_loss: 0.2778 - val_accuracy: 0.9149\n",
      "Epoch 39/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.2000 - accuracy: 0.9425\n",
      "Epoch 00039: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1999 - accuracy: 0.9425 - val_loss: 0.3105 - val_accuracy: 0.8993\n",
      "Epoch 40/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1990 - accuracy: 0.9424\n",
      "Epoch 00040: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1987 - accuracy: 0.9425 - val_loss: 0.2773 - val_accuracy: 0.9142\n",
      "Epoch 41/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1933 - accuracy: 0.9432\n",
      "Epoch 00041: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1929 - accuracy: 0.9432 - val_loss: 0.2803 - val_accuracy: 0.9156\n",
      "Epoch 42/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1942 - accuracy: 0.9437\n",
      "Epoch 00042: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1941 - accuracy: 0.9438 - val_loss: 0.3002 - val_accuracy: 0.9113\n",
      "Epoch 43/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9412\n",
      "Epoch 00043: val_accuracy improved from 0.91636 to 0.91855, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1938 - accuracy: 0.9414 - val_loss: 0.2797 - val_accuracy: 0.9185\n",
      "Epoch 44/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1919 - accuracy: 0.9450\n",
      "Epoch 00044: val_accuracy did not improve from 0.91855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1912 - accuracy: 0.9453 - val_loss: 0.2745 - val_accuracy: 0.9182\n",
      "Epoch 45/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.9462\n",
      "Epoch 00045: val_accuracy did not improve from 0.91855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1875 - accuracy: 0.9460 - val_loss: 0.2895 - val_accuracy: 0.9113\n",
      "Epoch 46/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9445\n",
      "Epoch 00046: val_accuracy improved from 0.91855 to 0.92473, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1881 - accuracy: 0.9444 - val_loss: 0.2525 - val_accuracy: 0.9247\n",
      "Epoch 47/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9467\n",
      "Epoch 00047: val_accuracy did not improve from 0.92473\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1835 - accuracy: 0.9469 - val_loss: 0.2636 - val_accuracy: 0.9215\n",
      "Epoch 48/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/774 [============================>.] - ETA: 0s - loss: 0.1789 - accuracy: 0.9479\n",
      "Epoch 00048: val_accuracy did not improve from 0.92473\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1793 - accuracy: 0.9477 - val_loss: 0.2625 - val_accuracy: 0.9211\n",
      "Epoch 49/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9475\n",
      "Epoch 00049: val_accuracy improved from 0.92473 to 0.92545, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1804 - accuracy: 0.9475 - val_loss: 0.2558 - val_accuracy: 0.9255\n",
      "Epoch 50/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1782 - accuracy: 0.9488\n",
      "Epoch 00050: val_accuracy did not improve from 0.92545\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1783 - accuracy: 0.9488 - val_loss: 0.2625 - val_accuracy: 0.9207\n",
      "Epoch 51/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1767 - accuracy: 0.9473\n",
      "Epoch 00051: val_accuracy did not improve from 0.92545\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1768 - accuracy: 0.9472 - val_loss: 0.2524 - val_accuracy: 0.9233\n",
      "Epoch 52/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1747 - accuracy: 0.9490\n",
      "Epoch 00052: val_accuracy did not improve from 0.92545\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1749 - accuracy: 0.9490 - val_loss: 0.2535 - val_accuracy: 0.9251\n",
      "Epoch 53/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9496\n",
      "Epoch 00053: val_accuracy did not improve from 0.92545\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1749 - accuracy: 0.9496 - val_loss: 0.2610 - val_accuracy: 0.9240\n",
      "Epoch 54/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1729 - accuracy: 0.9503\n",
      "Epoch 00054: val_accuracy improved from 0.92545 to 0.92873, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1725 - accuracy: 0.9503 - val_loss: 0.2564 - val_accuracy: 0.9287\n",
      "Epoch 55/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.9498\n",
      "Epoch 00055: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1718 - accuracy: 0.9499 - val_loss: 0.2566 - val_accuracy: 0.9218\n",
      "Epoch 56/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9514\n",
      "Epoch 00056: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1717 - accuracy: 0.9513 - val_loss: 0.2555 - val_accuracy: 0.9211\n",
      "Epoch 57/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.9523\n",
      "Epoch 00057: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1699 - accuracy: 0.9522 - val_loss: 0.2548 - val_accuracy: 0.9262\n",
      "Epoch 58/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1655 - accuracy: 0.9519\n",
      "Epoch 00058: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1657 - accuracy: 0.9517 - val_loss: 0.2679 - val_accuracy: 0.9240\n",
      "Epoch 59/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9527\n",
      "Epoch 00059: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1656 - accuracy: 0.9528 - val_loss: 0.2642 - val_accuracy: 0.9262\n",
      "Epoch 60/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.9518\n",
      "Epoch 00060: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1668 - accuracy: 0.9518 - val_loss: 0.2581 - val_accuracy: 0.9229\n",
      "Epoch 61/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1664 - accuracy: 0.9531\n",
      "Epoch 00061: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1662 - accuracy: 0.9532 - val_loss: 0.2632 - val_accuracy: 0.9265\n",
      "Epoch 62/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9527\n",
      "Epoch 00062: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1623 - accuracy: 0.9527 - val_loss: 0.2445 - val_accuracy: 0.9280\n",
      "Epoch 63/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9553\n",
      "Epoch 00063: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1579 - accuracy: 0.9548 - val_loss: 0.2715 - val_accuracy: 0.9236\n",
      "Epoch 64/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9543\n",
      "Epoch 00064: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1624 - accuracy: 0.9544 - val_loss: 0.2560 - val_accuracy: 0.9240\n",
      "Epoch 65/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1597 - accuracy: 0.9551\n",
      "Epoch 00065: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1595 - accuracy: 0.9552 - val_loss: 0.2405 - val_accuracy: 0.9269\n",
      "Epoch 66/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1596 - accuracy: 0.9540\n",
      "Epoch 00066: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1596 - accuracy: 0.9540 - val_loss: 0.2567 - val_accuracy: 0.9204\n",
      "Epoch 67/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9542\n",
      "Epoch 00067: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1616 - accuracy: 0.9541 - val_loss: 0.2459 - val_accuracy: 0.9280\n",
      "Epoch 68/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9556\n",
      "Epoch 00068: val_accuracy improved from 0.92873 to 0.92909, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1546 - accuracy: 0.9557 - val_loss: 0.2438 - val_accuracy: 0.9291\n",
      "Epoch 69/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9566\n",
      "Epoch 00069: val_accuracy improved from 0.92909 to 0.93091, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1544 - accuracy: 0.9566 - val_loss: 0.2467 - val_accuracy: 0.9309\n",
      "Epoch 70/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.9567\n",
      "Epoch 00070: val_accuracy improved from 0.93091 to 0.93273, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1531 - accuracy: 0.9567 - val_loss: 0.2440 - val_accuracy: 0.9327\n",
      "Epoch 71/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9577\n",
      "Epoch 00071: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1525 - accuracy: 0.9579 - val_loss: 0.2605 - val_accuracy: 0.9244\n",
      "Epoch 72/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1505 - accuracy: 0.9581\n",
      "Epoch 00072: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1506 - accuracy: 0.9581 - val_loss: 0.2532 - val_accuracy: 0.9258\n",
      "Epoch 73/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9565\n",
      "Epoch 00073: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1531 - accuracy: 0.9565 - val_loss: 0.2520 - val_accuracy: 0.9247\n",
      "Epoch 74/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/774 [============================>.] - ETA: 0s - loss: 0.1487 - accuracy: 0.9594\n",
      "Epoch 00074: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1484 - accuracy: 0.9596 - val_loss: 0.2483 - val_accuracy: 0.9291\n",
      "Epoch 75/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9583\n",
      "Epoch 00075: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1479 - accuracy: 0.9583 - val_loss: 0.2391 - val_accuracy: 0.9298\n",
      "Epoch 76/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1435 - accuracy: 0.9596\n",
      "Epoch 00076: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1435 - accuracy: 0.9596 - val_loss: 0.2811 - val_accuracy: 0.9240\n",
      "Epoch 77/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.9598\n",
      "Epoch 00077: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1450 - accuracy: 0.9598 - val_loss: 0.2356 - val_accuracy: 0.9324\n",
      "Epoch 78/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1431 - accuracy: 0.9605\n",
      "Epoch 00078: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1438 - accuracy: 0.9603 - val_loss: 0.2666 - val_accuracy: 0.9225\n",
      "Epoch 79/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9590\n",
      "Epoch 00079: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1465 - accuracy: 0.9589 - val_loss: 0.2347 - val_accuracy: 0.9269\n",
      "Epoch 80/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.9592\n",
      "Epoch 00080: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1451 - accuracy: 0.9591 - val_loss: 0.2489 - val_accuracy: 0.9291\n",
      "Epoch 81/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.9594\n",
      "Epoch 00081: val_accuracy did not improve from 0.93273\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1443 - accuracy: 0.9594 - val_loss: 0.2625 - val_accuracy: 0.9225\n",
      "Epoch 82/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1438 - accuracy: 0.9599\n",
      "Epoch 00082: val_accuracy improved from 0.93273 to 0.93309, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1440 - accuracy: 0.9598 - val_loss: 0.2374 - val_accuracy: 0.9331\n",
      "Epoch 83/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9610\n",
      "Epoch 00083: val_accuracy did not improve from 0.93309\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1386 - accuracy: 0.9611 - val_loss: 0.2769 - val_accuracy: 0.9189\n",
      "Epoch 84/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.9610\n",
      "Epoch 00084: val_accuracy did not improve from 0.93309\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1402 - accuracy: 0.9609 - val_loss: 0.2393 - val_accuracy: 0.9276\n",
      "Epoch 85/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1378 - accuracy: 0.9626\n",
      "Epoch 00085: val_accuracy did not improve from 0.93309\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1377 - accuracy: 0.9626 - val_loss: 0.2545 - val_accuracy: 0.9309\n",
      "Epoch 86/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9618\n",
      "Epoch 00086: val_accuracy did not improve from 0.93309\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1387 - accuracy: 0.9619 - val_loss: 0.2448 - val_accuracy: 0.9302\n",
      "Epoch 87/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9617\n",
      "Epoch 00087: val_accuracy did not improve from 0.93309\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1382 - accuracy: 0.9620 - val_loss: 0.2404 - val_accuracy: 0.9327\n",
      "Epoch 88/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9631\n",
      "Epoch 00088: val_accuracy did not improve from 0.93309\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1347 - accuracy: 0.9629 - val_loss: 0.2539 - val_accuracy: 0.9313\n",
      "Epoch 89/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1346 - accuracy: 0.9617\n",
      "Epoch 00089: val_accuracy improved from 0.93309 to 0.93636, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1348 - accuracy: 0.9617 - val_loss: 0.2339 - val_accuracy: 0.9364\n",
      "Epoch 90/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9615\n",
      "Epoch 00090: val_accuracy did not improve from 0.93636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1371 - accuracy: 0.9615 - val_loss: 0.2388 - val_accuracy: 0.9284\n",
      "Epoch 91/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9634\n",
      "Epoch 00091: val_accuracy did not improve from 0.93636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1330 - accuracy: 0.9634 - val_loss: 0.2337 - val_accuracy: 0.9338\n",
      "Epoch 92/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9624\n",
      "Epoch 00092: val_accuracy did not improve from 0.93636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1343 - accuracy: 0.9622 - val_loss: 0.2447 - val_accuracy: 0.9287\n",
      "Epoch 93/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1318 - accuracy: 0.9633\n",
      "Epoch 00093: val_accuracy did not improve from 0.93636\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1319 - accuracy: 0.9631 - val_loss: 0.2421 - val_accuracy: 0.9302\n",
      "Epoch 94/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9628\n",
      "Epoch 00094: val_accuracy improved from 0.93636 to 0.93855, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_norm_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1319 - accuracy: 0.9626 - val_loss: 0.2359 - val_accuracy: 0.9385\n",
      "Epoch 95/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9649\n",
      "Epoch 00095: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1274 - accuracy: 0.9650 - val_loss: 0.2466 - val_accuracy: 0.9295\n",
      "Epoch 96/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9623\n",
      "Epoch 00096: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1303 - accuracy: 0.9622 - val_loss: 0.2687 - val_accuracy: 0.9200\n",
      "Epoch 97/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9662\n",
      "Epoch 00097: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1287 - accuracy: 0.9661 - val_loss: 0.2306 - val_accuracy: 0.9335\n",
      "Epoch 98/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9639\n",
      "Epoch 00098: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1304 - accuracy: 0.9638 - val_loss: 0.2406 - val_accuracy: 0.9298\n",
      "Epoch 99/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9664\n",
      "Epoch 00099: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1254 - accuracy: 0.9664 - val_loss: 0.2291 - val_accuracy: 0.9356\n",
      "Epoch 100/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1280 - accuracy: 0.9638\n",
      "Epoch 00100: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1280 - accuracy: 0.9637 - val_loss: 0.2381 - val_accuracy: 0.9269\n",
      "Epoch 101/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772/774 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9661\n",
      "Epoch 00101: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1258 - accuracy: 0.9661 - val_loss: 0.2572 - val_accuracy: 0.9302\n",
      "Epoch 102/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9649\n",
      "Epoch 00102: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1264 - accuracy: 0.9650 - val_loss: 0.2340 - val_accuracy: 0.9353\n",
      "Epoch 103/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9642\n",
      "Epoch 00103: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1285 - accuracy: 0.9642 - val_loss: 0.2434 - val_accuracy: 0.9324\n",
      "Epoch 104/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9655\n",
      "Epoch 00104: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1273 - accuracy: 0.9654 - val_loss: 0.2386 - val_accuracy: 0.9255\n",
      "Epoch 105/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9672\n",
      "Epoch 00105: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1258 - accuracy: 0.9669 - val_loss: 0.2474 - val_accuracy: 0.9356\n",
      "Epoch 106/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9672\n",
      "Epoch 00106: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1227 - accuracy: 0.9672 - val_loss: 0.2370 - val_accuracy: 0.9335\n",
      "Epoch 107/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9654\n",
      "Epoch 00107: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1241 - accuracy: 0.9655 - val_loss: 0.2417 - val_accuracy: 0.9320\n",
      "Epoch 108/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9657\n",
      "Epoch 00108: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1237 - accuracy: 0.9657 - val_loss: 0.2350 - val_accuracy: 0.9342\n",
      "Epoch 109/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9662\n",
      "Epoch 00109: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1200 - accuracy: 0.9662 - val_loss: 0.2388 - val_accuracy: 0.9324\n",
      "Epoch 110/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9672\n",
      "Epoch 00110: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1211 - accuracy: 0.9672 - val_loss: 0.2431 - val_accuracy: 0.9335\n",
      "Epoch 111/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9674\n",
      "Epoch 00111: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1206 - accuracy: 0.9674 - val_loss: 0.2380 - val_accuracy: 0.9284\n",
      "Epoch 112/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9674\n",
      "Epoch 00112: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1189 - accuracy: 0.9673 - val_loss: 0.2443 - val_accuracy: 0.9287\n",
      "Epoch 113/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9674\n",
      "Epoch 00113: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1217 - accuracy: 0.9674 - val_loss: 0.2556 - val_accuracy: 0.9247\n",
      "Epoch 114/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9652\n",
      "Epoch 00114: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1235 - accuracy: 0.9650 - val_loss: 0.2464 - val_accuracy: 0.9324\n",
      "Epoch 115/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9678\n",
      "Epoch 00115: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1176 - accuracy: 0.9678 - val_loss: 0.2501 - val_accuracy: 0.9291\n",
      "Epoch 116/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9664\n",
      "Epoch 00116: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1223 - accuracy: 0.9664 - val_loss: 0.2436 - val_accuracy: 0.9324\n",
      "Epoch 117/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9682\n",
      "Epoch 00117: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1175 - accuracy: 0.9682 - val_loss: 0.2386 - val_accuracy: 0.9309\n",
      "Epoch 118/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9669\n",
      "Epoch 00118: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1172 - accuracy: 0.9670 - val_loss: 0.2368 - val_accuracy: 0.9335\n",
      "Epoch 119/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1158 - accuracy: 0.9689\n",
      "Epoch 00119: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1155 - accuracy: 0.9688 - val_loss: 0.2590 - val_accuracy: 0.9342\n",
      "Epoch 120/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9681\n",
      "Epoch 00120: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1178 - accuracy: 0.9682 - val_loss: 0.2380 - val_accuracy: 0.9356\n",
      "Epoch 121/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9682\n",
      "Epoch 00121: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1166 - accuracy: 0.9682 - val_loss: 0.2390 - val_accuracy: 0.9367\n",
      "Epoch 122/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9694\n",
      "Epoch 00122: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1142 - accuracy: 0.9694 - val_loss: 0.2438 - val_accuracy: 0.9335\n",
      "Epoch 123/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9706\n",
      "Epoch 00123: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1125 - accuracy: 0.9707 - val_loss: 0.2316 - val_accuracy: 0.9305\n",
      "Epoch 124/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9694\n",
      "Epoch 00124: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1164 - accuracy: 0.9693 - val_loss: 0.2445 - val_accuracy: 0.9360\n",
      "Epoch 125/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9694\n",
      "Epoch 00125: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1139 - accuracy: 0.9694 - val_loss: 0.2276 - val_accuracy: 0.9320\n",
      "Epoch 126/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9713\n",
      "Epoch 00126: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1115 - accuracy: 0.9714 - val_loss: 0.2684 - val_accuracy: 0.9335\n",
      "Epoch 127/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9691\n",
      "Epoch 00127: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1144 - accuracy: 0.9691 - val_loss: 0.2459 - val_accuracy: 0.9316\n",
      "Epoch 128/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9704\n",
      "Epoch 00128: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1119 - accuracy: 0.9704 - val_loss: 0.2448 - val_accuracy: 0.9291\n",
      "Epoch 129/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9699\n",
      "Epoch 00129: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1130 - accuracy: 0.9698 - val_loss: 0.2242 - val_accuracy: 0.9364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1100 - accuracy: 0.9717\n",
      "Epoch 00130: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1099 - accuracy: 0.9718 - val_loss: 0.2456 - val_accuracy: 0.9305\n",
      "Epoch 131/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9695\n",
      "Epoch 00131: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1125 - accuracy: 0.9695 - val_loss: 0.2456 - val_accuracy: 0.9316\n",
      "Epoch 132/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9694\n",
      "Epoch 00132: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1145 - accuracy: 0.9695 - val_loss: 0.2520 - val_accuracy: 0.9291\n",
      "Epoch 133/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1113 - accuracy: 0.9709\n",
      "Epoch 00133: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1108 - accuracy: 0.9711 - val_loss: 0.2270 - val_accuracy: 0.9364\n",
      "Epoch 134/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1098 - accuracy: 0.9713\n",
      "Epoch 00134: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1097 - accuracy: 0.9713 - val_loss: 0.2522 - val_accuracy: 0.9298\n",
      "Epoch 135/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1069 - accuracy: 0.9728\n",
      "Epoch 00135: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1070 - accuracy: 0.9727 - val_loss: 0.2690 - val_accuracy: 0.9298\n",
      "Epoch 136/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9706\n",
      "Epoch 00136: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1104 - accuracy: 0.9706 - val_loss: 0.2414 - val_accuracy: 0.9375\n",
      "Epoch 137/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9711\n",
      "Epoch 00137: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1093 - accuracy: 0.9709 - val_loss: 0.2422 - val_accuracy: 0.9327\n",
      "Epoch 138/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1066 - accuracy: 0.9731\n",
      "Epoch 00138: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1067 - accuracy: 0.9732 - val_loss: 0.2477 - val_accuracy: 0.9335\n",
      "Epoch 139/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1040 - accuracy: 0.9732\n",
      "Epoch 00139: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1044 - accuracy: 0.9730 - val_loss: 0.2452 - val_accuracy: 0.9327\n",
      "Epoch 140/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1095 - accuracy: 0.9703\n",
      "Epoch 00140: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1094 - accuracy: 0.9704 - val_loss: 0.2397 - val_accuracy: 0.9298\n",
      "Epoch 141/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1063 - accuracy: 0.9719\n",
      "Epoch 00141: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1064 - accuracy: 0.9719 - val_loss: 0.2383 - val_accuracy: 0.9356\n",
      "Epoch 142/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1045 - accuracy: 0.9729\n",
      "Epoch 00142: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1045 - accuracy: 0.9729 - val_loss: 0.2439 - val_accuracy: 0.9349\n",
      "Epoch 143/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9717\n",
      "Epoch 00143: val_accuracy did not improve from 0.93855\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1067 - accuracy: 0.9718 - val_loss: 0.2592 - val_accuracy: 0.9316\n",
      "Epoch 144/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1080 - accuracy: 0.9715\n",
      "Epoch 00144: val_accuracy did not improve from 0.93855\n",
      "Restoring model weights from the end of the best epoch.\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1078 - accuracy: 0.9716 - val_loss: 0.2647 - val_accuracy: 0.9284\n",
      "Epoch 00144: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size_CNN_1D = 32\n",
    "epochs_CNN_1D     = 150\n",
    "\n",
    "history_CNN_1D    = model_CNN_1D.fit(X_train[..., np.newaxis], y_train_OHEV,\n",
    "                                     batch_size      = batch_size_CNN_1D,\n",
    "                                     epochs          = epochs_CNN_1D,\n",
    "                                     verbose         = 1,\n",
    "                                     validation_data =(X_test[..., np.newaxis], y_test_OHEV),\n",
    "                                     callbacks       = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "331031ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.23589277267456055\n",
      "Test accuracy: 0.9385454654693604\n"
     ]
    }
   ],
   "source": [
    "score_CNN_1D = model_CNN_1D.evaluate(X_test[..., np.newaxis], y_test_OHEV, verbose=0, batch_size = 32)\n",
    "print('Test loss:', score_CNN_1D[0])\n",
    "print('Test accuracy:', score_CNN_1D[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "975bf891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385454654693604"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_CNN_1D[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42b09bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "fig.suptitle(nom_dataset + norm_type + model_surname + ' - CNN 1D - Training / Testing loss and accuracy', fontsize = 18)\n",
    "ax[0].plot(history_CNN_1D.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history_CNN_1D.history['val_loss'], color='r', label=\"Testing loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True, fontsize = 14)\n",
    "ax[0].tick_params(axis='x', labelsize=14)\n",
    "ax[0].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "ax[1].plot(history_CNN_1D.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history_CNN_1D.history['val_accuracy'], color='r',label=\"Testing accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True, fontsize = 14)\n",
    "ax[1].tick_params(axis='x', labelsize=14)\n",
    "ax[1].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(os.path.join(path_pic, picture_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6bb21a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_CNN_1D = np.argmax(model_CNN_1D.predict(X_val_norm[..., np.newaxis]),axis=1)\n",
    "y_pred_CNN_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "37047e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc = np.argmax(y_OHEV_val, axis=1)\n",
    "y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "04983276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.83      0.73      0.78       756\n",
      "        car_horn       0.84      0.88      0.86       252\n",
      "children_playing       0.70      0.76      0.73       700\n",
      "        dog_bark       0.76      0.85      0.80       700\n",
      "           siren       0.80      0.71      0.75       602\n",
      "\n",
      "        accuracy                           0.78      3010\n",
      "       macro avg       0.79      0.79      0.78      3010\n",
      "    weighted avg       0.78      0.78      0.77      3010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_set_CNN_1D = classification_report(y_test_enc, y_pred_CNN_1D, target_names=nom_classes)\n",
    "print(metrics_set_CNN_1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b8d8936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_1D\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 369, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 369, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 369, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 184, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 184, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10304)             0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                515250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 526,291\n",
      "Trainable params: 526,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the highest accuracy\n",
    "\n",
    "model_CNN_1D_saved = load_model(os.path.join(path_models, 'Model_CNN_1D_weights_0_best' + norm_type + model_surname + '.hdf5'))\n",
    "model_CNN_1D_saved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e7d38f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 2ms/step - loss: 1.7807 - accuracy: 0.7751\n",
      "Test loss: 1.7807343006134033\n",
      "Test accuracy: 0.775083065032959\n"
     ]
    }
   ],
   "source": [
    "score_CNN_1D_saved = model_CNN_1D_saved.evaluate(X_val_norm[..., np.newaxis], y_OHEV_val, verbose=1, batch_size = 32)\n",
    "print('Test loss:', score_CNN_1D_saved[0])\n",
    "print('Test accuracy:', score_CNN_1D_saved[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8b118cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_CNN_1D_saved = np.argmax(model_CNN_1D_saved.predict(X_val_norm[..., np.newaxis]),axis=1)\n",
    "y_pred_CNN_1D_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a8731110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.83      0.73      0.78       756\n",
      "        car_horn       0.84      0.88      0.86       252\n",
      "children_playing       0.70      0.76      0.73       700\n",
      "        dog_bark       0.76      0.85      0.80       700\n",
      "           siren       0.80      0.71      0.75       602\n",
      "\n",
      "        accuracy                           0.78      3010\n",
      "       macro avg       0.79      0.79      0.78      3010\n",
      "    weighted avg       0.78      0.78      0.77      3010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_set_CNN_1D_saved = classification_report(y_test_enc, y_pred_CNN_1D_saved, target_names=nom_classes)\n",
    "print(metrics_set_CNN_1D_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f26e1dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple confusion matrix\n",
    "\n",
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(y_test_enc, y_pred_CNN_1D_saved)\n",
    "title = nom_dataset + norm_type + model_surname + ' - Classifier CNN 1D (best model) - Highest accuracy test: '+ str(\"{:0.2f}%\".format(score_CNN_1D_saved[1]*100))\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True, \n",
    "            fmt='g', \n",
    "            cmap=cmap_cm, \n",
    "            annot_kws={\"size\": 8}, \n",
    "            xticklabels=nom_classes, \n",
    "            yticklabels=nom_classes)\n",
    "plt.title(title, fontsize = 12)\n",
    "plt.savefig(os.path.join(path_pic, picture_name))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0322d5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv1D at 0x19afa90d220>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x19afac02460>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x19afa90d520>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling1D at 0x19afa66ed60>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x19afa8dadf0>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x19afa8b9ee0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x19afa8b9280>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x19afa8daca0>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CNN_1D_saved.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0847ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.31187347,  0.56965   ,  0.39437732, -0.20509183,\n",
      "          0.19913703,  0.2268352 ,  0.09268779,  0.18787232,\n",
      "          0.04275443,  0.13827178, -0.17619027, -0.4258645 ,\n",
      "         -1.331382  , -0.14218913, -0.02965143, -0.16826132,\n",
      "          0.01487918,  0.3717704 ,  0.3878506 , -0.05878924,\n",
      "         -0.30350992,  0.5127523 , -0.22276865, -0.2655716 ,\n",
      "          0.20564559, -0.05680934,  0.04388108, -0.24845566]],\n",
      "\n",
      "       [[ 0.14944993, -0.24126102,  0.4915105 ,  0.14564605,\n",
      "         -0.17802507, -0.3725471 ,  0.04374021, -0.476927  ,\n",
      "         -0.27167296, -0.3029667 , -0.10299665,  0.32777965,\n",
      "         -1.4822747 , -0.47129154, -0.0410324 ,  0.03127728,\n",
      "         -1.2463312 , -0.08369651, -0.04190935, -0.17118697,\n",
      "         -0.3997618 ,  0.20154734, -0.36921975,  0.4081965 ,\n",
      "         -0.34341362,  0.19413154,  0.05621165, -0.26439404]],\n",
      "\n",
      "       [[ 0.36339745,  0.0211691 , -0.07887495, -0.18689027,\n",
      "         -0.31200123,  0.30339035,  0.19391611, -0.50361925,\n",
      "         -0.03620312, -0.3525988 ,  0.21338344,  0.01008343,\n",
      "         -1.8110774 ,  0.07215063, -0.13442454, -0.53618866,\n",
      "          0.2535772 ,  0.11744373,  0.16394459, -0.80254424,\n",
      "          0.02869316,  0.09549306,  0.3075659 ,  0.28781235,\n",
      "         -0.12072361,  0.05699543,  0.08493742,  0.04117731]],\n",
      "\n",
      "       [[-0.4041297 , -0.06396037, -0.0348426 ,  0.45677403,\n",
      "          0.06914423, -0.22879538, -0.00738837,  0.28598493,\n",
      "          0.24946487,  0.24485874,  0.05518118, -0.145792  ,\n",
      "         -2.1615233 ,  0.29147223, -0.24544139, -0.15757565,\n",
      "          0.09651324, -0.03856415,  0.3126379 ,  0.3846643 ,\n",
      "          0.01738135, -0.33120143, -0.12256071,  0.29632688,\n",
      "         -1.4553899 ,  0.42244405,  0.30496076,  0.24612361]],\n",
      "\n",
      "       [[-0.19915485,  0.14207721, -0.02329264,  0.26243395,\n",
      "         -0.38818538,  0.10495155,  0.11600974,  0.20580208,\n",
      "         -0.03249079,  0.3442743 ,  0.23824573,  0.32312372,\n",
      "         -2.3920329 ,  0.14754552, -0.07085085,  0.12821233,\n",
      "          0.30422783,  0.04165489, -0.42260647, -0.28773782,\n",
      "          0.37232026, -0.0580176 ,  0.18490884,  0.29274395,\n",
      "          0.12254251, -0.12266418,  0.23155987,  0.20989236]],\n",
      "\n",
      "       [[-0.02110151, -0.36904034, -0.17552987, -0.20780824,\n",
      "          0.48162782, -0.16834481,  0.14610934,  0.13552165,\n",
      "         -0.05147399,  0.04005809,  0.19876356,  0.15940367,\n",
      "         -2.0395734 , -0.02413435,  0.34074944,  0.01586675,\n",
      "         -0.07385228,  0.15200932, -0.11934877,  0.28636768,\n",
      "          0.28953907, -0.23332238,  0.01229841, -0.37029573,\n",
      "          0.06043968, -0.4944323 , -0.14057477,  0.16448773]],\n",
      "\n",
      "       [[-0.2632703 ,  0.15594684, -0.28394368,  0.17682256,\n",
      "         -0.34924492, -0.36642972,  0.19789836,  0.02456089,\n",
      "          0.6177604 ,  0.30277154, -0.17692642, -0.37254122,\n",
      "         -1.4360664 ,  0.33547333,  0.40263498,  0.43488958,\n",
      "         -0.22267449, -0.09287748, -0.0866462 ,  0.22514941,\n",
      "          0.12571402, -0.09286863,  0.5296262 , -0.08613417,\n",
      "          0.02955201,  0.13836753, -0.2566903 ,  0.3082075 ]]],\n",
      "      dtype=float32), array([ 0.01229936,  0.02390167,  0.01814952,  0.01805719,  0.01693872,\n",
      "        0.05384118, -0.00182857,  0.07697234,  0.02401023,  0.03646649,\n",
      "        0.01171728,  0.00601055,  0.05976195,  0.02696385,  0.02056577,\n",
      "        0.0092317 ,  0.08323789,  0.00073479,  0.02839067,  0.02847736,\n",
      "        0.05422768,  0.00578657,  0.02990885,  0.01780236,  0.10815489,\n",
      "        0.0411162 ,  0.00260897,  0.01833375], dtype=float32)]\n",
      "[array([[[-1.45188853e-01,  8.91917869e-02,  1.18335001e-01, ...,\n",
      "         -7.60607272e-02,  1.18332515e-02,  3.52971777e-02],\n",
      "        [-3.93189490e-02,  9.62826908e-02,  1.05583213e-01, ...,\n",
      "         -4.15034443e-02,  8.99300128e-02,  6.11096807e-02],\n",
      "        [-7.82277137e-02,  1.02269001e-01,  4.93606329e-02, ...,\n",
      "         -1.35181919e-01,  1.65224984e-01,  2.72900425e-02],\n",
      "        ...,\n",
      "        [-4.14645933e-02,  2.03921758e-02, -1.07306033e-01, ...,\n",
      "         -9.86575149e-04, -7.93165714e-02, -1.51817435e-02],\n",
      "        [-5.22955554e-03,  1.35244969e-02,  3.34720537e-02, ...,\n",
      "         -1.28162140e-02,  5.25295846e-02, -1.26430849e-02],\n",
      "        [ 6.87567294e-02, -9.44259986e-02, -3.20777185e-02, ...,\n",
      "          6.15207814e-02, -2.47065611e-02, -1.44058038e-02]],\n",
      "\n",
      "       [[-4.00464460e-02,  4.61739600e-02,  3.92522523e-03, ...,\n",
      "         -3.17187794e-02, -5.68934642e-02,  9.85604990e-03],\n",
      "        [-7.66036287e-03,  5.24785854e-02,  1.57446018e-03, ...,\n",
      "         -1.16324194e-01,  3.87777276e-02, -5.96321467e-03],\n",
      "        [-4.16752659e-02,  6.47462308e-02,  1.07929332e-03, ...,\n",
      "         -1.00628473e-02,  4.51490246e-02,  7.89682120e-02],\n",
      "        ...,\n",
      "        [ 4.65702005e-02,  2.86893104e-03,  7.07445219e-02, ...,\n",
      "          4.22061235e-03, -2.58431528e-02, -2.10375222e-03],\n",
      "        [ 3.27378213e-02,  1.96828153e-02,  3.39409225e-02, ...,\n",
      "          7.00320769e-03,  5.81385987e-03,  1.22366063e-02],\n",
      "        [ 5.41489832e-02, -7.16300532e-02, -3.18620019e-02, ...,\n",
      "         -4.28860821e-02,  2.55171638e-02, -3.45404372e-02]],\n",
      "\n",
      "       [[-1.94498058e-02,  9.22757946e-03,  5.79847983e-05, ...,\n",
      "          2.43013408e-02, -4.31804657e-02, -5.15638664e-02],\n",
      "        [ 2.53164582e-02,  6.06130771e-02, -1.31678339e-02, ...,\n",
      "          4.47228365e-02,  1.46195628e-02,  2.83654742e-02],\n",
      "        [-1.42524913e-02,  3.25125158e-02,  5.90134449e-02, ...,\n",
      "          2.56685577e-02, -1.70465130e-02, -8.17098841e-03],\n",
      "        ...,\n",
      "        [-2.82200868e-03,  1.15799485e-02,  8.92456993e-02, ...,\n",
      "          5.21034449e-02,  1.96590461e-02,  4.89655472e-02],\n",
      "        [ 2.04624366e-02,  8.83880444e-03,  1.81192870e-03, ...,\n",
      "          4.22366429e-03, -2.69058831e-02,  2.68180296e-02],\n",
      "        [ 1.08130887e-01, -5.12355752e-02, -1.78118292e-02, ...,\n",
      "         -1.62721425e-02,  2.52931416e-02,  6.71323854e-04]],\n",
      "\n",
      "       [[-5.50692109e-03,  1.88617427e-02,  1.72234580e-01, ...,\n",
      "          2.91990098e-02, -1.31235747e-02,  5.62772667e-03],\n",
      "        [-5.05609587e-02,  4.07300424e-03,  6.97910860e-02, ...,\n",
      "         -5.54702021e-02,  1.33343618e-02, -9.81664956e-02],\n",
      "        [ 7.41738975e-02, -1.42254354e-02, -3.06741707e-02, ...,\n",
      "         -2.41039600e-02, -1.13796629e-02, -2.28710696e-02],\n",
      "        ...,\n",
      "        [-1.03249121e-02,  1.38397003e-02, -8.88128057e-02, ...,\n",
      "          3.81437987e-02, -1.04546333e-02,  2.56420858e-02],\n",
      "        [ 6.04422353e-02, -1.30920652e-02, -4.10667956e-02, ...,\n",
      "         -1.95002276e-02, -1.23148151e-02, -2.20722035e-02],\n",
      "        [ 1.40616624e-02,  3.15018743e-03, -9.50860456e-02, ...,\n",
      "          2.13661417e-02, -1.49060367e-02, -1.04245702e-02]],\n",
      "\n",
      "       [[-8.94594267e-02,  5.04380427e-02,  1.63754486e-02, ...,\n",
      "          7.47351209e-03, -9.91828390e-04, -3.34821753e-02],\n",
      "        [ 1.09722398e-01, -2.79619154e-02, -6.17768802e-02, ...,\n",
      "          1.38936611e-02,  2.61173099e-02,  3.96347754e-02],\n",
      "        [ 1.65158641e-02,  3.26946029e-03,  1.14392035e-01, ...,\n",
      "         -4.95829619e-02,  8.03607106e-02, -1.42331352e-03],\n",
      "        ...,\n",
      "        [ 6.33445233e-02, -2.76259799e-02,  6.32481277e-02, ...,\n",
      "          6.17897650e-03,  2.95312051e-02,  1.73451491e-02],\n",
      "        [ 7.45161623e-02, -6.21765759e-03,  2.36707851e-02, ...,\n",
      "         -5.40816039e-02,  8.71097818e-02, -6.41735196e-02],\n",
      "        [ 1.08263105e-01, -2.56983377e-03, -5.56424558e-02, ...,\n",
      "          6.08411543e-02, -4.67736013e-02,  3.12318951e-02]]],\n",
      "      dtype=float32), array([-2.0447433e-02, -3.2388982e-03,  9.1116093e-03, -1.3877642e-03,\n",
      "       -2.5062913e-02,  4.2040080e-02,  1.1576412e-02,  2.5953449e-02,\n",
      "       -2.7260618e-02, -1.6682118e-03,  3.3442609e-03, -3.3674285e-02,\n",
      "       -7.5898413e-03, -1.6439613e-02, -1.1930410e-02, -1.2833230e-02,\n",
      "       -1.9716317e-04, -2.7907509e-03, -1.1809398e-02,  1.7688740e-02,\n",
      "       -4.1683331e-02,  6.1887436e-02,  3.7144732e-02,  1.0001903e-02,\n",
      "        8.5150339e-03,  3.6376420e-02,  2.1435530e-03,  2.5429780e-02,\n",
      "        1.1995630e-02, -6.3051333e-13, -4.8511427e-24,  5.7070120e-03,\n",
      "        5.4892447e-02,  5.2701388e-02], dtype=float32)]\n",
      "[array([[[ 1.81545597e-02, -4.52801175e-02,  1.35679487e-02, ...,\n",
      "         -1.17016204e-01, -5.91274071e-03, -1.10751115e-01],\n",
      "        [-3.76785845e-02,  5.27144112e-02, -2.30099428e-02, ...,\n",
      "          2.69937962e-02, -2.28065923e-02,  4.51828875e-02],\n",
      "        [-8.41016124e-04,  9.31953192e-02,  2.30742563e-02, ...,\n",
      "          4.99254907e-04, -4.70746011e-02, -1.73988029e-01],\n",
      "        ...,\n",
      "        [ 4.00913693e-02, -9.11247283e-02, -5.60962595e-02, ...,\n",
      "          2.17438396e-02,  8.28443590e-05, -9.51339211e-03],\n",
      "        [-3.85705940e-02,  6.55149892e-02,  3.46293710e-02, ...,\n",
      "          3.90131995e-02, -1.07526101e-01, -7.08300546e-02],\n",
      "        [-4.57764268e-02, -4.93374187e-03,  1.60493460e-02, ...,\n",
      "          7.49543682e-03,  8.89974274e-03, -3.33671905e-02]],\n",
      "\n",
      "       [[-1.04050092e-01, -5.74980602e-02, -5.51085174e-02, ...,\n",
      "          1.04845362e-02,  4.98644933e-02, -6.74912147e-03],\n",
      "        [ 7.91034922e-02, -1.76968258e-02,  3.59407403e-02, ...,\n",
      "          5.04607782e-02, -1.34134823e-02, -1.64910534e-03],\n",
      "        [-1.30444653e-02, -4.42129225e-02, -3.46370377e-02, ...,\n",
      "         -1.11022122e-01, -5.95633350e-02,  5.32545298e-02],\n",
      "        ...,\n",
      "        [ 2.87167709e-02, -2.00525410e-02, -4.96567115e-02, ...,\n",
      "          2.59099603e-02,  5.98661788e-03, -8.64297152e-04],\n",
      "        [ 2.86568403e-02, -6.86074495e-02,  2.81374902e-02, ...,\n",
      "         -2.48594079e-02, -5.41281588e-02,  4.40138839e-02],\n",
      "        [ 7.37629384e-02,  1.63999964e-02,  1.17667429e-02, ...,\n",
      "          7.30531290e-02,  3.12524289e-02,  3.93941030e-02]],\n",
      "\n",
      "       [[ 1.30584056e-03, -1.32002747e-02, -8.16720501e-02, ...,\n",
      "         -1.12070337e-01, -4.71822508e-02, -3.61837931e-02],\n",
      "        [ 1.80009101e-02, -1.60237458e-02,  3.88959721e-02, ...,\n",
      "         -3.45147550e-02, -1.60938613e-02,  1.24569470e-02],\n",
      "        [-5.47782704e-03, -1.06654495e-01,  1.19061032e-02, ...,\n",
      "         -6.22432195e-02,  6.07457757e-02,  2.31555365e-02],\n",
      "        ...,\n",
      "        [ 9.25539806e-03,  6.43556118e-02, -2.81965993e-02, ...,\n",
      "         -6.37483690e-03, -7.37885106e-03, -5.00765927e-02],\n",
      "        [ 1.00085050e-01, -3.71106081e-02,  2.08334345e-02, ...,\n",
      "         -7.66132772e-02, -2.79340334e-03, -4.99160737e-02],\n",
      "        [ 2.69018803e-02,  5.71926944e-02,  4.88847084e-02, ...,\n",
      "         -4.14025709e-02,  1.52710881e-02, -5.91912866e-02]]],\n",
      "      dtype=float32), array([ 2.1561077e-02,  2.0528536e-02,  4.7815084e-02,  2.2086501e-02,\n",
      "        8.9813219e-03,  1.1036132e-02,  3.5731871e-02,  4.5247017e-03,\n",
      "        5.3929258e-02,  2.9273031e-02,  4.1362975e-02,  2.1567846e-02,\n",
      "        2.3222242e-02, -2.0432845e-03,  1.7253600e-02,  3.0196154e-02,\n",
      "       -4.6842531e-03, -6.4225852e-02,  2.9093575e-02, -4.3367282e-02,\n",
      "       -3.9439861e-02,  1.0260554e-02,  1.2033156e-03, -2.6936730e-02,\n",
      "        6.2687449e-02, -1.8749189e-02,  6.1029095e-02, -6.6766913e-31,\n",
      "        8.3617335e-03, -9.0027265e-03,  2.0040153e-02, -1.1894413e-02,\n",
      "        9.5136807e-04,  3.1679578e-02,  2.6103582e-02,  3.0706611e-02,\n",
      "        2.3956198e-02, -9.8447641e-03,  2.5941415e-02,  1.2625937e-02,\n",
      "        1.8367926e-02, -4.6178471e-02, -2.2635829e-02,  4.0219914e-02,\n",
      "        4.9152851e-02,  3.0607445e-02,  1.0383484e-02, -3.3541147e-03,\n",
      "       -5.3250715e-03,  2.2144269e-02, -2.0280153e-02,  3.3951737e-02,\n",
      "        1.7986875e-02,  3.9647024e-02,  2.9409559e-02,  1.6112885e-02],\n",
      "      dtype=float32)]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[array([[-0.21120474, -0.00524259, -0.10829461, ...,  0.18491855,\n",
      "        -0.08189836,  0.25463513],\n",
      "       [ 0.06178788, -0.08838436, -0.00133886, ..., -0.0498901 ,\n",
      "         0.03992855,  0.04226467],\n",
      "       [-0.01321401,  0.01931093, -0.00066955, ...,  0.02854181,\n",
      "        -0.00504359, -0.03477243],\n",
      "       ...,\n",
      "       [ 0.07789508, -0.02968639,  0.0194552 , ..., -0.02637846,\n",
      "         0.04612647, -0.08181136],\n",
      "       [ 0.12343042, -0.0457854 ,  0.09930021, ..., -0.11146163,\n",
      "         0.03012786,  0.05161817],\n",
      "       [ 0.07426894, -0.13274333, -0.00615935, ...,  0.01478388,\n",
      "         0.18285838, -0.04706763]], dtype=float32), array([-0.01274817,  0.00644557, -0.00020243, -0.00715124,  0.01326905,\n",
      "       -0.01429185, -0.00172754, -0.00853652,  0.01951392, -0.00194459,\n",
      "        0.00291082, -0.0135257 , -0.00611873, -0.00458056,  0.01016462,\n",
      "       -0.01233897, -0.00978371, -0.01365671,  0.00838499, -0.01572184,\n",
      "       -0.0148635 , -0.01546774, -0.0015862 ,  0.00542443, -0.01156904,\n",
      "       -0.00350419, -0.01441918,  0.00711705, -0.01351845,  0.00150768,\n",
      "        0.00962905, -0.00121522, -0.00921537,  0.00599659, -0.00947774,\n",
      "       -0.01136281,  0.00695461,  0.00472802, -0.00586253, -0.02192194,\n",
      "       -0.00159143,  0.00199725, -0.0179156 , -0.00493053, -0.00028185,\n",
      "       -0.0052609 ,  0.02217016,  0.00444668, -0.01140025,  0.00022821],\n",
      "      dtype=float32)]\n",
      "[array([[ 0.0483472 , -0.7207894 , -0.95205873,  0.24067989,  0.59088063],\n",
      "       [-0.05668647, -0.11619767, -0.02967926, -0.86406255,  0.4024094 ],\n",
      "       [ 1.028875  , -0.8569481 , -0.5035664 , -0.04798215,  0.43731964],\n",
      "       [ 0.96334445,  0.60583264, -0.48962724,  0.08984878, -0.549255  ],\n",
      "       [ 0.24317382, -1.5116599 ,  0.3356443 ,  0.01184117,  0.30155358],\n",
      "       [ 0.05171666,  0.19797209, -0.1823294 ,  0.2096783 , -1.4952668 ],\n",
      "       [-0.31253782,  1.1526184 , -0.4224875 , -0.5267371 ,  0.80186284],\n",
      "       [ 0.5139751 , -0.5948004 ,  0.08038782,  0.67578816, -1.2074375 ],\n",
      "       [-0.4905093 , -0.19092913,  1.0226547 , -0.43861902,  0.36503193],\n",
      "       [-0.55094683, -0.773498  , -0.25585634,  0.20701505,  0.62132895],\n",
      "       [ 0.11820248,  0.9404702 , -0.08494069, -0.49986318, -0.6422994 ],\n",
      "       [-0.6416694 ,  0.7681377 , -0.6677892 ,  0.14152557,  0.74220955],\n",
      "       [-0.88901556, -0.06209291,  0.20974836,  0.50209606,  0.11902687],\n",
      "       [-0.7596252 ,  0.6950528 , -0.23299643, -0.19444253,  0.6854838 ],\n",
      "       [ 0.7165942 , -0.20887637,  0.25853527, -0.36041284, -0.62685436],\n",
      "       [-0.611899  ,  1.2011639 ,  0.03013394,  0.69631255, -0.7400794 ],\n",
      "       [ 0.05788507,  0.84604156,  0.00638928, -0.1035725 , -1.0453199 ],\n",
      "       [ 0.01845466, -0.7592364 , -0.7341976 ,  0.5434144 ,  0.60772747],\n",
      "       [-0.13148609, -1.7890879 ,  0.28225705,  0.37689182,  0.48682648],\n",
      "       [ 0.21705225, -0.02668503, -1.113931  ,  0.19446589,  0.5939224 ],\n",
      "       [-0.6373418 ,  1.1211706 , -0.50786746,  0.28724533,  0.17348121],\n",
      "       [-0.14522515, -0.21826251, -0.6575717 ,  0.5986803 ,  0.35479254],\n",
      "       [ 0.06786754, -0.9756574 ,  0.07665167,  0.3696169 , -0.9060415 ],\n",
      "       [ 0.5435779 ,  0.35373974,  0.08168558, -0.467762  , -0.5263986 ],\n",
      "       [ 0.46292496,  1.0584913 , -0.5606581 , -0.21754241, -0.19455443],\n",
      "       [-0.08665712, -0.31151512,  0.2436966 ,  0.8433919 , -0.5845647 ],\n",
      "       [ 0.34092912,  0.14720298,  0.0341424 ,  0.6938928 , -1.3639386 ],\n",
      "       [-0.26405132,  0.2239208 , -0.33871013, -0.48182315,  1.1841913 ],\n",
      "       [ 0.7717048 ,  0.14734961, -0.96788365,  0.22358754, -0.40890458],\n",
      "       [-0.8678449 , -0.31688318,  0.47977346,  0.4513964 , -0.30133998],\n",
      "       [ 0.38401294,  0.40478787,  0.52065164, -0.5546976 , -0.41657618],\n",
      "       [-0.72058827,  0.02977445,  0.5993577 ,  0.54123443, -0.4896619 ],\n",
      "       [ 0.3234256 , -0.52582866, -0.35057816,  1.0431626 , -0.2114787 ],\n",
      "       [ 0.14916705,  1.0442936 ,  0.39580056, -0.453498  , -0.35644603],\n",
      "       [-0.13598979,  1.2765428 , -0.3213663 , -0.11025617, -0.21002588],\n",
      "       [-0.51010525,  1.3291155 , -0.00785091,  0.6165566 , -0.5571902 ],\n",
      "       [ 0.5556219 , -0.01639465, -0.2170563 , -0.7359952 ,  0.30461544],\n",
      "       [ 0.50088215,  0.3618904 ,  0.14886133, -0.52577114, -0.6463083 ],\n",
      "       [ 0.5043021 ,  1.0487463 , -0.4471909 , -0.44253927, -0.20893314],\n",
      "       [ 0.47191504,  0.44309378, -1.0020914 ,  0.4469821 ,  0.18976104],\n",
      "       [-0.6995225 , -0.33049744, -0.27388754, -0.0027945 ,  0.74855024],\n",
      "       [-0.72681475,  1.0184207 , -0.12355307, -0.4678916 ,  0.50530475],\n",
      "       [ 0.520915  ,  0.7650956 , -0.72822535,  0.2368354 , -0.39689425],\n",
      "       [ 0.00199262,  1.5768391 , -0.12328649, -0.20022187,  0.5511538 ],\n",
      "       [-0.59266734, -0.34514666, -0.33459136,  0.00710912,  1.2035501 ],\n",
      "       [-0.6344024 , -0.09641112,  0.23712696,  0.69055206, -0.49214694],\n",
      "       [ 0.07419783, -0.9562736 ,  0.616741  , -0.5570219 ,  0.16453314],\n",
      "       [-0.40769663,  1.6615322 ,  0.36178952, -0.31243804, -0.21652757],\n",
      "       [-0.28830746,  0.13812226, -0.39478585,  0.7091564 , -0.37854254],\n",
      "       [ 0.26999223,  0.06656395,  0.39160714, -0.07745495, -1.4387277 ]],\n",
      "      dtype=float32), array([-0.00699311, -0.0268539 ,  0.01635103, -0.01566892, -0.00180505],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model_CNN_1D_saved.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e2b7df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00699311, -0.0268539 ,  0.01635103, -0.01566892, -0.00180505],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model_CNN_1D_saved.get_layer('Output').get_weights()\n",
    "weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73169e4d",
   "metadata": {},
   "source": [
    "## Metrics for the classifiers\n",
    "\n",
    "\n",
    "1. Accuracy: Accuracy is a measure of how many correct predictions a model makes overall, i.e., the ratio of correct predictions to the total number of predictions. It's a commonly used metric for evaluating models, but it may not be suitable in certain situations.\n",
    "\n",
    "2. Precision: Precision measures the ratio of true positives (correctly predicted positive instances) to all instances predicted as positive. It focuses on the accuracy of positive predictions.\n",
    "\n",
    "3. Recall: Recall, also known as sensitivity or true positive rate, measures the ratio of true positives to all actual positive instances. It focuses on how well a model captures all the positive instances.\n",
    "\n",
    "4. F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure that takes into account both false positives and false negatives. The F1 score is especially useful when you want to strike a balance between precision and recall.\n",
    "\n",
    "\n",
    "The F1 score is a metric that combines precision and recall, and it is particularly useful in situations where class imbalance or unequal misclassification costs are present. In such contexts, the F1 score can be more informative and meaningful than accuracy.\n",
    "\n",
    "A context where considering the F1 score makes more sense than accuracy:\n",
    "\n",
    "**Medical Diagnosis:**\n",
    "\n",
    "Imagine you're developing a model to diagnose a rare disease, and only 5% of the population has this disease. In this case, you have a significant class imbalance, where the majority of cases are negative (non-disease) and only a small fraction are positive (disease). If you were to use accuracy as the evaluation metric, the model could achieve a high accuracy by simply predicting \"negative\" for every case, because it would be correct 95% of the time due to the class imbalance. However, this would be entirely useless for detecting the actual disease.\n",
    "\n",
    "In this scenario, you'd be more interested in the F1 score. The F1 score considers both precision and recall, helping you find a balance between correctly identifying the disease (high recall) and not making too many false positive predictions (high precision). A high F1 score in this context indicates that your model is effective at correctly identifying the disease while minimizing false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "46413ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['ANN', 'CNN_1D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1a186ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to run the classifiers and their metrics\n",
    "\n",
    "def model_classifiers(classifiers:list, \n",
    "                      db: pd.DataFrame, \n",
    "                      scalerOpt: str, \n",
    "                      use_PCA = False):\n",
    "    \n",
    "    # Clear the session to start a new training\n",
    "    K.clear_session()\n",
    " \n",
    "    es = EarlyStopping(monitor='accuracy', min_delta=0.0001, patience=50, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    \n",
    "    count       = 1\n",
    "    batch_size  = 32\n",
    "    verbose     = True\n",
    "    models      = []\n",
    "    acc_set     = pd.DataFrame(index=None, columns=['Model',\n",
    "                                                    'Fold',\n",
    "                                                    'Accuracy(Train)',\n",
    "                                                    'Accuracy(Val)',\n",
    "                                                    'F1(Train)',\n",
    "                                                    'F1(Val)', \n",
    "                                                    'Precision(Train)',\n",
    "                                                    'Precision(Val)', \n",
    "                                                    'Recall(Train)',\n",
    "                                                    'Recall(Val)', \n",
    "                                                    'Conf_M',\n",
    "                                                    'Process_time',                                                     \n",
    "                                                    'Class_report(Val)'])\n",
    "    \n",
    "    for fold in np.unique(db['Fold']):\n",
    "        print(f\"Validation fold: {fold}\")\n",
    "\n",
    "        DB_VAL = db[db['Fold'] == fold]\n",
    "        DB_TRN = db[db['Fold'] != fold]\n",
    "\n",
    "        X      = DB_TRN.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "        y      = np.array(DB_TRN.Class_categorical.to_list())\n",
    "        y_OHEV = np.array(DB_TRN.Class_OHEV.to_list())\n",
    "\n",
    "        X_val      = DB_VAL.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "        y_val      = np.array(DB_VAL.Class_categorical.to_list())\n",
    "        y_OHEV_val = np.array(DB_VAL.Class_OHEV.to_list())\n",
    "        \n",
    "        neurons  = X.shape[1]\n",
    "        \n",
    "        X_statistics = pd.DataFrame({'mean': X.mean(), 'std': X.std(), 'min': X.min(), 'max': X.max()})\n",
    "\n",
    "        X_mean   = X_statistics.values[:, 0]\n",
    "        X_std    = X_statistics.values[:, 1]\n",
    "        X_min    = X_statistics.values[:, 2]\n",
    "        X_max    = X_statistics.values[:, 3]\n",
    "        \n",
    "        if scalerOpt == \"normalization\":\n",
    "            X_train_norm = (X.values - X_min) / (X_max - X_min)\n",
    "            X_val_norm   = (X_val.values - X_min) / (X_max - X_min)\n",
    "            batch_type    = '_norm'\n",
    "            print(f'X_train_norm shape...:{X_train_norm.shape}')\n",
    "            print(f'X_val_norm shape.....:{X_val_norm.shape}\\n')\n",
    "            \n",
    "        elif scalerOpt == \"standardization\":\n",
    "            X_train_norm = (X.values - X_mean) / X_std\n",
    "            X_val_norm   = (X_val.values - X_mean) / X_std\n",
    "            batch_type    = '_std'\n",
    "            print(f'X_train_norm shape...:{X_train_norm.shape}')\n",
    "            print(f'X_val_norm shape.....:{X_val_norm.shape}\\n')\n",
    "            \n",
    "        else:\n",
    "            sys.exit()\n",
    "            \n",
    "        if use_PCA:\n",
    "            pcaT = PCA()\n",
    "            pcaT.fit(X_train_norm)\n",
    "            ratio = pcaT.explained_variance_ratio_\n",
    "\n",
    "            batch_type = batch_type + '_PCA'\n",
    "\n",
    "            T           = 0.98\n",
    "            current_sum = 0\n",
    "            countComp   = 0\n",
    "\n",
    "            for element in ratio:\n",
    "                current_sum += element\n",
    "                countComp   += 1\n",
    "\n",
    "                if current_sum >= T:\n",
    "                    break\n",
    "\n",
    "            # Print the result\n",
    "            print(\"Sum of elements:\", current_sum)\n",
    "            print(\"Number of elements summed:\", countComp)           \n",
    "\n",
    "            pca          = PCA(n_components = countComp)\n",
    "            X_train_norm = pca.fit_transform(X_train_norm)\n",
    "            X_val_norm   = pca.transform(X_val_norm)\n",
    "            neurons      = countComp\n",
    "        \n",
    "        # The training dataset will be 10% reduced compared with the ML techniques, separating a test set to monitor\n",
    "        # the accuracy during training\n",
    "        X_train_norm, X_test_norm, y_train, y_test = train_test_split(X_train_norm, y_OHEV, test_size=0.1, random_state=42, stratify=y_OHEV)\n",
    "\n",
    "        for i in tqdm(range(len(classifiers))):\n",
    "            \n",
    "            name         = classifiers[i]\n",
    "            model_name   = ('Model_' + classifiers[i] + '_' + str(count))\n",
    "            count        = count + 1\n",
    "            \n",
    "            if classifiers[i] == 'ANN':\n",
    "                \n",
    "                filepath       = os.path.join(path_models, 'Model_ANN_weights_0_best' + norm_type + model_surname + '.hdf5')\n",
    "                checkpoint     = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "                callbacks_list = [checkpoint, es]\n",
    "               \n",
    "                model = build_ANN_model(model_name, neurons)\n",
    "                model.summary()\n",
    "                print(name)\n",
    "                print(np.shape(X_train_norm))    \n",
    "\n",
    "                model.fit(X_train_norm, \n",
    "                          y_train, \n",
    "                          batch_size      = batch_size, \n",
    "                          epochs          = 350, \n",
    "                          verbose         = verbose,                               \n",
    "                          validation_data = (X_test_norm, y_test),\n",
    "                          callbacks       = callbacks_list)\n",
    "                \n",
    "                model= load_model(os.path.join(path_models, 'Model_ANN_weights_0_best' + norm_type + model_surname + '.hdf5'))\n",
    "                print('Best model loaded')\n",
    "\n",
    "            else:\n",
    "\n",
    "                filepath       = os.path.join(path_models, 'Model_CNN_1D_weights_0_best' + norm_type + model_surname + '.hdf5')\n",
    "                checkpoint     = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "                callbacks_list = [checkpoint]                \n",
    "\n",
    "                X_train_norm = X_train_norm[..., np.newaxis]\n",
    "                X_val_norm   = X_val_norm[..., np.newaxis]\n",
    "                X_test_norm  = X_test_norm[..., np.newaxis]\n",
    "\n",
    "                model = build_CNN_1D_model(model_name, neurons)\n",
    "                model.summary()\n",
    "                print(name)\n",
    "                print(np.shape(X_train_norm))    \n",
    "                \n",
    "                model.fit(X_train_norm, \n",
    "                          y_train, \n",
    "                          batch_size = batch_size, \n",
    "                          epochs = 150, \n",
    "                          verbose = verbose,                          \n",
    "                          validation_data = (X_test_norm, y_test),\n",
    "                          callbacks       = callbacks_list)\n",
    "\n",
    "                model= load_model(os.path.join(path_models, 'Model_CNN_1D_weights_0_best' + norm_type + model_surname + '.hdf5'))\n",
    "                print('Best model loaded')\n",
    "\n",
    "            # Get the model predictions\n",
    "            y_train_enc = np.argmax(y_train, axis=1)\n",
    "            y_val_enc   = np.argmax(y_OHEV_val, axis=1)\n",
    "\n",
    "            y_train_predicted = np.argmax(model.predict(X_train_norm), axis=1)\n",
    "            \n",
    "            t_srt             = time.process_time_ns()\n",
    "            y_val_predicted   = np.argmax(model.predict(X_val_norm), axis=1)\n",
    "            t_end             = time.process_time_ns()\n",
    "            proc_time         = ((t_end - t_srt) / 1000000)         \n",
    "    \n",
    "            # Compute the classifier metrics\n",
    "            accuracy_train = metrics.accuracy_score(y_train_enc, y_train_predicted)\n",
    "            accuracy_val   = metrics.accuracy_score(y_val_enc,  y_val_predicted)\n",
    "\n",
    "            f1_Score_train = metrics.f1_score(y_train_enc, y_train_predicted, average = 'weighted')\n",
    "            f1_Score_val   = metrics.f1_score(y_val_enc,  y_val_predicted,  average = 'weighted')\n",
    "\n",
    "            precision_score_train = metrics.precision_score(y_train_enc, y_train_predicted, average = 'weighted')\n",
    "            precision_score_val   = metrics.precision_score(y_val_enc,  y_val_predicted,  average = 'weighted')\n",
    "\n",
    "            recall_score_train = metrics.recall_score(y_train_enc, y_train_predicted, average = 'weighted')\n",
    "            recall_score_val   = metrics.recall_score(y_val_enc,  y_val_predicted,  average = 'weighted')\n",
    "\n",
    "            class_report_val = classification_report(y_val_enc, y_val_predicted, target_names = nom_classes)\n",
    "            print(class_report_val)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            CM = metrics.confusion_matrix(y_val_enc, y_val_predicted)\n",
    "            y_val_enc       = []\n",
    "            y_val_predicted = []\n",
    "\n",
    "            # Store the name, test accuracy results and model\n",
    "            models.append((name, accuracy_val, model))\n",
    "            \n",
    "            K.clear_session()\n",
    "            del model\n",
    "                    \n",
    "            acc_set = pd.concat([acc_set, pd.DataFrame({'Model': [name],\n",
    "                                                        'Fold': [fold],\n",
    "                                                        'Accuracy(Train)': [accuracy_train],\n",
    "                                                        'Accuracy(Val)': [accuracy_val],\n",
    "                                                        'F1(Train)': [f1_Score_train],\n",
    "                                                        'F1(Val)': [f1_Score_val],\n",
    "                                                        'Precision(Train)': [precision_score_train],\n",
    "                                                        'Precision(Val)': [precision_score_val],\n",
    "                                                        'Recall(Train)': [recall_score_train],\n",
    "                                                        'Recall(Val)': [recall_score_val],\n",
    "                                                        'Conf_M': [CM],\n",
    "                                                        'Process_time': [proc_time],\n",
    "                                                        'Class_report(Val)': class_report_val})], ignore_index = True)\n",
    "                   \n",
    "    return acc_set, models, batch_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "358bab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation fold: 1\n",
      "X_train_norm shape...:(27496, 375)\n",
      "X_val_norm shape.....:(3010, 375)\n",
      "\n",
      "Sum of elements: 0.9801846635764112\n",
      "Number of elements summed: 231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 231)               53592     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 231)               53592     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 231)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               174000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 284,939\n",
      "Trainable params: 284,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24746, 231)\n",
      "Epoch 1/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.7861 - accuracy: 0.7111\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.81745, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.7816 - accuracy: 0.7131 - val_loss: 0.5138 - val_accuracy: 0.8175\n",
      "Epoch 2/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.4588 - accuracy: 0.8343\n",
      "Epoch 00002: val_accuracy improved from 0.81745 to 0.84800, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.4589 - accuracy: 0.8342 - val_loss: 0.4131 - val_accuracy: 0.8480\n",
      "Epoch 3/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.3533 - accuracy: 0.8731\n",
      "Epoch 00003: val_accuracy improved from 0.84800 to 0.87455, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.3541 - accuracy: 0.8726 - val_loss: 0.3526 - val_accuracy: 0.8745\n",
      "Epoch 4/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8944\n",
      "Epoch 00004: val_accuracy improved from 0.87455 to 0.88691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2910 - accuracy: 0.8946 - val_loss: 0.3129 - val_accuracy: 0.8869\n",
      "Epoch 5/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.9117\n",
      "Epoch 00005: val_accuracy improved from 0.88691 to 0.89564, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.2463 - accuracy: 0.9117 - val_loss: 0.2878 - val_accuracy: 0.8956\n",
      "Epoch 6/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9263\n",
      "Epoch 00006: val_accuracy improved from 0.89564 to 0.90909, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.2073 - accuracy: 0.9266 - val_loss: 0.2548 - val_accuracy: 0.9091\n",
      "Epoch 7/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9406\n",
      "Epoch 00007: val_accuracy improved from 0.90909 to 0.91782, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1737 - accuracy: 0.9404 - val_loss: 0.2428 - val_accuracy: 0.9178\n",
      "Epoch 8/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9498\n",
      "Epoch 00008: val_accuracy improved from 0.91782 to 0.91964, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1477 - accuracy: 0.9496 - val_loss: 0.2305 - val_accuracy: 0.9196\n",
      "Epoch 9/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9573\n",
      "Epoch 00009: val_accuracy improved from 0.91964 to 0.92618, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1268 - accuracy: 0.9573 - val_loss: 0.2239 - val_accuracy: 0.9262\n",
      "Epoch 10/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9641\n",
      "Epoch 00010: val_accuracy improved from 0.92618 to 0.93273, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1075 - accuracy: 0.9641 - val_loss: 0.2130 - val_accuracy: 0.9327\n",
      "Epoch 11/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9702\n",
      "Epoch 00011: val_accuracy improved from 0.93273 to 0.93709, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0920 - accuracy: 0.9703 - val_loss: 0.2081 - val_accuracy: 0.9371\n",
      "Epoch 12/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9757\n",
      "Epoch 00012: val_accuracy did not improve from 0.93709\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0770 - accuracy: 0.9752 - val_loss: 0.2134 - val_accuracy: 0.9353\n",
      "Epoch 13/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9779\n",
      "Epoch 00013: val_accuracy improved from 0.93709 to 0.93891, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0682 - accuracy: 0.9779 - val_loss: 0.2085 - val_accuracy: 0.9389\n",
      "Epoch 14/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9829\n",
      "Epoch 00014: val_accuracy improved from 0.93891 to 0.94073, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0560 - accuracy: 0.9829 - val_loss: 0.2077 - val_accuracy: 0.9407\n",
      "Epoch 15/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9864\n",
      "Epoch 00015: val_accuracy improved from 0.94073 to 0.94218, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0467 - accuracy: 0.9865 - val_loss: 0.2040 - val_accuracy: 0.9422\n",
      "Epoch 16/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9875\n",
      "Epoch 00016: val_accuracy did not improve from 0.94218\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0409 - accuracy: 0.9875 - val_loss: 0.2101 - val_accuracy: 0.9407\n",
      "Epoch 17/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0357 - accuracy: 0.9900\n",
      "Epoch 00017: val_accuracy did not improve from 0.94218\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0358 - accuracy: 0.9899 - val_loss: 0.2124 - val_accuracy: 0.9415\n",
      "Epoch 18/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 0.9901\n",
      "Epoch 00018: val_accuracy did not improve from 0.94218\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0334 - accuracy: 0.9901 - val_loss: 0.2109 - val_accuracy: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0273 - accuracy: 0.9925\n",
      "Epoch 00019: val_accuracy improved from 0.94218 to 0.94255, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0273 - accuracy: 0.9925 - val_loss: 0.2144 - val_accuracy: 0.9425\n",
      "Epoch 20/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9932\n",
      "Epoch 00020: val_accuracy improved from 0.94255 to 0.94291, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0244 - accuracy: 0.9932 - val_loss: 0.2160 - val_accuracy: 0.9429\n",
      "Epoch 21/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9947\n",
      "Epoch 00021: val_accuracy improved from 0.94291 to 0.94400, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0200 - accuracy: 0.9947 - val_loss: 0.2236 - val_accuracy: 0.9440\n",
      "Epoch 22/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9955\n",
      "Epoch 00022: val_accuracy did not improve from 0.94400\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.2296 - val_accuracy: 0.9415\n",
      "Epoch 23/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9969\n",
      "Epoch 00023: val_accuracy improved from 0.94400 to 0.94691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0148 - accuracy: 0.9969 - val_loss: 0.2378 - val_accuracy: 0.9469\n",
      "Epoch 24/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9961\n",
      "Epoch 00024: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0144 - accuracy: 0.9962 - val_loss: 0.2344 - val_accuracy: 0.9436\n",
      "Epoch 25/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9969\n",
      "Epoch 00025: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 0.2297 - val_accuracy: 0.9422\n",
      "Epoch 26/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9970\n",
      "Epoch 00026: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.2314 - val_accuracy: 0.9465\n",
      "Epoch 27/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9972\n",
      "Epoch 00027: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0118 - accuracy: 0.9972 - val_loss: 0.2286 - val_accuracy: 0.9425\n",
      "Epoch 28/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9982\n",
      "Epoch 00028: val_accuracy did not improve from 0.94691\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.2357 - val_accuracy: 0.9469\n",
      "Epoch 29/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9978\n",
      "Epoch 00029: val_accuracy improved from 0.94691 to 0.94909, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0091 - accuracy: 0.9979 - val_loss: 0.2422 - val_accuracy: 0.9491\n",
      "Epoch 30/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9979\n",
      "Epoch 00030: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 0.2437 - val_accuracy: 0.9455\n",
      "Epoch 31/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9973\n",
      "Epoch 00031: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.2458 - val_accuracy: 0.9447\n",
      "Epoch 32/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9980\n",
      "Epoch 00032: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.2393 - val_accuracy: 0.9476\n",
      "Epoch 33/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9983\n",
      "Epoch 00033: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.2519 - val_accuracy: 0.9480\n",
      "Epoch 34/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9986\n",
      "Epoch 00034: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 0.2534 - val_accuracy: 0.9484\n",
      "Epoch 35/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9986\n",
      "Epoch 00035: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9986 - val_loss: 0.2501 - val_accuracy: 0.9469\n",
      "Epoch 36/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9988\n",
      "Epoch 00036: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.2503 - val_accuracy: 0.9465\n",
      "Epoch 37/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989\n",
      "Epoch 00037: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 0.2588 - val_accuracy: 0.9455\n",
      "Epoch 38/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9989\n",
      "Epoch 00038: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 0.2605 - val_accuracy: 0.9462\n",
      "Epoch 39/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 00039: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.2673 - val_accuracy: 0.9458\n",
      "Epoch 40/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.2592 - val_accuracy: 0.9451\n",
      "Epoch 41/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9992\n",
      "Epoch 00041: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.2681 - val_accuracy: 0.9465\n",
      "Epoch 42/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9995\n",
      "Epoch 00042: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.2673 - val_accuracy: 0.9473\n",
      "Epoch 43/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 00043: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2713 - val_accuracy: 0.9469\n",
      "Epoch 44/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9992\n",
      "Epoch 00044: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.2779 - val_accuracy: 0.9469\n",
      "Epoch 45/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9991\n",
      "Epoch 00045: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.2711 - val_accuracy: 0.9469\n",
      "Epoch 46/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 00046: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.2685 - val_accuracy: 0.9491\n",
      "Epoch 47/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 00047: val_accuracy did not improve from 0.94909\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.2809 - val_accuracy: 0.9480\n",
      "Epoch 48/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 00048: val_accuracy improved from 0.94909 to 0.94945, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.2824 - val_accuracy: 0.9495\n",
      "Epoch 49/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9990\n",
      "Epoch 00049: val_accuracy did not improve from 0.94945\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.2836 - val_accuracy: 0.9480\n",
      "Epoch 50/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00050: val_accuracy improved from 0.94945 to 0.95055, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2767 - val_accuracy: 0.9505\n",
      "Epoch 51/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0026 - accuracy: 0.9997\n",
      "Epoch 00051: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2770 - val_accuracy: 0.9480\n",
      "Epoch 52/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9996\n",
      "Epoch 00052: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2779 - val_accuracy: 0.9469\n",
      "Epoch 53/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00053: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.2800 - val_accuracy: 0.9495\n",
      "Epoch 54/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9996\n",
      "Epoch 00054: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2800 - val_accuracy: 0.9469\n",
      "Epoch 55/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2797 - val_accuracy: 0.9505\n",
      "Epoch 56/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 00056: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.2851 - val_accuracy: 0.9487\n",
      "Epoch 57/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00057: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2729 - val_accuracy: 0.9476\n",
      "Epoch 58/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.2763 - val_accuracy: 0.9495\n",
      "Epoch 59/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2841 - val_accuracy: 0.9495\n",
      "Epoch 60/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00060: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2856 - val_accuracy: 0.9476\n",
      "Epoch 61/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2837 - val_accuracy: 0.9498\n",
      "Epoch 62/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 00062: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.2917 - val_accuracy: 0.9465\n",
      "Epoch 63/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00063: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2854 - val_accuracy: 0.9495\n",
      "Epoch 64/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00064: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2875 - val_accuracy: 0.9491\n",
      "Epoch 65/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00065: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.2949 - val_accuracy: 0.9473\n",
      "Epoch 66/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00066: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2924 - val_accuracy: 0.9502\n",
      "Epoch 67/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00067: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2945 - val_accuracy: 0.9476\n",
      "Epoch 68/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.3004 - val_accuracy: 0.9502\n",
      "Epoch 69/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3000 - val_accuracy: 0.9487\n",
      "Epoch 70/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00070: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9498\n",
      "Epoch 71/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00071: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 9s 12ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3007 - val_accuracy: 0.9505\n",
      "Epoch 72/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00072: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 4s 6ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2977 - val_accuracy: 0.9502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00073: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2984 - val_accuracy: 0.9498\n",
      "Epoch 74/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 00074: val_accuracy did not improve from 0.95055\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.3096 - val_accuracy: 0.9495\n",
      "Epoch 75/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 00075: val_accuracy improved from 0.95055 to 0.95164, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.2966 - val_accuracy: 0.9516\n",
      "Epoch 76/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00076: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2970 - val_accuracy: 0.9487\n",
      "Epoch 77/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 00077: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.3011 - val_accuracy: 0.9505\n",
      "Epoch 78/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 00078: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2959 - val_accuracy: 0.9484\n",
      "Epoch 79/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9993\n",
      "Epoch 00079: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.3073 - val_accuracy: 0.9484\n",
      "Epoch 80/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00080: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.3129 - val_accuracy: 0.9473\n",
      "Epoch 81/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00081: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3002 - val_accuracy: 0.9487\n",
      "Epoch 82/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 00082: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.2949 - val_accuracy: 0.9480\n",
      "Epoch 83/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00083: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2928 - val_accuracy: 0.9509\n",
      "Epoch 84/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00084: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2939 - val_accuracy: 0.9495\n",
      "Epoch 85/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00085: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9505\n",
      "Epoch 86/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00086: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2903 - val_accuracy: 0.9498\n",
      "Epoch 87/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2895 - val_accuracy: 0.9495\n",
      "Epoch 88/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00088: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2927 - val_accuracy: 0.9498\n",
      "Epoch 89/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00089: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2915 - val_accuracy: 0.9513\n",
      "Epoch 90/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00090: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2942 - val_accuracy: 0.9498\n",
      "Epoch 91/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00091: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2926 - val_accuracy: 0.9495\n",
      "Epoch 92/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 9.4045e-04 - accuracy: 0.9999\n",
      "Epoch 00092: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.3956e-04 - accuracy: 0.9999 - val_loss: 0.2970 - val_accuracy: 0.9495\n",
      "Epoch 93/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 9.2900e-04 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.2359e-04 - accuracy: 0.9998 - val_loss: 0.3104 - val_accuracy: 0.9476\n",
      "Epoch 94/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.3018 - val_accuracy: 0.9502\n",
      "Epoch 95/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 00095: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.2945 - val_accuracy: 0.9505\n",
      "Epoch 96/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3071 - val_accuracy: 0.9465\n",
      "Epoch 97/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9998\n",
      "Epoch 00097: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.3087 - val_accuracy: 0.9480\n",
      "Epoch 98/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 8.4612e-04 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.3823e-04 - accuracy: 1.0000 - val_loss: 0.3024 - val_accuracy: 0.9487\n",
      "Epoch 99/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 8.9195e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.8792e-04 - accuracy: 0.9998 - val_loss: 0.2985 - val_accuracy: 0.9484\n",
      "Epoch 100/350\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00100: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3008 - val_accuracy: 0.9487\n",
      "Epoch 101/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 8.6876e-04 - accuracy: 0.9998\n",
      "Epoch 00101: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 8.5476e-04 - accuracy: 0.9998 - val_loss: 0.2961 - val_accuracy: 0.9498\n",
      "Epoch 102/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 8.3309e-04 - accuracy: 0.9998\n",
      "Epoch 00102: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.2661e-04 - accuracy: 0.9998 - val_loss: 0.2985 - val_accuracy: 0.9487\n",
      "Epoch 103/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 6.4217e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 6.3415e-04 - accuracy: 1.0000 - val_loss: 0.2971 - val_accuracy: 0.9509\n",
      "Epoch 104/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 8.0594e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 8.1530e-04 - accuracy: 0.9999 - val_loss: 0.3010 - val_accuracy: 0.9513\n",
      "Epoch 105/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 7.1510e-04 - accuracy: 0.9999\n",
      "Epoch 00105: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 7.2042e-04 - accuracy: 0.9999 - val_loss: 0.2972 - val_accuracy: 0.9516\n",
      "Epoch 106/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 9.4884e-04 - accuracy: 0.9998\n",
      "Epoch 00106: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 9.4229e-04 - accuracy: 0.9998 - val_loss: 0.3002 - val_accuracy: 0.9495\n",
      "Epoch 107/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 9.5456e-04 - accuracy: 0.9998\n",
      "Epoch 00107: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 9.4721e-04 - accuracy: 0.9998 - val_loss: 0.2976 - val_accuracy: 0.9516\n",
      "Epoch 108/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 7.4247e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 7.3437e-04 - accuracy: 0.9999 - val_loss: 0.2989 - val_accuracy: 0.9513\n",
      "Epoch 109/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 7.4481e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 7.4892e-04 - accuracy: 0.9999 - val_loss: 0.3037 - val_accuracy: 0.9509\n",
      "Epoch 110/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 6.8301e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 6.7830e-04 - accuracy: 0.9999 - val_loss: 0.3053 - val_accuracy: 0.9513\n",
      "Epoch 111/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 6.7810e-04 - accuracy: 0.9999\n",
      "Epoch 00111: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.6412e-04 - accuracy: 0.9999 - val_loss: 0.3032 - val_accuracy: 0.9502\n",
      "Epoch 112/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 8.9072e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.8035e-04 - accuracy: 0.9998 - val_loss: 0.3015 - val_accuracy: 0.9509\n",
      "Epoch 113/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 6.9413e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.9209e-04 - accuracy: 0.9999 - val_loss: 0.3062 - val_accuracy: 0.9498\n",
      "Epoch 114/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 4.4941e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.4871e-04 - accuracy: 1.0000 - val_loss: 0.3027 - val_accuracy: 0.9491\n",
      "Epoch 115/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 7.0670e-04 - accuracy: 0.9999\n",
      "Epoch 00115: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 11s 15ms/step - loss: 7.0554e-04 - accuracy: 0.9999 - val_loss: 0.3053 - val_accuracy: 0.9509\n",
      "Epoch 116/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 8.0874e-04 - accuracy: 0.9998\n",
      "Epoch 00116: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.0104e-04 - accuracy: 0.9998 - val_loss: 0.3125 - val_accuracy: 0.9509\n",
      "Epoch 117/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 5.5600e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5485e-04 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.9498\n",
      "Epoch 118/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 5.8558e-04 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.8643e-04 - accuracy: 1.0000 - val_loss: 0.3043 - val_accuracy: 0.9502\n",
      "Epoch 119/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 8.1697e-04 - accuracy: 0.9998\n",
      "Epoch 00119: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.1327e-04 - accuracy: 0.9998 - val_loss: 0.3005 - val_accuracy: 0.9513\n",
      "Epoch 120/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 6.0401e-04 - accuracy: 0.9998\n",
      "Epoch 00120: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.0548e-04 - accuracy: 0.9998 - val_loss: 0.3009 - val_accuracy: 0.9516\n",
      "Epoch 121/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 9.5034e-04 - accuracy: 0.9997\n",
      "Epoch 00121: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 0.3060 - val_accuracy: 0.9484\n",
      "Epoch 122/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 7.3797e-04 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95164\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.3806e-04 - accuracy: 0.9999 - val_loss: 0.3033 - val_accuracy: 0.9476\n",
      "Epoch 123/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 9.1324e-04 - accuracy: 0.9998\n",
      "Epoch 00123: val_accuracy improved from 0.95164 to 0.95200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.0775e-04 - accuracy: 0.9998 - val_loss: 0.3038 - val_accuracy: 0.9520\n",
      "Epoch 124/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 6.7440e-04 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy improved from 0.95200 to 0.95236, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8017e-04 - accuracy: 1.0000 - val_loss: 0.3071 - val_accuracy: 0.9524\n",
      "Epoch 125/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 6.5047e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.4779e-04 - accuracy: 0.9999 - val_loss: 0.3117 - val_accuracy: 0.9498\n",
      "Epoch 126/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 6.2325e-04 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.2291e-04 - accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 0.9524\n",
      "Epoch 127/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 7.8766e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.7550e-04 - accuracy: 0.9999 - val_loss: 0.3144 - val_accuracy: 0.9509\n",
      "Epoch 128/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745/774 [===========================>..] - ETA: 0s - loss: 6.1379e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.5040e-04 - accuracy: 0.9999 - val_loss: 0.3159 - val_accuracy: 0.9487\n",
      "Epoch 129/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 8.1760e-04 - accuracy: 0.9998\n",
      "Epoch 00129: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.0915e-04 - accuracy: 0.9998 - val_loss: 0.3170 - val_accuracy: 0.9495\n",
      "Epoch 130/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 4.2138e-04 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.2613e-04 - accuracy: 1.0000 - val_loss: 0.3179 - val_accuracy: 0.9487\n",
      "Epoch 131/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 5.5918e-04 - accuracy: 0.9999\n",
      "Epoch 00131: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5918e-04 - accuracy: 0.9999 - val_loss: 0.3215 - val_accuracy: 0.9502\n",
      "Epoch 132/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 5.7124e-04 - accuracy: 1.0000\n",
      "Epoch 00132: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.6541e-04 - accuracy: 1.0000 - val_loss: 0.3166 - val_accuracy: 0.9513\n",
      "Epoch 133/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999    \n",
      "Epoch 00133: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.3249 - val_accuracy: 0.9491\n",
      "Epoch 134/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 6.1199e-04 - accuracy: 0.9999\n",
      "Epoch 00134: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 6.2723e-04 - accuracy: 0.9999 - val_loss: 0.3209 - val_accuracy: 0.9516\n",
      "Epoch 135/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 5.7706e-04 - accuracy: 0.9999\n",
      "Epoch 00135: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.7683e-04 - accuracy: 0.9999 - val_loss: 0.3205 - val_accuracy: 0.9505\n",
      "Epoch 136/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 5.8151e-04 - accuracy: 0.9999\n",
      "Epoch 00136: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.7075e-04 - accuracy: 0.9999 - val_loss: 0.3246 - val_accuracy: 0.9520\n",
      "Epoch 137/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 4.2551e-04 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.4616e-04 - accuracy: 1.0000 - val_loss: 0.3203 - val_accuracy: 0.9516\n",
      "Epoch 138/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 5.5412e-04 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5346e-04 - accuracy: 1.0000 - val_loss: 0.3191 - val_accuracy: 0.9516\n",
      "Epoch 139/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 6.5319e-04 - accuracy: 0.9998\n",
      "Epoch 00139: val_accuracy did not improve from 0.95236\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.5000e-04 - accuracy: 0.9998 - val_loss: 0.3143 - val_accuracy: 0.9505\n",
      "Epoch 140/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 7.0714e-04 - accuracy: 0.9998\n",
      "Epoch 00140: val_accuracy did not improve from 0.95236\n",
      "Restoring model weights from the end of the best epoch.\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.9907e-04 - accuracy: 0.9998 - val_loss: 0.3253 - val_accuracy: 0.9491\n",
      "Epoch 00140: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:59<03:59, 239.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.83      0.80      0.81       756\n",
      "        car_horn       0.83      0.92      0.87       252\n",
      "children_playing       0.77      0.78      0.77       700\n",
      "        dog_bark       0.77      0.84      0.80       700\n",
      "           siren       0.85      0.74      0.79       602\n",
      "\n",
      "        accuracy                           0.80      3010\n",
      "       macro avg       0.81      0.82      0.81      3010\n",
      "    weighted avg       0.81      0.80      0.80      3010\n",
      "\n",
      "Model: \"Model_CNN_1D_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 225, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 225, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 225, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 112, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 112, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                313650    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 324,691\n",
      "Trainable params: 324,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24746, 231, 1)\n",
      "Epoch 1/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.7394 - accuracy: 0.7625\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83091, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.7394 - accuracy: 0.7625 - val_loss: 0.5348 - val_accuracy: 0.8309\n",
      "Epoch 2/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.4969 - accuracy: 0.8501\n",
      "Epoch 00002: val_accuracy improved from 0.83091 to 0.83564, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.4970 - accuracy: 0.8501 - val_loss: 0.5185 - val_accuracy: 0.8356\n",
      "Epoch 3/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.4255 - accuracy: 0.8737\n",
      "Epoch 00003: val_accuracy improved from 0.83564 to 0.86582, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.4254 - accuracy: 0.8737 - val_loss: 0.4282 - val_accuracy: 0.8658\n",
      "Epoch 4/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8902\n",
      "Epoch 00004: val_accuracy improved from 0.86582 to 0.86836, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3766 - accuracy: 0.8902 - val_loss: 0.4096 - val_accuracy: 0.8684\n",
      "Epoch 5/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.3493 - accuracy: 0.8995\n",
      "Epoch 00005: val_accuracy improved from 0.86836 to 0.87600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3487 - accuracy: 0.8997 - val_loss: 0.3877 - val_accuracy: 0.8760\n",
      "Epoch 6/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.9102\n",
      "Epoch 00006: val_accuracy improved from 0.87600 to 0.88691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3171 - accuracy: 0.9105 - val_loss: 0.3628 - val_accuracy: 0.8869\n",
      "Epoch 7/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.9159\n",
      "Epoch 00007: val_accuracy did not improve from 0.88691\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2987 - accuracy: 0.9159 - val_loss: 0.3691 - val_accuracy: 0.8793\n",
      "Epoch 8/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.2786 - accuracy: 0.9220\n",
      "Epoch 00008: val_accuracy improved from 0.88691 to 0.88909, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2787 - accuracy: 0.9220 - val_loss: 0.3518 - val_accuracy: 0.8891\n",
      "Epoch 9/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.2606 - accuracy: 0.9290\n",
      "Epoch 00009: val_accuracy improved from 0.88909 to 0.90291, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2610 - accuracy: 0.9286 - val_loss: 0.3409 - val_accuracy: 0.9029\n",
      "Epoch 10/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.9324\n",
      "Epoch 00010: val_accuracy did not improve from 0.90291\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2489 - accuracy: 0.9323 - val_loss: 0.3669 - val_accuracy: 0.8873\n",
      "Epoch 11/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.2411 - accuracy: 0.9347\n",
      "Epoch 00011: val_accuracy did not improve from 0.90291\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2411 - accuracy: 0.9347 - val_loss: 0.3413 - val_accuracy: 0.9018\n",
      "Epoch 12/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9423\n",
      "Epoch 00012: val_accuracy did not improve from 0.90291\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2266 - accuracy: 0.9420 - val_loss: 0.3418 - val_accuracy: 0.8996\n",
      "Epoch 13/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.2194 - accuracy: 0.9430\n",
      "Epoch 00013: val_accuracy improved from 0.90291 to 0.90327, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2193 - accuracy: 0.9431 - val_loss: 0.3251 - val_accuracy: 0.9033\n",
      "Epoch 14/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.9448\n",
      "Epoch 00014: val_accuracy improved from 0.90327 to 0.90582, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2120 - accuracy: 0.9447 - val_loss: 0.3313 - val_accuracy: 0.9058\n",
      "Epoch 15/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.9487\n",
      "Epoch 00015: val_accuracy did not improve from 0.90582\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2018 - accuracy: 0.9487 - val_loss: 0.3522 - val_accuracy: 0.8964\n",
      "Epoch 16/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9501\n",
      "Epoch 00016: val_accuracy did not improve from 0.90582\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1970 - accuracy: 0.9500 - val_loss: 0.3376 - val_accuracy: 0.9051\n",
      "Epoch 17/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1929 - accuracy: 0.9527\n",
      "Epoch 00017: val_accuracy did not improve from 0.90582\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1932 - accuracy: 0.9526 - val_loss: 0.3473 - val_accuracy: 0.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9525\n",
      "Epoch 00018: val_accuracy improved from 0.90582 to 0.91091, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1847 - accuracy: 0.9525 - val_loss: 0.3313 - val_accuracy: 0.9109\n",
      "Epoch 19/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1769 - accuracy: 0.9559\n",
      "Epoch 00019: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.1770 - accuracy: 0.9558 - val_loss: 0.3201 - val_accuracy: 0.9102\n",
      "Epoch 20/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9572\n",
      "Epoch 00020: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1719 - accuracy: 0.9572 - val_loss: 0.3350 - val_accuracy: 0.9084\n",
      "Epoch 21/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9589\n",
      "Epoch 00021: val_accuracy did not improve from 0.91091\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1687 - accuracy: 0.9587 - val_loss: 0.3424 - val_accuracy: 0.9080\n",
      "Epoch 22/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9603\n",
      "Epoch 00022: val_accuracy improved from 0.91091 to 0.91200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1680 - accuracy: 0.9602 - val_loss: 0.3346 - val_accuracy: 0.9120\n",
      "Epoch 23/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9610\n",
      "Epoch 00023: val_accuracy improved from 0.91200 to 0.91345, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1622 - accuracy: 0.9606 - val_loss: 0.3264 - val_accuracy: 0.9135\n",
      "Epoch 24/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9620\n",
      "Epoch 00024: val_accuracy did not improve from 0.91345\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1598 - accuracy: 0.9619 - val_loss: 0.3273 - val_accuracy: 0.9120\n",
      "Epoch 25/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1595 - accuracy: 0.9617\n",
      "Epoch 00025: val_accuracy did not improve from 0.91345\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1596 - accuracy: 0.9617 - val_loss: 0.3341 - val_accuracy: 0.9109\n",
      "Epoch 26/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1498 - accuracy: 0.9648\n",
      "Epoch 00026: val_accuracy did not improve from 0.91345\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1496 - accuracy: 0.9650 - val_loss: 0.3286 - val_accuracy: 0.9135\n",
      "Epoch 27/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1502 - accuracy: 0.9647\n",
      "Epoch 00027: val_accuracy did not improve from 0.91345\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1505 - accuracy: 0.9647 - val_loss: 0.3425 - val_accuracy: 0.9084\n",
      "Epoch 28/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.9683\n",
      "Epoch 00028: val_accuracy improved from 0.91345 to 0.91600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1436 - accuracy: 0.9684 - val_loss: 0.3217 - val_accuracy: 0.9160\n",
      "Epoch 29/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9680\n",
      "Epoch 00029: val_accuracy did not improve from 0.91600\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1416 - accuracy: 0.9680 - val_loss: 0.3361 - val_accuracy: 0.9160\n",
      "Epoch 30/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9685\n",
      "Epoch 00030: val_accuracy did not improve from 0.91600\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1392 - accuracy: 0.9683 - val_loss: 0.3316 - val_accuracy: 0.9149\n",
      "Epoch 31/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.9686\n",
      "Epoch 00031: val_accuracy improved from 0.91600 to 0.91636, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1388 - accuracy: 0.9685 - val_loss: 0.3324 - val_accuracy: 0.9164\n",
      "Epoch 32/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.9696\n",
      "Epoch 00032: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1372 - accuracy: 0.9696 - val_loss: 0.3375 - val_accuracy: 0.9145\n",
      "Epoch 33/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1330 - accuracy: 0.9710\n",
      "Epoch 00033: val_accuracy did not improve from 0.91636\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1325 - accuracy: 0.9711 - val_loss: 0.3232 - val_accuracy: 0.9105\n",
      "Epoch 34/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9697\n",
      "Epoch 00034: val_accuracy improved from 0.91636 to 0.91673, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1311 - accuracy: 0.9698 - val_loss: 0.3330 - val_accuracy: 0.9167\n",
      "Epoch 35/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9721\n",
      "Epoch 00035: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9723 - val_loss: 0.3529 - val_accuracy: 0.9109\n",
      "Epoch 36/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9718\n",
      "Epoch 00036: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1291 - accuracy: 0.9720 - val_loss: 0.3439 - val_accuracy: 0.9105\n",
      "Epoch 37/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9717\n",
      "Epoch 00037: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1252 - accuracy: 0.9717 - val_loss: 0.3425 - val_accuracy: 0.9131\n",
      "Epoch 38/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9724\n",
      "Epoch 00038: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1230 - accuracy: 0.9724 - val_loss: 0.3343 - val_accuracy: 0.9156\n",
      "Epoch 39/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9750\n",
      "Epoch 00039: val_accuracy did not improve from 0.91673\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1193 - accuracy: 0.9749 - val_loss: 0.3584 - val_accuracy: 0.9116\n",
      "Epoch 40/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9735\n",
      "Epoch 00040: val_accuracy improved from 0.91673 to 0.91745, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1234 - accuracy: 0.9735 - val_loss: 0.3386 - val_accuracy: 0.9175\n",
      "Epoch 41/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9750\n",
      "Epoch 00041: val_accuracy did not improve from 0.91745\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1180 - accuracy: 0.9749 - val_loss: 0.3355 - val_accuracy: 0.9149\n",
      "Epoch 42/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9753\n",
      "Epoch 00042: val_accuracy did not improve from 0.91745\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1170 - accuracy: 0.9753 - val_loss: 0.3443 - val_accuracy: 0.9145\n",
      "Epoch 43/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9746\n",
      "Epoch 00043: val_accuracy did not improve from 0.91745\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1181 - accuracy: 0.9747 - val_loss: 0.3442 - val_accuracy: 0.9084\n",
      "Epoch 44/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9747\n",
      "Epoch 00044: val_accuracy did not improve from 0.91745\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1178 - accuracy: 0.9747 - val_loss: 0.3407 - val_accuracy: 0.9105\n",
      "Epoch 45/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9745\n",
      "Epoch 00045: val_accuracy improved from 0.91745 to 0.91782, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1169 - accuracy: 0.9746 - val_loss: 0.3513 - val_accuracy: 0.9178\n",
      "Epoch 46/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9751\n",
      "Epoch 00046: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 6s 7ms/step - loss: 0.1124 - accuracy: 0.9751 - val_loss: 0.3440 - val_accuracy: 0.9178\n",
      "Epoch 47/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9762\n",
      "Epoch 00047: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 8s 11ms/step - loss: 0.1134 - accuracy: 0.9763 - val_loss: 0.3579 - val_accuracy: 0.9156\n",
      "Epoch 48/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9769\n",
      "Epoch 00048: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.1104 - accuracy: 0.9767 - val_loss: 0.3472 - val_accuracy: 0.9175\n",
      "Epoch 49/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1079 - accuracy: 0.9787\n",
      "Epoch 00049: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1078 - accuracy: 0.9787 - val_loss: 0.3515 - val_accuracy: 0.9167\n",
      "Epoch 50/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9768\n",
      "Epoch 00050: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1111 - accuracy: 0.9768 - val_loss: 0.3422 - val_accuracy: 0.9156\n",
      "Epoch 51/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9787\n",
      "Epoch 00051: val_accuracy did not improve from 0.91782\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1057 - accuracy: 0.9787 - val_loss: 0.3431 - val_accuracy: 0.9138\n",
      "Epoch 52/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9786\n",
      "Epoch 00052: val_accuracy improved from 0.91782 to 0.91927, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1047 - accuracy: 0.9786 - val_loss: 0.3472 - val_accuracy: 0.9193\n",
      "Epoch 53/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9790\n",
      "Epoch 00053: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1020 - accuracy: 0.9787 - val_loss: 0.3446 - val_accuracy: 0.9178\n",
      "Epoch 54/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9780\n",
      "Epoch 00054: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1037 - accuracy: 0.9778 - val_loss: 0.3485 - val_accuracy: 0.9178\n",
      "Epoch 55/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9773\n",
      "Epoch 00055: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1056 - accuracy: 0.9775 - val_loss: 0.3460 - val_accuracy: 0.9160\n",
      "Epoch 56/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9795\n",
      "Epoch 00056: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1012 - accuracy: 0.9795 - val_loss: 0.3582 - val_accuracy: 0.9124\n",
      "Epoch 57/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0997 - accuracy: 0.9799\n",
      "Epoch 00057: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0997 - accuracy: 0.9799 - val_loss: 0.3427 - val_accuracy: 0.9135\n",
      "Epoch 58/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9792\n",
      "Epoch 00058: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0992 - accuracy: 0.9791 - val_loss: 0.3616 - val_accuracy: 0.9131\n",
      "Epoch 59/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9792\n",
      "Epoch 00059: val_accuracy did not improve from 0.91927\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0992 - accuracy: 0.9793 - val_loss: 0.3547 - val_accuracy: 0.9160\n",
      "Epoch 60/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9802\n",
      "Epoch 00060: val_accuracy improved from 0.91927 to 0.91964, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0988 - accuracy: 0.9801 - val_loss: 0.3489 - val_accuracy: 0.9196\n",
      "Epoch 61/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0961 - accuracy: 0.9803\n",
      "Epoch 00061: val_accuracy did not improve from 0.91964\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0966 - accuracy: 0.9802 - val_loss: 0.3544 - val_accuracy: 0.9164\n",
      "Epoch 62/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0979 - accuracy: 0.9798\n",
      "Epoch 00062: val_accuracy did not improve from 0.91964\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0980 - accuracy: 0.9798 - val_loss: 0.3554 - val_accuracy: 0.9196\n",
      "Epoch 63/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 0.9816\n",
      "Epoch 00063: val_accuracy improved from 0.91964 to 0.92145, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9812 - val_loss: 0.3467 - val_accuracy: 0.9215\n",
      "Epoch 64/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0937 - accuracy: 0.9812\n",
      "Epoch 00064: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0935 - accuracy: 0.9812 - val_loss: 0.3463 - val_accuracy: 0.9207\n",
      "Epoch 65/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9805\n",
      "Epoch 00065: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0958 - accuracy: 0.9804 - val_loss: 0.3505 - val_accuracy: 0.9204\n",
      "Epoch 66/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0931 - accuracy: 0.9815\n",
      "Epoch 00066: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0929 - accuracy: 0.9816 - val_loss: 0.3518 - val_accuracy: 0.9193\n",
      "Epoch 67/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0946 - accuracy: 0.9802\n",
      "Epoch 00067: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0946 - accuracy: 0.9802 - val_loss: 0.3538 - val_accuracy: 0.9207\n",
      "Epoch 68/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9824\n",
      "Epoch 00068: val_accuracy did not improve from 0.92145\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0901 - accuracy: 0.9823 - val_loss: 0.3350 - val_accuracy: 0.9185\n",
      "Epoch 69/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0888 - accuracy: 0.9831\n",
      "Epoch 00069: val_accuracy improved from 0.92145 to 0.92255, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0888 - accuracy: 0.9831 - val_loss: 0.3468 - val_accuracy: 0.9225\n",
      "Epoch 70/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9831\n",
      "Epoch 00070: val_accuracy did not improve from 0.92255\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0869 - accuracy: 0.9831 - val_loss: 0.3440 - val_accuracy: 0.9211\n",
      "Epoch 71/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9831\n",
      "Epoch 00071: val_accuracy improved from 0.92255 to 0.92400, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0868 - accuracy: 0.9832 - val_loss: 0.3645 - val_accuracy: 0.9240\n",
      "Epoch 72/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9810\n",
      "Epoch 00072: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9807 - val_loss: 0.3583 - val_accuracy: 0.9211\n",
      "Epoch 73/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9839\n",
      "Epoch 00073: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0852 - accuracy: 0.9838 - val_loss: 0.3682 - val_accuracy: 0.9156\n",
      "Epoch 74/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0854 - accuracy: 0.9845\n",
      "Epoch 00074: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0853 - accuracy: 0.9846 - val_loss: 0.3537 - val_accuracy: 0.9233\n",
      "Epoch 75/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9824\n",
      "Epoch 00075: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0879 - accuracy: 0.9823 - val_loss: 0.3570 - val_accuracy: 0.9185\n",
      "Epoch 76/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0844 - accuracy: 0.9838\n",
      "Epoch 00076: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0843 - accuracy: 0.9838 - val_loss: 0.3437 - val_accuracy: 0.9207\n",
      "Epoch 77/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0876 - accuracy: 0.9830\n",
      "Epoch 00077: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0880 - accuracy: 0.9828 - val_loss: 0.3658 - val_accuracy: 0.9196\n",
      "Epoch 78/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9844\n",
      "Epoch 00078: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 6s 7ms/step - loss: 0.0828 - accuracy: 0.9843 - val_loss: 0.3573 - val_accuracy: 0.9175\n",
      "Epoch 79/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9829\n",
      "Epoch 00079: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0864 - accuracy: 0.9828 - val_loss: 0.3656 - val_accuracy: 0.9189\n",
      "Epoch 80/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0825 - accuracy: 0.9838\n",
      "Epoch 00080: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 7s 9ms/step - loss: 0.0825 - accuracy: 0.9838 - val_loss: 0.3651 - val_accuracy: 0.9207\n",
      "Epoch 81/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9847\n",
      "Epoch 00081: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0831 - accuracy: 0.9847 - val_loss: 0.3414 - val_accuracy: 0.9218\n",
      "Epoch 82/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0821 - accuracy: 0.9841\n",
      "Epoch 00082: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0821 - accuracy: 0.9841 - val_loss: 0.3286 - val_accuracy: 0.9225\n",
      "Epoch 83/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9846\n",
      "Epoch 00083: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0825 - accuracy: 0.9846 - val_loss: 0.3626 - val_accuracy: 0.9175\n",
      "Epoch 84/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9846\n",
      "Epoch 00084: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0810 - accuracy: 0.9845 - val_loss: 0.3496 - val_accuracy: 0.9233\n",
      "Epoch 85/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9839\n",
      "Epoch 00085: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0820 - accuracy: 0.9840 - val_loss: 0.3601 - val_accuracy: 0.9189\n",
      "Epoch 86/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9848\n",
      "Epoch 00086: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0802 - accuracy: 0.9848 - val_loss: 0.3530 - val_accuracy: 0.9215\n",
      "Epoch 87/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9832\n",
      "Epoch 00087: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0836 - accuracy: 0.9832 - val_loss: 0.3536 - val_accuracy: 0.9175\n",
      "Epoch 88/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9847\n",
      "Epoch 00088: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0794 - accuracy: 0.9848 - val_loss: 0.3453 - val_accuracy: 0.9215\n",
      "Epoch 89/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9858\n",
      "Epoch 00089: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0766 - accuracy: 0.9858 - val_loss: 0.3676 - val_accuracy: 0.9182\n",
      "Epoch 90/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.9852\n",
      "Epoch 00090: val_accuracy did not improve from 0.92400\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0791 - accuracy: 0.9853 - val_loss: 0.3517 - val_accuracy: 0.9185\n",
      "Epoch 91/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9852\n",
      "Epoch 00091: val_accuracy improved from 0.92400 to 0.92436, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0770 - accuracy: 0.9851 - val_loss: 0.3599 - val_accuracy: 0.9244\n",
      "Epoch 92/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9849\n",
      "Epoch 00092: val_accuracy did not improve from 0.92436\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0801 - accuracy: 0.9849 - val_loss: 0.3416 - val_accuracy: 0.9185\n",
      "Epoch 93/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9851\n",
      "Epoch 00093: val_accuracy did not improve from 0.92436\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0788 - accuracy: 0.9850 - val_loss: 0.3494 - val_accuracy: 0.9244\n",
      "Epoch 94/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9859\n",
      "Epoch 00094: val_accuracy did not improve from 0.92436\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0736 - accuracy: 0.9859 - val_loss: 0.3538 - val_accuracy: 0.9207\n",
      "Epoch 95/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9860\n",
      "Epoch 00095: val_accuracy improved from 0.92436 to 0.92509, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0763 - accuracy: 0.9861 - val_loss: 0.3482 - val_accuracy: 0.9251\n",
      "Epoch 96/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9858\n",
      "Epoch 00096: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0737 - accuracy: 0.9859 - val_loss: 0.3501 - val_accuracy: 0.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.9840\n",
      "Epoch 00097: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9840 - val_loss: 0.3587 - val_accuracy: 0.9149\n",
      "Epoch 98/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9863\n",
      "Epoch 00098: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0739 - accuracy: 0.9863 - val_loss: 0.3408 - val_accuracy: 0.9251\n",
      "Epoch 99/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9872\n",
      "Epoch 00099: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0736 - accuracy: 0.9871 - val_loss: 0.3589 - val_accuracy: 0.9171\n",
      "Epoch 100/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9863\n",
      "Epoch 00100: val_accuracy did not improve from 0.92509\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0713 - accuracy: 0.9863 - val_loss: 0.3614 - val_accuracy: 0.9225\n",
      "Epoch 101/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9852\n",
      "Epoch 00101: val_accuracy improved from 0.92509 to 0.92618, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 6s 8ms/step - loss: 0.0784 - accuracy: 0.9851 - val_loss: 0.3517 - val_accuracy: 0.9262\n",
      "Epoch 102/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9870\n",
      "Epoch 00102: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 9s 12ms/step - loss: 0.0701 - accuracy: 0.9870 - val_loss: 0.3604 - val_accuracy: 0.9218\n",
      "Epoch 103/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9861\n",
      "Epoch 00103: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9860 - val_loss: 0.3496 - val_accuracy: 0.9233\n",
      "Epoch 104/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9865\n",
      "Epoch 00104: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0740 - accuracy: 0.9865 - val_loss: 0.3582 - val_accuracy: 0.9207\n",
      "Epoch 105/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9874\n",
      "Epoch 00105: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0716 - accuracy: 0.9874 - val_loss: 0.3577 - val_accuracy: 0.9200\n",
      "Epoch 106/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9849\n",
      "Epoch 00106: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0738 - accuracy: 0.9849 - val_loss: 0.3694 - val_accuracy: 0.9185\n",
      "Epoch 107/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9865\n",
      "Epoch 00107: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 13s 17ms/step - loss: 0.0717 - accuracy: 0.9865 - val_loss: 0.3493 - val_accuracy: 0.9233\n",
      "Epoch 108/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9859\n",
      "Epoch 00108: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0731 - accuracy: 0.9859 - val_loss: 0.3469 - val_accuracy: 0.9247\n",
      "Epoch 109/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9861\n",
      "Epoch 00109: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0733 - accuracy: 0.9860 - val_loss: 0.3570 - val_accuracy: 0.9258\n",
      "Epoch 110/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9880\n",
      "Epoch 00110: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0693 - accuracy: 0.9880 - val_loss: 0.3532 - val_accuracy: 0.9229\n",
      "Epoch 111/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9884\n",
      "Epoch 00111: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0682 - accuracy: 0.9882 - val_loss: 0.3466 - val_accuracy: 0.9247\n",
      "Epoch 112/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9865\n",
      "Epoch 00112: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9864 - val_loss: 0.3590 - val_accuracy: 0.9200\n",
      "Epoch 113/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9872\n",
      "Epoch 00113: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0711 - accuracy: 0.9871 - val_loss: 0.3443 - val_accuracy: 0.9262\n",
      "Epoch 114/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9881\n",
      "Epoch 00114: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9882 - val_loss: 0.3312 - val_accuracy: 0.9262\n",
      "Epoch 115/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9875\n",
      "Epoch 00115: val_accuracy did not improve from 0.92618\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9875 - val_loss: 0.3432 - val_accuracy: 0.9211\n",
      "Epoch 116/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9878\n",
      "Epoch 00116: val_accuracy improved from 0.92618 to 0.92655, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0673 - accuracy: 0.9877 - val_loss: 0.3459 - val_accuracy: 0.9265\n",
      "Epoch 117/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9873\n",
      "Epoch 00117: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0703 - accuracy: 0.9873 - val_loss: 0.3353 - val_accuracy: 0.9247\n",
      "Epoch 118/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9883\n",
      "Epoch 00118: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9882 - val_loss: 0.3492 - val_accuracy: 0.9233\n",
      "Epoch 119/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9876\n",
      "Epoch 00119: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9877 - val_loss: 0.3495 - val_accuracy: 0.9204\n",
      "Epoch 120/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9874\n",
      "Epoch 00120: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0677 - accuracy: 0.9875 - val_loss: 0.3465 - val_accuracy: 0.9229\n",
      "Epoch 121/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9879\n",
      "Epoch 00121: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0669 - accuracy: 0.9879 - val_loss: 0.3536 - val_accuracy: 0.9233\n",
      "Epoch 122/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9891\n",
      "Epoch 00122: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9891 - val_loss: 0.3572 - val_accuracy: 0.9222\n",
      "Epoch 123/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9873\n",
      "Epoch 00123: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0679 - accuracy: 0.9873 - val_loss: 0.3564 - val_accuracy: 0.9233\n",
      "Epoch 124/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9890\n",
      "Epoch 00124: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9890 - val_loss: 0.3484 - val_accuracy: 0.9215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9892\n",
      "Epoch 00125: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0640 - accuracy: 0.9892 - val_loss: 0.3457 - val_accuracy: 0.9255\n",
      "Epoch 126/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9869\n",
      "Epoch 00126: val_accuracy did not improve from 0.92655\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0668 - accuracy: 0.9869 - val_loss: 0.3456 - val_accuracy: 0.9258\n",
      "Epoch 127/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9868\n",
      "Epoch 00127: val_accuracy improved from 0.92655 to 0.92800, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9869 - val_loss: 0.3429 - val_accuracy: 0.9280\n",
      "Epoch 128/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9876\n",
      "Epoch 00128: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0656 - accuracy: 0.9876 - val_loss: 0.3449 - val_accuracy: 0.9273\n",
      "Epoch 129/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9879\n",
      "Epoch 00129: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9878 - val_loss: 0.3700 - val_accuracy: 0.9211\n",
      "Epoch 130/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9887\n",
      "Epoch 00130: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0638 - accuracy: 0.9886 - val_loss: 0.3486 - val_accuracy: 0.9247\n",
      "Epoch 131/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0632 - accuracy: 0.9892\n",
      "Epoch 00131: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9891 - val_loss: 0.3654 - val_accuracy: 0.9218\n",
      "Epoch 132/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9901\n",
      "Epoch 00132: val_accuracy did not improve from 0.92800\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0614 - accuracy: 0.9900 - val_loss: 0.3605 - val_accuracy: 0.9236\n",
      "Epoch 133/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9880\n",
      "Epoch 00133: val_accuracy improved from 0.92800 to 0.92873, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0654 - accuracy: 0.9879 - val_loss: 0.3542 - val_accuracy: 0.9287\n",
      "Epoch 134/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9895\n",
      "Epoch 00134: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0611 - accuracy: 0.9893 - val_loss: 0.3677 - val_accuracy: 0.9182\n",
      "Epoch 135/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9880\n",
      "Epoch 00135: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9879 - val_loss: 0.3638 - val_accuracy: 0.9225\n",
      "Epoch 136/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9894\n",
      "Epoch 00136: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0625 - accuracy: 0.9893 - val_loss: 0.3603 - val_accuracy: 0.9247\n",
      "Epoch 137/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9887\n",
      "Epoch 00137: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9887 - val_loss: 0.3651 - val_accuracy: 0.9233\n",
      "Epoch 138/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9890\n",
      "Epoch 00138: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9890 - val_loss: 0.3554 - val_accuracy: 0.9229\n",
      "Epoch 139/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9888\n",
      "Epoch 00139: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0608 - accuracy: 0.9888 - val_loss: 0.3479 - val_accuracy: 0.9276\n",
      "Epoch 140/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9893\n",
      "Epoch 00140: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0603 - accuracy: 0.9893 - val_loss: 0.3549 - val_accuracy: 0.9240\n",
      "Epoch 141/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0591 - accuracy: 0.9897\n",
      "Epoch 00141: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0591 - accuracy: 0.9896 - val_loss: 0.3516 - val_accuracy: 0.9255\n",
      "Epoch 142/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9888\n",
      "Epoch 00142: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0640 - accuracy: 0.9887 - val_loss: 0.3403 - val_accuracy: 0.9273\n",
      "Epoch 143/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9886\n",
      "Epoch 00143: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9886 - val_loss: 0.3630 - val_accuracy: 0.9225\n",
      "Epoch 144/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9906\n",
      "Epoch 00144: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0562 - accuracy: 0.9906 - val_loss: 0.3615 - val_accuracy: 0.9251\n",
      "Epoch 145/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9893\n",
      "Epoch 00145: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0595 - accuracy: 0.9894 - val_loss: 0.3579 - val_accuracy: 0.9218\n",
      "Epoch 146/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9878\n",
      "Epoch 00146: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9879 - val_loss: 0.3662 - val_accuracy: 0.9211\n",
      "Epoch 147/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9887\n",
      "Epoch 00147: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0606 - accuracy: 0.9887 - val_loss: 0.3526 - val_accuracy: 0.9262\n",
      "Epoch 148/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9907\n",
      "Epoch 00148: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0568 - accuracy: 0.9906 - val_loss: 0.3553 - val_accuracy: 0.9244\n",
      "Epoch 149/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9896\n",
      "Epoch 00149: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0585 - accuracy: 0.9896 - val_loss: 0.3627 - val_accuracy: 0.9244\n",
      "Epoch 150/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9897\n",
      "Epoch 00150: val_accuracy did not improve from 0.92873\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0588 - accuracy: 0.9898 - val_loss: 0.3664 - val_accuracy: 0.9265\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [10:45<00:00, 322.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.87      0.76      0.81       756\n",
      "        car_horn       0.81      0.90      0.85       252\n",
      "children_playing       0.73      0.80      0.76       700\n",
      "        dog_bark       0.74      0.83      0.78       700\n",
      "           siren       0.88      0.76      0.82       602\n",
      "\n",
      "        accuracy                           0.80      3010\n",
      "       macro avg       0.81      0.81      0.81      3010\n",
      "    weighted avg       0.81      0.80      0.80      3010\n",
      "\n",
      "Validation fold: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27573, 375)\n",
      "X_val_norm shape.....:(2933, 375)\n",
      "\n",
      "Sum of elements: 0.9800710149454922\n",
      "Number of elements summed: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 234)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               176250    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 289,985\n",
      "Trainable params: 289,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24815, 234)\n",
      "Epoch 1/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.7900 - accuracy: 0.7127\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83104, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 2ms/step - loss: 0.7846 - accuracy: 0.7150 - val_loss: 0.4820 - val_accuracy: 0.8310\n",
      "Epoch 2/350\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.4401 - accuracy: 0.8455\n",
      "Epoch 00002: val_accuracy improved from 0.83104 to 0.86403, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.4402 - accuracy: 0.8455 - val_loss: 0.3785 - val_accuracy: 0.8640\n",
      "Epoch 3/350\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.3426 - accuracy: 0.8779\n",
      "Epoch 00003: val_accuracy improved from 0.86403 to 0.88434, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.3427 - accuracy: 0.8777 - val_loss: 0.3245 - val_accuracy: 0.8843\n",
      "Epoch 4/350\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.8984\n",
      "Epoch 00004: val_accuracy improved from 0.88434 to 0.89521, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.2834 - accuracy: 0.8983 - val_loss: 0.2985 - val_accuracy: 0.8952\n",
      "Epoch 5/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.2363 - accuracy: 0.9164\n",
      "Epoch 00005: val_accuracy improved from 0.89521 to 0.90537, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.2365 - accuracy: 0.9164 - val_loss: 0.2640 - val_accuracy: 0.9054\n",
      "Epoch 6/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9331\n",
      "Epoch 00006: val_accuracy improved from 0.90537 to 0.91842, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.1969 - accuracy: 0.9333 - val_loss: 0.2382 - val_accuracy: 0.9184\n",
      "Epoch 7/350\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.1665 - accuracy: 0.9420\n",
      "Epoch 00007: val_accuracy improved from 0.91842 to 0.92059, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.1668 - accuracy: 0.9417 - val_loss: 0.2290 - val_accuracy: 0.9206\n",
      "Epoch 8/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9525\n",
      "Epoch 00008: val_accuracy improved from 0.92059 to 0.92748, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.1405 - accuracy: 0.9522 - val_loss: 0.2156 - val_accuracy: 0.9275\n",
      "Epoch 9/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9609\n",
      "Epoch 00009: val_accuracy improved from 0.92748 to 0.93038, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.1175 - accuracy: 0.9610 - val_loss: 0.2109 - val_accuracy: 0.9304\n",
      "Epoch 10/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9672\n",
      "Epoch 00010: val_accuracy improved from 0.93038 to 0.93510, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0996 - accuracy: 0.9674 - val_loss: 0.2002 - val_accuracy: 0.9351\n",
      "Epoch 11/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9729\n",
      "Epoch 00011: val_accuracy improved from 0.93510 to 0.93655, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0845 - accuracy: 0.9730 - val_loss: 0.1981 - val_accuracy: 0.9365\n",
      "Epoch 12/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9767\n",
      "Epoch 00012: val_accuracy improved from 0.93655 to 0.93945, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0724 - accuracy: 0.9767 - val_loss: 0.1992 - val_accuracy: 0.9394\n",
      "Epoch 13/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9799\n",
      "Epoch 00013: val_accuracy improved from 0.93945 to 0.94162, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0624 - accuracy: 0.9801 - val_loss: 0.1971 - val_accuracy: 0.9416\n",
      "Epoch 14/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9837\n",
      "Epoch 00014: val_accuracy improved from 0.94162 to 0.94271, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0522 - accuracy: 0.9834 - val_loss: 0.1909 - val_accuracy: 0.9427\n",
      "Epoch 15/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0475 - accuracy: 0.9853\n",
      "Epoch 00015: val_accuracy did not improve from 0.94271\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0475 - accuracy: 0.9854 - val_loss: 0.1947 - val_accuracy: 0.9420\n",
      "Epoch 16/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9889\n",
      "Epoch 00016: val_accuracy improved from 0.94271 to 0.94344, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0385 - accuracy: 0.9889 - val_loss: 0.1946 - val_accuracy: 0.9434\n",
      "Epoch 17/350\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.0333 - accuracy: 0.9907\n",
      "Epoch 00017: val_accuracy improved from 0.94344 to 0.94634, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9907 - val_loss: 0.1947 - val_accuracy: 0.9463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0284 - accuracy: 0.9924\n",
      "Epoch 00018: val_accuracy did not improve from 0.94634\n",
      "776/776 [==============================] - 2s 2ms/step - loss: 0.0284 - accuracy: 0.9923 - val_loss: 0.1951 - val_accuracy: 0.9453\n",
      "Epoch 19/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9926\n",
      "Epoch 00019: val_accuracy did not improve from 0.94634\n",
      "776/776 [==============================] - 2s 2ms/step - loss: 0.0255 - accuracy: 0.9925 - val_loss: 0.1976 - val_accuracy: 0.9456\n",
      "Epoch 20/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0213 - accuracy: 0.9943\n",
      "Epoch 00020: val_accuracy improved from 0.94634 to 0.94851, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9944 - val_loss: 0.1997 - val_accuracy: 0.9485\n",
      "Epoch 21/350\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9948\n",
      "Epoch 00021: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9948 - val_loss: 0.1997 - val_accuracy: 0.9482\n",
      "Epoch 22/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9948\n",
      "Epoch 00022: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9948 - val_loss: 0.2072 - val_accuracy: 0.9456\n",
      "Epoch 23/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9966\n",
      "Epoch 00023: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0141 - accuracy: 0.9966 - val_loss: 0.2020 - val_accuracy: 0.9453\n",
      "Epoch 24/350\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9966\n",
      "Epoch 00024: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9966 - val_loss: 0.2051 - val_accuracy: 0.9485\n",
      "Epoch 25/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9967\n",
      "Epoch 00025: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0136 - accuracy: 0.9967 - val_loss: 0.2076 - val_accuracy: 0.9474\n",
      "Epoch 26/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9970\n",
      "Epoch 00026: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.2137 - val_accuracy: 0.9467\n",
      "Epoch 27/350\n",
      "756/776 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9966\n",
      "Epoch 00027: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0121 - accuracy: 0.9967 - val_loss: 0.2140 - val_accuracy: 0.9460\n",
      "Epoch 28/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9974\n",
      "Epoch 00028: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0107 - accuracy: 0.9974 - val_loss: 0.2167 - val_accuracy: 0.9463\n",
      "Epoch 29/350\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9981\n",
      "Epoch 00029: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.2132 - val_accuracy: 0.9467\n",
      "Epoch 30/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9981\n",
      "Epoch 00030: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.2194 - val_accuracy: 0.9485\n",
      "Epoch 31/350\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9984\n",
      "Epoch 00031: val_accuracy did not improve from 0.94851\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9984 - val_loss: 0.2159 - val_accuracy: 0.9478\n",
      "Epoch 32/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9983\n",
      "Epoch 00032: val_accuracy improved from 0.94851 to 0.94960, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9983 - val_loss: 0.2171 - val_accuracy: 0.9496\n",
      "Epoch 33/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9985\n",
      "Epoch 00033: val_accuracy did not improve from 0.94960\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.2215 - val_accuracy: 0.9485\n",
      "Epoch 34/350\n",
      "750/776 [===========================>..] - ETA: 0s - loss: 0.0061 - accuracy: 0.9986\n",
      "Epoch 00034: val_accuracy improved from 0.94960 to 0.94996, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.2245 - val_accuracy: 0.9500\n",
      "Epoch 35/350\n",
      "745/776 [===========================>..] - ETA: 0s - loss: 0.0054 - accuracy: 0.9990\n",
      "Epoch 00035: val_accuracy did not improve from 0.94996\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 0.2270 - val_accuracy: 0.9489\n",
      "Epoch 36/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9989\n",
      "Epoch 00036: val_accuracy improved from 0.94996 to 0.95141, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.2218 - val_accuracy: 0.9514\n",
      "Epoch 37/350\n",
      "751/776 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00037: val_accuracy improved from 0.95141 to 0.95286, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.2277 - val_accuracy: 0.9529\n",
      "Epoch 38/350\n",
      "745/776 [===========================>..] - ETA: 0s - loss: 0.0041 - accuracy: 0.9993\n",
      "Epoch 00038: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.2296 - val_accuracy: 0.9503\n",
      "Epoch 39/350\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 00039: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.2388 - val_accuracy: 0.9492\n",
      "Epoch 40/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9989\n",
      "Epoch 00040: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.2420 - val_accuracy: 0.9482\n",
      "Epoch 41/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9990\n",
      "Epoch 00041: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.2352 - val_accuracy: 0.9500\n",
      "Epoch 42/350\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9992\n",
      "Epoch 00042: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.2416 - val_accuracy: 0.9492\n",
      "Epoch 43/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 00043: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.2364 - val_accuracy: 0.9489\n",
      "Epoch 44/350\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9989\n",
      "Epoch 00044: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.2431 - val_accuracy: 0.9485\n",
      "Epoch 45/350\n",
      "747/776 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 00045: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.2360 - val_accuracy: 0.9514\n",
      "Epoch 46/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9996\n",
      "Epoch 00046: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2302 - val_accuracy: 0.9507\n",
      "Epoch 47/350\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 00047: val_accuracy did not improve from 0.95286\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.2318 - val_accuracy: 0.9507\n",
      "Epoch 48/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 00048: val_accuracy improved from 0.95286 to 0.95323, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.2387 - val_accuracy: 0.9532\n",
      "Epoch 49/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 00049: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.2406 - val_accuracy: 0.9500\n",
      "Epoch 50/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9993\n",
      "Epoch 00050: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.2413 - val_accuracy: 0.9525\n",
      "Epoch 51/350\n",
      "750/776 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 0.9997\n",
      "Epoch 00051: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.2464 - val_accuracy: 0.9529\n",
      "Epoch 52/350\n",
      "749/776 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 0.9997\n",
      "Epoch 00052: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.2490 - val_accuracy: 0.9511\n",
      "Epoch 53/350\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 00053: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.2472 - val_accuracy: 0.9532\n",
      "Epoch 54/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 00054: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.2431 - val_accuracy: 0.9514\n",
      "Epoch 55/350\n",
      "751/776 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2439 - val_accuracy: 0.9507\n",
      "Epoch 56/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00056: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.2457 - val_accuracy: 0.9518\n",
      "Epoch 57/350\n",
      "748/776 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 00057: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2663 - val_accuracy: 0.9496\n",
      "Epoch 58/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 00058: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2536 - val_accuracy: 0.9511\n",
      "Epoch 59/350\n",
      "747/776 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 00059: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.2504 - val_accuracy: 0.9474\n",
      "Epoch 60/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 00060: val_accuracy did not improve from 0.95323\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.2497 - val_accuracy: 0.9521\n",
      "Epoch 61/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy improved from 0.95323 to 0.95359, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2493 - val_accuracy: 0.9536\n",
      "Epoch 62/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00062: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2531 - val_accuracy: 0.9503\n",
      "Epoch 63/350\n",
      "751/776 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00063: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2585 - val_accuracy: 0.9532\n",
      "Epoch 64/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00064: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2556 - val_accuracy: 0.9532\n",
      "Epoch 65/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00065: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2526 - val_accuracy: 0.9532\n",
      "Epoch 66/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 00066: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.2589 - val_accuracy: 0.9532\n",
      "Epoch 67/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00067: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2541 - val_accuracy: 0.9529\n",
      "Epoch 68/350\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 00068: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.2578 - val_accuracy: 0.9536\n",
      "Epoch 69/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00069: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2595 - val_accuracy: 0.9511\n",
      "Epoch 70/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00070: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2554 - val_accuracy: 0.9511\n",
      "Epoch 71/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2545 - val_accuracy: 0.9529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00072: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2578 - val_accuracy: 0.9525\n",
      "Epoch 73/350\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00073: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.2571 - val_accuracy: 0.9532\n",
      "Epoch 74/350\n",
      "753/776 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2574 - val_accuracy: 0.9529\n",
      "Epoch 75/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00075: val_accuracy did not improve from 0.95359\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2523 - val_accuracy: 0.9536\n",
      "Epoch 76/350\n",
      "752/776 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00076: val_accuracy improved from 0.95359 to 0.95395, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2567 - val_accuracy: 0.9540\n",
      "Epoch 77/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00077: val_accuracy did not improve from 0.95395\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2520 - val_accuracy: 0.9518\n",
      "Epoch 78/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00078: val_accuracy improved from 0.95395 to 0.95431, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2506 - val_accuracy: 0.9543\n",
      "Epoch 79/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95431\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2545 - val_accuracy: 0.9507\n",
      "Epoch 80/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00080: val_accuracy did not improve from 0.95431\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2553 - val_accuracy: 0.9529\n",
      "Epoch 81/350\n",
      "749/776 [===========================>..] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00081: val_accuracy did not improve from 0.95431\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2551 - val_accuracy: 0.9536\n",
      "Epoch 82/350\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00082: val_accuracy improved from 0.95431 to 0.95504, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2561 - val_accuracy: 0.9550\n",
      "Epoch 83/350\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00083: val_accuracy did not improve from 0.95504\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2569 - val_accuracy: 0.9514\n",
      "Epoch 84/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 00084: val_accuracy did not improve from 0.95504\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.2516 - val_accuracy: 0.9540\n",
      "Epoch 85/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00085: val_accuracy did not improve from 0.95504\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2525 - val_accuracy: 0.9543\n",
      "Epoch 86/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00086: val_accuracy improved from 0.95504 to 0.95540, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2576 - val_accuracy: 0.9554\n",
      "Epoch 87/350\n",
      "770/776 [============================>.] - ETA: 0s - loss: 9.3750e-04 - accuracy: 0.9999\n",
      "Epoch 00087: val_accuracy did not improve from 0.95540\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.3505e-04 - accuracy: 0.9999 - val_loss: 0.2626 - val_accuracy: 0.9543\n",
      "Epoch 88/350\n",
      "760/776 [============================>.] - ETA: 0s - loss: 9.4824e-04 - accuracy: 0.9999\n",
      "Epoch 00088: val_accuracy did not improve from 0.95540\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.5139e-04 - accuracy: 0.9999 - val_loss: 0.2617 - val_accuracy: 0.9550\n",
      "Epoch 89/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy improved from 0.95540 to 0.95577, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2614 - val_accuracy: 0.9558\n",
      "Epoch 90/350\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00090: val_accuracy improved from 0.95577 to 0.95649, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2604 - val_accuracy: 0.9565\n",
      "Epoch 91/350\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00091: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2664 - val_accuracy: 0.9532\n",
      "Epoch 92/350\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00092: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2630 - val_accuracy: 0.9525\n",
      "Epoch 93/350\n",
      "750/776 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2681 - val_accuracy: 0.9536\n",
      "Epoch 94/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 9.0326e-04 - accuracy: 1.0000\n",
      "Epoch 00094: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.9657e-04 - accuracy: 1.0000 - val_loss: 0.2702 - val_accuracy: 0.9536\n",
      "Epoch 95/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00095: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2716 - val_accuracy: 0.9532\n",
      "Epoch 96/350\n",
      "746/776 [===========================>..] - ETA: 0s - loss: 8.8391e-04 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.7649e-04 - accuracy: 0.9998 - val_loss: 0.2710 - val_accuracy: 0.9507\n",
      "Epoch 97/350\n",
      "745/776 [===========================>..] - ETA: 0s - loss: 7.6181e-04 - accuracy: 0.9999\n",
      "Epoch 00097: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.8058e-04 - accuracy: 0.9999 - val_loss: 0.2720 - val_accuracy: 0.9518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/350\n",
      "754/776 [============================>.] - ETA: 0s - loss: 7.7488e-04 - accuracy: 0.9999\n",
      "Epoch 00098: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.7267e-04 - accuracy: 0.9999 - val_loss: 0.2698 - val_accuracy: 0.9529\n",
      "Epoch 99/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 9.9292e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.8703e-04 - accuracy: 0.9998 - val_loss: 0.2662 - val_accuracy: 0.9554\n",
      "Epoch 100/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00100: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.9258e-04 - accuracy: 0.9998 - val_loss: 0.2662 - val_accuracy: 0.9547\n",
      "Epoch 101/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 9.8036e-04 - accuracy: 0.9997\n",
      "Epoch 00101: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.7969e-04 - accuracy: 0.9997 - val_loss: 0.2711 - val_accuracy: 0.9547\n",
      "Epoch 102/350\n",
      "758/776 [============================>.] - ETA: 0s - loss: 7.4438e-04 - accuracy: 0.9998\n",
      "Epoch 00102: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.4451e-04 - accuracy: 0.9998 - val_loss: 0.2734 - val_accuracy: 0.9543\n",
      "Epoch 103/350\n",
      "769/776 [============================>.] - ETA: 0s - loss: 9.5469e-04 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 9.4872e-04 - accuracy: 0.9998 - val_loss: 0.2702 - val_accuracy: 0.9543\n",
      "Epoch 104/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 7.2210e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.2087e-04 - accuracy: 0.9999 - val_loss: 0.2721 - val_accuracy: 0.9525\n",
      "Epoch 105/350\n",
      "762/776 [============================>.] - ETA: 0s - loss: 7.9323e-04 - accuracy: 0.9998\n",
      "Epoch 00105: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.8292e-04 - accuracy: 0.9998 - val_loss: 0.2776 - val_accuracy: 0.9507\n",
      "Epoch 106/350\n",
      "766/776 [============================>.] - ETA: 0s - loss: 8.2885e-04 - accuracy: 0.9998\n",
      "Epoch 00106: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.3165e-04 - accuracy: 0.9998 - val_loss: 0.2734 - val_accuracy: 0.9532\n",
      "Epoch 107/350\n",
      "761/776 [============================>.] - ETA: 0s - loss: 7.5584e-04 - accuracy: 0.9999\n",
      "Epoch 00107: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.5374e-04 - accuracy: 0.9999 - val_loss: 0.2665 - val_accuracy: 0.9536\n",
      "Epoch 108/350\n",
      "771/776 [============================>.] - ETA: 0s - loss: 6.4519e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.4189e-04 - accuracy: 0.9999 - val_loss: 0.2691 - val_accuracy: 0.9543\n",
      "Epoch 109/350\n",
      "772/776 [============================>.] - ETA: 0s - loss: 8.0081e-04 - accuracy: 0.9998\n",
      "Epoch 00109: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.9957e-04 - accuracy: 0.9998 - val_loss: 0.2758 - val_accuracy: 0.9550\n",
      "Epoch 110/350\n",
      "745/776 [===========================>..] - ETA: 0s - loss: 8.2919e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.2140e-04 - accuracy: 0.9999 - val_loss: 0.2692 - val_accuracy: 0.9540\n",
      "Epoch 111/350\n",
      "749/776 [===========================>..] - ETA: 0s - loss: 8.2616e-04 - accuracy: 0.9998\n",
      "Epoch 00111: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.3281e-04 - accuracy: 0.9998 - val_loss: 0.2642 - val_accuracy: 0.9532\n",
      "Epoch 112/350\n",
      "756/776 [============================>.] - ETA: 0s - loss: 6.0171e-04 - accuracy: 1.0000\n",
      "Epoch 00112: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 5.9872e-04 - accuracy: 1.0000 - val_loss: 0.2627 - val_accuracy: 0.9543\n",
      "Epoch 113/350\n",
      "768/776 [============================>.] - ETA: 0s - loss: 7.6883e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.6336e-04 - accuracy: 0.9999 - val_loss: 0.2673 - val_accuracy: 0.9550\n",
      "Epoch 114/350\n",
      "773/776 [============================>.] - ETA: 0s - loss: 6.6975e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.6817e-04 - accuracy: 1.0000 - val_loss: 0.2665 - val_accuracy: 0.9550\n",
      "Epoch 115/350\n",
      "765/776 [============================>.] - ETA: 0s - loss: 8.9503e-04 - accuracy: 0.9999\n",
      "Epoch 00115: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.8500e-04 - accuracy: 0.9999 - val_loss: 0.2632 - val_accuracy: 0.9565\n",
      "Epoch 116/350\n",
      "774/776 [============================>.] - ETA: 0s - loss: 6.9201e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.9077e-04 - accuracy: 1.0000 - val_loss: 0.2674 - val_accuracy: 0.9529\n",
      "Epoch 117/350\n",
      "757/776 [============================>.] - ETA: 0s - loss: 6.7184e-04 - accuracy: 0.9998\n",
      "Epoch 00117: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.6387e-04 - accuracy: 0.9998 - val_loss: 0.2739 - val_accuracy: 0.9529\n",
      "Epoch 118/350\n",
      "767/776 [============================>.] - ETA: 0s - loss: 7.1182e-04 - accuracy: 0.9999\n",
      "Epoch 00118: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 7.0507e-04 - accuracy: 0.9999 - val_loss: 0.2672 - val_accuracy: 0.9558\n",
      "Epoch 119/350\n",
      "750/776 [===========================>..] - ETA: 0s - loss: 6.6833e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 6.5967e-04 - accuracy: 0.9999 - val_loss: 0.2730 - val_accuracy: 0.9543\n",
      "Epoch 120/350\n",
      "755/776 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998  \n",
      "Epoch 00120: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2849 - val_accuracy: 0.9525\n",
      "Epoch 121/350\n",
      "759/776 [============================>.] - ETA: 0s - loss: 5.6476e-04 - accuracy: 0.9999\n",
      "Epoch 00121: val_accuracy did not improve from 0.95649\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 5.6383e-04 - accuracy: 0.9999 - val_loss: 0.2765 - val_accuracy: 0.9518\n",
      "Epoch 122/350\n",
      "764/776 [============================>.] - ETA: 0s - loss: 8.3316e-04 - accuracy: 0.9998\n",
      "Epoch 00122: val_accuracy did not improve from 0.95649\n",
      "Restoring model weights from the end of the best epoch.\n",
      "776/776 [==============================] - 1s 2ms/step - loss: 8.2685e-04 - accuracy: 0.9998 - val_loss: 0.2740 - val_accuracy: 0.9514\n",
      "Epoch 00122: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:50<02:50, 170.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.84      0.88      0.86       721\n",
      "        car_horn       0.79      0.80      0.79       231\n",
      "children_playing       0.68      0.80      0.74       700\n",
      "        dog_bark       0.73      0.77      0.75       700\n",
      "           siren       0.83      0.56      0.67       581\n",
      "\n",
      "        accuracy                           0.76      2933\n",
      "       macro avg       0.77      0.76      0.76      2933\n",
      "    weighted avg       0.77      0.76      0.76      2933\n",
      "\n",
      "Model: \"Model_CNN_1D_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 228, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 228, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 228, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6384)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                319250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 330,291\n",
      "Trainable params: 330,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24815, 234, 1)\n",
      "Epoch 1/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.7741\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82777, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 3s 4ms/step - loss: 0.7279 - accuracy: 0.7741 - val_loss: 0.5420 - val_accuracy: 0.8278\n",
      "Epoch 2/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.4891 - accuracy: 0.8517\n",
      "Epoch 00002: val_accuracy improved from 0.82777 to 0.85533, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.4877 - accuracy: 0.8524 - val_loss: 0.4537 - val_accuracy: 0.8553\n",
      "Epoch 3/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8757\n",
      "Epoch 00003: val_accuracy improved from 0.85533 to 0.87817, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.4174 - accuracy: 0.8761 - val_loss: 0.4066 - val_accuracy: 0.8782\n",
      "Epoch 4/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8918\n",
      "Epoch 00004: val_accuracy improved from 0.87817 to 0.88687, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.3774 - accuracy: 0.8916 - val_loss: 0.3818 - val_accuracy: 0.8869\n",
      "Epoch 5/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.9011\n",
      "Epoch 00005: val_accuracy improved from 0.88687 to 0.88941, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.3384 - accuracy: 0.9012 - val_loss: 0.3769 - val_accuracy: 0.8894\n",
      "Epoch 6/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.9126\n",
      "Epoch 00006: val_accuracy improved from 0.88941 to 0.89920, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.3140 - accuracy: 0.9126 - val_loss: 0.3537 - val_accuracy: 0.8992\n",
      "Epoch 7/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9174\n",
      "Epoch 00007: val_accuracy improved from 0.89920 to 0.90754, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2918 - accuracy: 0.9173 - val_loss: 0.3388 - val_accuracy: 0.9075\n",
      "Epoch 8/150\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.9239\n",
      "Epoch 00008: val_accuracy improved from 0.90754 to 0.90790, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2768 - accuracy: 0.9238 - val_loss: 0.3380 - val_accuracy: 0.9079\n",
      "Epoch 9/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.9310\n",
      "Epoch 00009: val_accuracy improved from 0.90790 to 0.91117, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2565 - accuracy: 0.9308 - val_loss: 0.3312 - val_accuracy: 0.9112\n",
      "Epoch 10/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.9351\n",
      "Epoch 00010: val_accuracy improved from 0.91117 to 0.91878, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2456 - accuracy: 0.9347 - val_loss: 0.3178 - val_accuracy: 0.9188\n",
      "Epoch 11/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.2353 - accuracy: 0.9382\n",
      "Epoch 00011: val_accuracy improved from 0.91878 to 0.92023, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2350 - accuracy: 0.9383 - val_loss: 0.3187 - val_accuracy: 0.9202\n",
      "Epoch 12/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9423\n",
      "Epoch 00012: val_accuracy did not improve from 0.92023\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2232 - accuracy: 0.9423 - val_loss: 0.3167 - val_accuracy: 0.9166\n",
      "Epoch 13/150\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.2159 - accuracy: 0.9429\n",
      "Epoch 00013: val_accuracy improved from 0.92023 to 0.92386, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2157 - accuracy: 0.9430 - val_loss: 0.3179 - val_accuracy: 0.9239\n",
      "Epoch 14/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9465\n",
      "Epoch 00014: val_accuracy did not improve from 0.92386\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.2092 - accuracy: 0.9465 - val_loss: 0.3175 - val_accuracy: 0.9159\n",
      "Epoch 15/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.1960 - accuracy: 0.9518\n",
      "Epoch 00015: val_accuracy did not improve from 0.92386\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1960 - accuracy: 0.9517 - val_loss: 0.3161 - val_accuracy: 0.9228\n",
      "Epoch 16/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9513\n",
      "Epoch 00016: val_accuracy did not improve from 0.92386\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1961 - accuracy: 0.9513 - val_loss: 0.3213 - val_accuracy: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9544\n",
      "Epoch 00017: val_accuracy did not improve from 0.92386\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1850 - accuracy: 0.9542 - val_loss: 0.3175 - val_accuracy: 0.9191\n",
      "Epoch 18/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9547\n",
      "Epoch 00018: val_accuracy improved from 0.92386 to 0.92676, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1822 - accuracy: 0.9546 - val_loss: 0.3075 - val_accuracy: 0.9268\n",
      "Epoch 19/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9579\n",
      "Epoch 00019: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1744 - accuracy: 0.9579 - val_loss: 0.2983 - val_accuracy: 0.9231\n",
      "Epoch 20/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9591\n",
      "Epoch 00020: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1697 - accuracy: 0.9590 - val_loss: 0.3184 - val_accuracy: 0.9191\n",
      "Epoch 21/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1663 - accuracy: 0.9592\n",
      "Epoch 00021: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1664 - accuracy: 0.9591 - val_loss: 0.3164 - val_accuracy: 0.9220\n",
      "Epoch 22/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.9608\n",
      "Epoch 00022: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1655 - accuracy: 0.9607 - val_loss: 0.3156 - val_accuracy: 0.9210\n",
      "Epoch 23/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9618\n",
      "Epoch 00023: val_accuracy did not improve from 0.92676\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1596 - accuracy: 0.9619 - val_loss: 0.3135 - val_accuracy: 0.9253\n",
      "Epoch 24/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1523 - accuracy: 0.9637\n",
      "Epoch 00024: val_accuracy improved from 0.92676 to 0.92712, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1524 - accuracy: 0.9635 - val_loss: 0.3175 - val_accuracy: 0.9271\n",
      "Epoch 25/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.1517 - accuracy: 0.9651\n",
      "Epoch 00025: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1517 - accuracy: 0.9651 - val_loss: 0.3278 - val_accuracy: 0.9210\n",
      "Epoch 26/150\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.9674\n",
      "Epoch 00026: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1464 - accuracy: 0.9676 - val_loss: 0.3204 - val_accuracy: 0.9213\n",
      "Epoch 27/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9649\n",
      "Epoch 00027: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1493 - accuracy: 0.9649 - val_loss: 0.3307 - val_accuracy: 0.9173\n",
      "Epoch 28/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9669\n",
      "Epoch 00028: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1416 - accuracy: 0.9668 - val_loss: 0.3232 - val_accuracy: 0.9260\n",
      "Epoch 29/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9696\n",
      "Epoch 00029: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1391 - accuracy: 0.9697 - val_loss: 0.3126 - val_accuracy: 0.9264\n",
      "Epoch 30/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1319 - accuracy: 0.9708\n",
      "Epoch 00030: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1318 - accuracy: 0.9709 - val_loss: 0.3128 - val_accuracy: 0.9206\n",
      "Epoch 31/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9690\n",
      "Epoch 00031: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1375 - accuracy: 0.9689 - val_loss: 0.3343 - val_accuracy: 0.9249\n",
      "Epoch 32/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9717\n",
      "Epoch 00032: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1304 - accuracy: 0.9717 - val_loss: 0.3111 - val_accuracy: 0.9228\n",
      "Epoch 33/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9717\n",
      "Epoch 00033: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1281 - accuracy: 0.9717 - val_loss: 0.3190 - val_accuracy: 0.9271\n",
      "Epoch 34/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9706\n",
      "Epoch 00034: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1299 - accuracy: 0.9705 - val_loss: 0.3284 - val_accuracy: 0.9249\n",
      "Epoch 35/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9724\n",
      "Epoch 00035: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1254 - accuracy: 0.9723 - val_loss: 0.3251 - val_accuracy: 0.9260\n",
      "Epoch 36/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9746\n",
      "Epoch 00036: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1208 - accuracy: 0.9746 - val_loss: 0.3168 - val_accuracy: 0.9264\n",
      "Epoch 37/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9726\n",
      "Epoch 00037: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1217 - accuracy: 0.9726 - val_loss: 0.3208 - val_accuracy: 0.9264\n",
      "Epoch 38/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9746\n",
      "Epoch 00038: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1198 - accuracy: 0.9745 - val_loss: 0.3195 - val_accuracy: 0.9228\n",
      "Epoch 39/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.1145 - accuracy: 0.9762\n",
      "Epoch 00039: val_accuracy did not improve from 0.92712\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1146 - accuracy: 0.9762 - val_loss: 0.3289 - val_accuracy: 0.9257\n",
      "Epoch 40/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9734\n",
      "Epoch 00040: val_accuracy improved from 0.92712 to 0.92821, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1193 - accuracy: 0.9734 - val_loss: 0.3288 - val_accuracy: 0.9282\n",
      "Epoch 41/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9746\n",
      "Epoch 00041: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1163 - accuracy: 0.9746 - val_loss: 0.3352 - val_accuracy: 0.9264\n",
      "Epoch 42/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9752\n",
      "Epoch 00042: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1134 - accuracy: 0.9752 - val_loss: 0.3402 - val_accuracy: 0.9228\n",
      "Epoch 43/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9764\n",
      "Epoch 00043: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1114 - accuracy: 0.9763 - val_loss: 0.3323 - val_accuracy: 0.9278\n",
      "Epoch 44/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9773\n",
      "Epoch 00044: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1131 - accuracy: 0.9771 - val_loss: 0.3301 - val_accuracy: 0.9246\n",
      "Epoch 45/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9773\n",
      "Epoch 00045: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1091 - accuracy: 0.9774 - val_loss: 0.3276 - val_accuracy: 0.9278\n",
      "Epoch 46/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9767\n",
      "Epoch 00046: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1096 - accuracy: 0.9767 - val_loss: 0.3217 - val_accuracy: 0.9231\n",
      "Epoch 47/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9774\n",
      "Epoch 00047: val_accuracy did not improve from 0.92821\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1077 - accuracy: 0.9773 - val_loss: 0.3454 - val_accuracy: 0.9213\n",
      "Epoch 48/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9771\n",
      "Epoch 00048: val_accuracy improved from 0.92821 to 0.92857, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1099 - accuracy: 0.9770 - val_loss: 0.3274 - val_accuracy: 0.9286\n",
      "Epoch 49/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.1048 - accuracy: 0.9792\n",
      "Epoch 00049: val_accuracy improved from 0.92857 to 0.92966, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1048 - accuracy: 0.9792 - val_loss: 0.3328 - val_accuracy: 0.9297\n",
      "Epoch 50/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9793\n",
      "Epoch 00050: val_accuracy did not improve from 0.92966\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1014 - accuracy: 0.9792 - val_loss: 0.3425 - val_accuracy: 0.9253\n",
      "Epoch 51/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0977 - accuracy: 0.9810\n",
      "Epoch 00051: val_accuracy did not improve from 0.92966\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0977 - accuracy: 0.9810 - val_loss: 0.3319 - val_accuracy: 0.9278\n",
      "Epoch 52/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9797\n",
      "Epoch 00052: val_accuracy did not improve from 0.92966\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1005 - accuracy: 0.9797 - val_loss: 0.3399 - val_accuracy: 0.9242\n",
      "Epoch 53/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9804\n",
      "Epoch 00053: val_accuracy did not improve from 0.92966\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0960 - accuracy: 0.9803 - val_loss: 0.3562 - val_accuracy: 0.9224\n",
      "Epoch 54/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9803\n",
      "Epoch 00054: val_accuracy improved from 0.92966 to 0.93002, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0980 - accuracy: 0.9803 - val_loss: 0.3403 - val_accuracy: 0.9300\n",
      "Epoch 55/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9790\n",
      "Epoch 00055: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.1016 - accuracy: 0.9787 - val_loss: 0.3499 - val_accuracy: 0.9242\n",
      "Epoch 56/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9802\n",
      "Epoch 00056: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0983 - accuracy: 0.9801 - val_loss: 0.3417 - val_accuracy: 0.9253\n",
      "Epoch 57/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0905 - accuracy: 0.9830\n",
      "Epoch 00057: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0905 - accuracy: 0.9831 - val_loss: 0.3394 - val_accuracy: 0.9286\n",
      "Epoch 58/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0950 - accuracy: 0.9804\n",
      "Epoch 00058: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0952 - accuracy: 0.9805 - val_loss: 0.3332 - val_accuracy: 0.9297\n",
      "Epoch 59/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9798\n",
      "Epoch 00059: val_accuracy did not improve from 0.93002\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0963 - accuracy: 0.9797 - val_loss: 0.3355 - val_accuracy: 0.9289\n",
      "Epoch 60/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9815\n",
      "Epoch 00060: val_accuracy improved from 0.93002 to 0.93075, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0947 - accuracy: 0.9815 - val_loss: 0.3341 - val_accuracy: 0.9307\n",
      "Epoch 61/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9804\n",
      "Epoch 00061: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0957 - accuracy: 0.9804 - val_loss: 0.3393 - val_accuracy: 0.9239\n",
      "Epoch 62/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9819\n",
      "Epoch 00062: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0955 - accuracy: 0.9819 - val_loss: 0.3402 - val_accuracy: 0.9264\n",
      "Epoch 63/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9829\n",
      "Epoch 00063: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0890 - accuracy: 0.9829 - val_loss: 0.3525 - val_accuracy: 0.9235\n",
      "Epoch 64/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9833\n",
      "Epoch 00064: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0888 - accuracy: 0.9832 - val_loss: 0.3429 - val_accuracy: 0.9282\n",
      "Epoch 65/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9811\n",
      "Epoch 00065: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.9811 - val_loss: 0.3482 - val_accuracy: 0.9289\n",
      "Epoch 66/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9829\n",
      "Epoch 00066: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0894 - accuracy: 0.9831 - val_loss: 0.3473 - val_accuracy: 0.9300\n",
      "Epoch 67/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9833\n",
      "Epoch 00067: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0873 - accuracy: 0.9833 - val_loss: 0.3491 - val_accuracy: 0.9246\n",
      "Epoch 68/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9822\n",
      "Epoch 00068: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9821 - val_loss: 0.3441 - val_accuracy: 0.9206\n",
      "Epoch 69/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9837\n",
      "Epoch 00069: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9836 - val_loss: 0.3427 - val_accuracy: 0.9282\n",
      "Epoch 70/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0879 - accuracy: 0.9817\n",
      "Epoch 00070: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0879 - accuracy: 0.9818 - val_loss: 0.3459 - val_accuracy: 0.9249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9827\n",
      "Epoch 00071: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9826 - val_loss: 0.3539 - val_accuracy: 0.9224\n",
      "Epoch 72/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0825 - accuracy: 0.9845\n",
      "Epoch 00072: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0827 - accuracy: 0.9844 - val_loss: 0.3405 - val_accuracy: 0.9253\n",
      "Epoch 73/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9831\n",
      "Epoch 00073: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9833 - val_loss: 0.3510 - val_accuracy: 0.9228\n",
      "Epoch 74/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9861\n",
      "Epoch 00074: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9861 - val_loss: 0.3532 - val_accuracy: 0.9242\n",
      "Epoch 75/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9853\n",
      "Epoch 00075: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0806 - accuracy: 0.9853 - val_loss: 0.3555 - val_accuracy: 0.9177\n",
      "Epoch 76/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9855\n",
      "Epoch 00076: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0799 - accuracy: 0.9855 - val_loss: 0.3270 - val_accuracy: 0.9307\n",
      "Epoch 77/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9840\n",
      "Epoch 00077: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.9839 - val_loss: 0.3306 - val_accuracy: 0.9286\n",
      "Epoch 78/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9844\n",
      "Epoch 00078: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0825 - accuracy: 0.9843 - val_loss: 0.3333 - val_accuracy: 0.9257\n",
      "Epoch 79/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9826\n",
      "Epoch 00079: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0849 - accuracy: 0.9826 - val_loss: 0.3343 - val_accuracy: 0.9268\n",
      "Epoch 80/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9833\n",
      "Epoch 00080: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0841 - accuracy: 0.9833 - val_loss: 0.3553 - val_accuracy: 0.9184\n",
      "Epoch 81/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9835\n",
      "Epoch 00081: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9836 - val_loss: 0.3541 - val_accuracy: 0.9271\n",
      "Epoch 82/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9853\n",
      "Epoch 00082: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0793 - accuracy: 0.9854 - val_loss: 0.3511 - val_accuracy: 0.9278\n",
      "Epoch 83/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9838\n",
      "Epoch 00083: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0829 - accuracy: 0.9837 - val_loss: 0.3403 - val_accuracy: 0.9304\n",
      "Epoch 84/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9842\n",
      "Epoch 00084: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0795 - accuracy: 0.9842 - val_loss: 0.3652 - val_accuracy: 0.9282\n",
      "Epoch 85/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9851\n",
      "Epoch 00085: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0764 - accuracy: 0.9851 - val_loss: 0.3349 - val_accuracy: 0.9271\n",
      "Epoch 86/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9854\n",
      "Epoch 00086: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9855 - val_loss: 0.3445 - val_accuracy: 0.9257\n",
      "Epoch 87/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9849\n",
      "Epoch 00087: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9848 - val_loss: 0.3363 - val_accuracy: 0.9282\n",
      "Epoch 88/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9861\n",
      "Epoch 00088: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0743 - accuracy: 0.9861 - val_loss: 0.3452 - val_accuracy: 0.9282\n",
      "Epoch 89/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9865\n",
      "Epoch 00089: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0748 - accuracy: 0.9865 - val_loss: 0.3481 - val_accuracy: 0.9282\n",
      "Epoch 90/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9857\n",
      "Epoch 00090: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0748 - accuracy: 0.9856 - val_loss: 0.3448 - val_accuracy: 0.9257\n",
      "Epoch 91/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9872\n",
      "Epoch 00091: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0730 - accuracy: 0.9871 - val_loss: 0.3628 - val_accuracy: 0.9224\n",
      "Epoch 92/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9868\n",
      "Epoch 00092: val_accuracy did not improve from 0.93075\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9868 - val_loss: 0.3575 - val_accuracy: 0.9260\n",
      "Epoch 93/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9855\n",
      "Epoch 00093: val_accuracy improved from 0.93075 to 0.93328, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0756 - accuracy: 0.9857 - val_loss: 0.3569 - val_accuracy: 0.9333\n",
      "Epoch 94/150\n",
      "761/776 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9867\n",
      "Epoch 00094: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0708 - accuracy: 0.9865 - val_loss: 0.3403 - val_accuracy: 0.9257\n",
      "Epoch 95/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9861\n",
      "Epoch 00095: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9859 - val_loss: 0.3371 - val_accuracy: 0.9286\n",
      "Epoch 96/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9869\n",
      "Epoch 00096: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9869 - val_loss: 0.3527 - val_accuracy: 0.9304\n",
      "Epoch 97/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9855\n",
      "Epoch 00097: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9854 - val_loss: 0.3449 - val_accuracy: 0.9275\n",
      "Epoch 98/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9842\n",
      "Epoch 00098: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0754 - accuracy: 0.9840 - val_loss: 0.3672 - val_accuracy: 0.9304\n",
      "Epoch 99/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9873\n",
      "Epoch 00099: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9873 - val_loss: 0.3597 - val_accuracy: 0.9268\n",
      "Epoch 100/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9868\n",
      "Epoch 00100: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0693 - accuracy: 0.9868 - val_loss: 0.3478 - val_accuracy: 0.9253\n",
      "Epoch 101/150\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9858\n",
      "Epoch 00101: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9858 - val_loss: 0.3421 - val_accuracy: 0.9253\n",
      "Epoch 102/150\n",
      "772/776 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9864\n",
      "Epoch 00102: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9864 - val_loss: 0.3447 - val_accuracy: 0.9264\n",
      "Epoch 103/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9857\n",
      "Epoch 00103: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0745 - accuracy: 0.9853 - val_loss: 0.3422 - val_accuracy: 0.9300\n",
      "Epoch 104/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9869\n",
      "Epoch 00104: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9869 - val_loss: 0.3452 - val_accuracy: 0.9264\n",
      "Epoch 105/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9871\n",
      "Epoch 00105: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9871 - val_loss: 0.3433 - val_accuracy: 0.9300\n",
      "Epoch 106/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9877\n",
      "Epoch 00106: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0664 - accuracy: 0.9877 - val_loss: 0.3678 - val_accuracy: 0.9271\n",
      "Epoch 107/150\n",
      "757/776 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9868\n",
      "Epoch 00107: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0700 - accuracy: 0.9868 - val_loss: 0.3682 - val_accuracy: 0.9322\n",
      "Epoch 108/150\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9861\n",
      "Epoch 00108: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0702 - accuracy: 0.9861 - val_loss: 0.3562 - val_accuracy: 0.9275\n",
      "Epoch 109/150\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9878\n",
      "Epoch 00109: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9877 - val_loss: 0.3441 - val_accuracy: 0.9253\n",
      "Epoch 110/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9880\n",
      "Epoch 00110: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0650 - accuracy: 0.9880 - val_loss: 0.3485 - val_accuracy: 0.9260\n",
      "Epoch 111/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9876\n",
      "Epoch 00111: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0670 - accuracy: 0.9876 - val_loss: 0.3420 - val_accuracy: 0.9246\n",
      "Epoch 112/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9897\n",
      "Epoch 00112: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9898 - val_loss: 0.3632 - val_accuracy: 0.9286\n",
      "Epoch 113/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9882\n",
      "Epoch 00113: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9882 - val_loss: 0.3746 - val_accuracy: 0.9220\n",
      "Epoch 114/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9894\n",
      "Epoch 00114: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0642 - accuracy: 0.9895 - val_loss: 0.3453 - val_accuracy: 0.9286\n",
      "Epoch 115/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9881\n",
      "Epoch 00115: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0658 - accuracy: 0.9881 - val_loss: 0.3519 - val_accuracy: 0.9311\n",
      "Epoch 116/150\n",
      "758/776 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9878\n",
      "Epoch 00116: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0665 - accuracy: 0.9879 - val_loss: 0.3450 - val_accuracy: 0.9275\n",
      "Epoch 117/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9885\n",
      "Epoch 00117: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0641 - accuracy: 0.9885 - val_loss: 0.3615 - val_accuracy: 0.9242\n",
      "Epoch 118/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9870\n",
      "Epoch 00118: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0664 - accuracy: 0.9871 - val_loss: 0.3519 - val_accuracy: 0.9293\n",
      "Epoch 119/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9879\n",
      "Epoch 00119: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9880 - val_loss: 0.3564 - val_accuracy: 0.9300\n",
      "Epoch 120/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9894\n",
      "Epoch 00120: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0603 - accuracy: 0.9895 - val_loss: 0.3548 - val_accuracy: 0.9322\n",
      "Epoch 121/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9895\n",
      "Epoch 00121: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0606 - accuracy: 0.9895 - val_loss: 0.3694 - val_accuracy: 0.9278\n",
      "Epoch 122/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9885\n",
      "Epoch 00122: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0645 - accuracy: 0.9885 - val_loss: 0.3523 - val_accuracy: 0.9275\n",
      "Epoch 123/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9895\n",
      "Epoch 00123: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0610 - accuracy: 0.9894 - val_loss: 0.3498 - val_accuracy: 0.9311\n",
      "Epoch 124/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9888\n",
      "Epoch 00124: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0632 - accuracy: 0.9888 - val_loss: 0.3383 - val_accuracy: 0.9293\n",
      "Epoch 125/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9873\n",
      "Epoch 00125: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9872 - val_loss: 0.3616 - val_accuracy: 0.9242\n",
      "Epoch 126/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9897\n",
      "Epoch 00126: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9898 - val_loss: 0.3491 - val_accuracy: 0.9307\n",
      "Epoch 127/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9889\n",
      "Epoch 00127: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0622 - accuracy: 0.9889 - val_loss: 0.3557 - val_accuracy: 0.9318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0581 - accuracy: 0.9899\n",
      "Epoch 00128: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0581 - accuracy: 0.9899 - val_loss: 0.3593 - val_accuracy: 0.9300\n",
      "Epoch 129/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9880\n",
      "Epoch 00129: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0631 - accuracy: 0.9880 - val_loss: 0.3571 - val_accuracy: 0.9278\n",
      "Epoch 130/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9878\n",
      "Epoch 00130: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9878 - val_loss: 0.3759 - val_accuracy: 0.9275\n",
      "Epoch 131/150\n",
      "765/776 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9868\n",
      "Epoch 00131: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0665 - accuracy: 0.9868 - val_loss: 0.3511 - val_accuracy: 0.9307\n",
      "Epoch 132/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9880\n",
      "Epoch 00132: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0621 - accuracy: 0.9880 - val_loss: 0.3535 - val_accuracy: 0.9326\n",
      "Epoch 133/150\n",
      "759/776 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9883\n",
      "Epoch 00133: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0603 - accuracy: 0.9883 - val_loss: 0.3677 - val_accuracy: 0.9318\n",
      "Epoch 134/150\n",
      "774/776 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9881\n",
      "Epoch 00134: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9882 - val_loss: 0.3405 - val_accuracy: 0.9278\n",
      "Epoch 135/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9889\n",
      "Epoch 00135: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0621 - accuracy: 0.9890 - val_loss: 0.3438 - val_accuracy: 0.9300\n",
      "Epoch 136/150\n",
      "771/776 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9889\n",
      "Epoch 00136: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9890 - val_loss: 0.3494 - val_accuracy: 0.9242\n",
      "Epoch 137/150\n",
      "766/776 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9892\n",
      "Epoch 00137: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0626 - accuracy: 0.9891 - val_loss: 0.3509 - val_accuracy: 0.9304\n",
      "Epoch 138/150\n",
      "763/776 [============================>.] - ETA: 0s - loss: 0.0546 - accuracy: 0.9915\n",
      "Epoch 00138: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0547 - accuracy: 0.9915 - val_loss: 0.3459 - val_accuracy: 0.9293\n",
      "Epoch 139/150\n",
      "768/776 [============================>.] - ETA: 0s - loss: 0.0603 - accuracy: 0.9886\n",
      "Epoch 00139: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0602 - accuracy: 0.9886 - val_loss: 0.3654 - val_accuracy: 0.9307\n",
      "Epoch 140/150\n",
      "762/776 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9893\n",
      "Epoch 00140: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0608 - accuracy: 0.9893 - val_loss: 0.3556 - val_accuracy: 0.9311\n",
      "Epoch 141/150\n",
      "776/776 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9904\n",
      "Epoch 00141: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0572 - accuracy: 0.9904 - val_loss: 0.3507 - val_accuracy: 0.9318\n",
      "Epoch 142/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0612 - accuracy: 0.9888\n",
      "Epoch 00142: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0611 - accuracy: 0.9888 - val_loss: 0.3451 - val_accuracy: 0.9333\n",
      "Epoch 143/150\n",
      "760/776 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9903\n",
      "Epoch 00143: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0572 - accuracy: 0.9904 - val_loss: 0.3652 - val_accuracy: 0.9260\n",
      "Epoch 144/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9881\n",
      "Epoch 00144: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9881 - val_loss: 0.3438 - val_accuracy: 0.9307\n",
      "Epoch 145/150\n",
      "764/776 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9877\n",
      "Epoch 00145: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9877 - val_loss: 0.3628 - val_accuracy: 0.9242\n",
      "Epoch 146/150\n",
      "767/776 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9882\n",
      "Epoch 00146: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9881 - val_loss: 0.3539 - val_accuracy: 0.9300\n",
      "Epoch 147/150\n",
      "769/776 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9886\n",
      "Epoch 00147: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0600 - accuracy: 0.9885 - val_loss: 0.3701 - val_accuracy: 0.9307\n",
      "Epoch 148/150\n",
      "770/776 [============================>.] - ETA: 0s - loss: 0.0592 - accuracy: 0.9886\n",
      "Epoch 00148: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0592 - accuracy: 0.9886 - val_loss: 0.3598 - val_accuracy: 0.9282\n",
      "Epoch 149/150\n",
      "775/776 [============================>.] - ETA: 0s - loss: 0.0580 - accuracy: 0.9900\n",
      "Epoch 00149: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 3s 3ms/step - loss: 0.0581 - accuracy: 0.9900 - val_loss: 0.3863 - val_accuracy: 0.9220\n",
      "Epoch 150/150\n",
      "773/776 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9905\n",
      "Epoch 00150: val_accuracy did not improve from 0.93328\n",
      "776/776 [==============================] - 2s 3ms/step - loss: 0.0576 - accuracy: 0.9904 - val_loss: 0.3650 - val_accuracy: 0.9307\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:38<00:00, 259.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.81      0.90      0.85       721\n",
      "        car_horn       0.87      0.76      0.81       231\n",
      "children_playing       0.62      0.82      0.71       700\n",
      "        dog_bark       0.77      0.74      0.76       700\n",
      "           siren       0.81      0.48      0.60       581\n",
      "\n",
      "        accuracy                           0.75      2933\n",
      "       macro avg       0.78      0.74      0.75      2933\n",
      "    weighted avg       0.76      0.75      0.74      2933\n",
      "\n",
      "Validation fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27503, 375)\n",
      "X_val_norm shape.....:(3003, 375)\n",
      "\n",
      "Sum of elements: 0.9802557517920008\n",
      "Number of elements summed: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 233)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               175500    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 288,299\n",
      "Trainable params: 288,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24752, 233)\n",
      "Epoch 1/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.7973 - accuracy: 0.7088\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.81716, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.7896 - accuracy: 0.7121 - val_loss: 0.4991 - val_accuracy: 0.8172\n",
      "Epoch 2/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.4473 - accuracy: 0.8391\n",
      "Epoch 00002: val_accuracy improved from 0.81716 to 0.85351, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 5s 7ms/step - loss: 0.4472 - accuracy: 0.8391 - val_loss: 0.4042 - val_accuracy: 0.8535\n",
      "Epoch 3/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.3492 - accuracy: 0.8751\n",
      "Epoch 00003: val_accuracy improved from 0.85351 to 0.87350, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 8s 10ms/step - loss: 0.3490 - accuracy: 0.8752 - val_loss: 0.3486 - val_accuracy: 0.8735\n",
      "Epoch 4/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.8997\n",
      "Epoch 00004: val_accuracy improved from 0.87350 to 0.88695, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.2890 - accuracy: 0.8996 - val_loss: 0.3132 - val_accuracy: 0.8870\n",
      "Epoch 5/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.2412 - accuracy: 0.9159\n",
      "Epoch 00005: val_accuracy improved from 0.88695 to 0.89531, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.2415 - accuracy: 0.9156 - val_loss: 0.2885 - val_accuracy: 0.8953\n",
      "Epoch 6/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.2051 - accuracy: 0.9292\n",
      "Epoch 00006: val_accuracy improved from 0.89531 to 0.90185, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.2037 - accuracy: 0.9295 - val_loss: 0.2698 - val_accuracy: 0.9019\n",
      "Epoch 7/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1715 - accuracy: 0.9416\n",
      "Epoch 00007: val_accuracy improved from 0.90185 to 0.90840, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.1713 - accuracy: 0.9416 - val_loss: 0.2533 - val_accuracy: 0.9084\n",
      "Epoch 8/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1511 - accuracy: 0.9477\n",
      "Epoch 00008: val_accuracy improved from 0.90840 to 0.91712, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1507 - accuracy: 0.9478 - val_loss: 0.2369 - val_accuracy: 0.9171\n",
      "Epoch 9/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9578\n",
      "Epoch 00009: val_accuracy improved from 0.91712 to 0.92185, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1266 - accuracy: 0.9576 - val_loss: 0.2312 - val_accuracy: 0.9218\n",
      "Epoch 10/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9647\n",
      "Epoch 00010: val_accuracy improved from 0.92185 to 0.92621, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.1086 - accuracy: 0.9646 - val_loss: 0.2239 - val_accuracy: 0.9262\n",
      "Epoch 11/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0895 - accuracy: 0.9705\n",
      "Epoch 00011: val_accuracy improved from 0.92621 to 0.92948, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0896 - accuracy: 0.9704 - val_loss: 0.2196 - val_accuracy: 0.9295\n",
      "Epoch 12/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9744\n",
      "Epoch 00012: val_accuracy did not improve from 0.92948\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0791 - accuracy: 0.9744 - val_loss: 0.2187 - val_accuracy: 0.9295\n",
      "Epoch 13/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9796\n",
      "Epoch 00013: val_accuracy did not improve from 0.92948\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0640 - accuracy: 0.9796 - val_loss: 0.2290 - val_accuracy: 0.9277\n",
      "Epoch 14/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0557 - accuracy: 0.9823\n",
      "Epoch 00014: val_accuracy improved from 0.92948 to 0.93021, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0558 - accuracy: 0.9822 - val_loss: 0.2189 - val_accuracy: 0.9302\n",
      "Epoch 15/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9860\n",
      "Epoch 00015: val_accuracy improved from 0.93021 to 0.93384, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0482 - accuracy: 0.9861 - val_loss: 0.2190 - val_accuracy: 0.9338\n",
      "Epoch 16/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0405 - accuracy: 0.9883\n",
      "Epoch 00016: val_accuracy did not improve from 0.93384\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0404 - accuracy: 0.9884 - val_loss: 0.2323 - val_accuracy: 0.9309\n",
      "Epoch 17/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.9885\n",
      "Epoch 00017: val_accuracy improved from 0.93384 to 0.93566, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0368 - accuracy: 0.9884 - val_loss: 0.2186 - val_accuracy: 0.9357\n",
      "Epoch 18/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9910\n",
      "Epoch 00018: val_accuracy improved from 0.93566 to 0.93748, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0314 - accuracy: 0.9909 - val_loss: 0.2263 - val_accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0273 - accuracy: 0.9920\n",
      "Epoch 00019: val_accuracy improved from 0.93748 to 0.93893, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0275 - accuracy: 0.9918 - val_loss: 0.2233 - val_accuracy: 0.9389\n",
      "Epoch 20/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9934\n",
      "Epoch 00020: val_accuracy did not improve from 0.93893\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0228 - accuracy: 0.9934 - val_loss: 0.2408 - val_accuracy: 0.9375\n",
      "Epoch 21/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9945\n",
      "Epoch 00021: val_accuracy improved from 0.93893 to 0.94111, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0199 - accuracy: 0.9945 - val_loss: 0.2326 - val_accuracy: 0.9411\n",
      "Epoch 22/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9958\n",
      "Epoch 00022: val_accuracy did not improve from 0.94111\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0182 - accuracy: 0.9958 - val_loss: 0.2358 - val_accuracy: 0.9404\n",
      "Epoch 23/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9968\n",
      "Epoch 00023: val_accuracy did not improve from 0.94111\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0149 - accuracy: 0.9968 - val_loss: 0.2406 - val_accuracy: 0.9404\n",
      "Epoch 24/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9962\n",
      "Epoch 00024: val_accuracy did not improve from 0.94111\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0154 - accuracy: 0.9962 - val_loss: 0.2509 - val_accuracy: 0.9397\n",
      "Epoch 25/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9968\n",
      "Epoch 00025: val_accuracy improved from 0.94111 to 0.94475, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0135 - accuracy: 0.9967 - val_loss: 0.2359 - val_accuracy: 0.9447\n",
      "Epoch 26/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0115 - accuracy: 0.9975\n",
      "Epoch 00026: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0115 - accuracy: 0.9975 - val_loss: 0.2456 - val_accuracy: 0.9415\n",
      "Epoch 27/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9973\n",
      "Epoch 00027: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0106 - accuracy: 0.9973 - val_loss: 0.2398 - val_accuracy: 0.9422\n",
      "Epoch 28/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9979\n",
      "Epoch 00028: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0100 - accuracy: 0.9979 - val_loss: 0.2495 - val_accuracy: 0.9411\n",
      "Epoch 29/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0090 - accuracy: 0.9982\n",
      "Epoch 00029: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9981 - val_loss: 0.2532 - val_accuracy: 0.9404\n",
      "Epoch 30/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 0.0088 - accuracy: 0.9978\n",
      "Epoch 00030: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 0.2448 - val_accuracy: 0.9422\n",
      "Epoch 31/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9988\n",
      "Epoch 00031: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.2534 - val_accuracy: 0.9418\n",
      "Epoch 32/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9982\n",
      "Epoch 00032: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9983 - val_loss: 0.2595 - val_accuracy: 0.9422\n",
      "Epoch 33/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n",
      "Epoch 00033: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.2884 - val_accuracy: 0.9411\n",
      "Epoch 34/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9985\n",
      "Epoch 00034: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0074 - accuracy: 0.9984 - val_loss: 0.2694 - val_accuracy: 0.9411\n",
      "Epoch 35/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9990\n",
      "Epoch 00035: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.2661 - val_accuracy: 0.9418\n",
      "Epoch 36/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9989\n",
      "Epoch 00036: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.2716 - val_accuracy: 0.9389\n",
      "Epoch 37/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9989\n",
      "Epoch 00037: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.2659 - val_accuracy: 0.9444\n",
      "Epoch 38/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9988\n",
      "Epoch 00038: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.2705 - val_accuracy: 0.9444\n",
      "Epoch 39/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9991\n",
      "Epoch 00039: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.2776 - val_accuracy: 0.9415\n",
      "Epoch 40/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9993\n",
      "Epoch 00040: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 0.2965 - val_accuracy: 0.9382\n",
      "Epoch 41/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 00041: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2839 - val_accuracy: 0.9389\n",
      "Epoch 42/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9996\n",
      "Epoch 00042: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.2806 - val_accuracy: 0.9418\n",
      "Epoch 43/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 00043: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.2891 - val_accuracy: 0.9444\n",
      "Epoch 44/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00044: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2723 - val_accuracy: 0.9437\n",
      "Epoch 45/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 00045: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.2805 - val_accuracy: 0.9415\n",
      "Epoch 46/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9997\n",
      "Epoch 00046: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.2743 - val_accuracy: 0.9433\n",
      "Epoch 47/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9995\n",
      "Epoch 00047: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.2828 - val_accuracy: 0.9433\n",
      "Epoch 48/350\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00048: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2916 - val_accuracy: 0.9433\n",
      "Epoch 49/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9995\n",
      "Epoch 00049: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.2880 - val_accuracy: 0.9429\n",
      "Epoch 50/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 00050: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.3037 - val_accuracy: 0.9426\n",
      "Epoch 51/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9996\n",
      "Epoch 00051: val_accuracy did not improve from 0.94475\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.2913 - val_accuracy: 0.9415\n",
      "Epoch 52/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9998\n",
      "Epoch 00052: val_accuracy improved from 0.94475 to 0.94547, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.2946 - val_accuracy: 0.9455\n",
      "Epoch 53/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00053: val_accuracy did not improve from 0.94547\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2957 - val_accuracy: 0.9444\n",
      "Epoch 54/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00054: val_accuracy did not improve from 0.94547\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2930 - val_accuracy: 0.9451\n",
      "Epoch 55/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy improved from 0.94547 to 0.94620, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2939 - val_accuracy: 0.9462\n",
      "Epoch 56/350\n",
      "753/774 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9995\n",
      "Epoch 00056: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.2911 - val_accuracy: 0.9447\n",
      "Epoch 57/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0019 - accuracy: 0.9998\n",
      "Epoch 00057: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.2968 - val_accuracy: 0.9440\n",
      "Epoch 58/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00058: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2935 - val_accuracy: 0.9458\n",
      "Epoch 59/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2992 - val_accuracy: 0.9458\n",
      "Epoch 60/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 00060: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2987 - val_accuracy: 0.9447\n",
      "Epoch 61/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00061: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.3022 - val_accuracy: 0.9444\n",
      "Epoch 62/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 00062: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.3034 - val_accuracy: 0.9455\n",
      "Epoch 63/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 00063: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.3061 - val_accuracy: 0.9451\n",
      "Epoch 64/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 00064: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.3017 - val_accuracy: 0.9447\n",
      "Epoch 65/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00065: val_accuracy did not improve from 0.94620\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.3126 - val_accuracy: 0.9451\n",
      "Epoch 66/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00066: val_accuracy improved from 0.94620 to 0.94766, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.3009 - val_accuracy: 0.9477\n",
      "Epoch 67/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00067: val_accuracy did not improve from 0.94766\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2975 - val_accuracy: 0.9469\n",
      "Epoch 68/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00068: val_accuracy improved from 0.94766 to 0.94838, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2984 - val_accuracy: 0.9484\n",
      "Epoch 69/350\n",
      "742/774 [===========================>..] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00069: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.3029 - val_accuracy: 0.9473\n",
      "Epoch 70/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00070: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3024 - val_accuracy: 0.9469\n",
      "Epoch 71/350\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.3007 - val_accuracy: 0.9466\n",
      "Epoch 72/350\n",
      "748/774 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00072: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.2996 - val_accuracy: 0.9477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 9.7374e-04 - accuracy: 0.9999\n",
      "Epoch 00073: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.7358e-04 - accuracy: 0.9999 - val_loss: 0.2967 - val_accuracy: 0.9466\n",
      "Epoch 74/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999  \n",
      "Epoch 00074: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3028 - val_accuracy: 0.9447\n",
      "Epoch 75/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00075: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3028 - val_accuracy: 0.9458\n",
      "Epoch 76/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00076: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3128 - val_accuracy: 0.9462\n",
      "Epoch 77/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00077: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3125 - val_accuracy: 0.9469\n",
      "Epoch 78/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.3108 - val_accuracy: 0.9477\n",
      "Epoch 79/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 9.9745e-04 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3066 - val_accuracy: 0.9466\n",
      "Epoch 80/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00080: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.3174 - val_accuracy: 0.9451\n",
      "Epoch 81/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00081: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3169 - val_accuracy: 0.9455\n",
      "Epoch 82/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 8.4576e-04 - accuracy: 0.9999\n",
      "Epoch 00082: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.4772e-04 - accuracy: 0.9999 - val_loss: 0.3157 - val_accuracy: 0.9477\n",
      "Epoch 83/350\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00083: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3120 - val_accuracy: 0.9480\n",
      "Epoch 84/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00084: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3170 - val_accuracy: 0.9466\n",
      "Epoch 85/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00085: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3188 - val_accuracy: 0.9462\n",
      "Epoch 86/350\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998  \n",
      "Epoch 00086: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3306 - val_accuracy: 0.9462\n",
      "Epoch 87/350\n",
      "745/774 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3186 - val_accuracy: 0.9480\n",
      "Epoch 88/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3179 - val_accuracy: 0.9458\n",
      "Epoch 89/350\n",
      "741/774 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 00089: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.3214 - val_accuracy: 0.9484\n",
      "Epoch 90/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998    \n",
      "Epoch 00090: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3237 - val_accuracy: 0.9451\n",
      "Epoch 91/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3152 - val_accuracy: 0.9462\n",
      "Epoch 92/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00092: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3169 - val_accuracy: 0.9477\n",
      "Epoch 93/350\n",
      "768/774 [============================>.] - ETA: 0s - loss: 9.4696e-04 - accuracy: 0.9997\n",
      "Epoch 00093: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.4317e-04 - accuracy: 0.9997 - val_loss: 0.3217 - val_accuracy: 0.9469\n",
      "Epoch 94/350\n",
      "766/774 [============================>.] - ETA: 0s - loss: 8.4531e-04 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.4114e-04 - accuracy: 0.9999 - val_loss: 0.3276 - val_accuracy: 0.9477\n",
      "Epoch 95/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 6.8297e-04 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8117e-04 - accuracy: 1.0000 - val_loss: 0.3222 - val_accuracy: 0.9480\n",
      "Epoch 96/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 9.5361e-04 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.6033e-04 - accuracy: 0.9998 - val_loss: 0.3232 - val_accuracy: 0.9477\n",
      "Epoch 97/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 8.8482e-04 - accuracy: 0.9999\n",
      "Epoch 00097: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 2s 2ms/step - loss: 8.7870e-04 - accuracy: 0.9999 - val_loss: 0.3258 - val_accuracy: 0.9469\n",
      "Epoch 98/350\n",
      "772/774 [============================>.] - ETA: 0s - loss: 7.8217e-04 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.8077e-04 - accuracy: 0.9998 - val_loss: 0.3284 - val_accuracy: 0.9477\n",
      "Epoch 99/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3212 - val_accuracy: 0.9477\n",
      "Epoch 100/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 7.7687e-04 - accuracy: 0.9999\n",
      "Epoch 00100: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.7499e-04 - accuracy: 0.9999 - val_loss: 0.3285 - val_accuracy: 0.9480\n",
      "Epoch 101/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 7.8357e-04 - accuracy: 0.9998\n",
      "Epoch 00101: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.6161e-04 - accuracy: 0.9998 - val_loss: 0.3281 - val_accuracy: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 8.4906e-04 - accuracy: 0.9999\n",
      "Epoch 00102: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.3986e-04 - accuracy: 0.9999 - val_loss: 0.3253 - val_accuracy: 0.9473\n",
      "Epoch 103/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3363 - val_accuracy: 0.9462\n",
      "Epoch 104/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00104: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3290 - val_accuracy: 0.9480\n",
      "Epoch 105/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 7.4116e-04 - accuracy: 0.9998\n",
      "Epoch 00105: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.5572e-04 - accuracy: 0.9998 - val_loss: 0.3390 - val_accuracy: 0.9466\n",
      "Epoch 106/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 8.2200e-04 - accuracy: 0.9999\n",
      "Epoch 00106: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.1486e-04 - accuracy: 0.9999 - val_loss: 0.3305 - val_accuracy: 0.9480\n",
      "Epoch 107/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 7.9671e-04 - accuracy: 0.9999\n",
      "Epoch 00107: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.1916e-04 - accuracy: 0.9999 - val_loss: 0.3321 - val_accuracy: 0.9480\n",
      "Epoch 108/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 7.7902e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.94838\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.7615e-04 - accuracy: 0.9999 - val_loss: 0.3279 - val_accuracy: 0.9480\n",
      "Epoch 109/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 8.0860e-04 - accuracy: 0.9998\n",
      "Epoch 00109: val_accuracy improved from 0.94838 to 0.94911, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.0618e-04 - accuracy: 0.9998 - val_loss: 0.3259 - val_accuracy: 0.9491\n",
      "Epoch 110/350\n",
      "752/774 [============================>.] - ETA: 0s - loss: 7.0060e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8937e-04 - accuracy: 0.9999 - val_loss: 0.3264 - val_accuracy: 0.9487\n",
      "Epoch 111/350\n",
      "761/774 [============================>.] - ETA: 0s - loss: 6.2394e-04 - accuracy: 0.9998\n",
      "Epoch 00111: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.2009e-04 - accuracy: 0.9998 - val_loss: 0.3342 - val_accuracy: 0.9462\n",
      "Epoch 112/350\n",
      "746/774 [===========================>..] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 9.9926e-04 - accuracy: 0.9998 - val_loss: 0.3379 - val_accuracy: 0.9480\n",
      "Epoch 113/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 7.0525e-04 - accuracy: 0.9998\n",
      "Epoch 00113: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.0280e-04 - accuracy: 0.9998 - val_loss: 0.3386 - val_accuracy: 0.9491\n",
      "Epoch 114/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 6.9352e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.7853e-04 - accuracy: 1.0000 - val_loss: 0.3382 - val_accuracy: 0.9477\n",
      "Epoch 115/350\n",
      "758/774 [============================>.] - ETA: 0s - loss: 8.3131e-04 - accuracy: 0.9998\n",
      "Epoch 00115: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.1833e-04 - accuracy: 0.9998 - val_loss: 0.3344 - val_accuracy: 0.9473\n",
      "Epoch 116/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 4.8985e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5264e-04 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.9477\n",
      "Epoch 117/350\n",
      "754/774 [============================>.] - ETA: 0s - loss: 7.4091e-04 - accuracy: 0.9998\n",
      "Epoch 00117: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.3433e-04 - accuracy: 0.9998 - val_loss: 0.3386 - val_accuracy: 0.9480\n",
      "Epoch 118/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 6.8442e-04 - accuracy: 0.9998\n",
      "Epoch 00118: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8349e-04 - accuracy: 0.9998 - val_loss: 0.3408 - val_accuracy: 0.9473\n",
      "Epoch 119/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 6.7650e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.6815e-04 - accuracy: 0.9999 - val_loss: 0.3384 - val_accuracy: 0.9455\n",
      "Epoch 120/350\n",
      "756/774 [============================>.] - ETA: 0s - loss: 4.8300e-04 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7888e-04 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.9469\n",
      "Epoch 121/350\n",
      "743/774 [===========================>..] - ETA: 0s - loss: 6.8129e-04 - accuracy: 0.9998\n",
      "Epoch 00121: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.7298e-04 - accuracy: 0.9998 - val_loss: 0.3438 - val_accuracy: 0.9484\n",
      "Epoch 122/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 4.7579e-04 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7579e-04 - accuracy: 1.0000 - val_loss: 0.3408 - val_accuracy: 0.9491\n",
      "Epoch 123/350\n",
      "771/774 [============================>.] - ETA: 0s - loss: 8.4642e-04 - accuracy: 0.9998\n",
      "Epoch 00123: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 8.4406e-04 - accuracy: 0.9998 - val_loss: 0.3408 - val_accuracy: 0.9466\n",
      "Epoch 124/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 6.2533e-04 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.2312e-04 - accuracy: 1.0000 - val_loss: 0.3524 - val_accuracy: 0.9466\n",
      "Epoch 125/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 5.5665e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.4897e-04 - accuracy: 0.9999 - val_loss: 0.3451 - val_accuracy: 0.9466\n",
      "Epoch 126/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 7.0781e-04 - accuracy: 0.9998\n",
      "Epoch 00126: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.0606e-04 - accuracy: 0.9998 - val_loss: 0.3299 - val_accuracy: 0.9466\n",
      "Epoch 127/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 5.0050e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.9986e-04 - accuracy: 0.9999 - val_loss: 0.3353 - val_accuracy: 0.9484\n",
      "Epoch 128/350\n",
      "764/774 [============================>.] - ETA: 0s - loss: 5.2962e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.2796e-04 - accuracy: 0.9999 - val_loss: 0.3328 - val_accuracy: 0.9462\n",
      "Epoch 129/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 4.5882e-04 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.5722e-04 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/350\n",
      "747/774 [===========================>..] - ETA: 0s - loss: 5.0176e-04 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.2114e-04 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.9473\n",
      "Epoch 131/350\n",
      "757/774 [============================>.] - ETA: 0s - loss: 5.4787e-04 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.4023e-04 - accuracy: 1.0000 - val_loss: 0.3374 - val_accuracy: 0.9469\n",
      "Epoch 132/350\n",
      "767/774 [============================>.] - ETA: 0s - loss: 6.8644e-04 - accuracy: 0.9998\n",
      "Epoch 00132: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.8093e-04 - accuracy: 0.9998 - val_loss: 0.3379 - val_accuracy: 0.9462\n",
      "Epoch 133/350\n",
      "744/774 [===========================>..] - ETA: 0s - loss: 5.4224e-04 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.3574e-04 - accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.9469\n",
      "Epoch 134/350\n",
      "759/774 [============================>.] - ETA: 0s - loss: 6.5291e-04 - accuracy: 0.9999\n",
      "Epoch 00134: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.4534e-04 - accuracy: 0.9999 - val_loss: 0.3384 - val_accuracy: 0.9477\n",
      "Epoch 135/350\n",
      "774/774 [==============================] - ETA: 0s - loss: 4.1053e-04 - accuracy: 1.0000\n",
      "Epoch 00135: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.1053e-04 - accuracy: 1.0000 - val_loss: 0.3321 - val_accuracy: 0.9469\n",
      "Epoch 136/350\n",
      "749/774 [============================>.] - ETA: 0s - loss: 4.6666e-04 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.5950e-04 - accuracy: 1.0000 - val_loss: 0.3409 - val_accuracy: 0.9458\n",
      "Epoch 137/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 4.7896e-04 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 4.7021e-04 - accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9473\n",
      "Epoch 138/350\n",
      "755/774 [============================>.] - ETA: 0s - loss: 6.3898e-04 - accuracy: 0.9999\n",
      "Epoch 00138: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.2921e-04 - accuracy: 0.9999 - val_loss: 0.3361 - val_accuracy: 0.9473\n",
      "Epoch 139/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 5.3260e-04 - accuracy: 0.9999\n",
      "Epoch 00139: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.5646e-04 - accuracy: 0.9998 - val_loss: 0.3360 - val_accuracy: 0.9484\n",
      "Epoch 140/350\n",
      "769/774 [============================>.] - ETA: 0s - loss: 7.2416e-04 - accuracy: 0.9998\n",
      "Epoch 00140: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 7.2020e-04 - accuracy: 0.9998 - val_loss: 0.3408 - val_accuracy: 0.9473\n",
      "Epoch 141/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 6.2621e-04 - accuracy: 0.9998\n",
      "Epoch 00141: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.5232e-04 - accuracy: 0.9998 - val_loss: 0.3443 - val_accuracy: 0.9480\n",
      "Epoch 142/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 5.5419e-04 - accuracy: 0.9998\n",
      "Epoch 00142: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.8138e-04 - accuracy: 0.9998 - val_loss: 0.3432 - val_accuracy: 0.9458\n",
      "Epoch 143/350\n",
      "770/774 [============================>.] - ETA: 0s - loss: 5.9109e-04 - accuracy: 0.9998\n",
      "Epoch 00143: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 5.8878e-04 - accuracy: 0.9998 - val_loss: 0.3535 - val_accuracy: 0.9462\n",
      "Epoch 144/350\n",
      "750/774 [============================>.] - ETA: 0s - loss: 6.2635e-04 - accuracy: 0.9998\n",
      "Epoch 00144: val_accuracy did not improve from 0.94911\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.1622e-04 - accuracy: 0.9998 - val_loss: 0.3477 - val_accuracy: 0.9484\n",
      "Epoch 145/350\n",
      "751/774 [============================>.] - ETA: 0s - loss: 6.6610e-04 - accuracy: 0.9999\n",
      "Epoch 00145: val_accuracy did not improve from 0.94911\n",
      "Restoring model weights from the end of the best epoch.\n",
      "774/774 [==============================] - 1s 2ms/step - loss: 6.6290e-04 - accuracy: 0.9999 - val_loss: 0.3447 - val_accuracy: 0.9484\n",
      "Epoch 00145: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:39<03:39, 219.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.70      0.61      0.66       672\n",
      "        car_horn       0.68      0.56      0.62       294\n",
      "children_playing       0.77      0.89      0.83       700\n",
      "        dog_bark       0.79      0.85      0.82       700\n",
      "           siren       0.70      0.68      0.69       637\n",
      "\n",
      "        accuracy                           0.74      3003\n",
      "       macro avg       0.73      0.72      0.72      3003\n",
      "    weighted avg       0.74      0.74      0.74      3003\n",
      "\n",
      "Model: \"Model_CNN_1D_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 227, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 227, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 227, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6328)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                316450    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 327,491\n",
      "Trainable params: 327,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24752, 233, 1)\n",
      "Epoch 1/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.7579 - accuracy: 0.7592\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82552, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.7579 - accuracy: 0.7592 - val_loss: 0.5663 - val_accuracy: 0.8255\n",
      "Epoch 2/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.4928 - accuracy: 0.8492\n",
      "Epoch 00002: val_accuracy improved from 0.82552 to 0.85896, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.4924 - accuracy: 0.8493 - val_loss: 0.4702 - val_accuracy: 0.8590\n",
      "Epoch 3/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.4218 - accuracy: 0.8728\n",
      "Epoch 00003: val_accuracy did not improve from 0.85896\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.4214 - accuracy: 0.8731 - val_loss: 0.4698 - val_accuracy: 0.8582\n",
      "Epoch 4/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.3748 - accuracy: 0.8916\n",
      "Epoch 00004: val_accuracy improved from 0.85896 to 0.87714, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3748 - accuracy: 0.8918 - val_loss: 0.4225 - val_accuracy: 0.8771\n",
      "Epoch 5/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.9041\n",
      "Epoch 00005: val_accuracy improved from 0.87714 to 0.88259, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3399 - accuracy: 0.9041 - val_loss: 0.4083 - val_accuracy: 0.8826\n",
      "Epoch 6/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.3182 - accuracy: 0.9086\n",
      "Epoch 00006: val_accuracy improved from 0.88259 to 0.88477, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.3182 - accuracy: 0.9086 - val_loss: 0.4105 - val_accuracy: 0.8848\n",
      "Epoch 7/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.9213\n",
      "Epoch 00007: val_accuracy improved from 0.88477 to 0.89059, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2900 - accuracy: 0.9213 - val_loss: 0.3979 - val_accuracy: 0.8906\n",
      "Epoch 8/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2683 - accuracy: 0.9260\n",
      "Epoch 00008: val_accuracy improved from 0.89059 to 0.89168, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2685 - accuracy: 0.9259 - val_loss: 0.4073 - val_accuracy: 0.8917\n",
      "Epoch 9/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.9306\n",
      "Epoch 00009: val_accuracy improved from 0.89168 to 0.89531, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2547 - accuracy: 0.9307 - val_loss: 0.3844 - val_accuracy: 0.8953\n",
      "Epoch 10/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.2454 - accuracy: 0.9357\n",
      "Epoch 00010: val_accuracy did not improve from 0.89531\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2454 - accuracy: 0.9359 - val_loss: 0.3781 - val_accuracy: 0.8935\n",
      "Epoch 11/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.2302 - accuracy: 0.9409\n",
      "Epoch 00011: val_accuracy did not improve from 0.89531\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2303 - accuracy: 0.9409 - val_loss: 0.3790 - val_accuracy: 0.8946\n",
      "Epoch 12/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9430\n",
      "Epoch 00012: val_accuracy improved from 0.89531 to 0.89749, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2247 - accuracy: 0.9431 - val_loss: 0.3817 - val_accuracy: 0.8975\n",
      "Epoch 13/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.2123 - accuracy: 0.9457\n",
      "Epoch 00013: val_accuracy improved from 0.89749 to 0.90513, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2136 - accuracy: 0.9453 - val_loss: 0.3692 - val_accuracy: 0.9051\n",
      "Epoch 14/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9480\n",
      "Epoch 00014: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.2070 - accuracy: 0.9480 - val_loss: 0.3760 - val_accuracy: 0.9029\n",
      "Epoch 15/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1950 - accuracy: 0.9500\n",
      "Epoch 00015: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1953 - accuracy: 0.9497 - val_loss: 0.3779 - val_accuracy: 0.9019\n",
      "Epoch 16/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9532\n",
      "Epoch 00016: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1917 - accuracy: 0.9530 - val_loss: 0.3665 - val_accuracy: 0.9015\n",
      "Epoch 17/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.1835 - accuracy: 0.9548\n",
      "Epoch 00017: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1833 - accuracy: 0.9548 - val_loss: 0.3801 - val_accuracy: 0.8997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9561\n",
      "Epoch 00018: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1798 - accuracy: 0.9561 - val_loss: 0.3785 - val_accuracy: 0.9019\n",
      "Epoch 19/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9594\n",
      "Epoch 00019: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1751 - accuracy: 0.9592 - val_loss: 0.3812 - val_accuracy: 0.9040\n",
      "Epoch 20/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.9598\n",
      "Epoch 00020: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1699 - accuracy: 0.9597 - val_loss: 0.4189 - val_accuracy: 0.8982\n",
      "Epoch 21/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.1632 - accuracy: 0.9616\n",
      "Epoch 00021: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1635 - accuracy: 0.9616 - val_loss: 0.3745 - val_accuracy: 0.9029\n",
      "Epoch 22/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9630\n",
      "Epoch 00022: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1578 - accuracy: 0.9629 - val_loss: 0.3908 - val_accuracy: 0.8997\n",
      "Epoch 23/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9621\n",
      "Epoch 00023: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1599 - accuracy: 0.9622 - val_loss: 0.3714 - val_accuracy: 0.9037\n",
      "Epoch 24/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9646\n",
      "Epoch 00024: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1500 - accuracy: 0.9649 - val_loss: 0.3921 - val_accuracy: 0.9000\n",
      "Epoch 25/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.1494 - accuracy: 0.9652\n",
      "Epoch 00025: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1492 - accuracy: 0.9653 - val_loss: 0.3797 - val_accuracy: 0.9033\n",
      "Epoch 26/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9660\n",
      "Epoch 00026: val_accuracy did not improve from 0.90513\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1492 - accuracy: 0.9659 - val_loss: 0.3877 - val_accuracy: 0.9051\n",
      "Epoch 27/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.9676\n",
      "Epoch 00027: val_accuracy improved from 0.90513 to 0.90585, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1433 - accuracy: 0.9675 - val_loss: 0.3871 - val_accuracy: 0.9059\n",
      "Epoch 28/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.9671\n",
      "Epoch 00028: val_accuracy improved from 0.90585 to 0.90912, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1452 - accuracy: 0.9672 - val_loss: 0.3889 - val_accuracy: 0.9091\n",
      "Epoch 29/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9718\n",
      "Epoch 00029: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1366 - accuracy: 0.9714 - val_loss: 0.4013 - val_accuracy: 0.9048\n",
      "Epoch 30/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9694\n",
      "Epoch 00030: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1360 - accuracy: 0.9693 - val_loss: 0.3876 - val_accuracy: 0.9077\n",
      "Epoch 31/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9705\n",
      "Epoch 00031: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1335 - accuracy: 0.9705 - val_loss: 0.4009 - val_accuracy: 0.9051\n",
      "Epoch 32/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9716\n",
      "Epoch 00032: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1312 - accuracy: 0.9715 - val_loss: 0.3915 - val_accuracy: 0.9088\n",
      "Epoch 33/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.9715\n",
      "Epoch 00033: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1298 - accuracy: 0.9717 - val_loss: 0.3990 - val_accuracy: 0.9051\n",
      "Epoch 34/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9746\n",
      "Epoch 00034: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1253 - accuracy: 0.9745 - val_loss: 0.4102 - val_accuracy: 0.9069\n",
      "Epoch 35/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9717\n",
      "Epoch 00035: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1289 - accuracy: 0.9718 - val_loss: 0.4024 - val_accuracy: 0.9066\n",
      "Epoch 36/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9755\n",
      "Epoch 00036: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1209 - accuracy: 0.9756 - val_loss: 0.3957 - val_accuracy: 0.9051\n",
      "Epoch 37/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9763\n",
      "Epoch 00037: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1189 - accuracy: 0.9762 - val_loss: 0.3988 - val_accuracy: 0.9026\n",
      "Epoch 38/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9758\n",
      "Epoch 00038: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1174 - accuracy: 0.9758 - val_loss: 0.4269 - val_accuracy: 0.9073\n",
      "Epoch 39/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9749\n",
      "Epoch 00039: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1178 - accuracy: 0.9750 - val_loss: 0.4169 - val_accuracy: 0.9088\n",
      "Epoch 40/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9740\n",
      "Epoch 00040: val_accuracy did not improve from 0.90912\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1194 - accuracy: 0.9740 - val_loss: 0.3998 - val_accuracy: 0.9051\n",
      "Epoch 41/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.1152 - accuracy: 0.9771\n",
      "Epoch 00041: val_accuracy improved from 0.90912 to 0.91167, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1150 - accuracy: 0.9770 - val_loss: 0.3890 - val_accuracy: 0.9117\n",
      "Epoch 42/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9770\n",
      "Epoch 00042: val_accuracy did not improve from 0.91167\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1122 - accuracy: 0.9771 - val_loss: 0.3917 - val_accuracy: 0.9113\n",
      "Epoch 43/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9770\n",
      "Epoch 00043: val_accuracy improved from 0.91167 to 0.91203, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1102 - accuracy: 0.9771 - val_loss: 0.3844 - val_accuracy: 0.9120\n",
      "Epoch 44/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9769\n",
      "Epoch 00044: val_accuracy did not improve from 0.91203\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1127 - accuracy: 0.9771 - val_loss: 0.3984 - val_accuracy: 0.9059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9814\n",
      "Epoch 00045: val_accuracy did not improve from 0.91203\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1026 - accuracy: 0.9814 - val_loss: 0.4033 - val_accuracy: 0.9080\n",
      "Epoch 46/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9776\n",
      "Epoch 00046: val_accuracy improved from 0.91203 to 0.91240, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9776 - val_loss: 0.3983 - val_accuracy: 0.9124\n",
      "Epoch 47/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.1058 - accuracy: 0.9790\n",
      "Epoch 00047: val_accuracy did not improve from 0.91240\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1058 - accuracy: 0.9790 - val_loss: 0.4051 - val_accuracy: 0.9106\n",
      "Epoch 48/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.1028 - accuracy: 0.9807\n",
      "Epoch 00048: val_accuracy did not improve from 0.91240\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1027 - accuracy: 0.9807 - val_loss: 0.4081 - val_accuracy: 0.9088\n",
      "Epoch 49/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9793\n",
      "Epoch 00049: val_accuracy did not improve from 0.91240\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1054 - accuracy: 0.9795 - val_loss: 0.4063 - val_accuracy: 0.9099\n",
      "Epoch 50/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9791\n",
      "Epoch 00050: val_accuracy improved from 0.91240 to 0.91385, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.1074 - accuracy: 0.9792 - val_loss: 0.3825 - val_accuracy: 0.9138\n",
      "Epoch 51/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9803\n",
      "Epoch 00051: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1020 - accuracy: 0.9804 - val_loss: 0.4202 - val_accuracy: 0.9051\n",
      "Epoch 52/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.1019 - accuracy: 0.9802\n",
      "Epoch 00052: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9801 - val_loss: 0.3960 - val_accuracy: 0.9080\n",
      "Epoch 53/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9813\n",
      "Epoch 00053: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9814 - val_loss: 0.3890 - val_accuracy: 0.9120\n",
      "Epoch 54/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9806\n",
      "Epoch 00054: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 4s 5ms/step - loss: 0.0985 - accuracy: 0.9806 - val_loss: 0.4162 - val_accuracy: 0.9080\n",
      "Epoch 55/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9806\n",
      "Epoch 00055: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 8s 10ms/step - loss: 0.0984 - accuracy: 0.9806 - val_loss: 0.4137 - val_accuracy: 0.9138\n",
      "Epoch 56/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9811\n",
      "Epoch 00056: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 5s 6ms/step - loss: 0.0987 - accuracy: 0.9810 - val_loss: 0.4003 - val_accuracy: 0.9113\n",
      "Epoch 57/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0949 - accuracy: 0.9824\n",
      "Epoch 00057: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0951 - accuracy: 0.9823 - val_loss: 0.4140 - val_accuracy: 0.9080\n",
      "Epoch 58/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9796\n",
      "Epoch 00058: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0989 - accuracy: 0.9796 - val_loss: 0.4163 - val_accuracy: 0.9088\n",
      "Epoch 59/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9809\n",
      "Epoch 00059: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0979 - accuracy: 0.9809 - val_loss: 0.4119 - val_accuracy: 0.9080\n",
      "Epoch 60/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9840\n",
      "Epoch 00060: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9840 - val_loss: 0.3992 - val_accuracy: 0.9106\n",
      "Epoch 61/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0917 - accuracy: 0.9824\n",
      "Epoch 00061: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9824 - val_loss: 0.4161 - val_accuracy: 0.9091\n",
      "Epoch 62/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9820\n",
      "Epoch 00062: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0937 - accuracy: 0.9820 - val_loss: 0.4100 - val_accuracy: 0.9062\n",
      "Epoch 63/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0912 - accuracy: 0.9822\n",
      "Epoch 00063: val_accuracy did not improve from 0.91385\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0912 - accuracy: 0.9822 - val_loss: 0.4133 - val_accuracy: 0.9128\n",
      "Epoch 64/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9837\n",
      "Epoch 00064: val_accuracy improved from 0.91385 to 0.91458, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0882 - accuracy: 0.9837 - val_loss: 0.4158 - val_accuracy: 0.9146\n",
      "Epoch 65/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9846\n",
      "Epoch 00065: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 5s 6ms/step - loss: 0.0866 - accuracy: 0.9846 - val_loss: 0.4121 - val_accuracy: 0.9135\n",
      "Epoch 66/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0884 - accuracy: 0.9831\n",
      "Epoch 00066: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 11s 14ms/step - loss: 0.0883 - accuracy: 0.9832 - val_loss: 0.4198 - val_accuracy: 0.9138\n",
      "Epoch 67/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0874 - accuracy: 0.9843\n",
      "Epoch 00067: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0873 - accuracy: 0.9844 - val_loss: 0.4069 - val_accuracy: 0.9120\n",
      "Epoch 68/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9827\n",
      "Epoch 00068: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0883 - accuracy: 0.9827 - val_loss: 0.4161 - val_accuracy: 0.9146\n",
      "Epoch 69/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0887 - accuracy: 0.9825\n",
      "Epoch 00069: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0887 - accuracy: 0.9825 - val_loss: 0.4047 - val_accuracy: 0.9135\n",
      "Epoch 70/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9857\n",
      "Epoch 00070: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0828 - accuracy: 0.9856 - val_loss: 0.4198 - val_accuracy: 0.9131\n",
      "Epoch 71/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9850\n",
      "Epoch 00071: val_accuracy did not improve from 0.91458\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0851 - accuracy: 0.9849 - val_loss: 0.4251 - val_accuracy: 0.9117\n",
      "Epoch 72/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9844\n",
      "Epoch 00072: val_accuracy improved from 0.91458 to 0.91567, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0854 - accuracy: 0.9842 - val_loss: 0.4176 - val_accuracy: 0.9157\n",
      "Epoch 73/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9835\n",
      "Epoch 00073: val_accuracy did not improve from 0.91567\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0873 - accuracy: 0.9835 - val_loss: 0.4226 - val_accuracy: 0.9135\n",
      "Epoch 74/150\n",
      "755/774 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9834\n",
      "Epoch 00074: val_accuracy did not improve from 0.91567\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0858 - accuracy: 0.9832 - val_loss: 0.4155 - val_accuracy: 0.9131\n",
      "Epoch 75/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0817 - accuracy: 0.9850\n",
      "Epoch 00075: val_accuracy did not improve from 0.91567\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9851 - val_loss: 0.4400 - val_accuracy: 0.9124\n",
      "Epoch 76/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0821 - accuracy: 0.9858\n",
      "Epoch 00076: val_accuracy did not improve from 0.91567\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0820 - accuracy: 0.9859 - val_loss: 0.4310 - val_accuracy: 0.9142\n",
      "Epoch 77/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.9855\n",
      "Epoch 00077: val_accuracy improved from 0.91567 to 0.91858, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0801 - accuracy: 0.9857 - val_loss: 0.4079 - val_accuracy: 0.9186\n",
      "Epoch 78/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9845\n",
      "Epoch 00078: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0832 - accuracy: 0.9846 - val_loss: 0.4202 - val_accuracy: 0.9135\n",
      "Epoch 79/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9839\n",
      "Epoch 00079: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9839 - val_loss: 0.4070 - val_accuracy: 0.9142\n",
      "Epoch 80/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9847\n",
      "Epoch 00080: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0821 - accuracy: 0.9848 - val_loss: 0.4026 - val_accuracy: 0.9146\n",
      "Epoch 81/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9867\n",
      "Epoch 00081: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0772 - accuracy: 0.9868 - val_loss: 0.4376 - val_accuracy: 0.9099\n",
      "Epoch 82/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9869\n",
      "Epoch 00082: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0783 - accuracy: 0.9868 - val_loss: 0.4033 - val_accuracy: 0.9142\n",
      "Epoch 83/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9849\n",
      "Epoch 00083: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0799 - accuracy: 0.9849 - val_loss: 0.4016 - val_accuracy: 0.9168\n",
      "Epoch 84/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9845\n",
      "Epoch 00084: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9844 - val_loss: 0.4119 - val_accuracy: 0.9149\n",
      "Epoch 85/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9865\n",
      "Epoch 00085: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0749 - accuracy: 0.9865 - val_loss: 0.4179 - val_accuracy: 0.9178\n",
      "Epoch 86/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.9846\n",
      "Epoch 00086: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9846 - val_loss: 0.4264 - val_accuracy: 0.9091\n",
      "Epoch 87/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9865\n",
      "Epoch 00087: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9864 - val_loss: 0.4013 - val_accuracy: 0.9171\n",
      "Epoch 88/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9893\n",
      "Epoch 00088: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9893 - val_loss: 0.4085 - val_accuracy: 0.9157\n",
      "Epoch 89/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9864\n",
      "Epoch 00089: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0765 - accuracy: 0.9861 - val_loss: 0.4326 - val_accuracy: 0.9131\n",
      "Epoch 90/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9864\n",
      "Epoch 00090: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9863 - val_loss: 0.4175 - val_accuracy: 0.9153\n",
      "Epoch 91/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9877\n",
      "Epoch 00091: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0735 - accuracy: 0.9877 - val_loss: 0.4181 - val_accuracy: 0.9138\n",
      "Epoch 92/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9871\n",
      "Epoch 00092: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9871 - val_loss: 0.4133 - val_accuracy: 0.9142\n",
      "Epoch 93/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9891\n",
      "Epoch 00093: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9891 - val_loss: 0.4268 - val_accuracy: 0.9146\n",
      "Epoch 94/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9867\n",
      "Epoch 00094: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9867 - val_loss: 0.4412 - val_accuracy: 0.9106\n",
      "Epoch 95/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9872\n",
      "Epoch 00095: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0736 - accuracy: 0.9873 - val_loss: 0.4218 - val_accuracy: 0.9117\n",
      "Epoch 96/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9870\n",
      "Epoch 00096: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0741 - accuracy: 0.9871 - val_loss: 0.4399 - val_accuracy: 0.9113\n",
      "Epoch 97/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9860\n",
      "Epoch 00097: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9860 - val_loss: 0.4088 - val_accuracy: 0.9146\n",
      "Epoch 98/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9865\n",
      "Epoch 00098: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0746 - accuracy: 0.9865 - val_loss: 0.4416 - val_accuracy: 0.9091\n",
      "Epoch 99/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9883\n",
      "Epoch 00099: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0687 - accuracy: 0.9883 - val_loss: 0.4296 - val_accuracy: 0.9146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9860\n",
      "Epoch 00100: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9860 - val_loss: 0.4268 - val_accuracy: 0.9124\n",
      "Epoch 101/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.9872\n",
      "Epoch 00101: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9872 - val_loss: 0.4160 - val_accuracy: 0.9149\n",
      "Epoch 102/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9870\n",
      "Epoch 00102: val_accuracy did not improve from 0.91858\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0729 - accuracy: 0.9870 - val_loss: 0.4248 - val_accuracy: 0.9157\n",
      "Epoch 103/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9869\n",
      "Epoch 00103: val_accuracy improved from 0.91858 to 0.92076, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0725 - accuracy: 0.9870 - val_loss: 0.4006 - val_accuracy: 0.9208\n",
      "Epoch 104/150\n",
      "763/774 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9876\n",
      "Epoch 00104: val_accuracy did not improve from 0.92076\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9876 - val_loss: 0.4039 - val_accuracy: 0.9171\n",
      "Epoch 105/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9874\n",
      "Epoch 00105: val_accuracy did not improve from 0.92076\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0710 - accuracy: 0.9874 - val_loss: 0.4241 - val_accuracy: 0.9128\n",
      "Epoch 106/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9879\n",
      "Epoch 00106: val_accuracy did not improve from 0.92076\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0701 - accuracy: 0.9877 - val_loss: 0.4212 - val_accuracy: 0.9168\n",
      "Epoch 107/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.9876\n",
      "Epoch 00107: val_accuracy did not improve from 0.92076\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0691 - accuracy: 0.9875 - val_loss: 0.4258 - val_accuracy: 0.9128\n",
      "Epoch 108/150\n",
      "756/774 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9885\n",
      "Epoch 00108: val_accuracy improved from 0.92076 to 0.92112, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0674 - accuracy: 0.9886 - val_loss: 0.4063 - val_accuracy: 0.9211\n",
      "Epoch 109/150\n",
      "758/774 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9874\n",
      "Epoch 00109: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0696 - accuracy: 0.9873 - val_loss: 0.4128 - val_accuracy: 0.9142\n",
      "Epoch 110/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9880\n",
      "Epoch 00110: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0672 - accuracy: 0.9881 - val_loss: 0.4130 - val_accuracy: 0.9171\n",
      "Epoch 111/150\n",
      "761/774 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9882\n",
      "Epoch 00111: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0668 - accuracy: 0.9882 - val_loss: 0.4133 - val_accuracy: 0.9149\n",
      "Epoch 112/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9886\n",
      "Epoch 00112: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0670 - accuracy: 0.9886 - val_loss: 0.4104 - val_accuracy: 0.9138\n",
      "Epoch 113/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9895\n",
      "Epoch 00113: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9896 - val_loss: 0.3952 - val_accuracy: 0.9157\n",
      "Epoch 114/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9899\n",
      "Epoch 00114: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9897 - val_loss: 0.4137 - val_accuracy: 0.9160\n",
      "Epoch 115/150\n",
      "766/774 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9875\n",
      "Epoch 00115: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9873 - val_loss: 0.4182 - val_accuracy: 0.9164\n",
      "Epoch 116/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9894\n",
      "Epoch 00116: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9893 - val_loss: 0.4406 - val_accuracy: 0.9135\n",
      "Epoch 117/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9863\n",
      "Epoch 00117: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0692 - accuracy: 0.9864 - val_loss: 0.4331 - val_accuracy: 0.9175\n",
      "Epoch 118/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9872\n",
      "Epoch 00118: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9872 - val_loss: 0.4014 - val_accuracy: 0.9146\n",
      "Epoch 119/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9888\n",
      "Epoch 00119: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9888 - val_loss: 0.4295 - val_accuracy: 0.9128\n",
      "Epoch 120/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9887\n",
      "Epoch 00120: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0664 - accuracy: 0.9887 - val_loss: 0.4078 - val_accuracy: 0.9160\n",
      "Epoch 121/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9884\n",
      "Epoch 00121: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9884 - val_loss: 0.4263 - val_accuracy: 0.9131\n",
      "Epoch 122/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9886\n",
      "Epoch 00122: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9886 - val_loss: 0.4250 - val_accuracy: 0.9157\n",
      "Epoch 123/150\n",
      "770/774 [============================>.] - ETA: 0s - loss: 0.0629 - accuracy: 0.9889\n",
      "Epoch 00123: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0628 - accuracy: 0.9889 - val_loss: 0.4100 - val_accuracy: 0.9157\n",
      "Epoch 124/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9879\n",
      "Epoch 00124: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0656 - accuracy: 0.9879 - val_loss: 0.4223 - val_accuracy: 0.9146\n",
      "Epoch 125/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9891\n",
      "Epoch 00125: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9892 - val_loss: 0.4233 - val_accuracy: 0.9182\n",
      "Epoch 126/150\n",
      "757/774 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9895\n",
      "Epoch 00126: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0623 - accuracy: 0.9896 - val_loss: 0.4354 - val_accuracy: 0.9160\n",
      "Epoch 127/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9887\n",
      "Epoch 00127: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9887 - val_loss: 0.4294 - val_accuracy: 0.9128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9875\n",
      "Epoch 00128: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9874 - val_loss: 0.4185 - val_accuracy: 0.9153\n",
      "Epoch 129/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9899\n",
      "Epoch 00129: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0615 - accuracy: 0.9899 - val_loss: 0.4355 - val_accuracy: 0.9113\n",
      "Epoch 130/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9883\n",
      "Epoch 00130: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9883 - val_loss: 0.4370 - val_accuracy: 0.9113\n",
      "Epoch 131/150\n",
      "771/774 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9873\n",
      "Epoch 00131: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0655 - accuracy: 0.9873 - val_loss: 0.4345 - val_accuracy: 0.9135\n",
      "Epoch 132/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9900\n",
      "Epoch 00132: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0618 - accuracy: 0.9900 - val_loss: 0.4431 - val_accuracy: 0.9171\n",
      "Epoch 133/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9893\n",
      "Epoch 00133: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0622 - accuracy: 0.9893 - val_loss: 0.4440 - val_accuracy: 0.9164\n",
      "Epoch 134/150\n",
      "774/774 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9884\n",
      "Epoch 00134: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9884 - val_loss: 0.4405 - val_accuracy: 0.9160\n",
      "Epoch 135/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9884\n",
      "Epoch 00135: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9884 - val_loss: 0.4478 - val_accuracy: 0.9102\n",
      "Epoch 136/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9900\n",
      "Epoch 00136: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9899 - val_loss: 0.4410 - val_accuracy: 0.9211\n",
      "Epoch 137/150\n",
      "765/774 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9902\n",
      "Epoch 00137: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0593 - accuracy: 0.9902 - val_loss: 0.4488 - val_accuracy: 0.9175\n",
      "Epoch 138/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9898\n",
      "Epoch 00138: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9897 - val_loss: 0.4242 - val_accuracy: 0.9208\n",
      "Epoch 139/150\n",
      "762/774 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9883\n",
      "Epoch 00139: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9884 - val_loss: 0.4323 - val_accuracy: 0.9208\n",
      "Epoch 140/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9893\n",
      "Epoch 00140: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 2s 3ms/step - loss: 0.0603 - accuracy: 0.9894 - val_loss: 0.4361 - val_accuracy: 0.9164\n",
      "Epoch 141/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9911\n",
      "Epoch 00141: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0586 - accuracy: 0.9910 - val_loss: 0.4211 - val_accuracy: 0.9178\n",
      "Epoch 142/150\n",
      "760/774 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9902\n",
      "Epoch 00142: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0582 - accuracy: 0.9903 - val_loss: 0.4444 - val_accuracy: 0.9128\n",
      "Epoch 143/150\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9903\n",
      "Epoch 00143: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0574 - accuracy: 0.9903 - val_loss: 0.4271 - val_accuracy: 0.9168\n",
      "Epoch 144/150\n",
      "768/774 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9902\n",
      "Epoch 00144: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0589 - accuracy: 0.9903 - val_loss: 0.4346 - val_accuracy: 0.9193\n",
      "Epoch 145/150\n",
      "759/774 [============================>.] - ETA: 0s - loss: 0.0559 - accuracy: 0.9911\n",
      "Epoch 00145: val_accuracy did not improve from 0.92112\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0556 - accuracy: 0.9912 - val_loss: 0.4498 - val_accuracy: 0.9164\n",
      "Epoch 146/150\n",
      "772/774 [============================>.] - ETA: 0s - loss: 0.0562 - accuracy: 0.9903\n",
      "Epoch 00146: val_accuracy improved from 0.92112 to 0.92330, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0562 - accuracy: 0.9903 - val_loss: 0.4278 - val_accuracy: 0.9233\n",
      "Epoch 147/150\n",
      "767/774 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9891\n",
      "Epoch 00147: val_accuracy did not improve from 0.92330\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0605 - accuracy: 0.9891 - val_loss: 0.4484 - val_accuracy: 0.9164\n",
      "Epoch 148/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9895\n",
      "Epoch 00148: val_accuracy did not improve from 0.92330\n",
      "774/774 [==============================] - 3s 4ms/step - loss: 0.0586 - accuracy: 0.9896 - val_loss: 0.4356 - val_accuracy: 0.9186\n",
      "Epoch 149/150\n",
      "764/774 [============================>.] - ETA: 0s - loss: 0.0611 - accuracy: 0.9897\n",
      "Epoch 00149: val_accuracy did not improve from 0.92330\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0610 - accuracy: 0.9898 - val_loss: 0.4100 - val_accuracy: 0.9197\n",
      "Epoch 150/150\n",
      "769/774 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9911\n",
      "Epoch 00150: val_accuracy did not improve from 0.92330\n",
      "774/774 [==============================] - 3s 3ms/step - loss: 0.0554 - accuracy: 0.9911 - val_loss: 0.4146 - val_accuracy: 0.9164\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [09:56<00:00, 298.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.65      0.66      0.66       672\n",
      "        car_horn       0.65      0.50      0.56       294\n",
      "children_playing       0.74      0.85      0.79       700\n",
      "        dog_bark       0.81      0.82      0.81       700\n",
      "           siren       0.76      0.69      0.72       637\n",
      "\n",
      "        accuracy                           0.73      3003\n",
      "       macro avg       0.72      0.70      0.71      3003\n",
      "    weighted avg       0.73      0.73      0.73      3003\n",
      "\n",
      "Validation fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27132, 375)\n",
      "X_val_norm shape.....:(3374, 375)\n",
      "\n",
      "Sum of elements: 0.9802037458487682\n",
      "Number of elements summed: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 233)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               175500    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 288,299\n",
      "Trainable params: 288,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24418, 233)\n",
      "Epoch 1/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.8154 - accuracy: 0.6989\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82461, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.8093 - accuracy: 0.7013 - val_loss: 0.4915 - val_accuracy: 0.8246\n",
      "Epoch 2/350\n",
      "736/764 [===========================>..] - ETA: 0s - loss: 0.4630 - accuracy: 0.8339\n",
      "Epoch 00002: val_accuracy improved from 0.82461 to 0.85298, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.4616 - accuracy: 0.8347 - val_loss: 0.3934 - val_accuracy: 0.8530\n",
      "Epoch 3/350\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.3606 - accuracy: 0.8720\n",
      "Epoch 00003: val_accuracy improved from 0.85298 to 0.87693, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.3591 - accuracy: 0.8727 - val_loss: 0.3357 - val_accuracy: 0.8769\n",
      "Epoch 4/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8945\n",
      "Epoch 00004: val_accuracy improved from 0.87693 to 0.89130, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.2974 - accuracy: 0.8946 - val_loss: 0.3007 - val_accuracy: 0.8913\n",
      "Epoch 5/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.9121\n",
      "Epoch 00005: val_accuracy improved from 0.89130 to 0.90015, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.2489 - accuracy: 0.9120 - val_loss: 0.2725 - val_accuracy: 0.9001\n",
      "Epoch 6/350\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9265\n",
      "Epoch 00006: val_accuracy improved from 0.90015 to 0.90789, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.2071 - accuracy: 0.9267 - val_loss: 0.2548 - val_accuracy: 0.9079\n",
      "Epoch 7/350\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9377\n",
      "Epoch 00007: val_accuracy improved from 0.90789 to 0.91489, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.9376 - val_loss: 0.2408 - val_accuracy: 0.9149\n",
      "Epoch 8/350\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.1503 - accuracy: 0.9478\n",
      "Epoch 00008: val_accuracy improved from 0.91489 to 0.91746, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.1504 - accuracy: 0.9477 - val_loss: 0.2267 - val_accuracy: 0.9175\n",
      "Epoch 9/350\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9555\n",
      "Epoch 00009: val_accuracy improved from 0.91746 to 0.92410, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.1298 - accuracy: 0.9554 - val_loss: 0.2223 - val_accuracy: 0.9241\n",
      "Epoch 10/350\n",
      "743/764 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9622\n",
      "Epoch 00010: val_accuracy improved from 0.92410 to 0.92962, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.1107 - accuracy: 0.9620 - val_loss: 0.2195 - val_accuracy: 0.9296\n",
      "Epoch 11/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9701\n",
      "Epoch 00011: val_accuracy improved from 0.92962 to 0.93073, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0920 - accuracy: 0.9700 - val_loss: 0.2118 - val_accuracy: 0.9307\n",
      "Epoch 12/350\n",
      "736/764 [===========================>..] - ETA: 0s - loss: 0.0792 - accuracy: 0.9746\n",
      "Epoch 00012: val_accuracy improved from 0.93073 to 0.93441, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.9746 - val_loss: 0.2117 - val_accuracy: 0.9344\n",
      "Epoch 13/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9787\n",
      "Epoch 00013: val_accuracy did not improve from 0.93441\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0670 - accuracy: 0.9788 - val_loss: 0.2100 - val_accuracy: 0.9344\n",
      "Epoch 14/350\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0572 - accuracy: 0.9831\n",
      "Epoch 00014: val_accuracy improved from 0.93441 to 0.93515, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0572 - accuracy: 0.9830 - val_loss: 0.2105 - val_accuracy: 0.9352\n",
      "Epoch 15/350\n",
      "741/764 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9853\n",
      "Epoch 00015: val_accuracy improved from 0.93515 to 0.93884, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0481 - accuracy: 0.9853 - val_loss: 0.2077 - val_accuracy: 0.9388\n",
      "Epoch 16/350\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9864\n",
      "Epoch 00016: val_accuracy improved from 0.93884 to 0.93957, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0427 - accuracy: 0.9864 - val_loss: 0.2104 - val_accuracy: 0.9396\n",
      "Epoch 17/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0369 - accuracy: 0.9882\n",
      "Epoch 00017: val_accuracy improved from 0.93957 to 0.93994, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0368 - accuracy: 0.9882 - val_loss: 0.2213 - val_accuracy: 0.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/350\n",
      "742/764 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9904\n",
      "Epoch 00018: val_accuracy improved from 0.93994 to 0.94068, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9904 - val_loss: 0.2198 - val_accuracy: 0.9407\n",
      "Epoch 19/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9920\n",
      "Epoch 00019: val_accuracy improved from 0.94068 to 0.94289, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0284 - accuracy: 0.9919 - val_loss: 0.2194 - val_accuracy: 0.9429\n",
      "Epoch 20/350\n",
      "733/764 [===========================>..] - ETA: 0s - loss: 0.0238 - accuracy: 0.9934\n",
      "Epoch 00020: val_accuracy did not improve from 0.94289\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0240 - accuracy: 0.9933 - val_loss: 0.2292 - val_accuracy: 0.9396\n",
      "Epoch 21/350\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9943\n",
      "Epoch 00021: val_accuracy did not improve from 0.94289\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9943 - val_loss: 0.2232 - val_accuracy: 0.9429\n",
      "Epoch 22/350\n",
      "737/764 [===========================>..] - ETA: 0s - loss: 0.0206 - accuracy: 0.9948\n",
      "Epoch 00022: val_accuracy improved from 0.94289 to 0.94547, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9948 - val_loss: 0.2265 - val_accuracy: 0.9455\n",
      "Epoch 23/350\n",
      "744/764 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9961\n",
      "Epoch 00023: val_accuracy improved from 0.94547 to 0.94768, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9961 - val_loss: 0.2346 - val_accuracy: 0.9477\n",
      "Epoch 24/350\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.0166 - accuracy: 0.9956\n",
      "Epoch 00024: val_accuracy improved from 0.94768 to 0.94805, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9956 - val_loss: 0.2333 - val_accuracy: 0.9480\n",
      "Epoch 25/350\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9967\n",
      "Epoch 00025: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9967 - val_loss: 0.2408 - val_accuracy: 0.9458\n",
      "Epoch 26/350\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9972\n",
      "Epoch 00026: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0126 - accuracy: 0.9971 - val_loss: 0.2449 - val_accuracy: 0.9469\n",
      "Epoch 27/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9972\n",
      "Epoch 00027: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9973 - val_loss: 0.2480 - val_accuracy: 0.9433\n",
      "Epoch 28/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9977\n",
      "Epoch 00028: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.2504 - val_accuracy: 0.9444\n",
      "Epoch 29/350\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9977\n",
      "Epoch 00029: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.2400 - val_accuracy: 0.9469\n",
      "Epoch 30/350\n",
      "736/764 [===========================>..] - ETA: 0s - loss: 0.0087 - accuracy: 0.9983\n",
      "Epoch 00030: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.2501 - val_accuracy: 0.9451\n",
      "Epoch 31/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9988\n",
      "Epoch 00031: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.2527 - val_accuracy: 0.9469\n",
      "Epoch 32/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9979\n",
      "Epoch 00032: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.2609 - val_accuracy: 0.9469\n",
      "Epoch 33/350\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9983\n",
      "Epoch 00033: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.2592 - val_accuracy: 0.9458\n",
      "Epoch 34/350\n",
      "738/764 [===========================>..] - ETA: 0s - loss: 0.0065 - accuracy: 0.9988\n",
      "Epoch 00034: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 0.2831 - val_accuracy: 0.9433\n",
      "Epoch 35/350\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9977\n",
      "Epoch 00035: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0097 - accuracy: 0.9977 - val_loss: 0.2749 - val_accuracy: 0.9469\n",
      "Epoch 36/350\n",
      "735/764 [===========================>..] - ETA: 0s - loss: 0.0064 - accuracy: 0.9987\n",
      "Epoch 00036: val_accuracy did not improve from 0.94805\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.2666 - val_accuracy: 0.9451\n",
      "Epoch 37/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9988\n",
      "Epoch 00037: val_accuracy improved from 0.94805 to 0.94915, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 0.2690 - val_accuracy: 0.9492\n",
      "Epoch 38/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9995\n",
      "Epoch 00038: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.2742 - val_accuracy: 0.9469\n",
      "Epoch 39/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9992\n",
      "Epoch 00039: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.2732 - val_accuracy: 0.9466\n",
      "Epoch 40/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.2742 - val_accuracy: 0.9458\n",
      "Epoch 41/350\n",
      "736/764 [===========================>..] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989\n",
      "Epoch 00041: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.2707 - val_accuracy: 0.9477\n",
      "Epoch 42/350\n",
      "742/764 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 00042: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2810 - val_accuracy: 0.9480\n",
      "Epoch 43/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9992\n",
      "Epoch 00043: val_accuracy did not improve from 0.94915\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 0.2755 - val_accuracy: 0.9477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/350\n",
      "734/764 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9993\n",
      "Epoch 00044: val_accuracy improved from 0.94915 to 0.94989, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.2737 - val_accuracy: 0.9499\n",
      "Epoch 45/350\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9990\n",
      "Epoch 00045: val_accuracy did not improve from 0.94989\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.2722 - val_accuracy: 0.9499\n",
      "Epoch 46/350\n",
      "731/764 [===========================>..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 00046: val_accuracy did not improve from 0.94989\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.2759 - val_accuracy: 0.9466\n",
      "Epoch 47/350\n",
      "739/764 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9995\n",
      "Epoch 00047: val_accuracy improved from 0.94989 to 0.95063, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.2785 - val_accuracy: 0.9506\n",
      "Epoch 48/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 00048: val_accuracy did not improve from 0.95063\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.2828 - val_accuracy: 0.9499\n",
      "Epoch 49/350\n",
      "739/764 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9992\n",
      "Epoch 00049: val_accuracy improved from 0.95063 to 0.95099, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.2806 - val_accuracy: 0.9510\n",
      "Epoch 50/350\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 00050: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.2855 - val_accuracy: 0.9510\n",
      "Epoch 51/350\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 00051: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.2784 - val_accuracy: 0.9492\n",
      "Epoch 52/350\n",
      "741/764 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 00052: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.2849 - val_accuracy: 0.9495\n",
      "Epoch 53/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9993\n",
      "Epoch 00053: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.2880 - val_accuracy: 0.9499\n",
      "Epoch 54/350\n",
      "734/764 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 00054: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.2937 - val_accuracy: 0.9469\n",
      "Epoch 55/350\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00055: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2956 - val_accuracy: 0.9480\n",
      "Epoch 56/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9996\n",
      "Epoch 00056: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2946 - val_accuracy: 0.9499\n",
      "Epoch 57/350\n",
      "743/764 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00057: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.3004 - val_accuracy: 0.9455\n",
      "Epoch 58/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.3004 - val_accuracy: 0.9484\n",
      "Epoch 59/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00059: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2965 - val_accuracy: 0.9488\n",
      "Epoch 60/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9996\n",
      "Epoch 00060: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2953 - val_accuracy: 0.9499\n",
      "Epoch 61/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9996\n",
      "Epoch 00061: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2949 - val_accuracy: 0.9480\n",
      "Epoch 62/350\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00062: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2933 - val_accuracy: 0.9484\n",
      "Epoch 63/350\n",
      "740/764 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00063: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2964 - val_accuracy: 0.9484\n",
      "Epoch 64/350\n",
      "741/764 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00064: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2996 - val_accuracy: 0.9462\n",
      "Epoch 65/350\n",
      "734/764 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00065: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2951 - val_accuracy: 0.9488\n",
      "Epoch 66/350\n",
      "744/764 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00066: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2953 - val_accuracy: 0.9492\n",
      "Epoch 67/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00067: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2945 - val_accuracy: 0.9480\n",
      "Epoch 68/350\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00068: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2975 - val_accuracy: 0.9503\n",
      "Epoch 69/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2963 - val_accuracy: 0.9499\n",
      "Epoch 70/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00070: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2996 - val_accuracy: 0.9506\n",
      "Epoch 71/350\n",
      "743/764 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3039 - val_accuracy: 0.9495\n",
      "Epoch 72/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 00072: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3049 - val_accuracy: 0.9506\n",
      "Epoch 73/350\n",
      "762/764 [============================>.] - ETA: 0s - loss: 9.9363e-04 - accuracy: 0.9999\n",
      "Epoch 00073: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.9288e-04 - accuracy: 0.9999 - val_loss: 0.3067 - val_accuracy: 0.9492\n",
      "Epoch 74/350\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3134 - val_accuracy: 0.9503\n",
      "Epoch 75/350\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00075: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.3111 - val_accuracy: 0.9477\n",
      "Epoch 76/350\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00076: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.3161 - val_accuracy: 0.9473\n",
      "Epoch 77/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00077: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.3108 - val_accuracy: 0.9492\n",
      "Epoch 78/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.3127 - val_accuracy: 0.9462\n",
      "Epoch 79/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3117 - val_accuracy: 0.9484\n",
      "Epoch 80/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 00080: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.3130 - val_accuracy: 0.9495\n",
      "Epoch 81/350\n",
      "734/764 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00081: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3098 - val_accuracy: 0.9499\n",
      "Epoch 82/350\n",
      "743/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 00082: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.3084 - val_accuracy: 0.9503\n",
      "Epoch 83/350\n",
      "739/764 [============================>.] - ETA: 0s - loss: 9.8283e-04 - accuracy: 0.9999\n",
      "Epoch 00083: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3124 - val_accuracy: 0.9510\n",
      "Epoch 84/350\n",
      "744/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00084: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3162 - val_accuracy: 0.9488\n",
      "Epoch 85/350\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 00085: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3059 - val_accuracy: 0.9495\n",
      "Epoch 86/350\n",
      "739/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00086: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3072 - val_accuracy: 0.9499\n",
      "Epoch 87/350\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3049 - val_accuracy: 0.9488\n",
      "Epoch 88/350\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3073 - val_accuracy: 0.9495\n",
      "Epoch 89/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00089: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3110 - val_accuracy: 0.9492\n",
      "Epoch 90/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00090: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3144 - val_accuracy: 0.9506\n",
      "Epoch 91/350\n",
      "741/764 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00091: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3145 - val_accuracy: 0.9503\n",
      "Epoch 92/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00092: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3278 - val_accuracy: 0.9495\n",
      "Epoch 93/350\n",
      "755/764 [============================>.] - ETA: 0s - loss: 9.2258e-04 - accuracy: 0.9999\n",
      "Epoch 00093: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.1965e-04 - accuracy: 0.9999 - val_loss: 0.3214 - val_accuracy: 0.9488\n",
      "Epoch 94/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00094: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.3167 - val_accuracy: 0.9503\n",
      "Epoch 95/350\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 00095: val_accuracy did not improve from 0.95099\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.3089 - val_accuracy: 0.9506\n",
      "Epoch 96/350\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00096: val_accuracy improved from 0.95099 to 0.95173, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3184 - val_accuracy: 0.9517\n",
      "Epoch 97/350\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00097: val_accuracy did not improve from 0.95173\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3218 - val_accuracy: 0.9499\n",
      "Epoch 98/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy improved from 0.95173 to 0.95210, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3225 - val_accuracy: 0.9521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/350\n",
      "747/764 [============================>.] - ETA: 0s - loss: 8.7839e-04 - accuracy: 0.9999\n",
      "Epoch 00099: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 8.7096e-04 - accuracy: 0.9999 - val_loss: 0.3212 - val_accuracy: 0.9521\n",
      "Epoch 100/350\n",
      "763/764 [============================>.] - ETA: 0s - loss: 9.5819e-04 - accuracy: 0.9999\n",
      "Epoch 00100: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 9.5811e-04 - accuracy: 0.9999 - val_loss: 0.3252 - val_accuracy: 0.9514\n",
      "Epoch 101/350\n",
      "751/764 [============================>.] - ETA: 0s - loss: 7.3124e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 7.2800e-04 - accuracy: 1.0000 - val_loss: 0.3260 - val_accuracy: 0.9503\n",
      "Epoch 102/350\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00102: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.3229 - val_accuracy: 0.9499\n",
      "Epoch 103/350\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3273 - val_accuracy: 0.9506\n",
      "Epoch 104/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 6.8103e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 6.8797e-04 - accuracy: 0.9999 - val_loss: 0.3287 - val_accuracy: 0.9517\n",
      "Epoch 105/350\n",
      "750/764 [============================>.] - ETA: 0s - loss: 8.8775e-04 - accuracy: 0.9999\n",
      "Epoch 00105: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 8.8259e-04 - accuracy: 0.9999 - val_loss: 0.3295 - val_accuracy: 0.9514\n",
      "Epoch 106/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 8.8855e-04 - accuracy: 0.9998\n",
      "Epoch 00106: val_accuracy did not improve from 0.95210\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 8.8843e-04 - accuracy: 0.9998 - val_loss: 0.3262 - val_accuracy: 0.9521\n",
      "Epoch 107/350\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00107: val_accuracy improved from 0.95210 to 0.95247, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.9877e-04 - accuracy: 0.9998 - val_loss: 0.3241 - val_accuracy: 0.9525\n",
      "Epoch 108/350\n",
      "733/764 [===========================>..] - ETA: 0s - loss: 7.5342e-04 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.5786e-04 - accuracy: 1.0000 - val_loss: 0.3258 - val_accuracy: 0.9503\n",
      "Epoch 109/350\n",
      "738/764 [===========================>..] - ETA: 0s - loss: 9.1189e-04 - accuracy: 0.9998\n",
      "Epoch 00109: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 8.9264e-04 - accuracy: 0.9998 - val_loss: 0.3263 - val_accuracy: 0.9506\n",
      "Epoch 110/350\n",
      "752/764 [============================>.] - ETA: 0s - loss: 7.6014e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 7.5461e-04 - accuracy: 0.9999 - val_loss: 0.3285 - val_accuracy: 0.9492\n",
      "Epoch 111/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 5.7749e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 5.8197e-04 - accuracy: 1.0000 - val_loss: 0.3283 - val_accuracy: 0.9480\n",
      "Epoch 112/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 7.6135e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.6109e-04 - accuracy: 0.9998 - val_loss: 0.3280 - val_accuracy: 0.9495\n",
      "Epoch 113/350\n",
      "743/764 [============================>.] - ETA: 0s - loss: 9.7460e-04 - accuracy: 0.9998\n",
      "Epoch 00113: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.7290e-04 - accuracy: 0.9998 - val_loss: 0.3251 - val_accuracy: 0.9480\n",
      "Epoch 114/350\n",
      "754/764 [============================>.] - ETA: 0s - loss: 6.9651e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.2860e-04 - accuracy: 0.9999 - val_loss: 0.3286 - val_accuracy: 0.9484\n",
      "Epoch 115/350\n",
      "749/764 [============================>.] - ETA: 0s - loss: 9.1768e-04 - accuracy: 0.9997\n",
      "Epoch 00115: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.0590e-04 - accuracy: 0.9998 - val_loss: 0.3275 - val_accuracy: 0.9495\n",
      "Epoch 116/350\n",
      "742/764 [============================>.] - ETA: 0s - loss: 9.5076e-04 - accuracy: 0.9998\n",
      "Epoch 00116: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.3935e-04 - accuracy: 0.9998 - val_loss: 0.3316 - val_accuracy: 0.9473\n",
      "Epoch 117/350\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00117: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3233 - val_accuracy: 0.9484\n",
      "Epoch 118/350\n",
      "738/764 [===========================>..] - ETA: 0s - loss: 8.8365e-04 - accuracy: 0.9999\n",
      "Epoch 00118: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 9.1470e-04 - accuracy: 0.9998 - val_loss: 0.3304 - val_accuracy: 0.9492\n",
      "Epoch 119/350\n",
      "737/764 [===========================>..] - ETA: 0s - loss: 7.2382e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.1051e-04 - accuracy: 0.9999 - val_loss: 0.3257 - val_accuracy: 0.9492\n",
      "Epoch 120/350\n",
      "752/764 [============================>.] - ETA: 0s - loss: 6.2829e-04 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 6.2325e-04 - accuracy: 1.0000 - val_loss: 0.3263 - val_accuracy: 0.9506\n",
      "Epoch 121/350\n",
      "758/764 [============================>.] - ETA: 0s - loss: 8.1448e-04 - accuracy: 0.9999\n",
      "Epoch 00121: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 8.1176e-04 - accuracy: 0.9999 - val_loss: 0.3284 - val_accuracy: 0.9506\n",
      "Epoch 122/350\n",
      "756/764 [============================>.] - ETA: 0s - loss: 7.7739e-04 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95247\n",
      "764/764 [==============================] - 1s 2ms/step - loss: 7.7177e-04 - accuracy: 0.9999 - val_loss: 0.3315 - val_accuracy: 0.9499\n",
      "Epoch 123/350\n",
      "753/764 [============================>.] - ETA: 0s - loss: 5.4531e-04 - accuracy: 0.9999\n",
      "Epoch 00123: val_accuracy did not improve from 0.95247\n",
      "Restoring model weights from the end of the best epoch.\n",
      "764/764 [==============================] - 2s 2ms/step - loss: 5.4452e-04 - accuracy: 0.9999 - val_loss: 0.3299 - val_accuracy: 0.9506\n",
      "Epoch 00123: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:50<02:50, 170.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.78      0.81      0.80       840\n",
      "        car_horn       0.86      0.98      0.91       301\n",
      "children_playing       0.78      0.78      0.78       700\n",
      "        dog_bark       0.84      0.80      0.82       700\n",
      "           siren       0.88      0.84      0.86       833\n",
      "\n",
      "        accuracy                           0.82      3374\n",
      "       macro avg       0.83      0.84      0.83      3374\n",
      "    weighted avg       0.82      0.82      0.82      3374\n",
      "\n",
      "Model: \"Model_CNN_1D_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 227, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 227, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 227, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6328)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                316450    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 327,491\n",
      "Trainable params: 327,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24418, 233, 1)\n",
      "Epoch 1/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.7465 - accuracy: 0.7609\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83014, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 4s 5ms/step - loss: 0.7465 - accuracy: 0.7609 - val_loss: 0.5227 - val_accuracy: 0.8301\n",
      "Epoch 2/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.4914 - accuracy: 0.8483\n",
      "Epoch 00002: val_accuracy improved from 0.83014 to 0.85814, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.4905 - accuracy: 0.8485 - val_loss: 0.4525 - val_accuracy: 0.8581\n",
      "Epoch 3/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8752\n",
      "Epoch 00003: val_accuracy improved from 0.85814 to 0.86035, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.4213 - accuracy: 0.8751 - val_loss: 0.4483 - val_accuracy: 0.8604\n",
      "Epoch 4/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.3722 - accuracy: 0.8917\n",
      "Epoch 00004: val_accuracy improved from 0.86035 to 0.86809, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.3727 - accuracy: 0.8915 - val_loss: 0.4307 - val_accuracy: 0.8681\n",
      "Epoch 5/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.9020\n",
      "Epoch 00005: val_accuracy improved from 0.86809 to 0.88025, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 3s 4ms/step - loss: 0.3439 - accuracy: 0.9019 - val_loss: 0.4044 - val_accuracy: 0.8803\n",
      "Epoch 6/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.3205 - accuracy: 0.9110\n",
      "Epoch 00006: val_accuracy did not improve from 0.88025\n",
      "764/764 [==============================] - 6s 8ms/step - loss: 0.3205 - accuracy: 0.9110 - val_loss: 0.3984 - val_accuracy: 0.8755\n",
      "Epoch 7/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9201\n",
      "Epoch 00007: val_accuracy did not improve from 0.88025\n",
      "764/764 [==============================] - 4s 5ms/step - loss: 0.2958 - accuracy: 0.9201 - val_loss: 0.4128 - val_accuracy: 0.8721\n",
      "Epoch 8/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.9234\n",
      "Epoch 00008: val_accuracy improved from 0.88025 to 0.89278, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2805 - accuracy: 0.9235 - val_loss: 0.3608 - val_accuracy: 0.8928\n",
      "Epoch 9/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.9267\n",
      "Epoch 00009: val_accuracy did not improve from 0.89278\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2697 - accuracy: 0.9266 - val_loss: 0.3755 - val_accuracy: 0.8880\n",
      "Epoch 10/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9325\n",
      "Epoch 00010: val_accuracy improved from 0.89278 to 0.89462, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2579 - accuracy: 0.9322 - val_loss: 0.3572 - val_accuracy: 0.8946\n",
      "Epoch 11/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.2386 - accuracy: 0.9386\n",
      "Epoch 00011: val_accuracy improved from 0.89462 to 0.89757, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2382 - accuracy: 0.9389 - val_loss: 0.3481 - val_accuracy: 0.8976\n",
      "Epoch 12/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9388\n",
      "Epoch 00012: val_accuracy improved from 0.89757 to 0.89867, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2356 - accuracy: 0.9386 - val_loss: 0.3564 - val_accuracy: 0.8987\n",
      "Epoch 13/150\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.2179 - accuracy: 0.9441\n",
      "Epoch 00013: val_accuracy did not improve from 0.89867\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2179 - accuracy: 0.9441 - val_loss: 0.3646 - val_accuracy: 0.8972\n",
      "Epoch 14/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9467\n",
      "Epoch 00014: val_accuracy improved from 0.89867 to 0.89978, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.2130 - accuracy: 0.9466 - val_loss: 0.3497 - val_accuracy: 0.8998\n",
      "Epoch 15/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.2111 - accuracy: 0.9454\n",
      "Epoch 00015: val_accuracy improved from 0.89978 to 0.90052, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 3s 3ms/step - loss: 0.2108 - accuracy: 0.9455 - val_loss: 0.3472 - val_accuracy: 0.9005\n",
      "Epoch 16/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9488\n",
      "Epoch 00016: val_accuracy did not improve from 0.90052\n",
      "764/764 [==============================] - 3s 3ms/step - loss: 0.2006 - accuracy: 0.9488 - val_loss: 0.3839 - val_accuracy: 0.8917\n",
      "Epoch 17/150\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9522\n",
      "Epoch 00017: val_accuracy improved from 0.90052 to 0.90125, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1930 - accuracy: 0.9521 - val_loss: 0.3539 - val_accuracy: 0.9013\n",
      "Epoch 18/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.1851 - accuracy: 0.9553\n",
      "Epoch 00018: val_accuracy improved from 0.90125 to 0.90567, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1853 - accuracy: 0.9552 - val_loss: 0.3535 - val_accuracy: 0.9057\n",
      "Epoch 19/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9543\n",
      "Epoch 00019: val_accuracy improved from 0.90567 to 0.90789, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1848 - accuracy: 0.9542 - val_loss: 0.3405 - val_accuracy: 0.9079\n",
      "Epoch 20/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9563\n",
      "Epoch 00020: val_accuracy did not improve from 0.90789\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1793 - accuracy: 0.9563 - val_loss: 0.3566 - val_accuracy: 0.9001\n",
      "Epoch 21/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.1738 - accuracy: 0.9589\n",
      "Epoch 00021: val_accuracy improved from 0.90789 to 0.90825, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1738 - accuracy: 0.9589 - val_loss: 0.3391 - val_accuracy: 0.9083\n",
      "Epoch 22/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.9613\n",
      "Epoch 00022: val_accuracy did not improve from 0.90825\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1648 - accuracy: 0.9614 - val_loss: 0.3558 - val_accuracy: 0.9038\n",
      "Epoch 23/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.1629 - accuracy: 0.9633\n",
      "Epoch 00023: val_accuracy did not improve from 0.90825\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1626 - accuracy: 0.9635 - val_loss: 0.3554 - val_accuracy: 0.9024\n",
      "Epoch 24/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1601 - accuracy: 0.9629\n",
      "Epoch 00024: val_accuracy improved from 0.90825 to 0.91304, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1607 - accuracy: 0.9627 - val_loss: 0.3384 - val_accuracy: 0.9130\n",
      "Epoch 25/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9637\n",
      "Epoch 00025: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1569 - accuracy: 0.9636 - val_loss: 0.3406 - val_accuracy: 0.9031\n",
      "Epoch 26/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1508 - accuracy: 0.9657\n",
      "Epoch 00026: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1510 - accuracy: 0.9658 - val_loss: 0.3613 - val_accuracy: 0.9083\n",
      "Epoch 27/150\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9652\n",
      "Epoch 00027: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1504 - accuracy: 0.9652 - val_loss: 0.3489 - val_accuracy: 0.9071\n",
      "Epoch 28/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.9648\n",
      "Epoch 00028: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1525 - accuracy: 0.9649 - val_loss: 0.3559 - val_accuracy: 0.9035\n",
      "Epoch 29/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9661\n",
      "Epoch 00029: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1478 - accuracy: 0.9660 - val_loss: 0.3542 - val_accuracy: 0.9086\n",
      "Epoch 30/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9691\n",
      "Epoch 00030: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1406 - accuracy: 0.9692 - val_loss: 0.3426 - val_accuracy: 0.9123\n",
      "Epoch 31/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.1456 - accuracy: 0.9674\n",
      "Epoch 00031: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1457 - accuracy: 0.9673 - val_loss: 0.3382 - val_accuracy: 0.9108\n",
      "Epoch 32/150\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9677\n",
      "Epoch 00032: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1394 - accuracy: 0.9679 - val_loss: 0.3325 - val_accuracy: 0.9097\n",
      "Epoch 33/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.1346 - accuracy: 0.9705\n",
      "Epoch 00033: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1345 - accuracy: 0.9706 - val_loss: 0.3556 - val_accuracy: 0.9086\n",
      "Epoch 34/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9710\n",
      "Epoch 00034: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1352 - accuracy: 0.9710 - val_loss: 0.3410 - val_accuracy: 0.9119\n",
      "Epoch 35/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9704\n",
      "Epoch 00035: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1362 - accuracy: 0.9703 - val_loss: 0.3413 - val_accuracy: 0.9112\n",
      "Epoch 36/150\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.9708\n",
      "Epoch 00036: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.9708 - val_loss: 0.3550 - val_accuracy: 0.9060\n",
      "Epoch 37/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9750\n",
      "Epoch 00037: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1227 - accuracy: 0.9750 - val_loss: 0.3531 - val_accuracy: 0.9119\n",
      "Epoch 38/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9704\n",
      "Epoch 00038: val_accuracy did not improve from 0.91304\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9706 - val_loss: 0.3507 - val_accuracy: 0.9071\n",
      "Epoch 39/150\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9715\n",
      "Epoch 00039: val_accuracy improved from 0.91304 to 0.91341, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9712 - val_loss: 0.3496 - val_accuracy: 0.9134\n",
      "Epoch 40/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9732\n",
      "Epoch 00040: val_accuracy did not improve from 0.91341\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1234 - accuracy: 0.9733 - val_loss: 0.3571 - val_accuracy: 0.9123\n",
      "Epoch 41/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9748\n",
      "Epoch 00041: val_accuracy improved from 0.91341 to 0.91378, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1213 - accuracy: 0.9746 - val_loss: 0.3423 - val_accuracy: 0.9138\n",
      "Epoch 42/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759/764 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9765\n",
      "Epoch 00042: val_accuracy did not improve from 0.91378\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1170 - accuracy: 0.9766 - val_loss: 0.3623 - val_accuracy: 0.9108\n",
      "Epoch 43/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9754\n",
      "Epoch 00043: val_accuracy improved from 0.91378 to 0.91525, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1161 - accuracy: 0.9755 - val_loss: 0.3575 - val_accuracy: 0.9153\n",
      "Epoch 44/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9733\n",
      "Epoch 00044: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1207 - accuracy: 0.9735 - val_loss: 0.3734 - val_accuracy: 0.9075\n",
      "Epoch 45/150\n",
      "746/764 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9755\n",
      "Epoch 00045: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1171 - accuracy: 0.9753 - val_loss: 0.3696 - val_accuracy: 0.9112\n",
      "Epoch 46/150\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9766\n",
      "Epoch 00046: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1156 - accuracy: 0.9765 - val_loss: 0.3504 - val_accuracy: 0.9090\n",
      "Epoch 47/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9778\n",
      "Epoch 00047: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1127 - accuracy: 0.9777 - val_loss: 0.3613 - val_accuracy: 0.9075\n",
      "Epoch 48/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.1102 - accuracy: 0.9768\n",
      "Epoch 00048: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1103 - accuracy: 0.9768 - val_loss: 0.3633 - val_accuracy: 0.9108\n",
      "Epoch 49/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9752\n",
      "Epoch 00049: val_accuracy did not improve from 0.91525\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1148 - accuracy: 0.9751 - val_loss: 0.3585 - val_accuracy: 0.9119\n",
      "Epoch 50/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.1078 - accuracy: 0.9787\n",
      "Epoch 00050: val_accuracy improved from 0.91525 to 0.91746, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1077 - accuracy: 0.9787 - val_loss: 0.3424 - val_accuracy: 0.9175\n",
      "Epoch 51/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9772\n",
      "Epoch 00051: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1083 - accuracy: 0.9775 - val_loss: 0.3759 - val_accuracy: 0.9153\n",
      "Epoch 52/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9777\n",
      "Epoch 00052: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1089 - accuracy: 0.9775 - val_loss: 0.3581 - val_accuracy: 0.9156\n",
      "Epoch 53/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9795\n",
      "Epoch 00053: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1042 - accuracy: 0.9795 - val_loss: 0.3719 - val_accuracy: 0.9083\n",
      "Epoch 54/150\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.1056 - accuracy: 0.9791\n",
      "Epoch 00054: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1057 - accuracy: 0.9791 - val_loss: 0.3524 - val_accuracy: 0.9153\n",
      "Epoch 55/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9801\n",
      "Epoch 00055: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1006 - accuracy: 0.9801 - val_loss: 0.3720 - val_accuracy: 0.9127\n",
      "Epoch 56/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9791\n",
      "Epoch 00056: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1034 - accuracy: 0.9790 - val_loss: 0.3536 - val_accuracy: 0.9119\n",
      "Epoch 57/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9804\n",
      "Epoch 00057: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9803 - val_loss: 0.3682 - val_accuracy: 0.9094\n",
      "Epoch 58/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.1018 - accuracy: 0.9803\n",
      "Epoch 00058: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.1014 - accuracy: 0.9805 - val_loss: 0.3537 - val_accuracy: 0.9112\n",
      "Epoch 59/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.9820\n",
      "Epoch 00059: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0954 - accuracy: 0.9820 - val_loss: 0.3593 - val_accuracy: 0.9130\n",
      "Epoch 60/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0965 - accuracy: 0.9816\n",
      "Epoch 00060: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0966 - accuracy: 0.9816 - val_loss: 0.3748 - val_accuracy: 0.9112\n",
      "Epoch 61/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0978 - accuracy: 0.9814\n",
      "Epoch 00061: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0977 - accuracy: 0.9815 - val_loss: 0.3528 - val_accuracy: 0.9127\n",
      "Epoch 62/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0969 - accuracy: 0.9810\n",
      "Epoch 00062: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0967 - accuracy: 0.9812 - val_loss: 0.3675 - val_accuracy: 0.9130\n",
      "Epoch 63/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9800\n",
      "Epoch 00063: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0983 - accuracy: 0.9800 - val_loss: 0.3744 - val_accuracy: 0.9108\n",
      "Epoch 64/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.0927 - accuracy: 0.9822\n",
      "Epoch 00064: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0932 - accuracy: 0.9821 - val_loss: 0.3623 - val_accuracy: 0.9116\n",
      "Epoch 65/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9810\n",
      "Epoch 00065: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0988 - accuracy: 0.9811 - val_loss: 0.3525 - val_accuracy: 0.9149\n",
      "Epoch 66/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0934 - accuracy: 0.9818\n",
      "Epoch 00066: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0933 - accuracy: 0.9819 - val_loss: 0.3545 - val_accuracy: 0.9141\n",
      "Epoch 67/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9813\n",
      "Epoch 00067: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0943 - accuracy: 0.9815 - val_loss: 0.3739 - val_accuracy: 0.9116\n",
      "Epoch 68/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9813\n",
      "Epoch 00068: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0931 - accuracy: 0.9814 - val_loss: 0.3565 - val_accuracy: 0.9108\n",
      "Epoch 69/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9825\n",
      "Epoch 00069: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0933 - accuracy: 0.9825 - val_loss: 0.3528 - val_accuracy: 0.9105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9833\n",
      "Epoch 00070: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0889 - accuracy: 0.9834 - val_loss: 0.3716 - val_accuracy: 0.9127\n",
      "Epoch 71/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9829\n",
      "Epoch 00071: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0909 - accuracy: 0.9828 - val_loss: 0.3647 - val_accuracy: 0.9141\n",
      "Epoch 72/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9830\n",
      "Epoch 00072: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0897 - accuracy: 0.9828 - val_loss: 0.3797 - val_accuracy: 0.9068\n",
      "Epoch 73/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9841\n",
      "Epoch 00073: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0871 - accuracy: 0.9842 - val_loss: 0.3767 - val_accuracy: 0.9097\n",
      "Epoch 74/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9822\n",
      "Epoch 00074: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0891 - accuracy: 0.9821 - val_loss: 0.3596 - val_accuracy: 0.9156\n",
      "Epoch 75/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9823\n",
      "Epoch 00075: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0886 - accuracy: 0.9823 - val_loss: 0.3597 - val_accuracy: 0.9141\n",
      "Epoch 76/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9828\n",
      "Epoch 00076: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0897 - accuracy: 0.9828 - val_loss: 0.3743 - val_accuracy: 0.9127\n",
      "Epoch 77/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9826\n",
      "Epoch 00077: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0905 - accuracy: 0.9826 - val_loss: 0.3664 - val_accuracy: 0.9153\n",
      "Epoch 78/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9832\n",
      "Epoch 00078: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9833 - val_loss: 0.3671 - val_accuracy: 0.9116\n",
      "Epoch 79/150\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9825\n",
      "Epoch 00079: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9824 - val_loss: 0.3708 - val_accuracy: 0.9119\n",
      "Epoch 80/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9840\n",
      "Epoch 00080: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0859 - accuracy: 0.9840 - val_loss: 0.3890 - val_accuracy: 0.9086\n",
      "Epoch 81/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9840\n",
      "Epoch 00081: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0843 - accuracy: 0.9840 - val_loss: 0.3628 - val_accuracy: 0.9112\n",
      "Epoch 82/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0806 - accuracy: 0.9851\n",
      "Epoch 00082: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0808 - accuracy: 0.9849 - val_loss: 0.3532 - val_accuracy: 0.9160\n",
      "Epoch 83/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9855\n",
      "Epoch 00083: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9855 - val_loss: 0.3663 - val_accuracy: 0.9153\n",
      "Epoch 84/150\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9820\n",
      "Epoch 00084: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0882 - accuracy: 0.9820 - val_loss: 0.3906 - val_accuracy: 0.9116\n",
      "Epoch 85/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9832\n",
      "Epoch 00085: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0843 - accuracy: 0.9831 - val_loss: 0.3688 - val_accuracy: 0.9138\n",
      "Epoch 86/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0830 - accuracy: 0.9842\n",
      "Epoch 00086: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0829 - accuracy: 0.9843 - val_loss: 0.3676 - val_accuracy: 0.9149\n",
      "Epoch 87/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9852\n",
      "Epoch 00087: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9852 - val_loss: 0.3689 - val_accuracy: 0.9145\n",
      "Epoch 88/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9856\n",
      "Epoch 00088: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0790 - accuracy: 0.9857 - val_loss: 0.3901 - val_accuracy: 0.9079\n",
      "Epoch 89/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9862\n",
      "Epoch 00089: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9863 - val_loss: 0.3778 - val_accuracy: 0.9153\n",
      "Epoch 90/150\n",
      "745/764 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9869\n",
      "Epoch 00090: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0762 - accuracy: 0.9869 - val_loss: 0.3565 - val_accuracy: 0.9145\n",
      "Epoch 91/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9867\n",
      "Epoch 00091: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9866 - val_loss: 0.3644 - val_accuracy: 0.9145\n",
      "Epoch 92/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9869\n",
      "Epoch 00092: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9870 - val_loss: 0.3704 - val_accuracy: 0.9116\n",
      "Epoch 93/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9850\n",
      "Epoch 00093: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0822 - accuracy: 0.9850 - val_loss: 0.3753 - val_accuracy: 0.9167\n",
      "Epoch 94/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9850\n",
      "Epoch 00094: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0784 - accuracy: 0.9851 - val_loss: 0.3823 - val_accuracy: 0.9145\n",
      "Epoch 95/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9867\n",
      "Epoch 00095: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9867 - val_loss: 0.3874 - val_accuracy: 0.9105\n",
      "Epoch 96/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9852\n",
      "Epoch 00096: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0783 - accuracy: 0.9851 - val_loss: 0.3758 - val_accuracy: 0.9112\n",
      "Epoch 97/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9874\n",
      "Epoch 00097: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0735 - accuracy: 0.9873 - val_loss: 0.3783 - val_accuracy: 0.9123\n",
      "Epoch 98/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9870\n",
      "Epoch 00098: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9871 - val_loss: 0.3870 - val_accuracy: 0.9156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9843\n",
      "Epoch 00099: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9844 - val_loss: 0.3903 - val_accuracy: 0.9123\n",
      "Epoch 100/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9867\n",
      "Epoch 00100: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0747 - accuracy: 0.9866 - val_loss: 0.3732 - val_accuracy: 0.9149\n",
      "Epoch 101/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9876\n",
      "Epoch 00101: val_accuracy did not improve from 0.91746\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0720 - accuracy: 0.9877 - val_loss: 0.3844 - val_accuracy: 0.9123\n",
      "Epoch 102/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9881\n",
      "Epoch 00102: val_accuracy improved from 0.91746 to 0.91968, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9882 - val_loss: 0.3739 - val_accuracy: 0.9197\n",
      "Epoch 103/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9877\n",
      "Epoch 00103: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9877 - val_loss: 0.3618 - val_accuracy: 0.9141\n",
      "Epoch 104/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9868\n",
      "Epoch 00104: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9867 - val_loss: 0.3746 - val_accuracy: 0.9171\n",
      "Epoch 105/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9873\n",
      "Epoch 00105: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9873 - val_loss: 0.3573 - val_accuracy: 0.9175\n",
      "Epoch 106/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9865\n",
      "Epoch 00106: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9865 - val_loss: 0.3769 - val_accuracy: 0.9153\n",
      "Epoch 107/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9854\n",
      "Epoch 00107: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9854 - val_loss: 0.3673 - val_accuracy: 0.9156\n",
      "Epoch 108/150\n",
      "752/764 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9865\n",
      "Epoch 00108: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9865 - val_loss: 0.3836 - val_accuracy: 0.9160\n",
      "Epoch 109/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9865\n",
      "Epoch 00109: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0725 - accuracy: 0.9865 - val_loss: 0.3744 - val_accuracy: 0.9130\n",
      "Epoch 110/150\n",
      "763/764 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9853\n",
      "Epoch 00110: val_accuracy did not improve from 0.91968\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0762 - accuracy: 0.9853 - val_loss: 0.3682 - val_accuracy: 0.9134\n",
      "Epoch 111/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9868\n",
      "Epoch 00111: val_accuracy improved from 0.91968 to 0.92004, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0722 - accuracy: 0.9868 - val_loss: 0.3661 - val_accuracy: 0.9200\n",
      "Epoch 112/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9863\n",
      "Epoch 00112: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9864 - val_loss: 0.3795 - val_accuracy: 0.9079\n",
      "Epoch 113/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9877\n",
      "Epoch 00113: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9878 - val_loss: 0.3934 - val_accuracy: 0.9145\n",
      "Epoch 114/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9877\n",
      "Epoch 00114: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0717 - accuracy: 0.9877 - val_loss: 0.3774 - val_accuracy: 0.9160\n",
      "Epoch 115/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9881\n",
      "Epoch 00115: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0682 - accuracy: 0.9880 - val_loss: 0.3739 - val_accuracy: 0.9145\n",
      "Epoch 116/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9870\n",
      "Epoch 00116: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0708 - accuracy: 0.9870 - val_loss: 0.3761 - val_accuracy: 0.9200\n",
      "Epoch 117/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9871\n",
      "Epoch 00117: val_accuracy did not improve from 0.92004\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0702 - accuracy: 0.9871 - val_loss: 0.3742 - val_accuracy: 0.9156\n",
      "Epoch 118/150\n",
      "764/764 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9869\n",
      "Epoch 00118: val_accuracy improved from 0.92004 to 0.92115, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0712 - accuracy: 0.9869 - val_loss: 0.3606 - val_accuracy: 0.9211\n",
      "Epoch 119/150\n",
      "747/764 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9877\n",
      "Epoch 00119: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9876 - val_loss: 0.3649 - val_accuracy: 0.9189\n",
      "Epoch 120/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9906\n",
      "Epoch 00120: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0615 - accuracy: 0.9906 - val_loss: 0.3717 - val_accuracy: 0.9186\n",
      "Epoch 121/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9892\n",
      "Epoch 00121: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0642 - accuracy: 0.9892 - val_loss: 0.3785 - val_accuracy: 0.9178\n",
      "Epoch 122/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9886\n",
      "Epoch 00122: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0653 - accuracy: 0.9887 - val_loss: 0.3729 - val_accuracy: 0.9145\n",
      "Epoch 123/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9891\n",
      "Epoch 00123: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0645 - accuracy: 0.9891 - val_loss: 0.3862 - val_accuracy: 0.9149\n",
      "Epoch 124/150\n",
      "748/764 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9895\n",
      "Epoch 00124: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9896 - val_loss: 0.3938 - val_accuracy: 0.9171\n",
      "Epoch 125/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9880\n",
      "Epoch 00125: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9880 - val_loss: 0.3821 - val_accuracy: 0.9153\n",
      "Epoch 126/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9892\n",
      "Epoch 00126: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0641 - accuracy: 0.9893 - val_loss: 0.3867 - val_accuracy: 0.9127\n",
      "Epoch 127/150\n",
      "759/764 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9881\n",
      "Epoch 00127: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9881 - val_loss: 0.4080 - val_accuracy: 0.9134\n",
      "Epoch 128/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9873\n",
      "Epoch 00128: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0693 - accuracy: 0.9872 - val_loss: 0.3694 - val_accuracy: 0.9153\n",
      "Epoch 129/150\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9884\n",
      "Epoch 00129: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0653 - accuracy: 0.9884 - val_loss: 0.3890 - val_accuracy: 0.9130\n",
      "Epoch 130/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9900\n",
      "Epoch 00130: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0616 - accuracy: 0.9900 - val_loss: 0.3814 - val_accuracy: 0.9156\n",
      "Epoch 131/150\n",
      "758/764 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9881\n",
      "Epoch 00131: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9881 - val_loss: 0.3861 - val_accuracy: 0.9156\n",
      "Epoch 132/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9886\n",
      "Epoch 00132: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0637 - accuracy: 0.9886 - val_loss: 0.3969 - val_accuracy: 0.9130\n",
      "Epoch 133/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9881\n",
      "Epoch 00133: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0679 - accuracy: 0.9881 - val_loss: 0.3788 - val_accuracy: 0.9193\n",
      "Epoch 134/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9892\n",
      "Epoch 00134: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9892 - val_loss: 0.3760 - val_accuracy: 0.9175\n",
      "Epoch 135/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9877\n",
      "Epoch 00135: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0667 - accuracy: 0.9878 - val_loss: 0.4076 - val_accuracy: 0.9141\n",
      "Epoch 136/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9889\n",
      "Epoch 00136: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0626 - accuracy: 0.9889 - val_loss: 0.3936 - val_accuracy: 0.9119\n",
      "Epoch 137/150\n",
      "762/764 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9897\n",
      "Epoch 00137: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0616 - accuracy: 0.9897 - val_loss: 0.3803 - val_accuracy: 0.9171\n",
      "Epoch 138/150\n",
      "761/764 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9892\n",
      "Epoch 00138: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9892 - val_loss: 0.3825 - val_accuracy: 0.9193\n",
      "Epoch 139/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9889\n",
      "Epoch 00139: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0646 - accuracy: 0.9889 - val_loss: 0.3731 - val_accuracy: 0.9164\n",
      "Epoch 140/150\n",
      "760/764 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9900\n",
      "Epoch 00140: val_accuracy did not improve from 0.92115\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9901 - val_loss: 0.3859 - val_accuracy: 0.9145\n",
      "Epoch 141/150\n",
      "755/764 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9896\n",
      "Epoch 00141: val_accuracy improved from 0.92115 to 0.92189, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0623 - accuracy: 0.9896 - val_loss: 0.3830 - val_accuracy: 0.9219\n",
      "Epoch 142/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9899\n",
      "Epoch 00142: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9899 - val_loss: 0.3943 - val_accuracy: 0.9193\n",
      "Epoch 143/150\n",
      "756/764 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9879\n",
      "Epoch 00143: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9879 - val_loss: 0.3847 - val_accuracy: 0.9141\n",
      "Epoch 144/150\n",
      "754/764 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9890\n",
      "Epoch 00144: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0627 - accuracy: 0.9889 - val_loss: 0.3733 - val_accuracy: 0.9186\n",
      "Epoch 145/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9888\n",
      "Epoch 00145: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0631 - accuracy: 0.9887 - val_loss: 0.3830 - val_accuracy: 0.9189\n",
      "Epoch 146/150\n",
      "757/764 [============================>.] - ETA: 0s - loss: 0.0579 - accuracy: 0.9905\n",
      "Epoch 00146: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0579 - accuracy: 0.9905 - val_loss: 0.3885 - val_accuracy: 0.9171\n",
      "Epoch 147/150\n",
      "753/764 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9871\n",
      "Epoch 00147: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0654 - accuracy: 0.9871 - val_loss: 0.3860 - val_accuracy: 0.9171\n",
      "Epoch 148/150\n",
      "750/764 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9892\n",
      "Epoch 00148: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0629 - accuracy: 0.9890 - val_loss: 0.3990 - val_accuracy: 0.9167\n",
      "Epoch 149/150\n",
      "749/764 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9909\n",
      "Epoch 00149: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0583 - accuracy: 0.9907 - val_loss: 0.3927 - val_accuracy: 0.9156\n",
      "Epoch 150/150\n",
      "751/764 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9889\n",
      "Epoch 00150: val_accuracy did not improve from 0.92189\n",
      "764/764 [==============================] - 2s 3ms/step - loss: 0.0626 - accuracy: 0.9888 - val_loss: 0.4004 - val_accuracy: 0.9108\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:42<00:00, 261.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.76      0.83      0.79       840\n",
      "        car_horn       0.87      0.96      0.91       301\n",
      "children_playing       0.75      0.79      0.77       700\n",
      "        dog_bark       0.81      0.77      0.79       700\n",
      "           siren       0.90      0.78      0.84       833\n",
      "\n",
      "        accuracy                           0.81      3374\n",
      "       macro avg       0.82      0.83      0.82      3374\n",
      "    weighted avg       0.81      0.81      0.81      3374\n",
      "\n",
      "Validation fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(26733, 375)\n",
      "X_val_norm shape.....:(3773, 375)\n",
      "\n",
      "Sum of elements: 0.9803348031146931\n",
      "Number of elements summed: 237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 237)               56406     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 237)               56406     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 237)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               178500    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 295,067\n",
      "Trainable params: 295,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24059, 237)\n",
      "Epoch 1/350\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.7819 - accuracy: 0.7090\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83583, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 2ms/step - loss: 0.7787 - accuracy: 0.7105 - val_loss: 0.4704 - val_accuracy: 0.8358\n",
      "Epoch 2/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.4350 - accuracy: 0.8470\n",
      "Epoch 00002: val_accuracy improved from 0.83583 to 0.87360, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.4338 - accuracy: 0.8475 - val_loss: 0.3601 - val_accuracy: 0.8736\n",
      "Epoch 3/350\n",
      "722/752 [===========================>..] - ETA: 0s - loss: 0.3344 - accuracy: 0.8806\n",
      "Epoch 00003: val_accuracy improved from 0.87360 to 0.89155, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.8807 - val_loss: 0.3039 - val_accuracy: 0.8915\n",
      "Epoch 4/350\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.9037\n",
      "Epoch 00004: val_accuracy improved from 0.89155 to 0.90726, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.2772 - accuracy: 0.9037 - val_loss: 0.2643 - val_accuracy: 0.9073\n",
      "Epoch 5/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.2256 - accuracy: 0.9217\n",
      "Epoch 00005: val_accuracy improved from 0.90726 to 0.91324, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.2249 - accuracy: 0.9219 - val_loss: 0.2416 - val_accuracy: 0.9132\n",
      "Epoch 6/350\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.9347\n",
      "Epoch 00006: val_accuracy improved from 0.91324 to 0.92221, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9347 - val_loss: 0.2234 - val_accuracy: 0.9222\n",
      "Epoch 7/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9458\n",
      "Epoch 00007: val_accuracy improved from 0.92221 to 0.93231, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.1602 - accuracy: 0.9459 - val_loss: 0.2060 - val_accuracy: 0.9323\n",
      "Epoch 8/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.9563\n",
      "Epoch 00008: val_accuracy improved from 0.93231 to 0.93455, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.1314 - accuracy: 0.9562 - val_loss: 0.1985 - val_accuracy: 0.9346\n",
      "Epoch 9/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.1104 - accuracy: 0.9640\n",
      "Epoch 00009: val_accuracy improved from 0.93455 to 0.93680, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.1094 - accuracy: 0.9645 - val_loss: 0.1920 - val_accuracy: 0.9368\n",
      "Epoch 10/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9705\n",
      "Epoch 00010: val_accuracy improved from 0.93680 to 0.93979, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0947 - accuracy: 0.9702 - val_loss: 0.1874 - val_accuracy: 0.9398\n",
      "Epoch 11/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9742\n",
      "Epoch 00011: val_accuracy improved from 0.93979 to 0.94241, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0802 - accuracy: 0.9742 - val_loss: 0.1768 - val_accuracy: 0.9424\n",
      "Epoch 12/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9799\n",
      "Epoch 00012: val_accuracy did not improve from 0.94241\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0654 - accuracy: 0.9800 - val_loss: 0.1815 - val_accuracy: 0.9405\n",
      "Epoch 13/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9830\n",
      "Epoch 00013: val_accuracy improved from 0.94241 to 0.94428, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0561 - accuracy: 0.9830 - val_loss: 0.1773 - val_accuracy: 0.9443\n",
      "Epoch 14/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9867\n",
      "Epoch 00014: val_accuracy improved from 0.94428 to 0.94615, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0457 - accuracy: 0.9866 - val_loss: 0.1793 - val_accuracy: 0.9461\n",
      "Epoch 15/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9875\n",
      "Epoch 00015: val_accuracy improved from 0.94615 to 0.94914, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9874 - val_loss: 0.1797 - val_accuracy: 0.9491\n",
      "Epoch 16/350\n",
      "723/752 [===========================>..] - ETA: 0s - loss: 0.0358 - accuracy: 0.9898\n",
      "Epoch 00016: val_accuracy did not improve from 0.94914\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0355 - accuracy: 0.9901 - val_loss: 0.1805 - val_accuracy: 0.9491\n",
      "Epoch 17/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9925\n",
      "Epoch 00017: val_accuracy improved from 0.94914 to 0.94951, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0279 - accuracy: 0.9924 - val_loss: 0.1817 - val_accuracy: 0.9495\n",
      "Epoch 18/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9930\n",
      "Epoch 00018: val_accuracy did not improve from 0.94951\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0256 - accuracy: 0.9931 - val_loss: 0.1881 - val_accuracy: 0.9480\n",
      "Epoch 19/350\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0205 - accuracy: 0.9941\n",
      "Epoch 00019: val_accuracy improved from 0.94951 to 0.95101, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9941 - val_loss: 0.1926 - val_accuracy: 0.9510\n",
      "Epoch 20/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9949\n",
      "Epoch 00020: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9949 - val_loss: 0.1881 - val_accuracy: 0.9488\n",
      "Epoch 21/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9953\n",
      "Epoch 00021: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1966 - val_accuracy: 0.9506\n",
      "Epoch 22/350\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9966\n",
      "Epoch 00022: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 0.2042 - val_accuracy: 0.9499\n",
      "Epoch 23/350\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9967\n",
      "Epoch 00023: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0143 - accuracy: 0.9966 - val_loss: 0.2042 - val_accuracy: 0.9469\n",
      "Epoch 24/350\n",
      "729/752 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9973\n",
      "Epoch 00024: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0121 - accuracy: 0.9973 - val_loss: 0.2025 - val_accuracy: 0.9503\n",
      "Epoch 25/350\n",
      "727/752 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9967\n",
      "Epoch 00025: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9968 - val_loss: 0.2036 - val_accuracy: 0.9488\n",
      "Epoch 26/350\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9976\n",
      "Epoch 00026: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9975 - val_loss: 0.2140 - val_accuracy: 0.9491\n",
      "Epoch 27/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9979\n",
      "Epoch 00027: val_accuracy did not improve from 0.95101\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.2091 - val_accuracy: 0.9499\n",
      "Epoch 28/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9984\n",
      "Epoch 00028: val_accuracy improved from 0.95101 to 0.95251, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.2108 - val_accuracy: 0.9525\n",
      "Epoch 29/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9984\n",
      "Epoch 00029: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.2196 - val_accuracy: 0.9521\n",
      "Epoch 30/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9980\n",
      "Epoch 00030: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.2173 - val_accuracy: 0.9514\n",
      "Epoch 31/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9988\n",
      "Epoch 00031: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.2230 - val_accuracy: 0.9506\n",
      "Epoch 32/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9980\n",
      "Epoch 00032: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.2210 - val_accuracy: 0.9495\n",
      "Epoch 33/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9988\n",
      "Epoch 00033: val_accuracy did not improve from 0.95251\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 0.2161 - val_accuracy: 0.9518\n",
      "Epoch 34/350\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9991\n",
      "Epoch 00034: val_accuracy improved from 0.95251 to 0.95325, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.2125 - val_accuracy: 0.9533\n",
      "Epoch 35/350\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9992\n",
      "Epoch 00035: val_accuracy did not improve from 0.95325\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.2135 - val_accuracy: 0.9518\n",
      "Epoch 36/350\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9991\n",
      "Epoch 00036: val_accuracy did not improve from 0.95325\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.2133 - val_accuracy: 0.9514\n",
      "Epoch 37/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 00037: val_accuracy improved from 0.95325 to 0.95400, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.2203 - val_accuracy: 0.9540\n",
      "Epoch 38/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9989\n",
      "Epoch 00038: val_accuracy did not improve from 0.95400\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2247 - val_accuracy: 0.9529\n",
      "Epoch 39/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9997\n",
      "Epoch 00039: val_accuracy did not improve from 0.95400\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 0.2240 - val_accuracy: 0.9529\n",
      "Epoch 40/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.95400\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.2262 - val_accuracy: 0.9518\n",
      "Epoch 41/350\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 00041: val_accuracy improved from 0.95400 to 0.95475, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.2234 - val_accuracy: 0.9547\n",
      "Epoch 42/350\n",
      "732/752 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9993\n",
      "Epoch 00042: val_accuracy did not improve from 0.95475\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.2223 - val_accuracy: 0.9521\n",
      "Epoch 43/350\n",
      "727/752 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00043: val_accuracy did not improve from 0.95475\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.2224 - val_accuracy: 0.9529\n",
      "Epoch 44/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726/752 [===========================>..] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00044: val_accuracy did not improve from 0.95475\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2223 - val_accuracy: 0.9529\n",
      "Epoch 45/350\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9998\n",
      "Epoch 00045: val_accuracy improved from 0.95475 to 0.95587, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.2249 - val_accuracy: 0.9559\n",
      "Epoch 46/350\n",
      "725/752 [===========================>..] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 00046: val_accuracy did not improve from 0.95587\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.2310 - val_accuracy: 0.9510\n",
      "Epoch 47/350\n",
      "726/752 [===========================>..] - ETA: 0s - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 00047: val_accuracy did not improve from 0.95587\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.2241 - val_accuracy: 0.9525\n",
      "Epoch 48/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00048: val_accuracy improved from 0.95587 to 0.95662, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2236 - val_accuracy: 0.9566\n",
      "Epoch 49/350\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9997\n",
      "Epoch 00049: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.2305 - val_accuracy: 0.9555\n",
      "Epoch 50/350\n",
      "728/752 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 00050: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2308 - val_accuracy: 0.9555\n",
      "Epoch 51/350\n",
      "722/752 [===========================>..] - ETA: 0s - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 00051: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.2299 - val_accuracy: 0.9555\n",
      "Epoch 52/350\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9998\n",
      "Epoch 00052: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.2340 - val_accuracy: 0.9544\n",
      "Epoch 53/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9997\n",
      "Epoch 00053: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.2396 - val_accuracy: 0.9544\n",
      "Epoch 54/350\n",
      "724/752 [===========================>..] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00054: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.2367 - val_accuracy: 0.9540\n",
      "Epoch 55/350\n",
      "724/752 [===========================>..] - ETA: 0s - loss: 0.0017 - accuracy: 0.9999\n",
      "Epoch 00055: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.2387 - val_accuracy: 0.9544\n",
      "Epoch 56/350\n",
      "733/752 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00056: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2475 - val_accuracy: 0.9510\n",
      "Epoch 57/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00057: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2433 - val_accuracy: 0.9559\n",
      "Epoch 58/350\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2426 - val_accuracy: 0.9547\n",
      "Epoch 59/350\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2436 - val_accuracy: 0.9559\n",
      "Epoch 60/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00060: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.2546 - val_accuracy: 0.9551\n",
      "Epoch 61/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00061: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2499 - val_accuracy: 0.9547\n",
      "Epoch 62/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9996\n",
      "Epoch 00062: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2456 - val_accuracy: 0.9559\n",
      "Epoch 63/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00063: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2444 - val_accuracy: 0.9547\n",
      "Epoch 64/350\n",
      "727/752 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 00064: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2538 - val_accuracy: 0.9540\n",
      "Epoch 65/350\n",
      "720/752 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00065: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2498 - val_accuracy: 0.9547\n",
      "Epoch 66/350\n",
      "724/752 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00066: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2513 - val_accuracy: 0.9547\n",
      "Epoch 67/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 00067: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.2536 - val_accuracy: 0.9544\n",
      "Epoch 68/350\n",
      "731/752 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.2560 - val_accuracy: 0.9521\n",
      "Epoch 69/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2566 - val_accuracy: 0.9533\n",
      "Epoch 70/350\n",
      "724/752 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00070: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2572 - val_accuracy: 0.9529\n",
      "Epoch 71/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00071: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2603 - val_accuracy: 0.9547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00072: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2529 - val_accuracy: 0.9544\n",
      "Epoch 73/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00073: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2542 - val_accuracy: 0.9529\n",
      "Epoch 74/350\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2559 - val_accuracy: 0.9525\n",
      "Epoch 75/350\n",
      "732/752 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00075: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.2612 - val_accuracy: 0.9529\n",
      "Epoch 76/350\n",
      "740/752 [============================>.] - ETA: 0s - loss: 9.3636e-04 - accuracy: 0.9999\n",
      "Epoch 00076: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.7911e-04 - accuracy: 0.9999 - val_loss: 0.2552 - val_accuracy: 0.9525\n",
      "Epoch 77/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00077: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2547 - val_accuracy: 0.9529\n",
      "Epoch 78/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 7.7684e-04 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.7569e-04 - accuracy: 0.9999 - val_loss: 0.2528 - val_accuracy: 0.9521\n",
      "Epoch 79/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00079: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2587 - val_accuracy: 0.9533\n",
      "Epoch 80/350\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00080: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2631 - val_accuracy: 0.9536\n",
      "Epoch 81/350\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00081: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2556 - val_accuracy: 0.9533\n",
      "Epoch 82/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00082: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2536 - val_accuracy: 0.9529\n",
      "Epoch 83/350\n",
      "720/752 [===========================>..] - ETA: 0s - loss: 8.9969e-04 - accuracy: 0.9999\n",
      "Epoch 00083: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.0779e-04 - accuracy: 0.9999 - val_loss: 0.2567 - val_accuracy: 0.9533\n",
      "Epoch 84/350\n",
      "721/752 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998  \n",
      "Epoch 00084: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2594 - val_accuracy: 0.9544\n",
      "Epoch 85/350\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00085: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2581 - val_accuracy: 0.9544\n",
      "Epoch 86/350\n",
      "739/752 [============================>.] - ETA: 0s - loss: 9.3011e-04 - accuracy: 0.9999\n",
      "Epoch 00086: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.3475e-04 - accuracy: 0.9999 - val_loss: 0.2606 - val_accuracy: 0.9529\n",
      "Epoch 87/350\n",
      "742/752 [============================>.] - ETA: 0s - loss: 9.5449e-04 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.5361e-04 - accuracy: 0.9998 - val_loss: 0.2639 - val_accuracy: 0.9536\n",
      "Epoch 88/350\n",
      "743/752 [============================>.] - ETA: 0s - loss: 9.0137e-04 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.9457e-04 - accuracy: 0.9998 - val_loss: 0.2633 - val_accuracy: 0.9544\n",
      "Epoch 89/350\n",
      "719/752 [===========================>..] - ETA: 0s - loss: 9.2043e-04 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.0831e-04 - accuracy: 0.9998 - val_loss: 0.2608 - val_accuracy: 0.9540\n",
      "Epoch 90/350\n",
      "751/752 [============================>.] - ETA: 0s - loss: 8.6417e-04 - accuracy: 0.9999\n",
      "Epoch 00090: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.6333e-04 - accuracy: 0.9999 - val_loss: 0.2591 - val_accuracy: 0.9566\n",
      "Epoch 91/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 9.4409e-04 - accuracy: 0.9998\n",
      "Epoch 00091: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.4071e-04 - accuracy: 0.9998 - val_loss: 0.2644 - val_accuracy: 0.9551\n",
      "Epoch 92/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 7.3997e-04 - accuracy: 1.0000\n",
      "Epoch 00092: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.3488e-04 - accuracy: 1.0000 - val_loss: 0.2582 - val_accuracy: 0.9544\n",
      "Epoch 93/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 8.6813e-04 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.6743e-04 - accuracy: 0.9998 - val_loss: 0.2574 - val_accuracy: 0.9566\n",
      "Epoch 94/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 9.4020e-04 - accuracy: 0.9998\n",
      "Epoch 00094: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.3513e-04 - accuracy: 0.9998 - val_loss: 0.2559 - val_accuracy: 0.9547\n",
      "Epoch 95/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 8.2032e-04 - accuracy: 0.9998\n",
      "Epoch 00095: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.1385e-04 - accuracy: 0.9998 - val_loss: 0.2580 - val_accuracy: 0.9559\n",
      "Epoch 96/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2560 - val_accuracy: 0.9555\n",
      "Epoch 97/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 7.3361e-04 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.9007e-04 - accuracy: 0.9999 - val_loss: 0.2539 - val_accuracy: 0.9547\n",
      "Epoch 98/350\n",
      "741/752 [============================>.] - ETA: 0s - loss: 7.0821e-04 - accuracy: 0.9999\n",
      "Epoch 00098: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.4546e-04 - accuracy: 0.9999 - val_loss: 0.2513 - val_accuracy: 0.9551\n",
      "Epoch 99/350\n",
      "744/752 [============================>.] - ETA: 0s - loss: 9.1096e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 9.0452e-04 - accuracy: 0.9998 - val_loss: 0.2572 - val_accuracy: 0.9551\n",
      "Epoch 100/350\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00100: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2571 - val_accuracy: 0.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/350\n",
      "746/752 [============================>.] - ETA: 0s - loss: 7.7267e-04 - accuracy: 0.9999\n",
      "Epoch 00101: val_accuracy did not improve from 0.95662\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.6910e-04 - accuracy: 0.9999 - val_loss: 0.2536 - val_accuracy: 0.9540\n",
      "Epoch 102/350\n",
      "749/752 [============================>.] - ETA: 0s - loss: 8.0665e-04 - accuracy: 0.9999\n",
      "Epoch 00102: val_accuracy improved from 0.95662 to 0.95699, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 8.0448e-04 - accuracy: 0.9999 - val_loss: 0.2488 - val_accuracy: 0.9570\n",
      "Epoch 103/350\n",
      "747/752 [============================>.] - ETA: 0s - loss: 7.0428e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 0.95699\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 7.0140e-04 - accuracy: 1.0000 - val_loss: 0.2515 - val_accuracy: 0.9544\n",
      "Epoch 104/350\n",
      "730/752 [============================>.] - ETA: 0s - loss: 6.5164e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95699\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 6.6094e-04 - accuracy: 0.9999 - val_loss: 0.2562 - val_accuracy: 0.9540\n",
      "Epoch 105/350\n",
      "750/752 [============================>.] - ETA: 0s - loss: 6.0028e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.95699\n",
      "Restoring model weights from the end of the best epoch.\n",
      "752/752 [==============================] - 1s 2ms/step - loss: 5.9939e-04 - accuracy: 1.0000 - val_loss: 0.2632 - val_accuracy: 0.9544\n",
      "Epoch 00105: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:15<02:15, 135.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.71      0.79      0.75       798\n",
      "        car_horn       0.82      0.59      0.68       413\n",
      "children_playing       0.66      0.64      0.65       700\n",
      "        dog_bark       0.69      0.84      0.76       700\n",
      "           siren       0.91      0.83      0.87      1162\n",
      "\n",
      "        accuracy                           0.76      3773\n",
      "       macro avg       0.76      0.74      0.74      3773\n",
      "    weighted avg       0.77      0.76      0.76      3773\n",
      "\n",
      "Model: \"Model_CNN_1D_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 231, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 231, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 231, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 115, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 115, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6440)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                322050    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 333,091\n",
      "Trainable params: 333,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24059, 237, 1)\n",
      "Epoch 1/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.7161 - accuracy: 0.7722\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85116, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 4s 5ms/step - loss: 0.7161 - accuracy: 0.7722 - val_loss: 0.5091 - val_accuracy: 0.8512\n",
      "Epoch 2/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.8568\n",
      "Epoch 00002: val_accuracy improved from 0.85116 to 0.87472, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.4741 - accuracy: 0.8569 - val_loss: 0.4358 - val_accuracy: 0.8747\n",
      "Epoch 3/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8791\n",
      "Epoch 00003: val_accuracy did not improve from 0.87472\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.4073 - accuracy: 0.8787 - val_loss: 0.4249 - val_accuracy: 0.8672\n",
      "Epoch 4/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.8946\n",
      "Epoch 00004: val_accuracy improved from 0.87472 to 0.89043, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.3653 - accuracy: 0.8947 - val_loss: 0.3876 - val_accuracy: 0.8904\n",
      "Epoch 5/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.9049\n",
      "Epoch 00005: val_accuracy did not improve from 0.89043\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.3328 - accuracy: 0.9049 - val_loss: 0.3917 - val_accuracy: 0.8882\n",
      "Epoch 6/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.9136\n",
      "Epoch 00006: val_accuracy improved from 0.89043 to 0.89604, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.3102 - accuracy: 0.9137 - val_loss: 0.3555 - val_accuracy: 0.8960\n",
      "Epoch 7/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9196\n",
      "Epoch 00007: val_accuracy improved from 0.89604 to 0.90202, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2892 - accuracy: 0.9197 - val_loss: 0.3530 - val_accuracy: 0.9020\n",
      "Epoch 8/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.2726 - accuracy: 0.9270\n",
      "Epoch 00008: val_accuracy did not improve from 0.90202\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2726 - accuracy: 0.9269 - val_loss: 0.3497 - val_accuracy: 0.9016\n",
      "Epoch 9/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.2611 - accuracy: 0.9306\n",
      "Epoch 00009: val_accuracy improved from 0.90202 to 0.90800, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2617 - accuracy: 0.9305 - val_loss: 0.3363 - val_accuracy: 0.9080\n",
      "Epoch 10/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.9322\n",
      "Epoch 00010: val_accuracy did not improve from 0.90800\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2504 - accuracy: 0.9320 - val_loss: 0.3453 - val_accuracy: 0.9035\n",
      "Epoch 11/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9373\n",
      "Epoch 00011: val_accuracy did not improve from 0.90800\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2371 - accuracy: 0.9369 - val_loss: 0.3394 - val_accuracy: 0.9024\n",
      "Epoch 12/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9416\n",
      "Epoch 00012: val_accuracy improved from 0.90800 to 0.90987, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2261 - accuracy: 0.9419 - val_loss: 0.3320 - val_accuracy: 0.9099\n",
      "Epoch 13/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9443\n",
      "Epoch 00013: val_accuracy improved from 0.90987 to 0.91062, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2161 - accuracy: 0.9444 - val_loss: 0.3328 - val_accuracy: 0.9106\n",
      "Epoch 14/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.2110 - accuracy: 0.9465\n",
      "Epoch 00014: val_accuracy did not improve from 0.91062\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2110 - accuracy: 0.9465 - val_loss: 0.3464 - val_accuracy: 0.8998\n",
      "Epoch 15/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9499\n",
      "Epoch 00015: val_accuracy improved from 0.91062 to 0.91548, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.2015 - accuracy: 0.9496 - val_loss: 0.3258 - val_accuracy: 0.9155\n",
      "Epoch 16/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9501\n",
      "Epoch 00016: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1972 - accuracy: 0.9502 - val_loss: 0.3278 - val_accuracy: 0.9106\n",
      "Epoch 17/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9531\n",
      "Epoch 00017: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1895 - accuracy: 0.9530 - val_loss: 0.3259 - val_accuracy: 0.9132\n",
      "Epoch 18/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.1832 - accuracy: 0.9544\n",
      "Epoch 00018: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1834 - accuracy: 0.9542 - val_loss: 0.3326 - val_accuracy: 0.9106\n",
      "Epoch 19/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9544\n",
      "Epoch 00019: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1815 - accuracy: 0.9544 - val_loss: 0.3300 - val_accuracy: 0.9144\n",
      "Epoch 20/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.1738 - accuracy: 0.9570\n",
      "Epoch 00020: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1738 - accuracy: 0.9571 - val_loss: 0.3358 - val_accuracy: 0.9132\n",
      "Epoch 21/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.1674 - accuracy: 0.9611\n",
      "Epoch 00021: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1677 - accuracy: 0.9608 - val_loss: 0.3446 - val_accuracy: 0.9069\n",
      "Epoch 22/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.9608\n",
      "Epoch 00022: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1642 - accuracy: 0.9609 - val_loss: 0.3320 - val_accuracy: 0.9144\n",
      "Epoch 23/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.1616 - accuracy: 0.9606\n",
      "Epoch 00023: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1615 - accuracy: 0.9606 - val_loss: 0.3312 - val_accuracy: 0.9117\n",
      "Epoch 24/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9621\n",
      "Epoch 00024: val_accuracy did not improve from 0.91548\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1603 - accuracy: 0.9619 - val_loss: 0.3226 - val_accuracy: 0.9155\n",
      "Epoch 25/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.1569 - accuracy: 0.9622\n",
      "Epoch 00025: val_accuracy improved from 0.91548 to 0.91586, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1568 - accuracy: 0.9621 - val_loss: 0.3342 - val_accuracy: 0.9159\n",
      "Epoch 26/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.9627\n",
      "Epoch 00026: val_accuracy improved from 0.91586 to 0.91623, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1526 - accuracy: 0.9627 - val_loss: 0.3383 - val_accuracy: 0.9162\n",
      "Epoch 27/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.9666\n",
      "Epoch 00027: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1449 - accuracy: 0.9665 - val_loss: 0.3354 - val_accuracy: 0.9114\n",
      "Epoch 28/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.1440 - accuracy: 0.9664\n",
      "Epoch 00028: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1445 - accuracy: 0.9663 - val_loss: 0.3437 - val_accuracy: 0.9114\n",
      "Epoch 29/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.1414 - accuracy: 0.9683\n",
      "Epoch 00029: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1417 - accuracy: 0.9680 - val_loss: 0.3484 - val_accuracy: 0.9091\n",
      "Epoch 30/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9671\n",
      "Epoch 00030: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1416 - accuracy: 0.9671 - val_loss: 0.3445 - val_accuracy: 0.9132\n",
      "Epoch 31/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9703\n",
      "Epoch 00031: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1362 - accuracy: 0.9704 - val_loss: 0.3328 - val_accuracy: 0.9159\n",
      "Epoch 32/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9699\n",
      "Epoch 00032: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1325 - accuracy: 0.9697 - val_loss: 0.3382 - val_accuracy: 0.9129\n",
      "Epoch 33/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9716\n",
      "Epoch 00033: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1307 - accuracy: 0.9712 - val_loss: 0.3379 - val_accuracy: 0.9159\n",
      "Epoch 34/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9697\n",
      "Epoch 00034: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1309 - accuracy: 0.9695 - val_loss: 0.3431 - val_accuracy: 0.9125\n",
      "Epoch 35/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9728\n",
      "Epoch 00035: val_accuracy did not improve from 0.91623\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1266 - accuracy: 0.9729 - val_loss: 0.3426 - val_accuracy: 0.9117\n",
      "Epoch 36/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9719\n",
      "Epoch 00036: val_accuracy improved from 0.91623 to 0.91997, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1272 - accuracy: 0.9720 - val_loss: 0.3443 - val_accuracy: 0.9200\n",
      "Epoch 37/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9713\n",
      "Epoch 00037: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1251 - accuracy: 0.9712 - val_loss: 0.3498 - val_accuracy: 0.9121\n",
      "Epoch 38/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9740\n",
      "Epoch 00038: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1207 - accuracy: 0.9739 - val_loss: 0.3435 - val_accuracy: 0.9177\n",
      "Epoch 39/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9739\n",
      "Epoch 00039: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1207 - accuracy: 0.9736 - val_loss: 0.3386 - val_accuracy: 0.9140\n",
      "Epoch 40/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9750\n",
      "Epoch 00040: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1182 - accuracy: 0.9749 - val_loss: 0.3390 - val_accuracy: 0.9159\n",
      "Epoch 41/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9739\n",
      "Epoch 00041: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1193 - accuracy: 0.9738 - val_loss: 0.3538 - val_accuracy: 0.9147\n",
      "Epoch 42/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9730\n",
      "Epoch 00042: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1208 - accuracy: 0.9727 - val_loss: 0.3407 - val_accuracy: 0.9151\n",
      "Epoch 43/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1117 - accuracy: 0.9775\n",
      "Epoch 00043: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1118 - accuracy: 0.9774 - val_loss: 0.3555 - val_accuracy: 0.9155\n",
      "Epoch 44/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9767\n",
      "Epoch 00044: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1110 - accuracy: 0.9767 - val_loss: 0.3516 - val_accuracy: 0.9185\n",
      "Epoch 45/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/752 [============================>.] - ETA: 0s - loss: 0.1116 - accuracy: 0.9771\n",
      "Epoch 00045: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1116 - accuracy: 0.9770 - val_loss: 0.3684 - val_accuracy: 0.9185\n",
      "Epoch 46/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9774\n",
      "Epoch 00046: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1102 - accuracy: 0.9773 - val_loss: 0.3585 - val_accuracy: 0.9151\n",
      "Epoch 47/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9771\n",
      "Epoch 00047: val_accuracy did not improve from 0.91997\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1059 - accuracy: 0.9772 - val_loss: 0.3693 - val_accuracy: 0.9110\n",
      "Epoch 48/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9776\n",
      "Epoch 00048: val_accuracy improved from 0.91997 to 0.92147, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1091 - accuracy: 0.9775 - val_loss: 0.3551 - val_accuracy: 0.9215\n",
      "Epoch 49/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9776\n",
      "Epoch 00049: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1083 - accuracy: 0.9776 - val_loss: 0.3674 - val_accuracy: 0.9166\n",
      "Epoch 50/150\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.1056 - accuracy: 0.9784\n",
      "Epoch 00050: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1057 - accuracy: 0.9784 - val_loss: 0.3660 - val_accuracy: 0.9155\n",
      "Epoch 51/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.1016 - accuracy: 0.9794\n",
      "Epoch 00051: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9794 - val_loss: 0.3659 - val_accuracy: 0.9170\n",
      "Epoch 52/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9798\n",
      "Epoch 00052: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9796 - val_loss: 0.3546 - val_accuracy: 0.9211\n",
      "Epoch 53/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.1028 - accuracy: 0.9791\n",
      "Epoch 00053: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1026 - accuracy: 0.9793 - val_loss: 0.3663 - val_accuracy: 0.9174\n",
      "Epoch 54/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9806\n",
      "Epoch 00054: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0996 - accuracy: 0.9806 - val_loss: 0.3608 - val_accuracy: 0.9207\n",
      "Epoch 55/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9787\n",
      "Epoch 00055: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1005 - accuracy: 0.9789 - val_loss: 0.3671 - val_accuracy: 0.9177\n",
      "Epoch 56/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9797\n",
      "Epoch 00056: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1015 - accuracy: 0.9797 - val_loss: 0.3680 - val_accuracy: 0.9174\n",
      "Epoch 57/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9795\n",
      "Epoch 00057: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0983 - accuracy: 0.9796 - val_loss: 0.3660 - val_accuracy: 0.9192\n",
      "Epoch 58/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9796\n",
      "Epoch 00058: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0988 - accuracy: 0.9796 - val_loss: 0.3556 - val_accuracy: 0.9215\n",
      "Epoch 59/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0953 - accuracy: 0.9807\n",
      "Epoch 00059: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0952 - accuracy: 0.9807 - val_loss: 0.3603 - val_accuracy: 0.9144\n",
      "Epoch 60/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9784\n",
      "Epoch 00060: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.1011 - accuracy: 0.9783 - val_loss: 0.3601 - val_accuracy: 0.9181\n",
      "Epoch 61/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9806\n",
      "Epoch 00061: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0981 - accuracy: 0.9807 - val_loss: 0.3583 - val_accuracy: 0.9196\n",
      "Epoch 62/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9815\n",
      "Epoch 00062: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0933 - accuracy: 0.9815 - val_loss: 0.3658 - val_accuracy: 0.9125\n",
      "Epoch 63/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9801\n",
      "Epoch 00063: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0951 - accuracy: 0.9803 - val_loss: 0.3623 - val_accuracy: 0.9155\n",
      "Epoch 64/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9819\n",
      "Epoch 00064: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9819 - val_loss: 0.3815 - val_accuracy: 0.9106\n",
      "Epoch 65/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9805\n",
      "Epoch 00065: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9805 - val_loss: 0.3756 - val_accuracy: 0.9185\n",
      "Epoch 66/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 0.9808\n",
      "Epoch 00066: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0923 - accuracy: 0.9808 - val_loss: 0.3693 - val_accuracy: 0.9177\n",
      "Epoch 67/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9825\n",
      "Epoch 00067: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0900 - accuracy: 0.9826 - val_loss: 0.3683 - val_accuracy: 0.9196\n",
      "Epoch 68/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0889 - accuracy: 0.9833\n",
      "Epoch 00068: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0889 - accuracy: 0.9832 - val_loss: 0.3668 - val_accuracy: 0.9181\n",
      "Epoch 69/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9817\n",
      "Epoch 00069: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0907 - accuracy: 0.9817 - val_loss: 0.3661 - val_accuracy: 0.9181\n",
      "Epoch 70/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9828\n",
      "Epoch 00070: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9828 - val_loss: 0.3583 - val_accuracy: 0.9162\n",
      "Epoch 71/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9825\n",
      "Epoch 00071: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0883 - accuracy: 0.9825 - val_loss: 0.3713 - val_accuracy: 0.9203\n",
      "Epoch 72/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9856\n",
      "Epoch 00072: val_accuracy did not improve from 0.92147\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0804 - accuracy: 0.9856 - val_loss: 0.3745 - val_accuracy: 0.9200\n",
      "Epoch 73/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9849\n",
      "Epoch 00073: val_accuracy improved from 0.92147 to 0.92221, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0852 - accuracy: 0.9847 - val_loss: 0.3535 - val_accuracy: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9844\n",
      "Epoch 00074: val_accuracy improved from 0.92221 to 0.92334, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0853 - accuracy: 0.9842 - val_loss: 0.3525 - val_accuracy: 0.9233\n",
      "Epoch 75/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9855\n",
      "Epoch 00075: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9854 - val_loss: 0.3727 - val_accuracy: 0.9218\n",
      "Epoch 76/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0836 - accuracy: 0.9835\n",
      "Epoch 00076: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0836 - accuracy: 0.9835 - val_loss: 0.3713 - val_accuracy: 0.9185\n",
      "Epoch 77/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9834\n",
      "Epoch 00077: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0831 - accuracy: 0.9835 - val_loss: 0.3844 - val_accuracy: 0.9174\n",
      "Epoch 78/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9833\n",
      "Epoch 00078: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0842 - accuracy: 0.9832 - val_loss: 0.3655 - val_accuracy: 0.9185\n",
      "Epoch 79/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9851\n",
      "Epoch 00079: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9852 - val_loss: 0.3712 - val_accuracy: 0.9188\n",
      "Epoch 80/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9846\n",
      "Epoch 00080: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0808 - accuracy: 0.9847 - val_loss: 0.3678 - val_accuracy: 0.9188\n",
      "Epoch 81/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9849\n",
      "Epoch 00081: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0788 - accuracy: 0.9849 - val_loss: 0.3711 - val_accuracy: 0.9226\n",
      "Epoch 82/150\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9838\n",
      "Epoch 00082: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9838 - val_loss: 0.3682 - val_accuracy: 0.9222\n",
      "Epoch 83/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9842\n",
      "Epoch 00083: val_accuracy did not improve from 0.92334\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0797 - accuracy: 0.9842 - val_loss: 0.3645 - val_accuracy: 0.9200\n",
      "Epoch 84/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9857\n",
      "Epoch 00084: val_accuracy improved from 0.92334 to 0.92371, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0787 - accuracy: 0.9856 - val_loss: 0.3627 - val_accuracy: 0.9237\n",
      "Epoch 85/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9841\n",
      "Epoch 00085: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0806 - accuracy: 0.9841 - val_loss: 0.3732 - val_accuracy: 0.9215\n",
      "Epoch 86/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0831 - accuracy: 0.9838\n",
      "Epoch 00086: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0831 - accuracy: 0.9838 - val_loss: 0.3754 - val_accuracy: 0.9200\n",
      "Epoch 87/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9880\n",
      "Epoch 00087: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9879 - val_loss: 0.3794 - val_accuracy: 0.9188\n",
      "Epoch 88/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9852\n",
      "Epoch 00088: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9853 - val_loss: 0.3627 - val_accuracy: 0.9181\n",
      "Epoch 89/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9861\n",
      "Epoch 00089: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9859 - val_loss: 0.3655 - val_accuracy: 0.9188\n",
      "Epoch 90/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9863\n",
      "Epoch 00090: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0750 - accuracy: 0.9862 - val_loss: 0.3783 - val_accuracy: 0.9174\n",
      "Epoch 91/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9865\n",
      "Epoch 00091: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0749 - accuracy: 0.9866 - val_loss: 0.3842 - val_accuracy: 0.9181\n",
      "Epoch 92/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9873\n",
      "Epoch 00092: val_accuracy did not improve from 0.92371\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9872 - val_loss: 0.3809 - val_accuracy: 0.9222\n",
      "Epoch 93/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9850\n",
      "Epoch 00093: val_accuracy improved from 0.92371 to 0.92670, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0777 - accuracy: 0.9848 - val_loss: 0.3774 - val_accuracy: 0.9267\n",
      "Epoch 94/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9857\n",
      "Epoch 00094: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0754 - accuracy: 0.9857 - val_loss: 0.3784 - val_accuracy: 0.9196\n",
      "Epoch 95/150\n",
      "737/752 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9877\n",
      "Epoch 00095: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0731 - accuracy: 0.9877 - val_loss: 0.3941 - val_accuracy: 0.9218\n",
      "Epoch 96/150\n",
      "736/752 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9873\n",
      "Epoch 00096: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0719 - accuracy: 0.9875 - val_loss: 0.3841 - val_accuracy: 0.9211\n",
      "Epoch 97/150\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9864\n",
      "Epoch 00097: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0723 - accuracy: 0.9864 - val_loss: 0.3848 - val_accuracy: 0.9226\n",
      "Epoch 98/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9877\n",
      "Epoch 00098: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9877 - val_loss: 0.3732 - val_accuracy: 0.9230\n",
      "Epoch 99/150\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9868\n",
      "Epoch 00099: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9867 - val_loss: 0.3763 - val_accuracy: 0.9192\n",
      "Epoch 100/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9862\n",
      "Epoch 00100: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0731 - accuracy: 0.9862 - val_loss: 0.3811 - val_accuracy: 0.9196\n",
      "Epoch 101/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9881\n",
      "Epoch 00101: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0677 - accuracy: 0.9881 - val_loss: 0.3686 - val_accuracy: 0.9215\n",
      "Epoch 102/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9860\n",
      "Epoch 00102: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0737 - accuracy: 0.9861 - val_loss: 0.3866 - val_accuracy: 0.9177\n",
      "Epoch 103/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9873\n",
      "Epoch 00103: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9872 - val_loss: 0.3691 - val_accuracy: 0.9230\n",
      "Epoch 104/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9875\n",
      "Epoch 00104: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9877 - val_loss: 0.3723 - val_accuracy: 0.9237\n",
      "Epoch 105/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 0.9881\n",
      "Epoch 00105: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0691 - accuracy: 0.9880 - val_loss: 0.3606 - val_accuracy: 0.9218\n",
      "Epoch 106/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9880\n",
      "Epoch 00106: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9879 - val_loss: 0.3938 - val_accuracy: 0.9211\n",
      "Epoch 107/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9857\n",
      "Epoch 00107: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0719 - accuracy: 0.9856 - val_loss: 0.3969 - val_accuracy: 0.9196\n",
      "Epoch 108/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9874\n",
      "Epoch 00108: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0690 - accuracy: 0.9874 - val_loss: 0.3643 - val_accuracy: 0.9267\n",
      "Epoch 109/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9877\n",
      "Epoch 00109: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9876 - val_loss: 0.3844 - val_accuracy: 0.9211\n",
      "Epoch 110/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9871\n",
      "Epoch 00110: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0704 - accuracy: 0.9872 - val_loss: 0.3520 - val_accuracy: 0.9248\n",
      "Epoch 111/150\n",
      "739/752 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9878\n",
      "Epoch 00111: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9878 - val_loss: 0.3638 - val_accuracy: 0.9233\n",
      "Epoch 112/150\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9858\n",
      "Epoch 00112: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9857 - val_loss: 0.3568 - val_accuracy: 0.9203\n",
      "Epoch 113/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9866\n",
      "Epoch 00113: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0705 - accuracy: 0.9867 - val_loss: 0.3643 - val_accuracy: 0.9211\n",
      "Epoch 114/150\n",
      "748/752 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9870\n",
      "Epoch 00114: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0687 - accuracy: 0.9870 - val_loss: 0.3651 - val_accuracy: 0.9237\n",
      "Epoch 115/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9871\n",
      "Epoch 00115: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0692 - accuracy: 0.9870 - val_loss: 0.3854 - val_accuracy: 0.9207\n",
      "Epoch 116/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9876\n",
      "Epoch 00116: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0672 - accuracy: 0.9875 - val_loss: 0.3809 - val_accuracy: 0.9218\n",
      "Epoch 117/150\n",
      "750/752 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9881\n",
      "Epoch 00117: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9881 - val_loss: 0.3634 - val_accuracy: 0.9267\n",
      "Epoch 118/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9895\n",
      "Epoch 00118: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0638 - accuracy: 0.9894 - val_loss: 0.3745 - val_accuracy: 0.9237\n",
      "Epoch 119/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9871\n",
      "Epoch 00119: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9872 - val_loss: 0.3772 - val_accuracy: 0.9230\n",
      "Epoch 120/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9887\n",
      "Epoch 00120: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0628 - accuracy: 0.9887 - val_loss: 0.3683 - val_accuracy: 0.9252\n",
      "Epoch 121/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9870\n",
      "Epoch 00121: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9869 - val_loss: 0.3802 - val_accuracy: 0.9218\n",
      "Epoch 122/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9892\n",
      "Epoch 00122: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0636 - accuracy: 0.9890 - val_loss: 0.3772 - val_accuracy: 0.9211\n",
      "Epoch 123/150\n",
      "734/752 [============================>.] - ETA: 0s - loss: 0.0632 - accuracy: 0.9886\n",
      "Epoch 00123: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0630 - accuracy: 0.9887 - val_loss: 0.3787 - val_accuracy: 0.9241\n",
      "Epoch 124/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9894\n",
      "Epoch 00124: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0624 - accuracy: 0.9894 - val_loss: 0.3848 - val_accuracy: 0.9252\n",
      "Epoch 125/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9898\n",
      "Epoch 00125: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0596 - accuracy: 0.9898 - val_loss: 0.4079 - val_accuracy: 0.9237\n",
      "Epoch 126/150\n",
      "744/752 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9877\n",
      "Epoch 00126: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9877 - val_loss: 0.3727 - val_accuracy: 0.9252\n",
      "Epoch 127/150\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9897\n",
      "Epoch 00127: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9897 - val_loss: 0.3962 - val_accuracy: 0.9181\n",
      "Epoch 128/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9888\n",
      "Epoch 00128: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9889 - val_loss: 0.4032 - val_accuracy: 0.9192\n",
      "Epoch 129/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9886\n",
      "Epoch 00129: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0631 - accuracy: 0.9887 - val_loss: 0.3838 - val_accuracy: 0.9241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9895\n",
      "Epoch 00130: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0615 - accuracy: 0.9894 - val_loss: 0.3782 - val_accuracy: 0.9263\n",
      "Epoch 131/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9902\n",
      "Epoch 00131: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0593 - accuracy: 0.9901 - val_loss: 0.3880 - val_accuracy: 0.9222\n",
      "Epoch 132/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9895\n",
      "Epoch 00132: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9893 - val_loss: 0.3859 - val_accuracy: 0.9196\n",
      "Epoch 133/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9888\n",
      "Epoch 00133: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0624 - accuracy: 0.9889 - val_loss: 0.3793 - val_accuracy: 0.9215\n",
      "Epoch 134/150\n",
      "735/752 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9889\n",
      "Epoch 00134: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0626 - accuracy: 0.9889 - val_loss: 0.3669 - val_accuracy: 0.9200\n",
      "Epoch 135/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9883\n",
      "Epoch 00135: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0621 - accuracy: 0.9883 - val_loss: 0.3925 - val_accuracy: 0.9196\n",
      "Epoch 136/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9913\n",
      "Epoch 00136: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0577 - accuracy: 0.9913 - val_loss: 0.3863 - val_accuracy: 0.9245\n",
      "Epoch 137/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9888\n",
      "Epoch 00137: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9889 - val_loss: 0.3861 - val_accuracy: 0.9230\n",
      "Epoch 138/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9889\n",
      "Epoch 00138: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0614 - accuracy: 0.9889 - val_loss: 0.3881 - val_accuracy: 0.9207\n",
      "Epoch 139/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9900\n",
      "Epoch 00139: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9900 - val_loss: 0.3742 - val_accuracy: 0.9248\n",
      "Epoch 140/150\n",
      "743/752 [============================>.] - ETA: 0s - loss: 0.0568 - accuracy: 0.9898\n",
      "Epoch 00140: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0567 - accuracy: 0.9899 - val_loss: 0.3826 - val_accuracy: 0.9248\n",
      "Epoch 141/150\n",
      "738/752 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9892\n",
      "Epoch 00141: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9892 - val_loss: 0.3892 - val_accuracy: 0.9211\n",
      "Epoch 142/150\n",
      "741/752 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9900\n",
      "Epoch 00142: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0594 - accuracy: 0.9900 - val_loss: 0.3844 - val_accuracy: 0.9260\n",
      "Epoch 143/150\n",
      "740/752 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9894\n",
      "Epoch 00143: val_accuracy did not improve from 0.92670\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9893 - val_loss: 0.3921 - val_accuracy: 0.9237\n",
      "Epoch 144/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0559 - accuracy: 0.9910\n",
      "Epoch 00144: val_accuracy improved from 0.92670 to 0.92857, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0560 - accuracy: 0.9910 - val_loss: 0.3761 - val_accuracy: 0.9286\n",
      "Epoch 145/150\n",
      "751/752 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9899\n",
      "Epoch 00145: val_accuracy improved from 0.92857 to 0.92932, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "752/752 [==============================] - 3s 3ms/step - loss: 0.0593 - accuracy: 0.9899 - val_loss: 0.3751 - val_accuracy: 0.9293\n",
      "Epoch 146/150\n",
      "747/752 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9897\n",
      "Epoch 00146: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0589 - accuracy: 0.9897 - val_loss: 0.3805 - val_accuracy: 0.9260\n",
      "Epoch 147/150\n",
      "746/752 [============================>.] - ETA: 0s - loss: 0.0564 - accuracy: 0.9901\n",
      "Epoch 00147: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0563 - accuracy: 0.9901 - val_loss: 0.3871 - val_accuracy: 0.9260\n",
      "Epoch 148/150\n",
      "749/752 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9896\n",
      "Epoch 00148: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0609 - accuracy: 0.9897 - val_loss: 0.3985 - val_accuracy: 0.9241\n",
      "Epoch 149/150\n",
      "742/752 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9891\n",
      "Epoch 00149: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0585 - accuracy: 0.9890 - val_loss: 0.3961 - val_accuracy: 0.9218\n",
      "Epoch 150/150\n",
      "745/752 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9876\n",
      "Epoch 00150: val_accuracy did not improve from 0.92932\n",
      "752/752 [==============================] - 2s 3ms/step - loss: 0.0623 - accuracy: 0.9877 - val_loss: 0.4001 - val_accuracy: 0.9256\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [07:53<00:00, 236.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.68      0.75      0.71       798\n",
      "        car_horn       0.88      0.50      0.63       413\n",
      "children_playing       0.64      0.64      0.64       700\n",
      "        dog_bark       0.68      0.84      0.75       700\n",
      "           siren       0.93      0.86      0.89      1162\n",
      "\n",
      "        accuracy                           0.76      3773\n",
      "       macro avg       0.76      0.72      0.73      3773\n",
      "    weighted avg       0.77      0.76      0.75      3773\n",
      "\n",
      "Validation fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27230, 375)\n",
      "X_val_norm shape.....:(3276, 375)\n",
      "\n",
      "Sum of elements: 0.9802872505448041\n",
      "Number of elements summed: 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 239)               57360     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 239)               57360     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 239)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               180000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 298,475\n",
      "Trainable params: 298,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24507, 239)\n",
      "Epoch 1/350\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.7898 - accuracy: 0.7081\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82336, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 2ms/step - loss: 0.7840 - accuracy: 0.7105 - val_loss: 0.4833 - val_accuracy: 0.8234\n",
      "Epoch 2/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.4366 - accuracy: 0.8465\n",
      "Epoch 00002: val_accuracy improved from 0.82336 to 0.86669, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.4366 - accuracy: 0.8465 - val_loss: 0.3799 - val_accuracy: 0.8667\n",
      "Epoch 3/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.8770\n",
      "Epoch 00003: val_accuracy improved from 0.86669 to 0.88469, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.3403 - accuracy: 0.8773 - val_loss: 0.3259 - val_accuracy: 0.8847\n",
      "Epoch 4/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.9022\n",
      "Epoch 00004: val_accuracy improved from 0.88469 to 0.90158, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.2781 - accuracy: 0.9022 - val_loss: 0.2904 - val_accuracy: 0.9016\n",
      "Epoch 5/350\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.2275 - accuracy: 0.9212\n",
      "Epoch 00005: val_accuracy improved from 0.90158 to 0.90378, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.2271 - accuracy: 0.9212 - val_loss: 0.2640 - val_accuracy: 0.9038\n",
      "Epoch 6/350\n",
      "742/766 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9361\n",
      "Epoch 00006: val_accuracy improved from 0.90378 to 0.91223, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9363 - val_loss: 0.2433 - val_accuracy: 0.9122\n",
      "Epoch 7/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 0.1613 - accuracy: 0.9468\n",
      "Epoch 00007: val_accuracy improved from 0.91223 to 0.91921, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.1600 - accuracy: 0.9473 - val_loss: 0.2245 - val_accuracy: 0.9192\n",
      "Epoch 8/350\n",
      "743/766 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9535\n",
      "Epoch 00008: val_accuracy improved from 0.91921 to 0.92104, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.1340 - accuracy: 0.9532 - val_loss: 0.2158 - val_accuracy: 0.9210\n",
      "Epoch 9/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9630\n",
      "Epoch 00009: val_accuracy improved from 0.92104 to 0.93169, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.1126 - accuracy: 0.9632 - val_loss: 0.2135 - val_accuracy: 0.9317\n",
      "Epoch 10/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 0.0964 - accuracy: 0.9675\n",
      "Epoch 00010: val_accuracy improved from 0.93169 to 0.93316, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0960 - accuracy: 0.9676 - val_loss: 0.1991 - val_accuracy: 0.9332\n",
      "Epoch 11/350\n",
      "737/766 [===========================>..] - ETA: 0s - loss: 0.0817 - accuracy: 0.9733\n",
      "Epoch 00011: val_accuracy did not improve from 0.93316\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0814 - accuracy: 0.9735 - val_loss: 0.2069 - val_accuracy: 0.9332\n",
      "Epoch 12/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.9783\n",
      "Epoch 00012: val_accuracy improved from 0.93316 to 0.93573, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0681 - accuracy: 0.9783 - val_loss: 0.2012 - val_accuracy: 0.9357\n",
      "Epoch 13/350\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0584 - accuracy: 0.9808\n",
      "Epoch 00013: val_accuracy improved from 0.93573 to 0.93977, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0581 - accuracy: 0.9809 - val_loss: 0.1986 - val_accuracy: 0.9398\n",
      "Epoch 14/350\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9862\n",
      "Epoch 00014: val_accuracy did not improve from 0.93977\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0458 - accuracy: 0.9862 - val_loss: 0.1993 - val_accuracy: 0.9398\n",
      "Epoch 15/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0423 - accuracy: 0.9871\n",
      "Epoch 00015: val_accuracy improved from 0.93977 to 0.94161, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9871 - val_loss: 0.1937 - val_accuracy: 0.9416\n",
      "Epoch 16/350\n",
      "747/766 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9899\n",
      "Epoch 00016: val_accuracy improved from 0.94161 to 0.94565, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9899 - val_loss: 0.1942 - val_accuracy: 0.9456\n",
      "Epoch 17/350\n",
      "738/766 [===========================>..] - ETA: 0s - loss: 0.0299 - accuracy: 0.9915\n",
      "Epoch 00017: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0299 - accuracy: 0.9914 - val_loss: 0.1978 - val_accuracy: 0.9427\n",
      "Epoch 18/350\n",
      "747/766 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9926\n",
      "Epoch 00018: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0257 - accuracy: 0.9925 - val_loss: 0.2022 - val_accuracy: 0.9442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "735/766 [===========================>..] - ETA: 0s - loss: 0.0221 - accuracy: 0.9948\n",
      "Epoch 00019: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9949 - val_loss: 0.2095 - val_accuracy: 0.9423\n",
      "Epoch 20/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9953\n",
      "Epoch 00020: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9953 - val_loss: 0.2033 - val_accuracy: 0.9431\n",
      "Epoch 21/350\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9960\n",
      "Epoch 00021: val_accuracy did not improve from 0.94565\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9959 - val_loss: 0.2076 - val_accuracy: 0.9453\n",
      "Epoch 22/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9970\n",
      "Epoch 00022: val_accuracy improved from 0.94565 to 0.94638, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 0.2143 - val_accuracy: 0.9464\n",
      "Epoch 23/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9971\n",
      "Epoch 00023: val_accuracy did not improve from 0.94638\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.2168 - val_accuracy: 0.9456\n",
      "Epoch 24/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 0.0125 - accuracy: 0.9964\n",
      "Epoch 00024: val_accuracy did not improve from 0.94638\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.2168 - val_accuracy: 0.9445\n",
      "Epoch 25/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 0.0111 - accuracy: 0.9975\n",
      "Epoch 00025: val_accuracy improved from 0.94638 to 0.94712, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9976 - val_loss: 0.2188 - val_accuracy: 0.9471\n",
      "Epoch 26/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 0.0096 - accuracy: 0.9977\n",
      "Epoch 00026: val_accuracy improved from 0.94712 to 0.94785, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 0.2140 - val_accuracy: 0.9479\n",
      "Epoch 27/350\n",
      "737/766 [===========================>..] - ETA: 0s - loss: 0.0080 - accuracy: 0.9985\n",
      "Epoch 00027: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.2326 - val_accuracy: 0.9471\n",
      "Epoch 28/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9980\n",
      "Epoch 00028: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.2364 - val_accuracy: 0.9456\n",
      "Epoch 29/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0081 - accuracy: 0.9982\n",
      "Epoch 00029: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9982 - val_loss: 0.2321 - val_accuracy: 0.9464\n",
      "Epoch 30/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 0.0069 - accuracy: 0.9989\n",
      "Epoch 00030: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0068 - accuracy: 0.9989 - val_loss: 0.2365 - val_accuracy: 0.9471\n",
      "Epoch 31/350\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9986\n",
      "Epoch 00031: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.2337 - val_accuracy: 0.9471\n",
      "Epoch 32/350\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9990\n",
      "Epoch 00032: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 0.2356 - val_accuracy: 0.9475\n",
      "Epoch 33/350\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9985\n",
      "Epoch 00033: val_accuracy did not improve from 0.94785\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.2440 - val_accuracy: 0.9467\n",
      "Epoch 34/350\n",
      "746/766 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9990\n",
      "Epoch 00034: val_accuracy improved from 0.94785 to 0.94859, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.2394 - val_accuracy: 0.9486\n",
      "Epoch 35/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9990\n",
      "Epoch 00035: val_accuracy did not improve from 0.94859\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.2442 - val_accuracy: 0.9464\n",
      "Epoch 36/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9991\n",
      "Epoch 00036: val_accuracy did not improve from 0.94859\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.2438 - val_accuracy: 0.9467\n",
      "Epoch 37/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00037: val_accuracy improved from 0.94859 to 0.94969, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.2353 - val_accuracy: 0.9497\n",
      "Epoch 38/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 00038: val_accuracy did not improve from 0.94969\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.2484 - val_accuracy: 0.9479\n",
      "Epoch 39/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9988\n",
      "Epoch 00039: val_accuracy improved from 0.94969 to 0.95006, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.2455 - val_accuracy: 0.9501\n",
      "Epoch 40/350\n",
      "746/766 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00040: val_accuracy did not improve from 0.95006\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.2493 - val_accuracy: 0.9490\n",
      "Epoch 41/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9990\n",
      "Epoch 00041: val_accuracy improved from 0.95006 to 0.95152, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 0.2422 - val_accuracy: 0.9515\n",
      "Epoch 42/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9993\n",
      "Epoch 00042: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2507 - val_accuracy: 0.9486\n",
      "Epoch 43/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 00043: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.2427 - val_accuracy: 0.9486\n",
      "Epoch 44/350\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9993\n",
      "Epoch 00044: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.2503 - val_accuracy: 0.9464\n",
      "Epoch 45/350\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9995\n",
      "Epoch 00045: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.2581 - val_accuracy: 0.9467\n",
      "Epoch 46/350\n",
      "745/766 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00046: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.2606 - val_accuracy: 0.9467\n",
      "Epoch 47/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 0.9997\n",
      "Epoch 00047: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2654 - val_accuracy: 0.9475\n",
      "Epoch 48/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9996\n",
      "Epoch 00048: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2572 - val_accuracy: 0.9486\n",
      "Epoch 49/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00049: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.2582 - val_accuracy: 0.9490\n",
      "Epoch 50/350\n",
      "733/766 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00050: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2588 - val_accuracy: 0.9490\n",
      "Epoch 51/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00051: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2575 - val_accuracy: 0.9490\n",
      "Epoch 52/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9997\n",
      "Epoch 00052: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.2639 - val_accuracy: 0.9475\n",
      "Epoch 53/350\n",
      "741/766 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 00053: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2682 - val_accuracy: 0.9479\n",
      "Epoch 54/350\n",
      "735/766 [===========================>..] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00054: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2718 - val_accuracy: 0.9456\n",
      "Epoch 55/350\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2772 - val_accuracy: 0.9456\n",
      "Epoch 56/350\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00056: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2767 - val_accuracy: 0.9467\n",
      "Epoch 57/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 00057: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2814 - val_accuracy: 0.9501\n",
      "Epoch 58/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.2752 - val_accuracy: 0.9493\n",
      "Epoch 59/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2693 - val_accuracy: 0.9467\n",
      "Epoch 60/350\n",
      "737/766 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 00060: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.2723 - val_accuracy: 0.9475\n",
      "Epoch 61/350\n",
      "746/766 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2749 - val_accuracy: 0.9490\n",
      "Epoch 62/350\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00062: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2651 - val_accuracy: 0.9504\n",
      "Epoch 63/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00063: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2664 - val_accuracy: 0.9508\n",
      "Epoch 64/350\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00064: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2748 - val_accuracy: 0.9497\n",
      "Epoch 65/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00065: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2702 - val_accuracy: 0.9471\n",
      "Epoch 66/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00066: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2710 - val_accuracy: 0.9479\n",
      "Epoch 67/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00067: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2710 - val_accuracy: 0.9497\n",
      "Epoch 68/350\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2698 - val_accuracy: 0.9490\n",
      "Epoch 69/350\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2705 - val_accuracy: 0.9493\n",
      "Epoch 70/350\n",
      "738/766 [===========================>..] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00070: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2718 - val_accuracy: 0.9486\n",
      "Epoch 71/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2700 - val_accuracy: 0.9490\n",
      "Epoch 72/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00072: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2812 - val_accuracy: 0.9479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00073: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2723 - val_accuracy: 0.9493\n",
      "Epoch 74/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2778 - val_accuracy: 0.9471\n",
      "Epoch 75/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00075: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2802 - val_accuracy: 0.9515\n",
      "Epoch 76/350\n",
      "738/766 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00076: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2847 - val_accuracy: 0.9508\n",
      "Epoch 77/350\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 00077: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2799 - val_accuracy: 0.9504\n",
      "Epoch 78/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 9.8091e-04 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 9.7198e-04 - accuracy: 0.9999 - val_loss: 0.2832 - val_accuracy: 0.9515\n",
      "Epoch 79/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2825 - val_accuracy: 0.9497\n",
      "Epoch 80/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 9.9272e-04 - accuracy: 0.9999\n",
      "Epoch 00080: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2857 - val_accuracy: 0.9508\n",
      "Epoch 81/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00081: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2832 - val_accuracy: 0.9501\n",
      "Epoch 82/350\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00082: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.2877 - val_accuracy: 0.9504\n",
      "Epoch 83/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 8.8393e-04 - accuracy: 0.9998\n",
      "Epoch 00083: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.8371e-04 - accuracy: 0.9998 - val_loss: 0.2868 - val_accuracy: 0.9482\n",
      "Epoch 84/350\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2869 - val_accuracy: 0.9497\n",
      "Epoch 85/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00085: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2815 - val_accuracy: 0.9501\n",
      "Epoch 86/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 9.9674e-04 - accuracy: 0.9998\n",
      "Epoch 00086: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 9.8979e-04 - accuracy: 0.9998 - val_loss: 0.2821 - val_accuracy: 0.9493\n",
      "Epoch 87/350\n",
      "741/766 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2843 - val_accuracy: 0.9512\n",
      "Epoch 88/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 9.1661e-04 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 9.1462e-04 - accuracy: 0.9998 - val_loss: 0.2919 - val_accuracy: 0.9493\n",
      "Epoch 89/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 8.8378e-04 - accuracy: 0.9999\n",
      "Epoch 00089: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.8326e-04 - accuracy: 0.9999 - val_loss: 0.2882 - val_accuracy: 0.9508\n",
      "Epoch 90/350\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00090: val_accuracy did not improve from 0.95152\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2874 - val_accuracy: 0.9504\n",
      "Epoch 91/350\n",
      "735/766 [===========================>..] - ETA: 0s - loss: 7.3540e-04 - accuracy: 1.0000\n",
      "Epoch 00091: val_accuracy improved from 0.95152 to 0.95189, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.2899e-04 - accuracy: 1.0000 - val_loss: 0.2852 - val_accuracy: 0.9519\n",
      "Epoch 92/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00092: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.2869 - val_accuracy: 0.9519\n",
      "Epoch 93/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00093: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2937 - val_accuracy: 0.9512\n",
      "Epoch 94/350\n",
      "743/766 [============================>.] - ETA: 0s - loss: 8.0667e-04 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.0454e-04 - accuracy: 0.9999 - val_loss: 0.2923 - val_accuracy: 0.9515\n",
      "Epoch 95/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00095: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3039 - val_accuracy: 0.9464\n",
      "Epoch 96/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2985 - val_accuracy: 0.9508\n",
      "Epoch 97/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9995  \n",
      "Epoch 00097: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.2986 - val_accuracy: 0.9460\n",
      "Epoch 98/350\n",
      "752/766 [============================>.] - ETA: 0s - loss: 8.9448e-04 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.8530e-04 - accuracy: 0.9998 - val_loss: 0.2937 - val_accuracy: 0.9475\n",
      "Epoch 99/350\n",
      "757/766 [============================>.] - ETA: 0s - loss: 7.8468e-04 - accuracy: 0.9999\n",
      "Epoch 00099: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.1082e-04 - accuracy: 0.9999 - val_loss: 0.3016 - val_accuracy: 0.9482\n",
      "Epoch 100/350\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00100: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2857 - val_accuracy: 0.9493\n",
      "Epoch 101/350\n",
      "737/766 [===========================>..] - ETA: 0s - loss: 9.4818e-04 - accuracy: 0.9998\n",
      "Epoch 00101: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 9.2151e-04 - accuracy: 0.9998 - val_loss: 0.2880 - val_accuracy: 0.9493\n",
      "Epoch 102/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 6.6016e-04 - accuracy: 1.0000\n",
      "Epoch 00102: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.2298e-04 - accuracy: 0.9999 - val_loss: 0.2858 - val_accuracy: 0.9508\n",
      "Epoch 103/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 8.9973e-04 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.9973e-04 - accuracy: 0.9998 - val_loss: 0.2965 - val_accuracy: 0.9493\n",
      "Epoch 104/350\n",
      "759/766 [============================>.] - ETA: 0s - loss: 5.7360e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.7383e-04 - accuracy: 1.0000 - val_loss: 0.2886 - val_accuracy: 0.9490\n",
      "Epoch 105/350\n",
      "759/766 [============================>.] - ETA: 0s - loss: 7.8251e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.8566e-04 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.9471\n",
      "Epoch 106/350\n",
      "764/766 [============================>.] - ETA: 0s - loss: 7.6376e-04 - accuracy: 0.9999\n",
      "Epoch 00106: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.6367e-04 - accuracy: 0.9999 - val_loss: 0.2884 - val_accuracy: 0.9497\n",
      "Epoch 107/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 6.2115e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.2115e-04 - accuracy: 1.0000 - val_loss: 0.2890 - val_accuracy: 0.9490\n",
      "Epoch 108/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 6.5839e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.5343e-04 - accuracy: 0.9999 - val_loss: 0.2976 - val_accuracy: 0.9486\n",
      "Epoch 109/350\n",
      "763/766 [============================>.] - ETA: 0s - loss: 6.3489e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.3523e-04 - accuracy: 0.9999 - val_loss: 0.2970 - val_accuracy: 0.9482\n",
      "Epoch 110/350\n",
      "735/766 [===========================>..] - ETA: 0s - loss: 5.2929e-04 - accuracy: 1.0000\n",
      "Epoch 00110: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.2720e-04 - accuracy: 1.0000 - val_loss: 0.3004 - val_accuracy: 0.9490\n",
      "Epoch 111/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 4.2693e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.2514e-04 - accuracy: 1.0000 - val_loss: 0.2993 - val_accuracy: 0.9490\n",
      "Epoch 112/350\n",
      "762/766 [============================>.] - ETA: 0s - loss: 8.3499e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.3427e-04 - accuracy: 0.9998 - val_loss: 0.3050 - val_accuracy: 0.9482\n",
      "Epoch 113/350\n",
      "733/766 [===========================>..] - ETA: 0s - loss: 6.1263e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.0058e-04 - accuracy: 0.9999 - val_loss: 0.2983 - val_accuracy: 0.9479\n",
      "Epoch 114/350\n",
      "761/766 [============================>.] - ETA: 0s - loss: 5.4240e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.4043e-04 - accuracy: 1.0000 - val_loss: 0.2976 - val_accuracy: 0.9482\n",
      "Epoch 115/350\n",
      "745/766 [============================>.] - ETA: 0s - loss: 5.2920e-04 - accuracy: 0.9999\n",
      "Epoch 00115: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.2283e-04 - accuracy: 0.9999 - val_loss: 0.2980 - val_accuracy: 0.9475\n",
      "Epoch 116/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 5.9168e-04 - accuracy: 0.9999\n",
      "Epoch 00116: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.9068e-04 - accuracy: 0.9999 - val_loss: 0.3006 - val_accuracy: 0.9504\n",
      "Epoch 117/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 5.7980e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.7645e-04 - accuracy: 1.0000 - val_loss: 0.2958 - val_accuracy: 0.9512\n",
      "Epoch 118/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 5.3260e-04 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.1963e-04 - accuracy: 1.0000 - val_loss: 0.2960 - val_accuracy: 0.9515\n",
      "Epoch 119/350\n",
      "742/766 [============================>.] - ETA: 0s - loss: 4.8446e-04 - accuracy: 1.0000\n",
      "Epoch 00119: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.7864e-04 - accuracy: 1.0000 - val_loss: 0.2982 - val_accuracy: 0.9515\n",
      "Epoch 120/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 4.2600e-04 - accuracy: 0.9999\n",
      "Epoch 00120: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.2600e-04 - accuracy: 0.9999 - val_loss: 0.2997 - val_accuracy: 0.9519\n",
      "Epoch 121/350\n",
      "742/766 [============================>.] - ETA: 0s - loss: 5.4810e-04 - accuracy: 0.9998\n",
      "Epoch 00121: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.3448e-04 - accuracy: 0.9998 - val_loss: 0.3009 - val_accuracy: 0.9515\n",
      "Epoch 122/350\n",
      "734/766 [===========================>..] - ETA: 0s - loss: 0.0018 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.2984 - val_accuracy: 0.9497\n",
      "Epoch 123/350\n",
      "745/766 [============================>.] - ETA: 0s - loss: 6.1383e-04 - accuracy: 0.9998\n",
      "Epoch 00123: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.0562e-04 - accuracy: 0.9998 - val_loss: 0.3017 - val_accuracy: 0.9504\n",
      "Epoch 124/350\n",
      "736/766 [===========================>..] - ETA: 0s - loss: 7.6714e-04 - accuracy: 0.9999\n",
      "Epoch 00124: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.6039e-04 - accuracy: 0.9999 - val_loss: 0.3046 - val_accuracy: 0.9501\n",
      "Epoch 125/350\n",
      "750/766 [============================>.] - ETA: 0s - loss: 5.5230e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.4818e-04 - accuracy: 0.9999 - val_loss: 0.3018 - val_accuracy: 0.9512\n",
      "Epoch 126/350\n",
      "752/766 [============================>.] - ETA: 0s - loss: 5.8212e-04 - accuracy: 0.9999\n",
      "Epoch 00126: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 6.3303e-04 - accuracy: 0.9999 - val_loss: 0.3064 - val_accuracy: 0.9508\n",
      "Epoch 127/350\n",
      "750/766 [============================>.] - ETA: 0s - loss: 4.9929e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.9237e-04 - accuracy: 0.9999 - val_loss: 0.3087 - val_accuracy: 0.9497\n",
      "Epoch 128/350\n",
      "748/766 [============================>.] - ETA: 0s - loss: 8.2013e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.0539e-04 - accuracy: 0.9999 - val_loss: 0.3112 - val_accuracy: 0.9497\n",
      "Epoch 129/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/766 [============================>.] - ETA: 0s - loss: 4.2174e-04 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.1764e-04 - accuracy: 1.0000 - val_loss: 0.3105 - val_accuracy: 0.9482\n",
      "Epoch 130/350\n",
      "760/766 [============================>.] - ETA: 0s - loss: 5.0147e-04 - accuracy: 0.9999\n",
      "Epoch 00130: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.0188e-04 - accuracy: 0.9999 - val_loss: 0.3209 - val_accuracy: 0.9512\n",
      "Epoch 131/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 3.4587e-04 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 3.4587e-04 - accuracy: 1.0000 - val_loss: 0.3140 - val_accuracy: 0.9479\n",
      "Epoch 132/350\n",
      "740/766 [===========================>..] - ETA: 0s - loss: 5.6295e-04 - accuracy: 0.9999\n",
      "Epoch 00132: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.5150e-04 - accuracy: 0.9999 - val_loss: 0.3026 - val_accuracy: 0.9490\n",
      "Epoch 133/350\n",
      "747/766 [============================>.] - ETA: 0s - loss: 4.7487e-04 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.6714e-04 - accuracy: 1.0000 - val_loss: 0.3121 - val_accuracy: 0.9486\n",
      "Epoch 134/350\n",
      "737/766 [===========================>..] - ETA: 0s - loss: 4.0578e-04 - accuracy: 0.9999\n",
      "Epoch 00134: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.2505e-04 - accuracy: 0.9999 - val_loss: 0.3076 - val_accuracy: 0.9486\n",
      "Epoch 135/350\n",
      "765/766 [============================>.] - ETA: 0s - loss: 5.0308e-04 - accuracy: 0.9999\n",
      "Epoch 00135: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 5.0292e-04 - accuracy: 0.9999 - val_loss: 0.3055 - val_accuracy: 0.9515\n",
      "Epoch 136/350\n",
      "758/766 [============================>.] - ETA: 0s - loss: 4.3965e-04 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.4522e-04 - accuracy: 1.0000 - val_loss: 0.3046 - val_accuracy: 0.9501\n",
      "Epoch 137/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 8.2346e-04 - accuracy: 0.9998\n",
      "Epoch 00137: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 8.4782e-04 - accuracy: 0.9998 - val_loss: 0.3020 - val_accuracy: 0.9501\n",
      "Epoch 138/350\n",
      "749/766 [============================>.] - ETA: 0s - loss: 7.4980e-04 - accuracy: 0.9998\n",
      "Epoch 00138: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 7.4410e-04 - accuracy: 0.9998 - val_loss: 0.3027 - val_accuracy: 0.9490\n",
      "Epoch 139/350\n",
      "754/766 [============================>.] - ETA: 0s - loss: 4.9177e-04 - accuracy: 1.0000\n",
      "Epoch 00139: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.8601e-04 - accuracy: 1.0000 - val_loss: 0.3048 - val_accuracy: 0.9493\n",
      "Epoch 140/350\n",
      "739/766 [===========================>..] - ETA: 0s - loss: 4.2164e-04 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 0.95189\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 4.1179e-04 - accuracy: 1.0000 - val_loss: 0.3027 - val_accuracy: 0.9490\n",
      "Epoch 141/350\n",
      "766/766 [==============================] - ETA: 0s - loss: 3.8838e-04 - accuracy: 1.0000\n",
      "Epoch 00141: val_accuracy did not improve from 0.95189\n",
      "Restoring model weights from the end of the best epoch.\n",
      "766/766 [==============================] - 1s 2ms/step - loss: 3.8838e-04 - accuracy: 1.0000 - val_loss: 0.3064 - val_accuracy: 0.9493\n",
      "Epoch 00141: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:06<03:06, 186.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.68      0.85      0.76       693\n",
      "        car_horn       0.93      0.67      0.78       686\n",
      "children_playing       0.76      0.74      0.75       700\n",
      "        dog_bark       0.74      0.77      0.75       700\n",
      "           siren       0.86      0.88      0.87       497\n",
      "\n",
      "        accuracy                           0.77      3276\n",
      "       macro avg       0.79      0.78      0.78      3276\n",
      "    weighted avg       0.79      0.77      0.77      3276\n",
      "\n",
      "Model: \"Model_CNN_1D_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 233, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 233, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 233, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 116, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 116, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6496)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                324850    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 335,891\n",
      "Trainable params: 335,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24507, 239, 1)\n",
      "Epoch 1/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.7523 - accuracy: 0.7622\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83548, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 4s 5ms/step - loss: 0.7523 - accuracy: 0.7622 - val_loss: 0.5443 - val_accuracy: 0.8355\n",
      "Epoch 2/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.4832 - accuracy: 0.8542\n",
      "Epoch 00002: val_accuracy improved from 0.83548 to 0.86926, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 3s 3ms/step - loss: 0.4836 - accuracy: 0.8539 - val_loss: 0.4540 - val_accuracy: 0.8693\n",
      "Epoch 3/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8764\n",
      "Epoch 00003: val_accuracy improved from 0.86926 to 0.88028, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.4168 - accuracy: 0.8766 - val_loss: 0.4128 - val_accuracy: 0.8803\n",
      "Epoch 4/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.8950\n",
      "Epoch 00004: val_accuracy improved from 0.88028 to 0.88322, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.3692 - accuracy: 0.8951 - val_loss: 0.4037 - val_accuracy: 0.8832\n",
      "Epoch 5/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.9040\n",
      "Epoch 00005: val_accuracy improved from 0.88322 to 0.89093, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.3380 - accuracy: 0.9038 - val_loss: 0.3984 - val_accuracy: 0.8909\n",
      "Epoch 6/150\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.9154\n",
      "Epoch 00006: val_accuracy improved from 0.89093 to 0.89974, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.3097 - accuracy: 0.9152 - val_loss: 0.3665 - val_accuracy: 0.8997\n",
      "Epoch 7/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9201\n",
      "Epoch 00007: val_accuracy did not improve from 0.89974\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2884 - accuracy: 0.9201 - val_loss: 0.3656 - val_accuracy: 0.8968\n",
      "Epoch 8/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.2681 - accuracy: 0.9291\n",
      "Epoch 00008: val_accuracy did not improve from 0.89974\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2686 - accuracy: 0.9290 - val_loss: 0.3678 - val_accuracy: 0.8920\n",
      "Epoch 9/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.9355\n",
      "Epoch 00009: val_accuracy improved from 0.89974 to 0.90562, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2528 - accuracy: 0.9351 - val_loss: 0.3541 - val_accuracy: 0.9056\n",
      "Epoch 10/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.2411 - accuracy: 0.9366\n",
      "Epoch 00010: val_accuracy did not improve from 0.90562\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2408 - accuracy: 0.9368 - val_loss: 0.3508 - val_accuracy: 0.9038\n",
      "Epoch 11/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9416\n",
      "Epoch 00011: val_accuracy improved from 0.90562 to 0.90709, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2269 - accuracy: 0.9417 - val_loss: 0.3421 - val_accuracy: 0.9071\n",
      "Epoch 12/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.2211 - accuracy: 0.9430\n",
      "Epoch 00012: val_accuracy did not improve from 0.90709\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2211 - accuracy: 0.9430 - val_loss: 0.3525 - val_accuracy: 0.9030\n",
      "Epoch 13/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.2114 - accuracy: 0.9469\n",
      "Epoch 00013: val_accuracy improved from 0.90709 to 0.91076, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2118 - accuracy: 0.9467 - val_loss: 0.3535 - val_accuracy: 0.9108\n",
      "Epoch 14/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.2072 - accuracy: 0.9475\n",
      "Epoch 00014: val_accuracy did not improve from 0.91076\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.2071 - accuracy: 0.9475 - val_loss: 0.3422 - val_accuracy: 0.9100\n",
      "Epoch 15/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9505\n",
      "Epoch 00015: val_accuracy improved from 0.91076 to 0.91149, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1992 - accuracy: 0.9505 - val_loss: 0.3340 - val_accuracy: 0.9115\n",
      "Epoch 16/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.9565\n",
      "Epoch 00016: val_accuracy improved from 0.91149 to 0.91443, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1870 - accuracy: 0.9565 - val_loss: 0.3365 - val_accuracy: 0.9144\n",
      "Epoch 17/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9554\n",
      "Epoch 00017: val_accuracy did not improve from 0.91443\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1837 - accuracy: 0.9555 - val_loss: 0.3970 - val_accuracy: 0.9008\n",
      "Epoch 18/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9565\n",
      "Epoch 00018: val_accuracy did not improve from 0.91443\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1807 - accuracy: 0.9565 - val_loss: 0.3347 - val_accuracy: 0.9141\n",
      "Epoch 19/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.9590\n",
      "Epoch 00019: val_accuracy did not improve from 0.91443\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1718 - accuracy: 0.9588 - val_loss: 0.3474 - val_accuracy: 0.9111\n",
      "Epoch 20/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.9601\n",
      "Epoch 00020: val_accuracy improved from 0.91443 to 0.91664, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1694 - accuracy: 0.9601 - val_loss: 0.3351 - val_accuracy: 0.9166\n",
      "Epoch 21/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.1601 - accuracy: 0.9631\n",
      "Epoch 00021: val_accuracy did not improve from 0.91664\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1603 - accuracy: 0.9631 - val_loss: 0.3390 - val_accuracy: 0.9152\n",
      "Epoch 22/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1593 - accuracy: 0.9623\n",
      "Epoch 00022: val_accuracy did not improve from 0.91664\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1593 - accuracy: 0.9623 - val_loss: 0.3461 - val_accuracy: 0.9144\n",
      "Epoch 23/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.1592 - accuracy: 0.9612\n",
      "Epoch 00023: val_accuracy did not improve from 0.91664\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1591 - accuracy: 0.9613 - val_loss: 0.3459 - val_accuracy: 0.9144\n",
      "Epoch 24/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.9631\n",
      "Epoch 00024: val_accuracy improved from 0.91664 to 0.91884, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1538 - accuracy: 0.9627 - val_loss: 0.3372 - val_accuracy: 0.9188\n",
      "Epoch 25/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.9658\n",
      "Epoch 00025: val_accuracy did not improve from 0.91884\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1501 - accuracy: 0.9657 - val_loss: 0.3414 - val_accuracy: 0.9170\n",
      "Epoch 26/150\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.1477 - accuracy: 0.9650\n",
      "Epoch 00026: val_accuracy did not improve from 0.91884\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1477 - accuracy: 0.9651 - val_loss: 0.3527 - val_accuracy: 0.9166\n",
      "Epoch 27/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.9660\n",
      "Epoch 00027: val_accuracy did not improve from 0.91884\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1468 - accuracy: 0.9659 - val_loss: 0.3393 - val_accuracy: 0.9185\n",
      "Epoch 28/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.9706\n",
      "Epoch 00028: val_accuracy did not improve from 0.91884\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1374 - accuracy: 0.9706 - val_loss: 0.3368 - val_accuracy: 0.9174\n",
      "Epoch 29/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9684\n",
      "Epoch 00029: val_accuracy improved from 0.91884 to 0.92104, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1408 - accuracy: 0.9685 - val_loss: 0.3361 - val_accuracy: 0.9210\n",
      "Epoch 30/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9697\n",
      "Epoch 00030: val_accuracy did not improve from 0.92104\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1372 - accuracy: 0.9698 - val_loss: 0.3567 - val_accuracy: 0.9137\n",
      "Epoch 31/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9708\n",
      "Epoch 00031: val_accuracy improved from 0.92104 to 0.92361, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.9708 - val_loss: 0.3322 - val_accuracy: 0.9236\n",
      "Epoch 32/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9727\n",
      "Epoch 00032: val_accuracy did not improve from 0.92361\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1297 - accuracy: 0.9727 - val_loss: 0.3281 - val_accuracy: 0.9214\n",
      "Epoch 33/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9718\n",
      "Epoch 00033: val_accuracy improved from 0.92361 to 0.92435, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1295 - accuracy: 0.9718 - val_loss: 0.3400 - val_accuracy: 0.9243\n",
      "Epoch 34/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9719\n",
      "Epoch 00034: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1285 - accuracy: 0.9719 - val_loss: 0.3485 - val_accuracy: 0.9177\n",
      "Epoch 35/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9716\n",
      "Epoch 00035: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1263 - accuracy: 0.9717 - val_loss: 0.3350 - val_accuracy: 0.9174\n",
      "Epoch 36/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9724\n",
      "Epoch 00036: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1246 - accuracy: 0.9724 - val_loss: 0.3363 - val_accuracy: 0.9181\n",
      "Epoch 37/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9734\n",
      "Epoch 00037: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1248 - accuracy: 0.9732 - val_loss: 0.3492 - val_accuracy: 0.9210\n",
      "Epoch 38/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9745\n",
      "Epoch 00038: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1200 - accuracy: 0.9745 - val_loss: 0.3386 - val_accuracy: 0.9218\n",
      "Epoch 39/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9746\n",
      "Epoch 00039: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1203 - accuracy: 0.9742 - val_loss: 0.3496 - val_accuracy: 0.9185\n",
      "Epoch 40/150\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9754\n",
      "Epoch 00040: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1180 - accuracy: 0.9751 - val_loss: 0.3448 - val_accuracy: 0.9229\n",
      "Epoch 41/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.1152 - accuracy: 0.9759\n",
      "Epoch 00041: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1150 - accuracy: 0.9759 - val_loss: 0.3587 - val_accuracy: 0.9218\n",
      "Epoch 42/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9757\n",
      "Epoch 00042: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1162 - accuracy: 0.9757 - val_loss: 0.3460 - val_accuracy: 0.9210\n",
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/766 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9761\n",
      "Epoch 00043: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1123 - accuracy: 0.9760 - val_loss: 0.3391 - val_accuracy: 0.9214\n",
      "Epoch 44/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9765\n",
      "Epoch 00044: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1099 - accuracy: 0.9766 - val_loss: 0.3518 - val_accuracy: 0.9170\n",
      "Epoch 45/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9785\n",
      "Epoch 00045: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1107 - accuracy: 0.9780 - val_loss: 0.3479 - val_accuracy: 0.9207\n",
      "Epoch 46/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9777\n",
      "Epoch 00046: val_accuracy did not improve from 0.92435\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1100 - accuracy: 0.9777 - val_loss: 0.3442 - val_accuracy: 0.9218\n",
      "Epoch 47/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.1095 - accuracy: 0.9782\n",
      "Epoch 00047: val_accuracy improved from 0.92435 to 0.92692, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1092 - accuracy: 0.9784 - val_loss: 0.3362 - val_accuracy: 0.9269\n",
      "Epoch 48/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9797\n",
      "Epoch 00048: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1023 - accuracy: 0.9798 - val_loss: 0.3485 - val_accuracy: 0.9196\n",
      "Epoch 49/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9779\n",
      "Epoch 00049: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1082 - accuracy: 0.9780 - val_loss: 0.3557 - val_accuracy: 0.9196\n",
      "Epoch 50/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9780\n",
      "Epoch 00050: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1039 - accuracy: 0.9780 - val_loss: 0.3798 - val_accuracy: 0.9170\n",
      "Epoch 51/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9782\n",
      "Epoch 00051: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.1071 - accuracy: 0.9781 - val_loss: 0.3558 - val_accuracy: 0.9225\n",
      "Epoch 52/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9799\n",
      "Epoch 00052: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 4ms/step - loss: 0.1012 - accuracy: 0.9800 - val_loss: 0.3558 - val_accuracy: 0.9181\n",
      "Epoch 53/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0974 - accuracy: 0.9814\n",
      "Epoch 00053: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 4ms/step - loss: 0.0974 - accuracy: 0.9814 - val_loss: 0.3712 - val_accuracy: 0.9185\n",
      "Epoch 54/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0978 - accuracy: 0.9804\n",
      "Epoch 00054: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 4ms/step - loss: 0.0977 - accuracy: 0.9805 - val_loss: 0.3654 - val_accuracy: 0.9192\n",
      "Epoch 55/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9813\n",
      "Epoch 00055: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0964 - accuracy: 0.9813 - val_loss: 0.3601 - val_accuracy: 0.9240\n",
      "Epoch 56/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0966 - accuracy: 0.9810\n",
      "Epoch 00056: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0965 - accuracy: 0.9811 - val_loss: 0.3642 - val_accuracy: 0.9203\n",
      "Epoch 57/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0940 - accuracy: 0.9816\n",
      "Epoch 00057: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0944 - accuracy: 0.9815 - val_loss: 0.3635 - val_accuracy: 0.9174\n",
      "Epoch 58/150\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.0962 - accuracy: 0.9810\n",
      "Epoch 00058: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0963 - accuracy: 0.9809 - val_loss: 0.3778 - val_accuracy: 0.9210\n",
      "Epoch 59/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0942 - accuracy: 0.9829\n",
      "Epoch 00059: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0946 - accuracy: 0.9827 - val_loss: 0.3656 - val_accuracy: 0.9207\n",
      "Epoch 60/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9837\n",
      "Epoch 00060: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9836 - val_loss: 0.3655 - val_accuracy: 0.9258\n",
      "Epoch 61/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9810\n",
      "Epoch 00061: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9810 - val_loss: 0.3672 - val_accuracy: 0.9229\n",
      "Epoch 62/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9820\n",
      "Epoch 00062: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0923 - accuracy: 0.9819 - val_loss: 0.3736 - val_accuracy: 0.9218\n",
      "Epoch 63/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9816\n",
      "Epoch 00063: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0950 - accuracy: 0.9817 - val_loss: 0.3643 - val_accuracy: 0.9225\n",
      "Epoch 64/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9822\n",
      "Epoch 00064: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0903 - accuracy: 0.9820 - val_loss: 0.3740 - val_accuracy: 0.9210\n",
      "Epoch 65/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9814\n",
      "Epoch 00065: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0907 - accuracy: 0.9814 - val_loss: 0.3713 - val_accuracy: 0.9203\n",
      "Epoch 66/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9827\n",
      "Epoch 00066: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0881 - accuracy: 0.9827 - val_loss: 0.3722 - val_accuracy: 0.9203\n",
      "Epoch 67/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0931 - accuracy: 0.9803\n",
      "Epoch 00067: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0931 - accuracy: 0.9801 - val_loss: 0.3605 - val_accuracy: 0.9214\n",
      "Epoch 68/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0854 - accuracy: 0.9839\n",
      "Epoch 00068: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0852 - accuracy: 0.9840 - val_loss: 0.3578 - val_accuracy: 0.9218\n",
      "Epoch 69/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9847\n",
      "Epoch 00069: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0833 - accuracy: 0.9847 - val_loss: 0.3664 - val_accuracy: 0.9192\n",
      "Epoch 70/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9827\n",
      "Epoch 00070: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0875 - accuracy: 0.9827 - val_loss: 0.3501 - val_accuracy: 0.9243\n",
      "Epoch 71/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9852\n",
      "Epoch 00071: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0804 - accuracy: 0.9851 - val_loss: 0.3796 - val_accuracy: 0.9229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9835\n",
      "Epoch 00072: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0843 - accuracy: 0.9835 - val_loss: 0.3694 - val_accuracy: 0.9218\n",
      "Epoch 73/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9840\n",
      "Epoch 00073: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0839 - accuracy: 0.9840 - val_loss: 0.3784 - val_accuracy: 0.9188\n",
      "Epoch 74/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9824\n",
      "Epoch 00074: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0871 - accuracy: 0.9824 - val_loss: 0.3618 - val_accuracy: 0.9207\n",
      "Epoch 75/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9826\n",
      "Epoch 00075: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0876 - accuracy: 0.9825 - val_loss: 0.3715 - val_accuracy: 0.9218\n",
      "Epoch 76/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9849\n",
      "Epoch 00076: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0812 - accuracy: 0.9850 - val_loss: 0.3687 - val_accuracy: 0.9243\n",
      "Epoch 77/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0829 - accuracy: 0.9834\n",
      "Epoch 00077: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0828 - accuracy: 0.9835 - val_loss: 0.3751 - val_accuracy: 0.9221\n",
      "Epoch 78/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9829\n",
      "Epoch 00078: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0857 - accuracy: 0.9830 - val_loss: 0.3780 - val_accuracy: 0.9166\n",
      "Epoch 79/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9833\n",
      "Epoch 00079: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0815 - accuracy: 0.9834 - val_loss: 0.3678 - val_accuracy: 0.9214\n",
      "Epoch 80/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9845\n",
      "Epoch 00080: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9845 - val_loss: 0.3647 - val_accuracy: 0.9243\n",
      "Epoch 81/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9856\n",
      "Epoch 00081: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9854 - val_loss: 0.3624 - val_accuracy: 0.9225\n",
      "Epoch 82/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9836\n",
      "Epoch 00082: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9836 - val_loss: 0.3719 - val_accuracy: 0.9221\n",
      "Epoch 83/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9854\n",
      "Epoch 00083: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9854 - val_loss: 0.3636 - val_accuracy: 0.9269\n",
      "Epoch 84/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9854\n",
      "Epoch 00084: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0775 - accuracy: 0.9856 - val_loss: 0.3620 - val_accuracy: 0.9218\n",
      "Epoch 85/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9855\n",
      "Epoch 00085: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9856 - val_loss: 0.3756 - val_accuracy: 0.9221\n",
      "Epoch 86/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9862\n",
      "Epoch 00086: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0763 - accuracy: 0.9863 - val_loss: 0.3879 - val_accuracy: 0.9185\n",
      "Epoch 87/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9849\n",
      "Epoch 00087: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0784 - accuracy: 0.9849 - val_loss: 0.3617 - val_accuracy: 0.9258\n",
      "Epoch 88/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9852\n",
      "Epoch 00088: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 3ms/step - loss: 0.0760 - accuracy: 0.9849 - val_loss: 0.3774 - val_accuracy: 0.9214\n",
      "Epoch 89/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9874\n",
      "Epoch 00089: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0727 - accuracy: 0.9873 - val_loss: 0.3802 - val_accuracy: 0.9229\n",
      "Epoch 90/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9864\n",
      "Epoch 00090: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0750 - accuracy: 0.9862 - val_loss: 0.3703 - val_accuracy: 0.9229\n",
      "Epoch 91/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9861\n",
      "Epoch 00091: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9861 - val_loss: 0.3723 - val_accuracy: 0.9221\n",
      "Epoch 92/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9859\n",
      "Epoch 00092: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0755 - accuracy: 0.9858 - val_loss: 0.3834 - val_accuracy: 0.9214\n",
      "Epoch 93/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9856\n",
      "Epoch 00093: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9856 - val_loss: 0.3660 - val_accuracy: 0.9251\n",
      "Epoch 94/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9855\n",
      "Epoch 00094: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0744 - accuracy: 0.9856 - val_loss: 0.3949 - val_accuracy: 0.9207\n",
      "Epoch 95/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9862\n",
      "Epoch 00095: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0761 - accuracy: 0.9862 - val_loss: 0.3805 - val_accuracy: 0.9207\n",
      "Epoch 96/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9868\n",
      "Epoch 00096: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0706 - accuracy: 0.9867 - val_loss: 0.3780 - val_accuracy: 0.9203\n",
      "Epoch 97/150\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9873\n",
      "Epoch 00097: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0723 - accuracy: 0.9874 - val_loss: 0.3667 - val_accuracy: 0.9232\n",
      "Epoch 98/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.9884\n",
      "Epoch 00098: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0679 - accuracy: 0.9885 - val_loss: 0.3940 - val_accuracy: 0.9192\n",
      "Epoch 99/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9865\n",
      "Epoch 00099: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9866 - val_loss: 0.3833 - val_accuracy: 0.9232\n",
      "Epoch 100/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9851\n",
      "Epoch 00100: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9850 - val_loss: 0.3681 - val_accuracy: 0.9240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9872\n",
      "Epoch 00101: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9871 - val_loss: 0.3625 - val_accuracy: 0.9262\n",
      "Epoch 102/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0699 - accuracy: 0.9874\n",
      "Epoch 00102: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9874 - val_loss: 0.3770 - val_accuracy: 0.9243\n",
      "Epoch 103/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9853\n",
      "Epoch 00103: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9852 - val_loss: 0.3792 - val_accuracy: 0.9170\n",
      "Epoch 104/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9871\n",
      "Epoch 00104: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0691 - accuracy: 0.9871 - val_loss: 0.3879 - val_accuracy: 0.9225\n",
      "Epoch 105/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9879\n",
      "Epoch 00105: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0677 - accuracy: 0.9879 - val_loss: 0.3773 - val_accuracy: 0.9214\n",
      "Epoch 106/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9878\n",
      "Epoch 00106: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0712 - accuracy: 0.9878 - val_loss: 0.3733 - val_accuracy: 0.9254\n",
      "Epoch 107/150\n",
      "757/766 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9894\n",
      "Epoch 00107: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0651 - accuracy: 0.9893 - val_loss: 0.3697 - val_accuracy: 0.9236\n",
      "Epoch 108/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9881\n",
      "Epoch 00108: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9882 - val_loss: 0.3667 - val_accuracy: 0.9269\n",
      "Epoch 109/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.9876\n",
      "Epoch 00109: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9876 - val_loss: 0.3923 - val_accuracy: 0.9236\n",
      "Epoch 110/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9876\n",
      "Epoch 00110: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0674 - accuracy: 0.9875 - val_loss: 0.3901 - val_accuracy: 0.9221\n",
      "Epoch 111/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9876\n",
      "Epoch 00111: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9876 - val_loss: 0.3891 - val_accuracy: 0.9203\n",
      "Epoch 112/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9877\n",
      "Epoch 00112: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9878 - val_loss: 0.3658 - val_accuracy: 0.9218\n",
      "Epoch 113/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9885\n",
      "Epoch 00113: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9886 - val_loss: 0.3712 - val_accuracy: 0.9232\n",
      "Epoch 114/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9900\n",
      "Epoch 00114: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9900 - val_loss: 0.3842 - val_accuracy: 0.9243\n",
      "Epoch 115/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9895\n",
      "Epoch 00115: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 3s 3ms/step - loss: 0.0644 - accuracy: 0.9895 - val_loss: 0.3905 - val_accuracy: 0.9192\n",
      "Epoch 116/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9877\n",
      "Epoch 00116: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0672 - accuracy: 0.9876 - val_loss: 0.3658 - val_accuracy: 0.9196\n",
      "Epoch 117/150\n",
      "756/766 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9887\n",
      "Epoch 00117: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0637 - accuracy: 0.9887 - val_loss: 0.3721 - val_accuracy: 0.9232\n",
      "Epoch 118/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9881\n",
      "Epoch 00118: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9879 - val_loss: 0.3645 - val_accuracy: 0.9240\n",
      "Epoch 119/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9870\n",
      "Epoch 00119: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9870 - val_loss: 0.3753 - val_accuracy: 0.9221\n",
      "Epoch 120/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.9883\n",
      "Epoch 00120: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0655 - accuracy: 0.9880 - val_loss: 0.3611 - val_accuracy: 0.9240\n",
      "Epoch 121/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9893\n",
      "Epoch 00121: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0638 - accuracy: 0.9891 - val_loss: 0.4126 - val_accuracy: 0.9210\n",
      "Epoch 122/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9878\n",
      "Epoch 00122: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9878 - val_loss: 0.3858 - val_accuracy: 0.9243\n",
      "Epoch 123/150\n",
      "759/766 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9879\n",
      "Epoch 00123: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0650 - accuracy: 0.9878 - val_loss: 0.3743 - val_accuracy: 0.9232\n",
      "Epoch 124/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9884\n",
      "Epoch 00124: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9884 - val_loss: 0.3802 - val_accuracy: 0.9243\n",
      "Epoch 125/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0588 - accuracy: 0.9903\n",
      "Epoch 00125: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0586 - accuracy: 0.9904 - val_loss: 0.3635 - val_accuracy: 0.9269\n",
      "Epoch 126/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9883\n",
      "Epoch 00126: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0623 - accuracy: 0.9885 - val_loss: 0.3791 - val_accuracy: 0.9214\n",
      "Epoch 127/150\n",
      "748/766 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9885\n",
      "Epoch 00127: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0630 - accuracy: 0.9887 - val_loss: 0.3746 - val_accuracy: 0.9210\n",
      "Epoch 128/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9880\n",
      "Epoch 00128: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9880 - val_loss: 0.3800 - val_accuracy: 0.9232\n",
      "Epoch 129/150\n",
      "753/766 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9899\n",
      "Epoch 00129: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0581 - accuracy: 0.9900 - val_loss: 0.3744 - val_accuracy: 0.9229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0597 - accuracy: 0.9896\n",
      "Epoch 00130: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9896 - val_loss: 0.3758 - val_accuracy: 0.9214\n",
      "Epoch 131/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9897\n",
      "Epoch 00131: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0594 - accuracy: 0.9898 - val_loss: 0.4008 - val_accuracy: 0.9199\n",
      "Epoch 132/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9882\n",
      "Epoch 00132: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0627 - accuracy: 0.9882 - val_loss: 0.3997 - val_accuracy: 0.9210\n",
      "Epoch 133/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9876\n",
      "Epoch 00133: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0642 - accuracy: 0.9877 - val_loss: 0.3982 - val_accuracy: 0.9199\n",
      "Epoch 134/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9882\n",
      "Epoch 00134: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9883 - val_loss: 0.3908 - val_accuracy: 0.9221\n",
      "Epoch 135/150\n",
      "751/766 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9902\n",
      "Epoch 00135: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0577 - accuracy: 0.9902 - val_loss: 0.3870 - val_accuracy: 0.9221\n",
      "Epoch 136/150\n",
      "765/766 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9889\n",
      "Epoch 00136: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0637 - accuracy: 0.9889 - val_loss: 0.3972 - val_accuracy: 0.9192\n",
      "Epoch 137/150\n",
      "766/766 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9893\n",
      "Epoch 00137: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0598 - accuracy: 0.9893 - val_loss: 0.3964 - val_accuracy: 0.9188\n",
      "Epoch 138/150\n",
      "754/766 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9887\n",
      "Epoch 00138: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0629 - accuracy: 0.9886 - val_loss: 0.3819 - val_accuracy: 0.9185\n",
      "Epoch 139/150\n",
      "763/766 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9898\n",
      "Epoch 00139: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0593 - accuracy: 0.9898 - val_loss: 0.3776 - val_accuracy: 0.9243\n",
      "Epoch 140/150\n",
      "752/766 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9894\n",
      "Epoch 00140: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0602 - accuracy: 0.9893 - val_loss: 0.3831 - val_accuracy: 0.9221\n",
      "Epoch 141/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0579 - accuracy: 0.9900\n",
      "Epoch 00141: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0578 - accuracy: 0.9901 - val_loss: 0.3936 - val_accuracy: 0.9203\n",
      "Epoch 142/150\n",
      "762/766 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9891\n",
      "Epoch 00142: val_accuracy did not improve from 0.92692\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0598 - accuracy: 0.9890 - val_loss: 0.3863 - val_accuracy: 0.9236\n",
      "Epoch 143/150\n",
      "758/766 [============================>.] - ETA: 0s - loss: 0.0591 - accuracy: 0.9904\n",
      "Epoch 00143: val_accuracy improved from 0.92692 to 0.92729, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0593 - accuracy: 0.9903 - val_loss: 0.3800 - val_accuracy: 0.9273\n",
      "Epoch 144/150\n",
      "755/766 [============================>.] - ETA: 0s - loss: 0.0566 - accuracy: 0.9903\n",
      "Epoch 00144: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 3s 3ms/step - loss: 0.0564 - accuracy: 0.9903 - val_loss: 0.3927 - val_accuracy: 0.9221\n",
      "Epoch 145/150\n",
      "761/766 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9903\n",
      "Epoch 00145: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0575 - accuracy: 0.9903 - val_loss: 0.3910 - val_accuracy: 0.9214\n",
      "Epoch 146/150\n",
      "749/766 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9894\n",
      "Epoch 00146: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0581 - accuracy: 0.9895 - val_loss: 0.3739 - val_accuracy: 0.9221\n",
      "Epoch 147/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9899\n",
      "Epoch 00147: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0554 - accuracy: 0.9899 - val_loss: 0.3771 - val_accuracy: 0.9254\n",
      "Epoch 148/150\n",
      "760/766 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9887\n",
      "Epoch 00148: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0589 - accuracy: 0.9887 - val_loss: 0.3675 - val_accuracy: 0.9232\n",
      "Epoch 149/150\n",
      "750/766 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9885\n",
      "Epoch 00149: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9884 - val_loss: 0.3779 - val_accuracy: 0.9225\n",
      "Epoch 150/150\n",
      "764/766 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9908\n",
      "Epoch 00150: val_accuracy did not improve from 0.92729\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.0563 - accuracy: 0.9908 - val_loss: 0.3884 - val_accuracy: 0.9229\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [09:03<00:00, 271.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.64      0.85      0.73       693\n",
      "        car_horn       0.93      0.67      0.78       686\n",
      "children_playing       0.69      0.71      0.70       700\n",
      "        dog_bark       0.78      0.74      0.76       700\n",
      "           siren       0.82      0.79      0.80       497\n",
      "\n",
      "        accuracy                           0.75      3276\n",
      "       macro avg       0.77      0.75      0.75      3276\n",
      "    weighted avg       0.77      0.75      0.75      3276\n",
      "\n",
      "Validation fold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27706, 375)\n",
      "X_val_norm shape.....:(2800, 375)\n",
      "\n",
      "Sum of elements: 0.9803388072660862\n",
      "Number of elements summed: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 233)               54522     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 233)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               175500    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 288,299\n",
      "Trainable params: 288,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24935, 233)\n",
      "Epoch 1/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.7935 - accuracy: 0.7109\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82533, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 2ms/step - loss: 0.7892 - accuracy: 0.7127 - val_loss: 0.4813 - val_accuracy: 0.8253\n",
      "Epoch 2/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.4496 - accuracy: 0.8399\n",
      "Epoch 00002: val_accuracy improved from 0.82533 to 0.87044, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.4493 - accuracy: 0.8400 - val_loss: 0.3700 - val_accuracy: 0.8704\n",
      "Epoch 3/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.3493 - accuracy: 0.8750\n",
      "Epoch 00003: val_accuracy improved from 0.87044 to 0.88813, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.3492 - accuracy: 0.8751 - val_loss: 0.3171 - val_accuracy: 0.8881\n",
      "Epoch 4/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9006\n",
      "Epoch 00004: val_accuracy improved from 0.88813 to 0.89643, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.2847 - accuracy: 0.9011 - val_loss: 0.2851 - val_accuracy: 0.8964\n",
      "Epoch 5/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.9158\n",
      "Epoch 00005: val_accuracy improved from 0.89643 to 0.90870, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.2433 - accuracy: 0.9157 - val_loss: 0.2539 - val_accuracy: 0.9087\n",
      "Epoch 6/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.2010 - accuracy: 0.9314\n",
      "Epoch 00006: val_accuracy improved from 0.90870 to 0.91519, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.2008 - accuracy: 0.9314 - val_loss: 0.2428 - val_accuracy: 0.9152\n",
      "Epoch 7/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.9407\n",
      "Epoch 00007: val_accuracy improved from 0.91519 to 0.92277, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.1694 - accuracy: 0.9411 - val_loss: 0.2222 - val_accuracy: 0.9228\n",
      "Epoch 8/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9516\n",
      "Epoch 00008: val_accuracy improved from 0.92277 to 0.92494, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.1424 - accuracy: 0.9513 - val_loss: 0.2147 - val_accuracy: 0.9249\n",
      "Epoch 9/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9590\n",
      "Epoch 00009: val_accuracy improved from 0.92494 to 0.93035, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.1225 - accuracy: 0.9590 - val_loss: 0.1985 - val_accuracy: 0.9304\n",
      "Epoch 10/350\n",
      "747/780 [===========================>..] - ETA: 0s - loss: 0.1063 - accuracy: 0.9636\n",
      "Epoch 00010: val_accuracy improved from 0.93035 to 0.93396, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.1060 - accuracy: 0.9641 - val_loss: 0.1946 - val_accuracy: 0.9340\n",
      "Epoch 11/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9715\n",
      "Epoch 00011: val_accuracy improved from 0.93396 to 0.93432, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.9716 - val_loss: 0.1925 - val_accuracy: 0.9343\n",
      "Epoch 12/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9762\n",
      "Epoch 00012: val_accuracy did not improve from 0.93432\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0771 - accuracy: 0.9763 - val_loss: 0.1908 - val_accuracy: 0.9322\n",
      "Epoch 13/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0647 - accuracy: 0.9794\n",
      "Epoch 00013: val_accuracy did not improve from 0.93432\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0646 - accuracy: 0.9795 - val_loss: 0.1902 - val_accuracy: 0.9329\n",
      "Epoch 14/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0524 - accuracy: 0.9839\n",
      "Epoch 00014: val_accuracy improved from 0.93432 to 0.94262, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0527 - accuracy: 0.9839 - val_loss: 0.1860 - val_accuracy: 0.9426\n",
      "Epoch 15/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9858\n",
      "Epoch 00015: val_accuracy did not improve from 0.94262\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0464 - accuracy: 0.9858 - val_loss: 0.1830 - val_accuracy: 0.9412\n",
      "Epoch 16/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9882\n",
      "Epoch 00016: val_accuracy did not improve from 0.94262\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9883 - val_loss: 0.1818 - val_accuracy: 0.9405\n",
      "Epoch 17/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0348 - accuracy: 0.9900\n",
      "Epoch 00017: val_accuracy improved from 0.94262 to 0.94406, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0348 - accuracy: 0.9900 - val_loss: 0.1842 - val_accuracy: 0.9441\n",
      "Epoch 18/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0287 - accuracy: 0.9925\n",
      "Epoch 00018: val_accuracy did not improve from 0.94406\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0287 - accuracy: 0.9924 - val_loss: 0.1890 - val_accuracy: 0.9419\n",
      "Epoch 19/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0268 - accuracy: 0.9927\n",
      "Epoch 00019: val_accuracy improved from 0.94406 to 0.94695, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9928 - val_loss: 0.1896 - val_accuracy: 0.9470\n",
      "Epoch 20/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 0.0220 - accuracy: 0.9947\n",
      "Epoch 00020: val_accuracy did not improve from 0.94695\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9947 - val_loss: 0.1966 - val_accuracy: 0.9423\n",
      "Epoch 21/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9948\n",
      "Epoch 00021: val_accuracy did not improve from 0.94695\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9948 - val_loss: 0.1981 - val_accuracy: 0.9444\n",
      "Epoch 22/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0189 - accuracy: 0.9955\n",
      "Epoch 00022: val_accuracy did not improve from 0.94695\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9955 - val_loss: 0.1998 - val_accuracy: 0.9462\n",
      "Epoch 23/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9971\n",
      "Epoch 00023: val_accuracy improved from 0.94695 to 0.94767, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0151 - accuracy: 0.9971 - val_loss: 0.2024 - val_accuracy: 0.9477\n",
      "Epoch 24/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9971\n",
      "Epoch 00024: val_accuracy improved from 0.94767 to 0.95020, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0140 - accuracy: 0.9971 - val_loss: 0.2008 - val_accuracy: 0.9502\n",
      "Epoch 25/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9967\n",
      "Epoch 00025: val_accuracy did not improve from 0.95020\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0129 - accuracy: 0.9967 - val_loss: 0.2069 - val_accuracy: 0.9488\n",
      "Epoch 26/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9972\n",
      "Epoch 00026: val_accuracy improved from 0.95020 to 0.95092, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.2019 - val_accuracy: 0.9509\n",
      "Epoch 27/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 0.0101 - accuracy: 0.9979\n",
      "Epoch 00027: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0101 - accuracy: 0.9979 - val_loss: 0.2070 - val_accuracy: 0.9509\n",
      "Epoch 28/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0092 - accuracy: 0.9977\n",
      "Epoch 00028: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0092 - accuracy: 0.9978 - val_loss: 0.2048 - val_accuracy: 0.9448\n",
      "Epoch 29/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0092 - accuracy: 0.9977\n",
      "Epoch 00029: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.2064 - val_accuracy: 0.9462\n",
      "Epoch 30/350\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9984\n",
      "Epoch 00030: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 0.2221 - val_accuracy: 0.9444\n",
      "Epoch 31/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9980\n",
      "Epoch 00031: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 0.2111 - val_accuracy: 0.9488\n",
      "Epoch 32/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9983\n",
      "Epoch 00032: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9983 - val_loss: 0.2168 - val_accuracy: 0.9462\n",
      "Epoch 33/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9988\n",
      "Epoch 00033: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.2193 - val_accuracy: 0.9488\n",
      "Epoch 34/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0062 - accuracy: 0.9988\n",
      "Epoch 00034: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 0.2160 - val_accuracy: 0.9491\n",
      "Epoch 35/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9990\n",
      "Epoch 00035: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.2210 - val_accuracy: 0.9470\n",
      "Epoch 36/350\n",
      "751/780 [===========================>..] - ETA: 0s - loss: 0.0048 - accuracy: 0.9993\n",
      "Epoch 00036: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.2255 - val_accuracy: 0.9462\n",
      "Epoch 37/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9989\n",
      "Epoch 00037: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9990 - val_loss: 0.2266 - val_accuracy: 0.9506\n",
      "Epoch 38/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 00038: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.2264 - val_accuracy: 0.9484\n",
      "Epoch 39/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9990\n",
      "Epoch 00039: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.2203 - val_accuracy: 0.9488\n",
      "Epoch 40/350\n",
      "747/780 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 00040: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.2236 - val_accuracy: 0.9509\n",
      "Epoch 41/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 00041: val_accuracy did not improve from 0.95092\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.2242 - val_accuracy: 0.9509\n",
      "Epoch 42/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 00042: val_accuracy improved from 0.95092 to 0.95200, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.2220 - val_accuracy: 0.9520\n",
      "Epoch 43/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9991\n",
      "Epoch 00043: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.2217 - val_accuracy: 0.9498\n",
      "Epoch 44/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9991\n",
      "Epoch 00044: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.2313 - val_accuracy: 0.9491\n",
      "Epoch 45/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749/780 [===========================>..] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00045: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2292 - val_accuracy: 0.9491\n",
      "Epoch 46/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 00046: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.2332 - val_accuracy: 0.9480\n",
      "Epoch 47/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 00047: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.2344 - val_accuracy: 0.9506\n",
      "Epoch 48/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 00048: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.2384 - val_accuracy: 0.9498\n",
      "Epoch 49/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9996\n",
      "Epoch 00049: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2364 - val_accuracy: 0.9484\n",
      "Epoch 50/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 00050: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.2365 - val_accuracy: 0.9509\n",
      "Epoch 51/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9996\n",
      "Epoch 00051: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.2367 - val_accuracy: 0.9498\n",
      "Epoch 52/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00052: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2400 - val_accuracy: 0.9506\n",
      "Epoch 53/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9998\n",
      "Epoch 00053: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.2426 - val_accuracy: 0.9502\n",
      "Epoch 54/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9996\n",
      "Epoch 00054: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.2381 - val_accuracy: 0.9506\n",
      "Epoch 55/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9997\n",
      "Epoch 00055: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2377 - val_accuracy: 0.9495\n",
      "Epoch 56/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9996\n",
      "Epoch 00056: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2353 - val_accuracy: 0.9520\n",
      "Epoch 57/350\n",
      "747/780 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9997\n",
      "Epoch 00057: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.2346 - val_accuracy: 0.9502\n",
      "Epoch 58/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00058: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2393 - val_accuracy: 0.9520\n",
      "Epoch 59/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00059: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2384 - val_accuracy: 0.9516\n",
      "Epoch 60/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00060: val_accuracy did not improve from 0.95200\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2361 - val_accuracy: 0.9520\n",
      "Epoch 61/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00061: val_accuracy improved from 0.95200 to 0.95345, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2386 - val_accuracy: 0.9534\n",
      "Epoch 62/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9997\n",
      "Epoch 00062: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.2364 - val_accuracy: 0.9516\n",
      "Epoch 63/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00063: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2404 - val_accuracy: 0.9534\n",
      "Epoch 64/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00064: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2419 - val_accuracy: 0.9513\n",
      "Epoch 65/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00065: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2471 - val_accuracy: 0.9491\n",
      "Epoch 66/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 00066: val_accuracy did not improve from 0.95345\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.2500 - val_accuracy: 0.9495\n",
      "Epoch 67/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 00067: val_accuracy improved from 0.95345 to 0.95381, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.2472 - val_accuracy: 0.9538\n",
      "Epoch 68/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2492 - val_accuracy: 0.9524\n",
      "Epoch 69/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00069: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2476 - val_accuracy: 0.9534\n",
      "Epoch 70/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00070: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.2450 - val_accuracy: 0.9516\n",
      "Epoch 71/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00071: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2461 - val_accuracy: 0.9531\n",
      "Epoch 72/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9994\n",
      "Epoch 00072: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.2431 - val_accuracy: 0.9513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00073: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2448 - val_accuracy: 0.9531\n",
      "Epoch 74/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 00074: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.2453 - val_accuracy: 0.9498\n",
      "Epoch 75/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00075: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2415 - val_accuracy: 0.9520\n",
      "Epoch 76/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9998\n",
      "Epoch 00076: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.2447 - val_accuracy: 0.9516\n",
      "Epoch 77/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00077: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2498 - val_accuracy: 0.9513\n",
      "Epoch 78/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00078: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2466 - val_accuracy: 0.9520\n",
      "Epoch 79/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2476 - val_accuracy: 0.9538\n",
      "Epoch 80/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00080: val_accuracy did not improve from 0.95381\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2466 - val_accuracy: 0.9538\n",
      "Epoch 81/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00081: val_accuracy improved from 0.95381 to 0.95417, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2451 - val_accuracy: 0.9542\n",
      "Epoch 82/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00082: val_accuracy did not improve from 0.95417\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2464 - val_accuracy: 0.9542\n",
      "Epoch 83/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00083: val_accuracy did not improve from 0.95417\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2485 - val_accuracy: 0.9542\n",
      "Epoch 84/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.95417\n",
      "780/780 [==============================] - 2s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2447 - val_accuracy: 0.9516\n",
      "Epoch 85/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00085: val_accuracy improved from 0.95417 to 0.95669, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2480 - val_accuracy: 0.9567\n",
      "Epoch 86/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00086: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2465 - val_accuracy: 0.9545\n",
      "Epoch 87/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 9.3253e-04 - accuracy: 0.9999\n",
      "Epoch 00087: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.7960e-04 - accuracy: 0.9998 - val_loss: 0.2493 - val_accuracy: 0.9520\n",
      "Epoch 88/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00088: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2520 - val_accuracy: 0.9524\n",
      "Epoch 89/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2538 - val_accuracy: 0.9553\n",
      "Epoch 90/350\n",
      "751/780 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00090: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2582 - val_accuracy: 0.9538\n",
      "Epoch 91/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 9.2646e-04 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.5668e-04 - accuracy: 0.9999 - val_loss: 0.2518 - val_accuracy: 0.9531\n",
      "Epoch 92/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 9.0188e-04 - accuracy: 0.9999\n",
      "Epoch 00092: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.0356e-04 - accuracy: 0.9999 - val_loss: 0.2541 - val_accuracy: 0.9531\n",
      "Epoch 93/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 9.8097e-04 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2544 - val_accuracy: 0.9542\n",
      "Epoch 94/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 7.1187e-04 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.1963e-04 - accuracy: 0.9999 - val_loss: 0.2553 - val_accuracy: 0.9534\n",
      "Epoch 95/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00095: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2564 - val_accuracy: 0.9534\n",
      "Epoch 96/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 9.6102e-04 - accuracy: 0.9999\n",
      "Epoch 00096: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.5234e-04 - accuracy: 0.9999 - val_loss: 0.2555 - val_accuracy: 0.9534\n",
      "Epoch 97/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 6.3338e-04 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.2553e-04 - accuracy: 1.0000 - val_loss: 0.2568 - val_accuracy: 0.9553\n",
      "Epoch 98/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 9.2115e-04 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.1297e-04 - accuracy: 0.9998 - val_loss: 0.2564 - val_accuracy: 0.9545\n",
      "Epoch 99/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 8.3270e-04 - accuracy: 0.9999\n",
      "Epoch 00099: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 8.2746e-04 - accuracy: 0.9999 - val_loss: 0.2573 - val_accuracy: 0.9549\n",
      "Epoch 100/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 7.6936e-04 - accuracy: 1.0000\n",
      "Epoch 00100: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.6506e-04 - accuracy: 1.0000 - val_loss: 0.2600 - val_accuracy: 0.9542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 7.6234e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.6733e-04 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 0.9531\n",
      "Epoch 102/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 7.1378e-04 - accuracy: 0.9998\n",
      "Epoch 00102: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.0584e-04 - accuracy: 0.9998 - val_loss: 0.2653 - val_accuracy: 0.9534\n",
      "Epoch 103/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 6.8157e-04 - accuracy: 0.9999\n",
      "Epoch 00103: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.8374e-04 - accuracy: 0.9999 - val_loss: 0.2593 - val_accuracy: 0.9524\n",
      "Epoch 104/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00104: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2580 - val_accuracy: 0.9542\n",
      "Epoch 105/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 8.5139e-04 - accuracy: 0.9998\n",
      "Epoch 00105: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.1012e-04 - accuracy: 0.9998 - val_loss: 0.2560 - val_accuracy: 0.9545\n",
      "Epoch 106/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 9.0172e-04 - accuracy: 0.9998\n",
      "Epoch 00106: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 9.1502e-04 - accuracy: 0.9998 - val_loss: 0.2543 - val_accuracy: 0.9542\n",
      "Epoch 107/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 6.9474e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.95669\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.9455e-04 - accuracy: 1.0000 - val_loss: 0.2532 - val_accuracy: 0.9563\n",
      "Epoch 108/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 6.6340e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy improved from 0.95669 to 0.95706, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.5450e-04 - accuracy: 0.9999 - val_loss: 0.2535 - val_accuracy: 0.9571\n",
      "Epoch 109/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 6.9743e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy improved from 0.95706 to 0.95778, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.9743e-04 - accuracy: 0.9999 - val_loss: 0.2555 - val_accuracy: 0.9578\n",
      "Epoch 110/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 8.0544e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.9386e-04 - accuracy: 0.9999 - val_loss: 0.2585 - val_accuracy: 0.9571\n",
      "Epoch 111/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 5.8293e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.7453e-04 - accuracy: 1.0000 - val_loss: 0.2626 - val_accuracy: 0.9567\n",
      "Epoch 112/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 5.8116e-04 - accuracy: 0.9999\n",
      "Epoch 00112: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.8041e-04 - accuracy: 0.9999 - val_loss: 0.2617 - val_accuracy: 0.9556\n",
      "Epoch 113/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 8.1534e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 8.1344e-04 - accuracy: 0.9999 - val_loss: 0.2626 - val_accuracy: 0.9563\n",
      "Epoch 114/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 7.6083e-04 - accuracy: 0.9999\n",
      "Epoch 00114: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.5057e-04 - accuracy: 0.9999 - val_loss: 0.2664 - val_accuracy: 0.9553\n",
      "Epoch 115/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 4.9412e-04 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.9399e-04 - accuracy: 1.0000 - val_loss: 0.2719 - val_accuracy: 0.9549\n",
      "Epoch 116/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 4.6711e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.6093e-04 - accuracy: 1.0000 - val_loss: 0.2704 - val_accuracy: 0.9567\n",
      "Epoch 117/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 4.7336e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.6921e-04 - accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 0.9560\n",
      "Epoch 118/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 4.1190e-04 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.1038e-04 - accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 0.9560\n",
      "Epoch 119/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 5.8303e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.8288e-04 - accuracy: 0.9999 - val_loss: 0.2736 - val_accuracy: 0.9571\n",
      "Epoch 120/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 5.8408e-04 - accuracy: 0.9999\n",
      "Epoch 00120: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.9158e-04 - accuracy: 0.9999 - val_loss: 0.2764 - val_accuracy: 0.9553\n",
      "Epoch 121/350\n",
      "746/780 [===========================>..] - ETA: 0s - loss: 6.8764e-04 - accuracy: 0.9998\n",
      "Epoch 00121: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.7241e-04 - accuracy: 0.9998 - val_loss: 0.2735 - val_accuracy: 0.9556\n",
      "Epoch 122/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 6.4510e-04 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.2948e-04 - accuracy: 0.9999 - val_loss: 0.2739 - val_accuracy: 0.9563\n",
      "Epoch 123/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 6.0940e-04 - accuracy: 0.9999\n",
      "Epoch 00123: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.1512e-04 - accuracy: 0.9999 - val_loss: 0.2741 - val_accuracy: 0.9531\n",
      "Epoch 124/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 7.9925e-04 - accuracy: 0.9998\n",
      "Epoch 00124: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.9086e-04 - accuracy: 0.9998 - val_loss: 0.2706 - val_accuracy: 0.9556\n",
      "Epoch 125/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 5.6709e-04 - accuracy: 1.0000\n",
      "Epoch 00125: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.5610e-04 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 0.9556\n",
      "Epoch 126/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 5.6227e-04 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.5969e-04 - accuracy: 1.0000 - val_loss: 0.2684 - val_accuracy: 0.9553\n",
      "Epoch 127/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 6.4342e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.4132e-04 - accuracy: 0.9999 - val_loss: 0.2691 - val_accuracy: 0.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 4.1995e-04 - accuracy: 1.0000\n",
      "Epoch 00128: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.1910e-04 - accuracy: 1.0000 - val_loss: 0.2686 - val_accuracy: 0.9563\n",
      "Epoch 129/350\n",
      "751/780 [===========================>..] - ETA: 0s - loss: 7.5277e-04 - accuracy: 0.9999\n",
      "Epoch 00129: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.3603e-04 - accuracy: 0.9999 - val_loss: 0.2713 - val_accuracy: 0.9553\n",
      "Epoch 130/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 5.2124e-04 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.4392e-04 - accuracy: 0.9999 - val_loss: 0.2762 - val_accuracy: 0.9560\n",
      "Epoch 131/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 4.7297e-04 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.7367e-04 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9567\n",
      "Epoch 132/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 5.5865e-04 - accuracy: 0.9999\n",
      "Epoch 00132: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.5724e-04 - accuracy: 0.9999 - val_loss: 0.2694 - val_accuracy: 0.9549\n",
      "Epoch 133/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 4.6089e-04 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.5814e-04 - accuracy: 1.0000 - val_loss: 0.2702 - val_accuracy: 0.9567\n",
      "Epoch 134/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 7.0386e-04 - accuracy: 0.9998\n",
      "Epoch 00134: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.9014e-04 - accuracy: 0.9998 - val_loss: 0.2762 - val_accuracy: 0.9563\n",
      "Epoch 135/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 7.1177e-04 - accuracy: 0.9999\n",
      "Epoch 00135: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.0166e-04 - accuracy: 0.9999 - val_loss: 0.2745 - val_accuracy: 0.9560\n",
      "Epoch 136/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 6.5052e-04 - accuracy: 0.9998\n",
      "Epoch 00136: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.5003e-04 - accuracy: 0.9998 - val_loss: 0.2742 - val_accuracy: 0.9563\n",
      "Epoch 137/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 7.1489e-04 - accuracy: 0.9998\n",
      "Epoch 00137: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.0297e-04 - accuracy: 0.9998 - val_loss: 0.2738 - val_accuracy: 0.9549\n",
      "Epoch 138/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 4.0678e-04 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.0799e-04 - accuracy: 1.0000 - val_loss: 0.2764 - val_accuracy: 0.9542\n",
      "Epoch 139/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 3.9605e-04 - accuracy: 1.0000\n",
      "Epoch 00139: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 3.9866e-04 - accuracy: 1.0000 - val_loss: 0.2761 - val_accuracy: 0.9553\n",
      "Epoch 140/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 4.6124e-04 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.5853e-04 - accuracy: 1.0000 - val_loss: 0.2781 - val_accuracy: 0.9556\n",
      "Epoch 141/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 7.8900e-04 - accuracy: 0.9998\n",
      "Epoch 00141: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.7060e-04 - accuracy: 0.9998 - val_loss: 0.2786 - val_accuracy: 0.9560\n",
      "Epoch 142/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 6.0213e-04 - accuracy: 0.9999\n",
      "Epoch 00142: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 6.0096e-04 - accuracy: 0.9999 - val_loss: 0.2753 - val_accuracy: 0.9567\n",
      "Epoch 143/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 6.8498e-04 - accuracy: 0.9999\n",
      "Epoch 00143: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.0306e-04 - accuracy: 0.9999 - val_loss: 0.2809 - val_accuracy: 0.9556\n",
      "Epoch 144/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 3.3956e-04 - accuracy: 1.0000\n",
      "Epoch 00144: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 3.3785e-04 - accuracy: 1.0000 - val_loss: 0.2793 - val_accuracy: 0.9549\n",
      "Epoch 145/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 4.8673e-04 - accuracy: 1.0000\n",
      "Epoch 00145: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.8652e-04 - accuracy: 1.0000 - val_loss: 0.2793 - val_accuracy: 0.9567\n",
      "Epoch 146/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 4.0518e-04 - accuracy: 1.0000\n",
      "Epoch 00146: val_accuracy did not improve from 0.95778\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 4.2379e-04 - accuracy: 1.0000 - val_loss: 0.2790 - val_accuracy: 0.9563\n",
      "Epoch 147/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 5.1547e-04 - accuracy: 0.9999\n",
      "Epoch 00147: val_accuracy did not improve from 0.95778\n",
      "Restoring model weights from the end of the best epoch.\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 5.0790e-04 - accuracy: 0.9999 - val_loss: 0.2771 - val_accuracy: 0.9571\n",
      "Epoch 00147: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:17<03:17, 197.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.73      0.75      0.74       686\n",
      "        car_horn       0.74      0.96      0.84       196\n",
      "children_playing       0.76      0.83      0.79       700\n",
      "        dog_bark       0.82      0.76      0.79       700\n",
      "           siren       0.81      0.68      0.74       518\n",
      "\n",
      "        accuracy                           0.77      2800\n",
      "       macro avg       0.77      0.79      0.78      2800\n",
      "    weighted avg       0.78      0.77      0.77      2800\n",
      "\n",
      "Model: \"Model_CNN_1D_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 227, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 227, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 227, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 113, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6328)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                316450    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 327,491\n",
      "Trainable params: 327,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24935, 233, 1)\n",
      "Epoch 1/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.7483 - accuracy: 0.7631\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83255, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 4s 5ms/step - loss: 0.7483 - accuracy: 0.7631 - val_loss: 0.5507 - val_accuracy: 0.8326\n",
      "Epoch 2/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.4938 - accuracy: 0.8504\n",
      "Epoch 00002: val_accuracy improved from 0.83255 to 0.86250, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.4926 - accuracy: 0.8509 - val_loss: 0.4323 - val_accuracy: 0.8625\n",
      "Epoch 3/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.8742\n",
      "Epoch 00003: val_accuracy improved from 0.86250 to 0.88307, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.4207 - accuracy: 0.8744 - val_loss: 0.3843 - val_accuracy: 0.8831\n",
      "Epoch 4/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.3748 - accuracy: 0.8925\n",
      "Epoch 00004: val_accuracy improved from 0.88307 to 0.89282, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3753 - accuracy: 0.8923 - val_loss: 0.3686 - val_accuracy: 0.8928\n",
      "Epoch 5/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.9018\n",
      "Epoch 00005: val_accuracy improved from 0.89282 to 0.89895, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3444 - accuracy: 0.9014 - val_loss: 0.3511 - val_accuracy: 0.8990\n",
      "Epoch 6/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.9120\n",
      "Epoch 00006: val_accuracy did not improve from 0.89895\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3172 - accuracy: 0.9121 - val_loss: 0.3652 - val_accuracy: 0.8881\n",
      "Epoch 7/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9177\n",
      "Epoch 00007: val_accuracy improved from 0.89895 to 0.90978, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2968 - accuracy: 0.9177 - val_loss: 0.3187 - val_accuracy: 0.9098\n",
      "Epoch 8/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.9265\n",
      "Epoch 00008: val_accuracy did not improve from 0.90978\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2754 - accuracy: 0.9263 - val_loss: 0.3225 - val_accuracy: 0.9087\n",
      "Epoch 9/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.2609 - accuracy: 0.9292\n",
      "Epoch 00009: val_accuracy did not improve from 0.90978\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2608 - accuracy: 0.9292 - val_loss: 0.3214 - val_accuracy: 0.9073\n",
      "Epoch 10/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9339\n",
      "Epoch 00010: val_accuracy improved from 0.90978 to 0.91050, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2494 - accuracy: 0.9339 - val_loss: 0.3107 - val_accuracy: 0.9105\n",
      "Epoch 11/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.2335 - accuracy: 0.9406\n",
      "Epoch 00011: val_accuracy improved from 0.91050 to 0.91231, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2340 - accuracy: 0.9404 - val_loss: 0.3175 - val_accuracy: 0.9123\n",
      "Epoch 12/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.2282 - accuracy: 0.9404\n",
      "Epoch 00012: val_accuracy improved from 0.91231 to 0.91772, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2283 - accuracy: 0.9404 - val_loss: 0.3088 - val_accuracy: 0.9177\n",
      "Epoch 13/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9436\n",
      "Epoch 00013: val_accuracy improved from 0.91772 to 0.92097, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2183 - accuracy: 0.9437 - val_loss: 0.3060 - val_accuracy: 0.9210\n",
      "Epoch 14/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.9459\n",
      "Epoch 00014: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2094 - accuracy: 0.9459 - val_loss: 0.3121 - val_accuracy: 0.9141\n",
      "Epoch 15/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.2001 - accuracy: 0.9490\n",
      "Epoch 00015: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2000 - accuracy: 0.9491 - val_loss: 0.3032 - val_accuracy: 0.9199\n",
      "Epoch 16/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9490\n",
      "Epoch 00016: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2014 - accuracy: 0.9490 - val_loss: 0.3263 - val_accuracy: 0.9130\n",
      "Epoch 17/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9511\n",
      "Epoch 00017: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1922 - accuracy: 0.9511 - val_loss: 0.2986 - val_accuracy: 0.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1847 - accuracy: 0.9547\n",
      "Epoch 00018: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1847 - accuracy: 0.9547 - val_loss: 0.3007 - val_accuracy: 0.9170\n",
      "Epoch 19/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.1782 - accuracy: 0.9563\n",
      "Epoch 00019: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1783 - accuracy: 0.9562 - val_loss: 0.3017 - val_accuracy: 0.9174\n",
      "Epoch 20/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9568\n",
      "Epoch 00020: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.1770 - accuracy: 0.9567 - val_loss: 0.3135 - val_accuracy: 0.9148\n",
      "Epoch 21/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9590\n",
      "Epoch 00021: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1717 - accuracy: 0.9589 - val_loss: 0.3058 - val_accuracy: 0.9188\n",
      "Epoch 22/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9630\n",
      "Epoch 00022: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1609 - accuracy: 0.9631 - val_loss: 0.3100 - val_accuracy: 0.9181\n",
      "Epoch 23/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.1631 - accuracy: 0.9609\n",
      "Epoch 00023: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1634 - accuracy: 0.9609 - val_loss: 0.2949 - val_accuracy: 0.9181\n",
      "Epoch 24/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9632\n",
      "Epoch 00024: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1586 - accuracy: 0.9630 - val_loss: 0.3102 - val_accuracy: 0.9206\n",
      "Epoch 25/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9651\n",
      "Epoch 00025: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1552 - accuracy: 0.9652 - val_loss: 0.3127 - val_accuracy: 0.9184\n",
      "Epoch 26/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9658\n",
      "Epoch 00026: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1517 - accuracy: 0.9659 - val_loss: 0.3094 - val_accuracy: 0.9192\n",
      "Epoch 27/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1483 - accuracy: 0.9671\n",
      "Epoch 00027: val_accuracy did not improve from 0.92097\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1481 - accuracy: 0.9673 - val_loss: 0.3199 - val_accuracy: 0.9188\n",
      "Epoch 28/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9664\n",
      "Epoch 00028: val_accuracy improved from 0.92097 to 0.92494, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1483 - accuracy: 0.9664 - val_loss: 0.3089 - val_accuracy: 0.9249\n",
      "Epoch 29/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9670\n",
      "Epoch 00029: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1481 - accuracy: 0.9668 - val_loss: 0.3089 - val_accuracy: 0.9159\n",
      "Epoch 30/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9703\n",
      "Epoch 00030: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1394 - accuracy: 0.9702 - val_loss: 0.3100 - val_accuracy: 0.9206\n",
      "Epoch 31/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9684\n",
      "Epoch 00031: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1390 - accuracy: 0.9685 - val_loss: 0.3109 - val_accuracy: 0.9199\n",
      "Epoch 32/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1370 - accuracy: 0.9695\n",
      "Epoch 00032: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1373 - accuracy: 0.9693 - val_loss: 0.3139 - val_accuracy: 0.9206\n",
      "Epoch 33/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9688\n",
      "Epoch 00033: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1391 - accuracy: 0.9690 - val_loss: 0.3198 - val_accuracy: 0.9119\n",
      "Epoch 34/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1321 - accuracy: 0.9701\n",
      "Epoch 00034: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1322 - accuracy: 0.9701 - val_loss: 0.3108 - val_accuracy: 0.9181\n",
      "Epoch 35/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1310 - accuracy: 0.9710\n",
      "Epoch 00035: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.1309 - accuracy: 0.9710 - val_loss: 0.3165 - val_accuracy: 0.9217\n",
      "Epoch 36/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9711\n",
      "Epoch 00036: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1311 - accuracy: 0.9711 - val_loss: 0.3181 - val_accuracy: 0.9224\n",
      "Epoch 37/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9705\n",
      "Epoch 00037: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1308 - accuracy: 0.9704 - val_loss: 0.3246 - val_accuracy: 0.9206\n",
      "Epoch 38/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9729\n",
      "Epoch 00038: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1259 - accuracy: 0.9726 - val_loss: 0.3159 - val_accuracy: 0.9181\n",
      "Epoch 39/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9726\n",
      "Epoch 00039: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1230 - accuracy: 0.9726 - val_loss: 0.3136 - val_accuracy: 0.9195\n",
      "Epoch 40/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9738\n",
      "Epoch 00040: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1222 - accuracy: 0.9738 - val_loss: 0.3106 - val_accuracy: 0.9249\n",
      "Epoch 41/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9741\n",
      "Epoch 00041: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1203 - accuracy: 0.9741 - val_loss: 0.3146 - val_accuracy: 0.9199\n",
      "Epoch 42/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9736\n",
      "Epoch 00042: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1213 - accuracy: 0.9736 - val_loss: 0.3242 - val_accuracy: 0.9206\n",
      "Epoch 43/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9731\n",
      "Epoch 00043: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1222 - accuracy: 0.9732 - val_loss: 0.3088 - val_accuracy: 0.9184\n",
      "Epoch 44/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9778\n",
      "Epoch 00044: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1125 - accuracy: 0.9778 - val_loss: 0.3090 - val_accuracy: 0.9210\n",
      "Epoch 45/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9756\n",
      "Epoch 00045: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1169 - accuracy: 0.9756 - val_loss: 0.3102 - val_accuracy: 0.9246\n",
      "Epoch 46/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9790\n",
      "Epoch 00046: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1073 - accuracy: 0.9790 - val_loss: 0.3148 - val_accuracy: 0.9249\n",
      "Epoch 47/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9785\n",
      "Epoch 00047: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1105 - accuracy: 0.9784 - val_loss: 0.3181 - val_accuracy: 0.9231\n",
      "Epoch 48/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9761\n",
      "Epoch 00048: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1130 - accuracy: 0.9760 - val_loss: 0.3169 - val_accuracy: 0.9235\n",
      "Epoch 49/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1067 - accuracy: 0.9786\n",
      "Epoch 00049: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1070 - accuracy: 0.9785 - val_loss: 0.3232 - val_accuracy: 0.9220\n",
      "Epoch 50/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9777\n",
      "Epoch 00050: val_accuracy did not improve from 0.92494\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1066 - accuracy: 0.9777 - val_loss: 0.3316 - val_accuracy: 0.9206\n",
      "Epoch 51/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9781\n",
      "Epoch 00051: val_accuracy improved from 0.92494 to 0.92602, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1084 - accuracy: 0.9781 - val_loss: 0.3135 - val_accuracy: 0.9260\n",
      "Epoch 52/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9795\n",
      "Epoch 00052: val_accuracy did not improve from 0.92602\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1041 - accuracy: 0.9796 - val_loss: 0.3303 - val_accuracy: 0.9213\n",
      "Epoch 53/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9777\n",
      "Epoch 00053: val_accuracy did not improve from 0.92602\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9777 - val_loss: 0.3282 - val_accuracy: 0.9235\n",
      "Epoch 54/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9795\n",
      "Epoch 00054: val_accuracy improved from 0.92602 to 0.92638, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1019 - accuracy: 0.9793 - val_loss: 0.3120 - val_accuracy: 0.9264\n",
      "Epoch 55/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.1045 - accuracy: 0.9791\n",
      "Epoch 00055: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1046 - accuracy: 0.9790 - val_loss: 0.3204 - val_accuracy: 0.9192\n",
      "Epoch 56/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9799\n",
      "Epoch 00056: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1010 - accuracy: 0.9799 - val_loss: 0.3199 - val_accuracy: 0.9199\n",
      "Epoch 57/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9784\n",
      "Epoch 00057: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1034 - accuracy: 0.9783 - val_loss: 0.3275 - val_accuracy: 0.9181\n",
      "Epoch 58/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9798\n",
      "Epoch 00058: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0994 - accuracy: 0.9798 - val_loss: 0.3251 - val_accuracy: 0.9202\n",
      "Epoch 59/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9816\n",
      "Epoch 00059: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0962 - accuracy: 0.9814 - val_loss: 0.3204 - val_accuracy: 0.9217\n",
      "Epoch 60/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0967 - accuracy: 0.9817\n",
      "Epoch 00060: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0965 - accuracy: 0.9818 - val_loss: 0.3396 - val_accuracy: 0.9148\n",
      "Epoch 61/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9803\n",
      "Epoch 00061: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0972 - accuracy: 0.9803 - val_loss: 0.3344 - val_accuracy: 0.9192\n",
      "Epoch 62/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.9813\n",
      "Epoch 00062: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0954 - accuracy: 0.9814 - val_loss: 0.3267 - val_accuracy: 0.9217\n",
      "Epoch 63/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9825\n",
      "Epoch 00063: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0941 - accuracy: 0.9822 - val_loss: 0.3192 - val_accuracy: 0.9231\n",
      "Epoch 64/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9803\n",
      "Epoch 00064: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0963 - accuracy: 0.9803 - val_loss: 0.3182 - val_accuracy: 0.9242\n",
      "Epoch 65/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0931 - accuracy: 0.9819\n",
      "Epoch 00065: val_accuracy did not improve from 0.92638\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9819 - val_loss: 0.3137 - val_accuracy: 0.9257\n",
      "Epoch 66/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0936 - accuracy: 0.9816\n",
      "Epoch 00066: val_accuracy improved from 0.92638 to 0.92710, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0936 - accuracy: 0.9816 - val_loss: 0.3229 - val_accuracy: 0.9271\n",
      "Epoch 67/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9796\n",
      "Epoch 00067: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0956 - accuracy: 0.9796 - val_loss: 0.3250 - val_accuracy: 0.9224\n",
      "Epoch 68/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9832\n",
      "Epoch 00068: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0912 - accuracy: 0.9833 - val_loss: 0.3259 - val_accuracy: 0.9239\n",
      "Epoch 69/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9820\n",
      "Epoch 00069: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0926 - accuracy: 0.9821 - val_loss: 0.3149 - val_accuracy: 0.9228\n",
      "Epoch 70/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9809\n",
      "Epoch 00070: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0924 - accuracy: 0.9807 - val_loss: 0.3201 - val_accuracy: 0.9206\n",
      "Epoch 71/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9823\n",
      "Epoch 00071: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0901 - accuracy: 0.9823 - val_loss: 0.3187 - val_accuracy: 0.9224\n",
      "Epoch 72/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9823\n",
      "Epoch 00072: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0907 - accuracy: 0.9823 - val_loss: 0.3444 - val_accuracy: 0.9210\n",
      "Epoch 73/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767/780 [============================>.] - ETA: 0s - loss: 0.0903 - accuracy: 0.9814\n",
      "Epoch 00073: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0904 - accuracy: 0.9813 - val_loss: 0.3313 - val_accuracy: 0.9228\n",
      "Epoch 74/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9838\n",
      "Epoch 00074: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0874 - accuracy: 0.9838 - val_loss: 0.3264 - val_accuracy: 0.9202\n",
      "Epoch 75/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0872 - accuracy: 0.9828\n",
      "Epoch 00075: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0873 - accuracy: 0.9827 - val_loss: 0.3138 - val_accuracy: 0.9217\n",
      "Epoch 76/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9825\n",
      "Epoch 00076: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0872 - accuracy: 0.9824 - val_loss: 0.3238 - val_accuracy: 0.9217\n",
      "Epoch 77/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9833\n",
      "Epoch 00077: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0865 - accuracy: 0.9833 - val_loss: 0.3259 - val_accuracy: 0.9249\n",
      "Epoch 78/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9815\n",
      "Epoch 00078: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0900 - accuracy: 0.9814 - val_loss: 0.3291 - val_accuracy: 0.9235\n",
      "Epoch 79/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.9834\n",
      "Epoch 00079: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0878 - accuracy: 0.9834 - val_loss: 0.3240 - val_accuracy: 0.9249\n",
      "Epoch 80/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0844 - accuracy: 0.9824\n",
      "Epoch 00080: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9824 - val_loss: 0.3120 - val_accuracy: 0.9260\n",
      "Epoch 81/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9856\n",
      "Epoch 00081: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0822 - accuracy: 0.9856 - val_loss: 0.3171 - val_accuracy: 0.9246\n",
      "Epoch 82/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9840\n",
      "Epoch 00082: val_accuracy did not improve from 0.92710\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9840 - val_loss: 0.3348 - val_accuracy: 0.9228\n",
      "Epoch 83/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9833\n",
      "Epoch 00083: val_accuracy improved from 0.92710 to 0.92927, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0850 - accuracy: 0.9833 - val_loss: 0.3195 - val_accuracy: 0.9293\n",
      "Epoch 84/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9830\n",
      "Epoch 00084: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9831 - val_loss: 0.3296 - val_accuracy: 0.9257\n",
      "Epoch 85/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9849\n",
      "Epoch 00085: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9848 - val_loss: 0.3300 - val_accuracy: 0.9282\n",
      "Epoch 86/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9849\n",
      "Epoch 00086: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9849 - val_loss: 0.3157 - val_accuracy: 0.9293\n",
      "Epoch 87/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0814 - accuracy: 0.9845\n",
      "Epoch 00087: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9844 - val_loss: 0.3307 - val_accuracy: 0.9213\n",
      "Epoch 88/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9847\n",
      "Epoch 00088: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0802 - accuracy: 0.9847 - val_loss: 0.3272 - val_accuracy: 0.9242\n",
      "Epoch 89/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9838\n",
      "Epoch 00089: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0825 - accuracy: 0.9838 - val_loss: 0.3263 - val_accuracy: 0.9271\n",
      "Epoch 90/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9858\n",
      "Epoch 00090: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0790 - accuracy: 0.9858 - val_loss: 0.3237 - val_accuracy: 0.9231\n",
      "Epoch 91/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9855\n",
      "Epoch 00091: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9855 - val_loss: 0.3337 - val_accuracy: 0.9235\n",
      "Epoch 92/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9837\n",
      "Epoch 00092: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9838 - val_loss: 0.3292 - val_accuracy: 0.9210\n",
      "Epoch 93/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9849\n",
      "Epoch 00093: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9850 - val_loss: 0.3256 - val_accuracy: 0.9278\n",
      "Epoch 94/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9846\n",
      "Epoch 00094: val_accuracy did not improve from 0.92927\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0783 - accuracy: 0.9847 - val_loss: 0.3348 - val_accuracy: 0.9231\n",
      "Epoch 95/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9856\n",
      "Epoch 00095: val_accuracy improved from 0.92927 to 0.92963, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9856 - val_loss: 0.3096 - val_accuracy: 0.9296\n",
      "Epoch 96/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9848\n",
      "Epoch 00096: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0774 - accuracy: 0.9848 - val_loss: 0.3213 - val_accuracy: 0.9285\n",
      "Epoch 97/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9852\n",
      "Epoch 00097: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0768 - accuracy: 0.9853 - val_loss: 0.3326 - val_accuracy: 0.9202\n",
      "Epoch 98/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9856\n",
      "Epoch 00098: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0774 - accuracy: 0.9855 - val_loss: 0.3417 - val_accuracy: 0.9199\n",
      "Epoch 99/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9852\n",
      "Epoch 00099: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9852 - val_loss: 0.3194 - val_accuracy: 0.9235\n",
      "Epoch 100/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9854\n",
      "Epoch 00100: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0754 - accuracy: 0.9854 - val_loss: 0.3261 - val_accuracy: 0.9235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9874\n",
      "Epoch 00101: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9874 - val_loss: 0.3229 - val_accuracy: 0.9253\n",
      "Epoch 102/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9871\n",
      "Epoch 00102: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0733 - accuracy: 0.9870 - val_loss: 0.3178 - val_accuracy: 0.9235\n",
      "Epoch 103/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9851\n",
      "Epoch 00103: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0786 - accuracy: 0.9852 - val_loss: 0.3415 - val_accuracy: 0.9188\n",
      "Epoch 104/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9851\n",
      "Epoch 00104: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9849 - val_loss: 0.3323 - val_accuracy: 0.9271\n",
      "Epoch 105/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9861\n",
      "Epoch 00105: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0752 - accuracy: 0.9862 - val_loss: 0.3306 - val_accuracy: 0.9278\n",
      "Epoch 106/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9869\n",
      "Epoch 00106: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0743 - accuracy: 0.9869 - val_loss: 0.3434 - val_accuracy: 0.9220\n",
      "Epoch 107/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9877\n",
      "Epoch 00107: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9876 - val_loss: 0.3402 - val_accuracy: 0.9249\n",
      "Epoch 108/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9880\n",
      "Epoch 00108: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9879 - val_loss: 0.3526 - val_accuracy: 0.9195\n",
      "Epoch 109/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9865\n",
      "Epoch 00109: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0716 - accuracy: 0.9865 - val_loss: 0.3390 - val_accuracy: 0.9210\n",
      "Epoch 110/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9858\n",
      "Epoch 00110: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.0747 - accuracy: 0.9859 - val_loss: 0.3352 - val_accuracy: 0.9246\n",
      "Epoch 111/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9854\n",
      "Epoch 00111: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9855 - val_loss: 0.3300 - val_accuracy: 0.9239\n",
      "Epoch 112/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9872\n",
      "Epoch 00112: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0717 - accuracy: 0.9870 - val_loss: 0.3250 - val_accuracy: 0.9224\n",
      "Epoch 113/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9850\n",
      "Epoch 00113: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.0745 - accuracy: 0.9850 - val_loss: 0.3405 - val_accuracy: 0.9257\n",
      "Epoch 114/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9885\n",
      "Epoch 00114: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0686 - accuracy: 0.9885 - val_loss: 0.3255 - val_accuracy: 0.9242\n",
      "Epoch 115/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9890\n",
      "Epoch 00115: val_accuracy did not improve from 0.92963\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0673 - accuracy: 0.9890 - val_loss: 0.3181 - val_accuracy: 0.9275\n",
      "Epoch 116/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9882\n",
      "Epoch 00116: val_accuracy improved from 0.92963 to 0.92999, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9882 - val_loss: 0.3101 - val_accuracy: 0.9300\n",
      "Epoch 117/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9873\n",
      "Epoch 00117: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0698 - accuracy: 0.9873 - val_loss: 0.3433 - val_accuracy: 0.9220\n",
      "Epoch 118/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9871\n",
      "Epoch 00118: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9870 - val_loss: 0.3464 - val_accuracy: 0.9184\n",
      "Epoch 119/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9870\n",
      "Epoch 00119: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0706 - accuracy: 0.9871 - val_loss: 0.3263 - val_accuracy: 0.9220\n",
      "Epoch 120/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9889\n",
      "Epoch 00120: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0664 - accuracy: 0.9888 - val_loss: 0.3276 - val_accuracy: 0.9220\n",
      "Epoch 121/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9873\n",
      "Epoch 00121: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0695 - accuracy: 0.9874 - val_loss: 0.3167 - val_accuracy: 0.9249\n",
      "Epoch 122/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9880\n",
      "Epoch 00122: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9879 - val_loss: 0.3221 - val_accuracy: 0.9264\n",
      "Epoch 123/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9855\n",
      "Epoch 00123: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0717 - accuracy: 0.9855 - val_loss: 0.3296 - val_accuracy: 0.9257\n",
      "Epoch 124/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9884\n",
      "Epoch 00124: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0650 - accuracy: 0.9884 - val_loss: 0.3230 - val_accuracy: 0.9249\n",
      "Epoch 125/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9878\n",
      "Epoch 00125: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0651 - accuracy: 0.9879 - val_loss: 0.3362 - val_accuracy: 0.9253\n",
      "Epoch 126/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9876\n",
      "Epoch 00126: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0670 - accuracy: 0.9876 - val_loss: 0.3294 - val_accuracy: 0.9260\n",
      "Epoch 127/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9882\n",
      "Epoch 00127: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0671 - accuracy: 0.9882 - val_loss: 0.3470 - val_accuracy: 0.9239\n",
      "Epoch 128/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9881\n",
      "Epoch 00128: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0677 - accuracy: 0.9882 - val_loss: 0.3384 - val_accuracy: 0.9282\n",
      "Epoch 129/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9895\n",
      "Epoch 00129: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9895 - val_loss: 0.3374 - val_accuracy: 0.9231\n",
      "Epoch 130/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9876\n",
      "Epoch 00130: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9876 - val_loss: 0.3456 - val_accuracy: 0.9239\n",
      "Epoch 131/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9873\n",
      "Epoch 00131: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0672 - accuracy: 0.9873 - val_loss: 0.3260 - val_accuracy: 0.9267\n",
      "Epoch 132/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9883\n",
      "Epoch 00132: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0655 - accuracy: 0.9881 - val_loss: 0.3173 - val_accuracy: 0.9239\n",
      "Epoch 133/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9889\n",
      "Epoch 00133: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9889 - val_loss: 0.3381 - val_accuracy: 0.9213\n",
      "Epoch 134/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9884\n",
      "Epoch 00134: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9882 - val_loss: 0.3378 - val_accuracy: 0.9239\n",
      "Epoch 135/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9870\n",
      "Epoch 00135: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0673 - accuracy: 0.9870 - val_loss: 0.3325 - val_accuracy: 0.9224\n",
      "Epoch 136/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9886\n",
      "Epoch 00136: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0638 - accuracy: 0.9886 - val_loss: 0.3236 - val_accuracy: 0.9228\n",
      "Epoch 137/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9871\n",
      "Epoch 00137: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9871 - val_loss: 0.3340 - val_accuracy: 0.9235\n",
      "Epoch 138/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9874\n",
      "Epoch 00138: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9874 - val_loss: 0.3218 - val_accuracy: 0.9271\n",
      "Epoch 139/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9889\n",
      "Epoch 00139: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0627 - accuracy: 0.9889 - val_loss: 0.3110 - val_accuracy: 0.9285\n",
      "Epoch 140/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9878\n",
      "Epoch 00140: val_accuracy did not improve from 0.92999\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9878 - val_loss: 0.3326 - val_accuracy: 0.9253\n",
      "Epoch 141/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9880\n",
      "Epoch 00141: val_accuracy improved from 0.92999 to 0.93143, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0649 - accuracy: 0.9881 - val_loss: 0.3230 - val_accuracy: 0.9314\n",
      "Epoch 142/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9888\n",
      "Epoch 00142: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9887 - val_loss: 0.3261 - val_accuracy: 0.9289\n",
      "Epoch 143/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9889\n",
      "Epoch 00143: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0628 - accuracy: 0.9889 - val_loss: 0.3445 - val_accuracy: 0.9217\n",
      "Epoch 144/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9883\n",
      "Epoch 00144: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9883 - val_loss: 0.3338 - val_accuracy: 0.9257\n",
      "Epoch 145/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9884\n",
      "Epoch 00145: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0643 - accuracy: 0.9884 - val_loss: 0.3372 - val_accuracy: 0.9264\n",
      "Epoch 146/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9879\n",
      "Epoch 00146: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0656 - accuracy: 0.9880 - val_loss: 0.3190 - val_accuracy: 0.9300\n",
      "Epoch 147/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0590 - accuracy: 0.9899\n",
      "Epoch 00147: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0596 - accuracy: 0.9897 - val_loss: 0.3173 - val_accuracy: 0.9285\n",
      "Epoch 148/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9897\n",
      "Epoch 00148: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0606 - accuracy: 0.9897 - val_loss: 0.3251 - val_accuracy: 0.9260\n",
      "Epoch 149/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9905\n",
      "Epoch 00149: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0575 - accuracy: 0.9906 - val_loss: 0.3357 - val_accuracy: 0.9271\n",
      "Epoch 150/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9901\n",
      "Epoch 00150: val_accuracy did not improve from 0.93143\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9901 - val_loss: 0.3457 - val_accuracy: 0.9239\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [09:16<00:00, 278.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.68      0.73      0.70       686\n",
      "        car_horn       0.80      0.88      0.83       196\n",
      "children_playing       0.74      0.83      0.78       700\n",
      "        dog_bark       0.80      0.72      0.76       700\n",
      "           siren       0.77      0.65      0.70       518\n",
      "\n",
      "        accuracy                           0.75      2800\n",
      "       macro avg       0.76      0.76      0.76      2800\n",
      "    weighted avg       0.75      0.75      0.75      2800\n",
      "\n",
      "Validation fold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27671, 375)\n",
      "X_val_norm shape.....:(2835, 375)\n",
      "\n",
      "Sum of elements: 0.9802503685859969\n",
      "Number of elements summed: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 234)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               176250    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 289,985\n",
      "Trainable params: 289,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24903, 234)\n",
      "Epoch 1/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.7829 - accuracy: 0.7130\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82767, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 2ms/step - loss: 0.7823 - accuracy: 0.7133 - val_loss: 0.4944 - val_accuracy: 0.8277\n",
      "Epoch 2/350\n",
      "759/779 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.8428\n",
      "Epoch 00002: val_accuracy improved from 0.82767 to 0.86163, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.4414 - accuracy: 0.8435 - val_loss: 0.3863 - val_accuracy: 0.8616\n",
      "Epoch 3/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.3449 - accuracy: 0.8767\n",
      "Epoch 00003: val_accuracy improved from 0.86163 to 0.88439, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.3451 - accuracy: 0.8767 - val_loss: 0.3315 - val_accuracy: 0.8844\n",
      "Epoch 4/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.2792 - accuracy: 0.9002\n",
      "Epoch 00004: val_accuracy improved from 0.88439 to 0.89704, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.2792 - accuracy: 0.9003 - val_loss: 0.2919 - val_accuracy: 0.8970\n",
      "Epoch 5/350\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.2335 - accuracy: 0.9165\n",
      "Epoch 00005: val_accuracy improved from 0.89704 to 0.89993, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.2330 - accuracy: 0.9167 - val_loss: 0.2702 - val_accuracy: 0.8999\n",
      "Epoch 6/350\n",
      "759/779 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9318\n",
      "Epoch 00006: val_accuracy improved from 0.89993 to 0.91329, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.1950 - accuracy: 0.9317 - val_loss: 0.2477 - val_accuracy: 0.9133\n",
      "Epoch 7/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.9434\n",
      "Epoch 00007: val_accuracy improved from 0.91329 to 0.91727, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.1639 - accuracy: 0.9434 - val_loss: 0.2314 - val_accuracy: 0.9173\n",
      "Epoch 8/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.9525\n",
      "Epoch 00008: val_accuracy improved from 0.91727 to 0.92305, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.1385 - accuracy: 0.9524 - val_loss: 0.2177 - val_accuracy: 0.9230\n",
      "Epoch 9/350\n",
      "751/779 [===========================>..] - ETA: 0s - loss: 0.1190 - accuracy: 0.9596\n",
      "Epoch 00009: val_accuracy improved from 0.92305 to 0.93208, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.1190 - accuracy: 0.9598 - val_loss: 0.2062 - val_accuracy: 0.9321\n",
      "Epoch 10/350\n",
      "745/779 [===========================>..] - ETA: 0s - loss: 0.0981 - accuracy: 0.9676\n",
      "Epoch 00010: val_accuracy did not improve from 0.93208\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0974 - accuracy: 0.9679 - val_loss: 0.2085 - val_accuracy: 0.9270\n",
      "Epoch 11/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9731\n",
      "Epoch 00011: val_accuracy improved from 0.93208 to 0.93353, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0832 - accuracy: 0.9731 - val_loss: 0.1998 - val_accuracy: 0.9335\n",
      "Epoch 12/350\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9762\n",
      "Epoch 00012: val_accuracy improved from 0.93353 to 0.93461, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0707 - accuracy: 0.9763 - val_loss: 0.1941 - val_accuracy: 0.9346\n",
      "Epoch 13/350\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9798\n",
      "Epoch 00013: val_accuracy did not improve from 0.93461\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0614 - accuracy: 0.9797 - val_loss: 0.1944 - val_accuracy: 0.9346\n",
      "Epoch 14/350\n",
      "751/779 [===========================>..] - ETA: 0s - loss: 0.0510 - accuracy: 0.9829\n",
      "Epoch 00014: val_accuracy improved from 0.93461 to 0.93931, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0513 - accuracy: 0.9828 - val_loss: 0.1933 - val_accuracy: 0.9393\n",
      "Epoch 15/350\n",
      "749/779 [===========================>..] - ETA: 0s - loss: 0.0444 - accuracy: 0.9870\n",
      "Epoch 00015: val_accuracy improved from 0.93931 to 0.94003, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0446 - accuracy: 0.9867 - val_loss: 0.1983 - val_accuracy: 0.9400\n",
      "Epoch 16/350\n",
      "750/779 [===========================>..] - ETA: 0s - loss: 0.0369 - accuracy: 0.9893\n",
      "Epoch 00016: val_accuracy did not improve from 0.94003\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9894 - val_loss: 0.2021 - val_accuracy: 0.9400\n",
      "Epoch 17/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9903\n",
      "Epoch 00017: val_accuracy improved from 0.94003 to 0.94147, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9903 - val_loss: 0.2059 - val_accuracy: 0.9415\n",
      "Epoch 18/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9924\n",
      "Epoch 00018: val_accuracy improved from 0.94147 to 0.94256, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9925 - val_loss: 0.2025 - val_accuracy: 0.9426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "752/779 [===========================>..] - ETA: 0s - loss: 0.0239 - accuracy: 0.9939\n",
      "Epoch 00019: val_accuracy improved from 0.94256 to 0.94364, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0239 - accuracy: 0.9938 - val_loss: 0.2021 - val_accuracy: 0.9436\n",
      "Epoch 20/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9950\n",
      "Epoch 00020: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9951 - val_loss: 0.2115 - val_accuracy: 0.9408\n",
      "Epoch 21/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9956\n",
      "Epoch 00021: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9956 - val_loss: 0.2070 - val_accuracy: 0.9436\n",
      "Epoch 22/350\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9967\n",
      "Epoch 00022: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9967 - val_loss: 0.2157 - val_accuracy: 0.9415\n",
      "Epoch 23/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9962\n",
      "Epoch 00023: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0150 - accuracy: 0.9962 - val_loss: 0.2125 - val_accuracy: 0.9436\n",
      "Epoch 24/350\n",
      "756/779 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9965\n",
      "Epoch 00024: val_accuracy did not improve from 0.94364\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9965 - val_loss: 0.2239 - val_accuracy: 0.9426\n",
      "Epoch 25/350\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9975\n",
      "Epoch 00025: val_accuracy improved from 0.94364 to 0.94617, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9976 - val_loss: 0.2173 - val_accuracy: 0.9462\n",
      "Epoch 26/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9972\n",
      "Epoch 00026: val_accuracy did not improve from 0.94617\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9973 - val_loss: 0.2238 - val_accuracy: 0.9451\n",
      "Epoch 27/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9973\n",
      "Epoch 00027: val_accuracy improved from 0.94617 to 0.94689, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9973 - val_loss: 0.2236 - val_accuracy: 0.9469\n",
      "Epoch 28/350\n",
      "754/779 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9979\n",
      "Epoch 00028: val_accuracy improved from 0.94689 to 0.94762, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.2246 - val_accuracy: 0.9476\n",
      "Epoch 29/350\n",
      "759/779 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9981\n",
      "Epoch 00029: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.2286 - val_accuracy: 0.9465\n",
      "Epoch 30/350\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9979\n",
      "Epoch 00030: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9978 - val_loss: 0.2344 - val_accuracy: 0.9465\n",
      "Epoch 31/350\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n",
      "Epoch 00031: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.2302 - val_accuracy: 0.9454\n",
      "Epoch 32/350\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9989\n",
      "Epoch 00032: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 0.2284 - val_accuracy: 0.9465\n",
      "Epoch 33/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9989\n",
      "Epoch 00033: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9989 - val_loss: 0.2343 - val_accuracy: 0.9476\n",
      "Epoch 34/350\n",
      "753/779 [===========================>..] - ETA: 0s - loss: 0.0058 - accuracy: 0.9988\n",
      "Epoch 00034: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9989 - val_loss: 0.2365 - val_accuracy: 0.9447\n",
      "Epoch 35/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989\n",
      "Epoch 00035: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.2353 - val_accuracy: 0.9447\n",
      "Epoch 36/350\n",
      "758/779 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9985\n",
      "Epoch 00036: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.2432 - val_accuracy: 0.9436\n",
      "Epoch 37/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9991\n",
      "Epoch 00037: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.2408 - val_accuracy: 0.9429\n",
      "Epoch 38/350\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 00038: val_accuracy did not improve from 0.94762\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.2440 - val_accuracy: 0.9465\n",
      "Epoch 39/350\n",
      "748/779 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9989\n",
      "Epoch 00039: val_accuracy improved from 0.94762 to 0.94798, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.2381 - val_accuracy: 0.9480\n",
      "Epoch 40/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2451 - val_accuracy: 0.9473\n",
      "Epoch 41/350\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00041: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.2550 - val_accuracy: 0.9465\n",
      "Epoch 42/350\n",
      "753/779 [===========================>..] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 00042: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.2510 - val_accuracy: 0.9462\n",
      "Epoch 43/350\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9991\n",
      "Epoch 00043: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.2493 - val_accuracy: 0.9473\n",
      "Epoch 44/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00044: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2568 - val_accuracy: 0.9451\n",
      "Epoch 45/350\n",
      "758/779 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00045: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.2549 - val_accuracy: 0.9465\n",
      "Epoch 46/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9995\n",
      "Epoch 00046: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.2564 - val_accuracy: 0.9476\n",
      "Epoch 47/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9992\n",
      "Epoch 00047: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.2601 - val_accuracy: 0.9480\n",
      "Epoch 48/350\n",
      "754/779 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 00048: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.2636 - val_accuracy: 0.9476\n",
      "Epoch 49/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 00049: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.2661 - val_accuracy: 0.9462\n",
      "Epoch 50/350\n",
      "757/779 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 00050: val_accuracy did not improve from 0.94798\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2701 - val_accuracy: 0.9469\n",
      "Epoch 51/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9996\n",
      "Epoch 00051: val_accuracy improved from 0.94798 to 0.94978, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2659 - val_accuracy: 0.9498\n",
      "Epoch 52/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00052: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2645 - val_accuracy: 0.9494\n",
      "Epoch 53/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00053: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2644 - val_accuracy: 0.9494\n",
      "Epoch 54/350\n",
      "746/779 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 00054: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2652 - val_accuracy: 0.9494\n",
      "Epoch 55/350\n",
      "752/779 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2595 - val_accuracy: 0.9480\n",
      "Epoch 56/350\n",
      "756/779 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9996\n",
      "Epoch 00056: val_accuracy did not improve from 0.94978\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2633 - val_accuracy: 0.9494\n",
      "Epoch 57/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00057: val_accuracy improved from 0.94978 to 0.95087, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2599 - val_accuracy: 0.9509\n",
      "Epoch 58/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9998\n",
      "Epoch 00058: val_accuracy improved from 0.95087 to 0.95195, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2659 - val_accuracy: 0.9520\n",
      "Epoch 59/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00059: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.2698 - val_accuracy: 0.9469\n",
      "Epoch 60/350\n",
      "746/779 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00060: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.2763 - val_accuracy: 0.9487\n",
      "Epoch 61/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 00061: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.2685 - val_accuracy: 0.9487\n",
      "Epoch 62/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00062: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2748 - val_accuracy: 0.9462\n",
      "Epoch 63/350\n",
      "746/779 [===========================>..] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00063: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2735 - val_accuracy: 0.9469\n",
      "Epoch 64/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00064: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.2727 - val_accuracy: 0.9465\n",
      "Epoch 65/350\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 00065: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.2825 - val_accuracy: 0.9480\n",
      "Epoch 66/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00066: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2843 - val_accuracy: 0.9483\n",
      "Epoch 67/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00067: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2825 - val_accuracy: 0.9487\n",
      "Epoch 68/350\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00068: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2836 - val_accuracy: 0.9476\n",
      "Epoch 69/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00069: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2807 - val_accuracy: 0.9480\n",
      "Epoch 70/350\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00070: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2841 - val_accuracy: 0.9476\n",
      "Epoch 71/350\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00071: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2820 - val_accuracy: 0.9473\n",
      "Epoch 72/350\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00072: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2788 - val_accuracy: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/350\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00073: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2862 - val_accuracy: 0.9469\n",
      "Epoch 74/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 00074: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2835 - val_accuracy: 0.9473\n",
      "Epoch 75/350\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00075: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.2842 - val_accuracy: 0.9501\n",
      "Epoch 76/350\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 00076: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.2830 - val_accuracy: 0.9469\n",
      "Epoch 77/350\n",
      "758/779 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 00077: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2904 - val_accuracy: 0.9494\n",
      "Epoch 78/350\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00078: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2911 - val_accuracy: 0.9491\n",
      "Epoch 79/350\n",
      "758/779 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00079: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2848 - val_accuracy: 0.9473\n",
      "Epoch 80/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00080: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.2901 - val_accuracy: 0.9491\n",
      "Epoch 81/350\n",
      "748/779 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 00081: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.2968 - val_accuracy: 0.9473\n",
      "Epoch 82/350\n",
      "748/779 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00082: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2928 - val_accuracy: 0.9491\n",
      "Epoch 83/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 9.6116e-04 - accuracy: 0.9999\n",
      "Epoch 00083: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.5890e-04 - accuracy: 0.9999 - val_loss: 0.2932 - val_accuracy: 0.9501\n",
      "Epoch 84/350\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2971 - val_accuracy: 0.9509\n",
      "Epoch 85/350\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00085: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2890 - val_accuracy: 0.9509\n",
      "Epoch 86/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00086: val_accuracy did not improve from 0.95195\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2825 - val_accuracy: 0.9505\n",
      "Epoch 87/350\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00087: val_accuracy improved from 0.95195 to 0.95267, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2865 - val_accuracy: 0.9527\n",
      "Epoch 88/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2866 - val_accuracy: 0.9480\n",
      "Epoch 89/350\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2876 - val_accuracy: 0.9501\n",
      "Epoch 90/350\n",
      "768/779 [============================>.] - ETA: 0s - loss: 6.5594e-04 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.5374e-04 - accuracy: 1.0000 - val_loss: 0.2882 - val_accuracy: 0.9505\n",
      "Epoch 91/350\n",
      "747/779 [===========================>..] - ETA: 0s - loss: 7.3749e-04 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.7024e-04 - accuracy: 0.9999 - val_loss: 0.2907 - val_accuracy: 0.9512\n",
      "Epoch 92/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 6.8097e-04 - accuracy: 0.9999\n",
      "Epoch 00092: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.7484e-04 - accuracy: 0.9999 - val_loss: 0.2920 - val_accuracy: 0.9491\n",
      "Epoch 93/350\n",
      "751/779 [===========================>..] - ETA: 0s - loss: 8.8508e-04 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.8310e-04 - accuracy: 0.9998 - val_loss: 0.2919 - val_accuracy: 0.9491\n",
      "Epoch 94/350\n",
      "756/779 [============================>.] - ETA: 0s - loss: 8.4343e-04 - accuracy: 0.9999\n",
      "Epoch 00094: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.2753e-04 - accuracy: 0.9999 - val_loss: 0.2894 - val_accuracy: 0.9498\n",
      "Epoch 95/350\n",
      "751/779 [===========================>..] - ETA: 0s - loss: 9.0789e-04 - accuracy: 0.9998\n",
      "Epoch 00095: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.9077e-04 - accuracy: 0.9998 - val_loss: 0.2807 - val_accuracy: 0.9498\n",
      "Epoch 96/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 7.7053e-04 - accuracy: 0.9999\n",
      "Epoch 00096: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.7525e-04 - accuracy: 0.9999 - val_loss: 0.2802 - val_accuracy: 0.9509\n",
      "Epoch 97/350\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00097: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2820 - val_accuracy: 0.9512\n",
      "Epoch 98/350\n",
      "762/779 [============================>.] - ETA: 0s - loss: 7.7438e-04 - accuracy: 0.9999\n",
      "Epoch 00098: val_accuracy did not improve from 0.95267\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.6313e-04 - accuracy: 0.9999 - val_loss: 0.2837 - val_accuracy: 0.9512\n",
      "Epoch 99/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 9.6537e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy improved from 0.95267 to 0.95340, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.5857e-04 - accuracy: 0.9998 - val_loss: 0.2833 - val_accuracy: 0.9534\n",
      "Epoch 100/350\n",
      "752/779 [===========================>..] - ETA: 0s - loss: 9.8481e-04 - accuracy: 0.9998\n",
      "Epoch 00100: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.2779 - val_accuracy: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 7.0540e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.0416e-04 - accuracy: 1.0000 - val_loss: 0.2837 - val_accuracy: 0.9512\n",
      "Epoch 102/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 9.0493e-04 - accuracy: 0.9998\n",
      "Epoch 00102: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.1504e-04 - accuracy: 0.9998 - val_loss: 0.2897 - val_accuracy: 0.9512\n",
      "Epoch 103/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 8.8748e-04 - accuracy: 0.9998\n",
      "Epoch 00103: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.8790e-04 - accuracy: 0.9998 - val_loss: 0.2924 - val_accuracy: 0.9501\n",
      "Epoch 104/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 6.2698e-04 - accuracy: 0.9999\n",
      "Epoch 00104: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.2560e-04 - accuracy: 0.9999 - val_loss: 0.2863 - val_accuracy: 0.9520\n",
      "Epoch 105/350\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 00105: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.2888 - val_accuracy: 0.9523\n",
      "Epoch 106/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 6.0772e-04 - accuracy: 1.0000\n",
      "Epoch 00106: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.0678e-04 - accuracy: 1.0000 - val_loss: 0.2914 - val_accuracy: 0.9512\n",
      "Epoch 107/350\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997    \n",
      "Epoch 00107: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.2939 - val_accuracy: 0.9505\n",
      "Epoch 108/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 7.7835e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.7589e-04 - accuracy: 0.9999 - val_loss: 0.2908 - val_accuracy: 0.9505\n",
      "Epoch 109/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 7.5469e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.5378e-04 - accuracy: 0.9999 - val_loss: 0.2922 - val_accuracy: 0.9512\n",
      "Epoch 110/350\n",
      "764/779 [============================>.] - ETA: 0s - loss: 6.4234e-04 - accuracy: 0.9999\n",
      "Epoch 00110: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.3255e-04 - accuracy: 0.9999 - val_loss: 0.2955 - val_accuracy: 0.9505\n",
      "Epoch 111/350\n",
      "762/779 [============================>.] - ETA: 0s - loss: 9.4776e-04 - accuracy: 0.9998\n",
      "Epoch 00111: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.3174e-04 - accuracy: 0.9998 - val_loss: 0.2925 - val_accuracy: 0.9516\n",
      "Epoch 112/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 8.0397e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.9370e-04 - accuracy: 0.9998 - val_loss: 0.3015 - val_accuracy: 0.9480\n",
      "Epoch 113/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 5.8683e-04 - accuracy: 1.0000\n",
      "Epoch 00113: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.8397e-04 - accuracy: 1.0000 - val_loss: 0.2944 - val_accuracy: 0.9498\n",
      "Epoch 114/350\n",
      "757/779 [============================>.] - ETA: 0s - loss: 7.7985e-04 - accuracy: 0.9998\n",
      "Epoch 00114: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.6828e-04 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9498\n",
      "Epoch 115/350\n",
      "768/779 [============================>.] - ETA: 0s - loss: 7.5491e-04 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.4996e-04 - accuracy: 1.0000 - val_loss: 0.2973 - val_accuracy: 0.9509\n",
      "Epoch 116/350\n",
      "779/779 [==============================] - ETA: 0s - loss: 8.4037e-04 - accuracy: 0.9998\n",
      "Epoch 00116: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.4037e-04 - accuracy: 0.9998 - val_loss: 0.2930 - val_accuracy: 0.9512\n",
      "Epoch 117/350\n",
      "759/779 [============================>.] - ETA: 0s - loss: 7.7162e-04 - accuracy: 0.9999\n",
      "Epoch 00117: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.8684e-04 - accuracy: 0.9999 - val_loss: 0.2890 - val_accuracy: 0.9498\n",
      "Epoch 118/350\n",
      "763/779 [============================>.] - ETA: 0s - loss: 5.6959e-04 - accuracy: 0.9998\n",
      "Epoch 00118: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.9848e-04 - accuracy: 0.9998 - val_loss: 0.2930 - val_accuracy: 0.9509\n",
      "Epoch 119/350\n",
      "757/779 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 00119: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.3033 - val_accuracy: 0.9509\n",
      "Epoch 120/350\n",
      "757/779 [============================>.] - ETA: 0s - loss: 6.7667e-04 - accuracy: 0.9999\n",
      "Epoch 00120: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.6394e-04 - accuracy: 0.9999 - val_loss: 0.2984 - val_accuracy: 0.9516\n",
      "Epoch 121/350\n",
      "776/779 [============================>.] - ETA: 0s - loss: 7.0531e-04 - accuracy: 0.9998\n",
      "Epoch 00121: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.0376e-04 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9523\n",
      "Epoch 122/350\n",
      "758/779 [============================>.] - ETA: 0s - loss: 6.9755e-04 - accuracy: 0.9999\n",
      "Epoch 00122: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.9440e-04 - accuracy: 0.9999 - val_loss: 0.2963 - val_accuracy: 0.9505\n",
      "Epoch 123/350\n",
      "763/779 [============================>.] - ETA: 0s - loss: 6.2099e-04 - accuracy: 0.9999\n",
      "Epoch 00123: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.1423e-04 - accuracy: 0.9999 - val_loss: 0.3045 - val_accuracy: 0.9523\n",
      "Epoch 124/350\n",
      "753/779 [===========================>..] - ETA: 0s - loss: 9.2865e-04 - accuracy: 0.9998\n",
      "Epoch 00124: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 9.3207e-04 - accuracy: 0.9998 - val_loss: 0.3018 - val_accuracy: 0.9509\n",
      "Epoch 125/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 7.2572e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.1765e-04 - accuracy: 0.9999 - val_loss: 0.3004 - val_accuracy: 0.9494\n",
      "Epoch 126/350\n",
      "767/779 [============================>.] - ETA: 0s - loss: 5.4412e-04 - accuracy: 0.9999\n",
      "Epoch 00126: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.4165e-04 - accuracy: 0.9999 - val_loss: 0.3003 - val_accuracy: 0.9501\n",
      "Epoch 127/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 8.9342e-04 - accuracy: 0.9998\n",
      "Epoch 00127: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 8.8136e-04 - accuracy: 0.9998 - val_loss: 0.3062 - val_accuracy: 0.9520\n",
      "Epoch 128/350\n",
      "761/779 [============================>.] - ETA: 0s - loss: 5.6326e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.5545e-04 - accuracy: 0.9999 - val_loss: 0.3005 - val_accuracy: 0.9498\n",
      "Epoch 129/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771/779 [============================>.] - ETA: 0s - loss: 6.6663e-04 - accuracy: 0.9999\n",
      "Epoch 00129: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.6199e-04 - accuracy: 0.9999 - val_loss: 0.3022 - val_accuracy: 0.9494\n",
      "Epoch 130/350\n",
      "769/779 [============================>.] - ETA: 0s - loss: 7.3118e-04 - accuracy: 0.9999\n",
      "Epoch 00130: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.2504e-04 - accuracy: 0.9999 - val_loss: 0.3024 - val_accuracy: 0.9498\n",
      "Epoch 131/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 7.3354e-04 - accuracy: 0.9998\n",
      "Epoch 00131: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.3240e-04 - accuracy: 0.9998 - val_loss: 0.2998 - val_accuracy: 0.9509\n",
      "Epoch 132/350\n",
      "777/779 [============================>.] - ETA: 0s - loss: 7.2816e-04 - accuracy: 0.9999\n",
      "Epoch 00132: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.2707e-04 - accuracy: 0.9999 - val_loss: 0.3025 - val_accuracy: 0.9498\n",
      "Epoch 133/350\n",
      "750/779 [===========================>..] - ETA: 0s - loss: 7.8338e-04 - accuracy: 0.9998\n",
      "Epoch 00133: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.7269e-04 - accuracy: 0.9998 - val_loss: 0.2999 - val_accuracy: 0.9505\n",
      "Epoch 134/350\n",
      "775/779 [============================>.] - ETA: 0s - loss: 5.4981e-04 - accuracy: 0.9999\n",
      "Epoch 00134: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.5048e-04 - accuracy: 0.9999 - val_loss: 0.3010 - val_accuracy: 0.9512\n",
      "Epoch 135/350\n",
      "767/779 [============================>.] - ETA: 0s - loss: 6.9401e-04 - accuracy: 0.9999\n",
      "Epoch 00135: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 6.8814e-04 - accuracy: 0.9999 - val_loss: 0.3014 - val_accuracy: 0.9498\n",
      "Epoch 136/350\n",
      "771/779 [============================>.] - ETA: 0s - loss: 4.1285e-04 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 4.1030e-04 - accuracy: 1.0000 - val_loss: 0.3021 - val_accuracy: 0.9487\n",
      "Epoch 137/350\n",
      "770/779 [============================>.] - ETA: 0s - loss: 4.9723e-04 - accuracy: 0.9999\n",
      "Epoch 00137: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 4.9295e-04 - accuracy: 0.9999 - val_loss: 0.3041 - val_accuracy: 0.9494\n",
      "Epoch 138/350\n",
      "766/779 [============================>.] - ETA: 0s - loss: 7.2816e-04 - accuracy: 0.9999\n",
      "Epoch 00138: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 7.1796e-04 - accuracy: 0.9999 - val_loss: 0.3029 - val_accuracy: 0.9480\n",
      "Epoch 139/350\n",
      "760/779 [============================>.] - ETA: 0s - loss: 5.8825e-04 - accuracy: 0.9999\n",
      "Epoch 00139: val_accuracy did not improve from 0.95340\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.8017e-04 - accuracy: 0.9999 - val_loss: 0.3082 - val_accuracy: 0.9498\n",
      "Epoch 140/350\n",
      "762/779 [============================>.] - ETA: 0s - loss: 5.5055e-04 - accuracy: 0.9999\n",
      "Epoch 00140: val_accuracy did not improve from 0.95340\n",
      "Restoring model weights from the end of the best epoch.\n",
      "779/779 [==============================] - 1s 2ms/step - loss: 5.4320e-04 - accuracy: 0.9999 - val_loss: 0.3047 - val_accuracy: 0.9505\n",
      "Epoch 00140: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [03:04<03:04, 184.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.71      0.73      0.72       700\n",
      "        car_horn       0.82      0.69      0.75       196\n",
      "children_playing       0.74      0.72      0.73       700\n",
      "        dog_bark       0.75      0.86      0.80       700\n",
      "           siren       0.73      0.63      0.67       539\n",
      "\n",
      "        accuracy                           0.74      2835\n",
      "       macro avg       0.75      0.73      0.74      2835\n",
      "    weighted avg       0.74      0.74      0.74      2835\n",
      "\n",
      "Model: \"Model_CNN_1D_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 228, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 228, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 228, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6384)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                319250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 330,291\n",
      "Trainable params: 330,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24903, 234, 1)\n",
      "Epoch 1/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.7451 - accuracy: 0.7629\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83454, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 3s 4ms/step - loss: 0.7451 - accuracy: 0.7629 - val_loss: 0.5461 - val_accuracy: 0.8345\n",
      "Epoch 2/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.8527\n",
      "Epoch 00002: val_accuracy improved from 0.83454 to 0.85730, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.4840 - accuracy: 0.8527 - val_loss: 0.4636 - val_accuracy: 0.8573\n",
      "Epoch 3/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8735\n",
      "Epoch 00003: val_accuracy improved from 0.85730 to 0.88078, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.4186 - accuracy: 0.8739 - val_loss: 0.4147 - val_accuracy: 0.8808\n",
      "Epoch 4/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.8912\n",
      "Epoch 00004: val_accuracy improved from 0.88078 to 0.88186, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.3734 - accuracy: 0.8914 - val_loss: 0.4093 - val_accuracy: 0.8819\n",
      "Epoch 5/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.9029\n",
      "Epoch 00005: val_accuracy improved from 0.88186 to 0.88584, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.3383 - accuracy: 0.9029 - val_loss: 0.3938 - val_accuracy: 0.8858\n",
      "Epoch 6/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.3145 - accuracy: 0.9120\n",
      "Epoch 00006: val_accuracy improved from 0.88584 to 0.90173, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.3146 - accuracy: 0.9117 - val_loss: 0.3598 - val_accuracy: 0.9017\n",
      "Epoch 7/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.9182\n",
      "Epoch 00007: val_accuracy improved from 0.90173 to 0.90210, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2953 - accuracy: 0.9183 - val_loss: 0.3491 - val_accuracy: 0.9021\n",
      "Epoch 8/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.2735 - accuracy: 0.9260\n",
      "Epoch 00008: val_accuracy improved from 0.90210 to 0.90607, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2742 - accuracy: 0.9256 - val_loss: 0.3411 - val_accuracy: 0.9061\n",
      "Epoch 9/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.9329\n",
      "Epoch 00009: val_accuracy did not improve from 0.90607\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2539 - accuracy: 0.9327 - val_loss: 0.3477 - val_accuracy: 0.9014\n",
      "Epoch 10/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.9367\n",
      "Epoch 00010: val_accuracy did not improve from 0.90607\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2444 - accuracy: 0.9363 - val_loss: 0.3717 - val_accuracy: 0.8963\n",
      "Epoch 11/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9381\n",
      "Epoch 00011: val_accuracy improved from 0.90607 to 0.91004, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2394 - accuracy: 0.9381 - val_loss: 0.3306 - val_accuracy: 0.9100\n",
      "Epoch 12/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.9413\n",
      "Epoch 00012: val_accuracy did not improve from 0.91004\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2268 - accuracy: 0.9413 - val_loss: 0.3443 - val_accuracy: 0.8988\n",
      "Epoch 13/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.2191 - accuracy: 0.9441\n",
      "Epoch 00013: val_accuracy improved from 0.91004 to 0.91113, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2189 - accuracy: 0.9440 - val_loss: 0.3245 - val_accuracy: 0.9111\n",
      "Epoch 14/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.2107 - accuracy: 0.9445\n",
      "Epoch 00014: val_accuracy did not improve from 0.91113\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.2106 - accuracy: 0.9444 - val_loss: 0.3484 - val_accuracy: 0.9068\n",
      "Epoch 15/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9500\n",
      "Epoch 00015: val_accuracy improved from 0.91113 to 0.91691, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1998 - accuracy: 0.9500 - val_loss: 0.3173 - val_accuracy: 0.9169\n",
      "Epoch 16/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9519\n",
      "Epoch 00016: val_accuracy improved from 0.91691 to 0.91727, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1931 - accuracy: 0.9520 - val_loss: 0.3209 - val_accuracy: 0.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.1907 - accuracy: 0.9522\n",
      "Epoch 00017: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1911 - accuracy: 0.9519 - val_loss: 0.3253 - val_accuracy: 0.9137\n",
      "Epoch 18/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9538\n",
      "Epoch 00018: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1878 - accuracy: 0.9537 - val_loss: 0.3188 - val_accuracy: 0.9147\n",
      "Epoch 19/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.1770 - accuracy: 0.9585\n",
      "Epoch 00019: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1771 - accuracy: 0.9586 - val_loss: 0.3226 - val_accuracy: 0.9144\n",
      "Epoch 20/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9574\n",
      "Epoch 00020: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1758 - accuracy: 0.9574 - val_loss: 0.3345 - val_accuracy: 0.9133\n",
      "Epoch 21/150\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9581\n",
      "Epoch 00021: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1722 - accuracy: 0.9581 - val_loss: 0.3198 - val_accuracy: 0.9173\n",
      "Epoch 22/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9607\n",
      "Epoch 00022: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1638 - accuracy: 0.9607 - val_loss: 0.3188 - val_accuracy: 0.9129\n",
      "Epoch 23/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.1634 - accuracy: 0.9623\n",
      "Epoch 00023: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1633 - accuracy: 0.9623 - val_loss: 0.3310 - val_accuracy: 0.9072\n",
      "Epoch 24/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9635\n",
      "Epoch 00024: val_accuracy did not improve from 0.91727\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1567 - accuracy: 0.9633 - val_loss: 0.3258 - val_accuracy: 0.9162\n",
      "Epoch 25/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9649\n",
      "Epoch 00025: val_accuracy improved from 0.91727 to 0.91799, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1532 - accuracy: 0.9649 - val_loss: 0.3205 - val_accuracy: 0.9180\n",
      "Epoch 26/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.9642\n",
      "Epoch 00026: val_accuracy did not improve from 0.91799\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1530 - accuracy: 0.9642 - val_loss: 0.3095 - val_accuracy: 0.9176\n",
      "Epoch 27/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9658\n",
      "Epoch 00027: val_accuracy improved from 0.91799 to 0.91835, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1484 - accuracy: 0.9658 - val_loss: 0.3104 - val_accuracy: 0.9184\n",
      "Epoch 28/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9652\n",
      "Epoch 00028: val_accuracy did not improve from 0.91835\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1488 - accuracy: 0.9653 - val_loss: 0.3141 - val_accuracy: 0.9184\n",
      "Epoch 29/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9666\n",
      "Epoch 00029: val_accuracy improved from 0.91835 to 0.91908, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1450 - accuracy: 0.9666 - val_loss: 0.3155 - val_accuracy: 0.9191\n",
      "Epoch 30/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.9702\n",
      "Epoch 00030: val_accuracy improved from 0.91908 to 0.92269, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1384 - accuracy: 0.9704 - val_loss: 0.3193 - val_accuracy: 0.9227\n",
      "Epoch 31/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.9718\n",
      "Epoch 00031: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1321 - accuracy: 0.9717 - val_loss: 0.3174 - val_accuracy: 0.9184\n",
      "Epoch 32/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.9704\n",
      "Epoch 00032: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1366 - accuracy: 0.9704 - val_loss: 0.3094 - val_accuracy: 0.9212\n",
      "Epoch 33/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.9702\n",
      "Epoch 00033: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1374 - accuracy: 0.9702 - val_loss: 0.3297 - val_accuracy: 0.9151\n",
      "Epoch 34/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1354 - accuracy: 0.9694\n",
      "Epoch 00034: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1351 - accuracy: 0.9694 - val_loss: 0.3163 - val_accuracy: 0.9223\n",
      "Epoch 35/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.9729\n",
      "Epoch 00035: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1333 - accuracy: 0.9729 - val_loss: 0.3291 - val_accuracy: 0.9198\n",
      "Epoch 36/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9728\n",
      "Epoch 00036: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1244 - accuracy: 0.9729 - val_loss: 0.3122 - val_accuracy: 0.9212\n",
      "Epoch 37/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9723\n",
      "Epoch 00037: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9723 - val_loss: 0.3265 - val_accuracy: 0.9187\n",
      "Epoch 38/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9738\n",
      "Epoch 00038: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1255 - accuracy: 0.9739 - val_loss: 0.3177 - val_accuracy: 0.9187\n",
      "Epoch 39/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9720\n",
      "Epoch 00039: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1263 - accuracy: 0.9718 - val_loss: 0.3233 - val_accuracy: 0.9173\n",
      "Epoch 40/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9741\n",
      "Epoch 00040: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1234 - accuracy: 0.9741 - val_loss: 0.3251 - val_accuracy: 0.9155\n",
      "Epoch 41/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9752\n",
      "Epoch 00041: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1187 - accuracy: 0.9752 - val_loss: 0.3222 - val_accuracy: 0.9151\n",
      "Epoch 42/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9741\n",
      "Epoch 00042: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1183 - accuracy: 0.9742 - val_loss: 0.3248 - val_accuracy: 0.9187\n",
      "Epoch 43/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9752\n",
      "Epoch 00043: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1182 - accuracy: 0.9752 - val_loss: 0.3175 - val_accuracy: 0.9216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 0.9759\n",
      "Epoch 00044: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1150 - accuracy: 0.9759 - val_loss: 0.3336 - val_accuracy: 0.9176\n",
      "Epoch 45/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9752\n",
      "Epoch 00045: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1144 - accuracy: 0.9753 - val_loss: 0.3349 - val_accuracy: 0.9187\n",
      "Epoch 46/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9763\n",
      "Epoch 00046: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1122 - accuracy: 0.9763 - val_loss: 0.3265 - val_accuracy: 0.9205\n",
      "Epoch 47/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.1122 - accuracy: 0.9766\n",
      "Epoch 00047: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1124 - accuracy: 0.9765 - val_loss: 0.3303 - val_accuracy: 0.9202\n",
      "Epoch 48/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.9777\n",
      "Epoch 00048: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9777 - val_loss: 0.3361 - val_accuracy: 0.9180\n",
      "Epoch 49/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9770\n",
      "Epoch 00049: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1095 - accuracy: 0.9773 - val_loss: 0.3370 - val_accuracy: 0.9180\n",
      "Epoch 50/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.1104 - accuracy: 0.9761\n",
      "Epoch 00050: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1104 - accuracy: 0.9761 - val_loss: 0.3259 - val_accuracy: 0.9191\n",
      "Epoch 51/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9785\n",
      "Epoch 00051: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1071 - accuracy: 0.9785 - val_loss: 0.3273 - val_accuracy: 0.9212\n",
      "Epoch 52/150\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9805\n",
      "Epoch 00052: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1043 - accuracy: 0.9805 - val_loss: 0.3356 - val_accuracy: 0.9205\n",
      "Epoch 53/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.1016 - accuracy: 0.9803\n",
      "Epoch 00053: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1016 - accuracy: 0.9804 - val_loss: 0.3252 - val_accuracy: 0.9223\n",
      "Epoch 54/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.1058 - accuracy: 0.9786\n",
      "Epoch 00054: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1058 - accuracy: 0.9786 - val_loss: 0.3251 - val_accuracy: 0.9216\n",
      "Epoch 55/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9782\n",
      "Epoch 00055: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1034 - accuracy: 0.9782 - val_loss: 0.3269 - val_accuracy: 0.9187\n",
      "Epoch 56/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.1054 - accuracy: 0.9784\n",
      "Epoch 00056: val_accuracy did not improve from 0.92269\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.1052 - accuracy: 0.9785 - val_loss: 0.3256 - val_accuracy: 0.9223\n",
      "Epoch 57/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9798\n",
      "Epoch 00057: val_accuracy improved from 0.92269 to 0.92630, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0996 - accuracy: 0.9798 - val_loss: 0.3281 - val_accuracy: 0.9263\n",
      "Epoch 58/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9803\n",
      "Epoch 00058: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0985 - accuracy: 0.9803 - val_loss: 0.3463 - val_accuracy: 0.9216\n",
      "Epoch 59/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9810\n",
      "Epoch 00059: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0979 - accuracy: 0.9809 - val_loss: 0.3517 - val_accuracy: 0.9212\n",
      "Epoch 60/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9822\n",
      "Epoch 00060: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0960 - accuracy: 0.9822 - val_loss: 0.3377 - val_accuracy: 0.9187\n",
      "Epoch 61/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9804\n",
      "Epoch 00061: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0961 - accuracy: 0.9804 - val_loss: 0.3251 - val_accuracy: 0.9249\n",
      "Epoch 62/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0962 - accuracy: 0.9810\n",
      "Epoch 00062: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0963 - accuracy: 0.9808 - val_loss: 0.3305 - val_accuracy: 0.9205\n",
      "Epoch 63/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9817\n",
      "Epoch 00063: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0964 - accuracy: 0.9816 - val_loss: 0.3173 - val_accuracy: 0.9249\n",
      "Epoch 64/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9820\n",
      "Epoch 00064: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0937 - accuracy: 0.9819 - val_loss: 0.3209 - val_accuracy: 0.9212\n",
      "Epoch 65/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9817\n",
      "Epoch 00065: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0926 - accuracy: 0.9816 - val_loss: 0.3308 - val_accuracy: 0.9234\n",
      "Epoch 66/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9816\n",
      "Epoch 00066: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0944 - accuracy: 0.9816 - val_loss: 0.3301 - val_accuracy: 0.9259\n",
      "Epoch 67/150\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9826\n",
      "Epoch 00067: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0897 - accuracy: 0.9826 - val_loss: 0.3267 - val_accuracy: 0.9252\n",
      "Epoch 68/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0902 - accuracy: 0.9820\n",
      "Epoch 00068: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0903 - accuracy: 0.9819 - val_loss: 0.3289 - val_accuracy: 0.9194\n",
      "Epoch 69/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9836\n",
      "Epoch 00069: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0882 - accuracy: 0.9835 - val_loss: 0.3286 - val_accuracy: 0.9241\n",
      "Epoch 70/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9838\n",
      "Epoch 00070: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0866 - accuracy: 0.9838 - val_loss: 0.3278 - val_accuracy: 0.9227\n",
      "Epoch 71/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9842\n",
      "Epoch 00071: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0878 - accuracy: 0.9842 - val_loss: 0.3215 - val_accuracy: 0.9249\n",
      "Epoch 72/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0928 - accuracy: 0.9803\n",
      "Epoch 00072: val_accuracy did not improve from 0.92630\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0929 - accuracy: 0.9804 - val_loss: 0.3222 - val_accuracy: 0.9223\n",
      "Epoch 73/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9837\n",
      "Epoch 00073: val_accuracy improved from 0.92630 to 0.92811, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9837 - val_loss: 0.3267 - val_accuracy: 0.9281\n",
      "Epoch 74/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9837\n",
      "Epoch 00074: val_accuracy did not improve from 0.92811\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9837 - val_loss: 0.3295 - val_accuracy: 0.9241\n",
      "Epoch 75/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0850 - accuracy: 0.9833\n",
      "Epoch 00075: val_accuracy did not improve from 0.92811\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0849 - accuracy: 0.9833 - val_loss: 0.3385 - val_accuracy: 0.9216\n",
      "Epoch 76/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9834\n",
      "Epoch 00076: val_accuracy improved from 0.92811 to 0.92883, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0864 - accuracy: 0.9835 - val_loss: 0.3246 - val_accuracy: 0.9288\n",
      "Epoch 77/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9846\n",
      "Epoch 00077: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0835 - accuracy: 0.9847 - val_loss: 0.3240 - val_accuracy: 0.9267\n",
      "Epoch 78/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9840\n",
      "Epoch 00078: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0833 - accuracy: 0.9840 - val_loss: 0.3267 - val_accuracy: 0.9270\n",
      "Epoch 79/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9838\n",
      "Epoch 00079: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0857 - accuracy: 0.9839 - val_loss: 0.3376 - val_accuracy: 0.9198\n",
      "Epoch 80/150\n",
      "769/779 [============================>.] - ETA: 0s - loss: 0.0821 - accuracy: 0.9850\n",
      "Epoch 00080: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0819 - accuracy: 0.9851 - val_loss: 0.3310 - val_accuracy: 0.9234\n",
      "Epoch 81/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9839\n",
      "Epoch 00081: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0834 - accuracy: 0.9839 - val_loss: 0.3350 - val_accuracy: 0.9230\n",
      "Epoch 82/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9856\n",
      "Epoch 00082: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9857 - val_loss: 0.3286 - val_accuracy: 0.9259\n",
      "Epoch 83/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9839\n",
      "Epoch 00083: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0844 - accuracy: 0.9840 - val_loss: 0.3312 - val_accuracy: 0.9212\n",
      "Epoch 84/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9854\n",
      "Epoch 00084: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.9852 - val_loss: 0.3273 - val_accuracy: 0.9256\n",
      "Epoch 85/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9868\n",
      "Epoch 00085: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0778 - accuracy: 0.9869 - val_loss: 0.3524 - val_accuracy: 0.9173\n",
      "Epoch 86/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9849\n",
      "Epoch 00086: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9848 - val_loss: 0.3217 - val_accuracy: 0.9263\n",
      "Epoch 87/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9858\n",
      "Epoch 00087: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0794 - accuracy: 0.9860 - val_loss: 0.3293 - val_accuracy: 0.9223\n",
      "Epoch 88/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9864\n",
      "Epoch 00088: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9863 - val_loss: 0.3351 - val_accuracy: 0.9234\n",
      "Epoch 89/150\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9867\n",
      "Epoch 00089: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9866 - val_loss: 0.3222 - val_accuracy: 0.9270\n",
      "Epoch 90/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9858\n",
      "Epoch 00090: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0771 - accuracy: 0.9857 - val_loss: 0.3586 - val_accuracy: 0.9198\n",
      "Epoch 91/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9860\n",
      "Epoch 00091: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0774 - accuracy: 0.9860 - val_loss: 0.3336 - val_accuracy: 0.9212\n",
      "Epoch 92/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9852\n",
      "Epoch 00092: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9846 - val_loss: 0.3289 - val_accuracy: 0.9241\n",
      "Epoch 93/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9866\n",
      "Epoch 00093: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0755 - accuracy: 0.9865 - val_loss: 0.3398 - val_accuracy: 0.9238\n",
      "Epoch 94/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9858\n",
      "Epoch 00094: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0770 - accuracy: 0.9858 - val_loss: 0.3471 - val_accuracy: 0.9205\n",
      "Epoch 95/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9853\n",
      "Epoch 00095: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9853 - val_loss: 0.3316 - val_accuracy: 0.9270\n",
      "Epoch 96/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9853\n",
      "Epoch 00096: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0786 - accuracy: 0.9853 - val_loss: 0.3402 - val_accuracy: 0.9216\n",
      "Epoch 97/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9850\n",
      "Epoch 00097: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9850 - val_loss: 0.3369 - val_accuracy: 0.9220\n",
      "Epoch 98/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9847\n",
      "Epoch 00098: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0780 - accuracy: 0.9847 - val_loss: 0.3318 - val_accuracy: 0.9209\n",
      "Epoch 99/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9876\n",
      "Epoch 00099: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9876 - val_loss: 0.3387 - val_accuracy: 0.9227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9868\n",
      "Epoch 00100: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0736 - accuracy: 0.9869 - val_loss: 0.3297 - val_accuracy: 0.9252\n",
      "Epoch 101/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9871\n",
      "Epoch 00101: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9872 - val_loss: 0.3508 - val_accuracy: 0.9198\n",
      "Epoch 102/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9861\n",
      "Epoch 00102: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0764 - accuracy: 0.9861 - val_loss: 0.3261 - val_accuracy: 0.9238\n",
      "Epoch 103/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9876\n",
      "Epoch 00103: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0706 - accuracy: 0.9876 - val_loss: 0.3426 - val_accuracy: 0.9252\n",
      "Epoch 104/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9877\n",
      "Epoch 00104: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9877 - val_loss: 0.3359 - val_accuracy: 0.9259\n",
      "Epoch 105/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9893\n",
      "Epoch 00105: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9892 - val_loss: 0.3387 - val_accuracy: 0.9238\n",
      "Epoch 106/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9877\n",
      "Epoch 00106: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0702 - accuracy: 0.9877 - val_loss: 0.3467 - val_accuracy: 0.9216\n",
      "Epoch 107/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9890\n",
      "Epoch 00107: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 3s 3ms/step - loss: 0.0676 - accuracy: 0.9890 - val_loss: 0.3397 - val_accuracy: 0.9270\n",
      "Epoch 108/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9865\n",
      "Epoch 00108: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9865 - val_loss: 0.3315 - val_accuracy: 0.9223\n",
      "Epoch 109/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9867\n",
      "Epoch 00109: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0708 - accuracy: 0.9867 - val_loss: 0.3473 - val_accuracy: 0.9241\n",
      "Epoch 110/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9866\n",
      "Epoch 00110: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0710 - accuracy: 0.9865 - val_loss: 0.3348 - val_accuracy: 0.9234\n",
      "Epoch 111/150\n",
      "764/779 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9881\n",
      "Epoch 00111: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9880 - val_loss: 0.3556 - val_accuracy: 0.9129\n",
      "Epoch 112/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9859\n",
      "Epoch 00112: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9859 - val_loss: 0.3519 - val_accuracy: 0.9216\n",
      "Epoch 113/150\n",
      "771/779 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9864\n",
      "Epoch 00113: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0720 - accuracy: 0.9863 - val_loss: 0.3357 - val_accuracy: 0.9241\n",
      "Epoch 114/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9872\n",
      "Epoch 00114: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0686 - accuracy: 0.9871 - val_loss: 0.3409 - val_accuracy: 0.9227\n",
      "Epoch 115/150\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9871\n",
      "Epoch 00115: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0699 - accuracy: 0.9871 - val_loss: 0.3352 - val_accuracy: 0.9270\n",
      "Epoch 116/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0630 - accuracy: 0.9896\n",
      "Epoch 00116: val_accuracy did not improve from 0.92883\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0630 - accuracy: 0.9896 - val_loss: 0.3446 - val_accuracy: 0.9274\n",
      "Epoch 117/150\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.0686 - accuracy: 0.9875\n",
      "Epoch 00117: val_accuracy improved from 0.92883 to 0.92991, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0686 - accuracy: 0.9874 - val_loss: 0.3293 - val_accuracy: 0.9299\n",
      "Epoch 118/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9893\n",
      "Epoch 00118: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9894 - val_loss: 0.3371 - val_accuracy: 0.9238\n",
      "Epoch 119/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9869\n",
      "Epoch 00119: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9867 - val_loss: 0.3295 - val_accuracy: 0.9259\n",
      "Epoch 120/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9890\n",
      "Epoch 00120: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0657 - accuracy: 0.9888 - val_loss: 0.3468 - val_accuracy: 0.9238\n",
      "Epoch 121/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9887\n",
      "Epoch 00121: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9888 - val_loss: 0.3468 - val_accuracy: 0.9230\n",
      "Epoch 122/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9888\n",
      "Epoch 00122: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9888 - val_loss: 0.3286 - val_accuracy: 0.9245\n",
      "Epoch 123/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0648 - accuracy: 0.9890\n",
      "Epoch 00123: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9889 - val_loss: 0.3354 - val_accuracy: 0.9216\n",
      "Epoch 124/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9883\n",
      "Epoch 00124: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0674 - accuracy: 0.9882 - val_loss: 0.3593 - val_accuracy: 0.9205\n",
      "Epoch 125/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9883\n",
      "Epoch 00125: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9882 - val_loss: 0.3351 - val_accuracy: 0.9249\n",
      "Epoch 126/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9876\n",
      "Epoch 00126: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9877 - val_loss: 0.3348 - val_accuracy: 0.9198\n",
      "Epoch 127/150\n",
      "770/779 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9881\n",
      "Epoch 00127: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0654 - accuracy: 0.9881 - val_loss: 0.3443 - val_accuracy: 0.9212\n",
      "Epoch 128/150\n",
      "761/779 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9886\n",
      "Epoch 00128: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9887 - val_loss: 0.3489 - val_accuracy: 0.9263\n",
      "Epoch 129/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9873\n",
      "Epoch 00129: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9873 - val_loss: 0.3472 - val_accuracy: 0.9252\n",
      "Epoch 130/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9869\n",
      "Epoch 00130: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0683 - accuracy: 0.9869 - val_loss: 0.3349 - val_accuracy: 0.9292\n",
      "Epoch 131/150\n",
      "760/779 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9891\n",
      "Epoch 00131: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0635 - accuracy: 0.9891 - val_loss: 0.3414 - val_accuracy: 0.9256\n",
      "Epoch 132/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9888\n",
      "Epoch 00132: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0624 - accuracy: 0.9889 - val_loss: 0.3560 - val_accuracy: 0.9245\n",
      "Epoch 133/150\n",
      "772/779 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9870\n",
      "Epoch 00133: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0650 - accuracy: 0.9870 - val_loss: 0.3454 - val_accuracy: 0.9216\n",
      "Epoch 134/150\n",
      "777/779 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9887\n",
      "Epoch 00134: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0646 - accuracy: 0.9887 - val_loss: 0.3560 - val_accuracy: 0.9227\n",
      "Epoch 135/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9875\n",
      "Epoch 00135: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0667 - accuracy: 0.9877 - val_loss: 0.3414 - val_accuracy: 0.9245\n",
      "Epoch 136/150\n",
      "778/779 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9890\n",
      "Epoch 00136: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9890 - val_loss: 0.3324 - val_accuracy: 0.9234\n",
      "Epoch 137/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9898\n",
      "Epoch 00137: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0605 - accuracy: 0.9898 - val_loss: 0.3334 - val_accuracy: 0.9216\n",
      "Epoch 138/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9894\n",
      "Epoch 00138: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9892 - val_loss: 0.3230 - val_accuracy: 0.9252\n",
      "Epoch 139/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0612 - accuracy: 0.9890\n",
      "Epoch 00139: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9890 - val_loss: 0.3175 - val_accuracy: 0.9277\n",
      "Epoch 140/150\n",
      "766/779 [============================>.] - ETA: 0s - loss: 0.0601 - accuracy: 0.9897\n",
      "Epoch 00140: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0601 - accuracy: 0.9896 - val_loss: 0.3351 - val_accuracy: 0.9238\n",
      "Epoch 141/150\n",
      "765/779 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9877\n",
      "Epoch 00141: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9874 - val_loss: 0.3207 - val_accuracy: 0.9299\n",
      "Epoch 142/150\n",
      "773/779 [============================>.] - ETA: 0s - loss: 0.0588 - accuracy: 0.9903\n",
      "Epoch 00142: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0591 - accuracy: 0.9902 - val_loss: 0.3325 - val_accuracy: 0.9281\n",
      "Epoch 143/150\n",
      "774/779 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9895\n",
      "Epoch 00143: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0600 - accuracy: 0.9895 - val_loss: 0.3247 - val_accuracy: 0.9238\n",
      "Epoch 144/150\n",
      "775/779 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9894\n",
      "Epoch 00144: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9895 - val_loss: 0.3524 - val_accuracy: 0.9263\n",
      "Epoch 145/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 0.9904\n",
      "Epoch 00145: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0567 - accuracy: 0.9906 - val_loss: 0.3470 - val_accuracy: 0.9205\n",
      "Epoch 146/150\n",
      "762/779 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9905\n",
      "Epoch 00146: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0585 - accuracy: 0.9904 - val_loss: 0.3271 - val_accuracy: 0.9252\n",
      "Epoch 147/150\n",
      "763/779 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9905\n",
      "Epoch 00147: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0577 - accuracy: 0.9904 - val_loss: 0.3577 - val_accuracy: 0.9212\n",
      "Epoch 148/150\n",
      "767/779 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9892\n",
      "Epoch 00148: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0612 - accuracy: 0.9893 - val_loss: 0.3363 - val_accuracy: 0.9227\n",
      "Epoch 149/150\n",
      "776/779 [============================>.] - ETA: 0s - loss: 0.0611 - accuracy: 0.9897\n",
      "Epoch 00149: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0610 - accuracy: 0.9898 - val_loss: 0.3512 - val_accuracy: 0.9209\n",
      "Epoch 150/150\n",
      "768/779 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9896\n",
      "Epoch 00150: val_accuracy did not improve from 0.92991\n",
      "779/779 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9897 - val_loss: 0.3488 - val_accuracy: 0.9249\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:46<00:00, 263.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.66      0.70      0.68       700\n",
      "        car_horn       0.82      0.63      0.71       196\n",
      "children_playing       0.69      0.70      0.70       700\n",
      "        dog_bark       0.77      0.86      0.81       700\n",
      "           siren       0.72      0.59      0.65       539\n",
      "\n",
      "        accuracy                           0.71      2835\n",
      "       macro avg       0.73      0.70      0.71      2835\n",
      "    weighted avg       0.72      0.71      0.71      2835\n",
      "\n",
      "Validation fold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27776, 375)\n",
      "X_val_norm shape.....:(2730, 375)\n",
      "\n",
      "Sum of elements: 0.9800451063769955\n",
      "Number of elements summed: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 234)               54990     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 234)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               176250    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 289,985\n",
      "Trainable params: 289,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24998, 234)\n",
      "Epoch 1/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.8196 - accuracy: 0.6999\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82793, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.8151 - accuracy: 0.7012 - val_loss: 0.5143 - val_accuracy: 0.8279\n",
      "Epoch 2/350\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.4584 - accuracy: 0.8368\n",
      "Epoch 00002: val_accuracy improved from 0.82793 to 0.86285, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4584 - accuracy: 0.8368 - val_loss: 0.4007 - val_accuracy: 0.8629\n",
      "Epoch 3/350\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.3532 - accuracy: 0.8733\n",
      "Epoch 00003: val_accuracy improved from 0.86285 to 0.88049, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.3527 - accuracy: 0.8733 - val_loss: 0.3464 - val_accuracy: 0.8805\n",
      "Epoch 4/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.8983\n",
      "Epoch 00004: val_accuracy improved from 0.88049 to 0.88949, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2870 - accuracy: 0.8982 - val_loss: 0.3139 - val_accuracy: 0.8895\n",
      "Epoch 5/350\n",
      "758/782 [============================>.] - ETA: 0s - loss: 0.2377 - accuracy: 0.9179\n",
      "Epoch 00005: val_accuracy improved from 0.88949 to 0.89921, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2361 - accuracy: 0.9185 - val_loss: 0.2915 - val_accuracy: 0.8992\n",
      "Epoch 6/350\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.1992 - accuracy: 0.9311\n",
      "Epoch 00006: val_accuracy improved from 0.89921 to 0.90929, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1993 - accuracy: 0.9310 - val_loss: 0.2690 - val_accuracy: 0.9093\n",
      "Epoch 7/350\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1674 - accuracy: 0.9422\n",
      "Epoch 00007: val_accuracy improved from 0.90929 to 0.91757, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1678 - accuracy: 0.9420 - val_loss: 0.2505 - val_accuracy: 0.9176\n",
      "Epoch 8/350\n",
      "758/782 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9522\n",
      "Epoch 00008: val_accuracy improved from 0.91757 to 0.92405, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1420 - accuracy: 0.9520 - val_loss: 0.2412 - val_accuracy: 0.9240\n",
      "Epoch 9/350\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9593\n",
      "Epoch 00009: val_accuracy improved from 0.92405 to 0.92729, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1209 - accuracy: 0.9597 - val_loss: 0.2295 - val_accuracy: 0.9273\n",
      "Epoch 10/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9672\n",
      "Epoch 00010: val_accuracy improved from 0.92729 to 0.93017, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1007 - accuracy: 0.9672 - val_loss: 0.2228 - val_accuracy: 0.9302\n",
      "Epoch 11/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9710\n",
      "Epoch 00011: val_accuracy did not improve from 0.93017\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0862 - accuracy: 0.9713 - val_loss: 0.2201 - val_accuracy: 0.9294\n",
      "Epoch 12/350\n",
      "761/782 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9759\n",
      "Epoch 00012: val_accuracy improved from 0.93017 to 0.93125, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0726 - accuracy: 0.9760 - val_loss: 0.2222 - val_accuracy: 0.9312\n",
      "Epoch 13/350\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9801\n",
      "Epoch 00013: val_accuracy improved from 0.93125 to 0.93521, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0619 - accuracy: 0.9802 - val_loss: 0.2199 - val_accuracy: 0.9352\n",
      "Epoch 14/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0541 - accuracy: 0.9830\n",
      "Epoch 00014: val_accuracy did not improve from 0.93521\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0539 - accuracy: 0.9830 - val_loss: 0.2109 - val_accuracy: 0.9352\n",
      "Epoch 15/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0438 - accuracy: 0.9870\n",
      "Epoch 00015: val_accuracy did not improve from 0.93521\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0438 - accuracy: 0.9870 - val_loss: 0.2181 - val_accuracy: 0.9352\n",
      "Epoch 16/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0382 - accuracy: 0.9886\n",
      "Epoch 00016: val_accuracy improved from 0.93521 to 0.93988, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.9887 - val_loss: 0.2157 - val_accuracy: 0.9399\n",
      "Epoch 17/350\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9900\n",
      "Epoch 00017: val_accuracy did not improve from 0.93988\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9900 - val_loss: 0.2441 - val_accuracy: 0.9356\n",
      "Epoch 18/350\n",
      "761/782 [============================>.] - ETA: 0s - loss: 0.0287 - accuracy: 0.9920\n",
      "Epoch 00018: val_accuracy improved from 0.93988 to 0.94096, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0290 - accuracy: 0.9918 - val_loss: 0.2222 - val_accuracy: 0.9410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9933\n",
      "Epoch 00019: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0262 - accuracy: 0.9932 - val_loss: 0.2243 - val_accuracy: 0.9392\n",
      "Epoch 20/350\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9947\n",
      "Epoch 00020: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9947 - val_loss: 0.2322 - val_accuracy: 0.9399\n",
      "Epoch 21/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9953\n",
      "Epoch 00021: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9953 - val_loss: 0.2381 - val_accuracy: 0.9381\n",
      "Epoch 22/350\n",
      "758/782 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9966\n",
      "Epoch 00022: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9965 - val_loss: 0.2438 - val_accuracy: 0.9381\n",
      "Epoch 23/350\n",
      "757/782 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9964\n",
      "Epoch 00023: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0152 - accuracy: 0.9963 - val_loss: 0.2502 - val_accuracy: 0.9406\n",
      "Epoch 24/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9969\n",
      "Epoch 00024: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.2423 - val_accuracy: 0.9406\n",
      "Epoch 25/350\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9972\n",
      "Epoch 00025: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9972 - val_loss: 0.2587 - val_accuracy: 0.9384\n",
      "Epoch 26/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9977\n",
      "Epoch 00026: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9977 - val_loss: 0.2527 - val_accuracy: 0.9399\n",
      "Epoch 27/350\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9975\n",
      "Epoch 00027: val_accuracy did not improve from 0.94096\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0102 - accuracy: 0.9974 - val_loss: 0.2613 - val_accuracy: 0.9384\n",
      "Epoch 28/350\n",
      "755/782 [===========================>..] - ETA: 0s - loss: 0.0093 - accuracy: 0.9978\n",
      "Epoch 00028: val_accuracy improved from 0.94096 to 0.94204, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.2639 - val_accuracy: 0.9420\n",
      "Epoch 29/350\n",
      "757/782 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9975\n",
      "Epoch 00029: val_accuracy did not improve from 0.94204\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.2602 - val_accuracy: 0.9381\n",
      "Epoch 30/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9982\n",
      "Epoch 00030: val_accuracy did not improve from 0.94204\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9982 - val_loss: 0.2593 - val_accuracy: 0.9420\n",
      "Epoch 31/350\n",
      "749/782 [===========================>..] - ETA: 0s - loss: 0.0066 - accuracy: 0.9989\n",
      "Epoch 00031: val_accuracy did not improve from 0.94204\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.2673 - val_accuracy: 0.9399\n",
      "Epoch 32/350\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9987\n",
      "Epoch 00032: val_accuracy did not improve from 0.94204\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 0.2717 - val_accuracy: 0.9413\n",
      "Epoch 33/350\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9985\n",
      "Epoch 00033: val_accuracy improved from 0.94204 to 0.94384, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.2661 - val_accuracy: 0.9438\n",
      "Epoch 34/350\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9991\n",
      "Epoch 00034: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.2696 - val_accuracy: 0.9428\n",
      "Epoch 35/350\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9994\n",
      "Epoch 00035: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.2740 - val_accuracy: 0.9413\n",
      "Epoch 36/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9989\n",
      "Epoch 00036: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.2830 - val_accuracy: 0.9424\n",
      "Epoch 37/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9989\n",
      "Epoch 00037: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.2885 - val_accuracy: 0.9438\n",
      "Epoch 38/350\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9985\n",
      "Epoch 00038: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.2874 - val_accuracy: 0.9431\n",
      "Epoch 39/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9989\n",
      "Epoch 00039: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.2854 - val_accuracy: 0.9435\n",
      "Epoch 40/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.94384\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.2864 - val_accuracy: 0.9435\n",
      "Epoch 41/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9993\n",
      "Epoch 00041: val_accuracy improved from 0.94384 to 0.94600, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.2828 - val_accuracy: 0.9460\n",
      "Epoch 42/350\n",
      "750/782 [===========================>..] - ETA: 0s - loss: 0.0039 - accuracy: 0.9992\n",
      "Epoch 00042: val_accuracy did not improve from 0.94600\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.2800 - val_accuracy: 0.9460\n",
      "Epoch 43/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00043: val_accuracy improved from 0.94600 to 0.94708, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2766 - val_accuracy: 0.9471\n",
      "Epoch 44/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9986\n",
      "Epoch 00044: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.2722 - val_accuracy: 0.9428\n",
      "Epoch 45/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 00045: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.2765 - val_accuracy: 0.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 00046: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.2767 - val_accuracy: 0.9442\n",
      "Epoch 47/350\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 00047: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.2806 - val_accuracy: 0.9428\n",
      "Epoch 48/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 00048: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.2830 - val_accuracy: 0.9388\n",
      "Epoch 49/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9996\n",
      "Epoch 00049: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.2807 - val_accuracy: 0.9438\n",
      "Epoch 50/350\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 00050: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2829 - val_accuracy: 0.9431\n",
      "Epoch 51/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9998\n",
      "Epoch 00051: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.2931 - val_accuracy: 0.9399\n",
      "Epoch 52/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00052: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2920 - val_accuracy: 0.9413\n",
      "Epoch 53/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9992\n",
      "Epoch 00053: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.2849 - val_accuracy: 0.9442\n",
      "Epoch 54/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 00054: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.2929 - val_accuracy: 0.9417\n",
      "Epoch 55/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9996\n",
      "Epoch 00055: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0037 - accuracy: 0.9996 - val_loss: 0.2963 - val_accuracy: 0.9413\n",
      "Epoch 56/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00056: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.2996 - val_accuracy: 0.9424\n",
      "Epoch 57/350\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 00057: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.2954 - val_accuracy: 0.9438\n",
      "Epoch 58/350\n",
      "762/782 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 00058: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.2927 - val_accuracy: 0.9431\n",
      "Epoch 59/350\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00059: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.3090 - val_accuracy: 0.9413\n",
      "Epoch 60/350\n",
      "760/782 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9998\n",
      "Epoch 00060: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.3023 - val_accuracy: 0.9428\n",
      "Epoch 61/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.3038 - val_accuracy: 0.9435\n",
      "Epoch 62/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9999\n",
      "Epoch 00062: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.3089 - val_accuracy: 0.9431\n",
      "Epoch 63/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00063: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3072 - val_accuracy: 0.9438\n",
      "Epoch 64/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00064: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.3053 - val_accuracy: 0.9431\n",
      "Epoch 65/350\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9997\n",
      "Epoch 00065: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.3054 - val_accuracy: 0.9428\n",
      "Epoch 66/350\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 00066: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.2998 - val_accuracy: 0.9438\n",
      "Epoch 67/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 00067: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.3094 - val_accuracy: 0.9431\n",
      "Epoch 68/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 00068: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9410\n",
      "Epoch 69/350\n",
      "760/782 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00069: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.3116 - val_accuracy: 0.9420\n",
      "Epoch 70/350\n",
      "760/782 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00070: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.3063 - val_accuracy: 0.9446\n",
      "Epoch 71/350\n",
      "752/782 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.3034 - val_accuracy: 0.9438\n",
      "Epoch 72/350\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00072: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.3078 - val_accuracy: 0.9428\n",
      "Epoch 73/350\n",
      "755/782 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00073: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2959 - val_accuracy: 0.9449\n",
      "Epoch 74/350\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00074: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3023 - val_accuracy: 0.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00075: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3026 - val_accuracy: 0.9442\n",
      "Epoch 76/350\n",
      "751/782 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00076: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3043 - val_accuracy: 0.9460\n",
      "Epoch 77/350\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00077: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3079 - val_accuracy: 0.9449\n",
      "Epoch 78/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00078: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3076 - val_accuracy: 0.9464\n",
      "Epoch 79/350\n",
      "754/782 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00079: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3100 - val_accuracy: 0.9453\n",
      "Epoch 80/350\n",
      "749/782 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00080: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3149 - val_accuracy: 0.9431\n",
      "Epoch 81/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 00081: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3056 - val_accuracy: 0.9446\n",
      "Epoch 82/350\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 00082: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.3065 - val_accuracy: 0.9460\n",
      "Epoch 83/350\n",
      "776/782 [============================>.] - ETA: 0s - loss: 9.9313e-04 - accuracy: 0.9999\n",
      "Epoch 00083: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3084 - val_accuracy: 0.9467\n",
      "Epoch 84/350\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3121 - val_accuracy: 0.9449\n",
      "Epoch 85/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00085: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.3182 - val_accuracy: 0.9460\n",
      "Epoch 86/350\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 00086: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.3138 - val_accuracy: 0.9446\n",
      "Epoch 87/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 00087: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.3097 - val_accuracy: 0.9456\n",
      "Epoch 88/350\n",
      "756/782 [============================>.] - ETA: 0s - loss: 9.1036e-04 - accuracy: 0.9999\n",
      "Epoch 00088: val_accuracy did not improve from 0.94708\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.3158 - val_accuracy: 0.9446\n",
      "Epoch 89/350\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00089: val_accuracy improved from 0.94708 to 0.94744, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3185 - val_accuracy: 0.9474\n",
      "Epoch 90/350\n",
      "780/782 [============================>.] - ETA: 0s - loss: 9.7362e-04 - accuracy: 0.9998\n",
      "Epoch 00090: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.7295e-04 - accuracy: 0.9998 - val_loss: 0.3172 - val_accuracy: 0.9424\n",
      "Epoch 91/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 9.9463e-04 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.8843e-04 - accuracy: 0.9999 - val_loss: 0.3180 - val_accuracy: 0.9449\n",
      "Epoch 92/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 9.6414e-04 - accuracy: 0.9998\n",
      "Epoch 00092: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.5039e-04 - accuracy: 0.9998 - val_loss: 0.3209 - val_accuracy: 0.9431\n",
      "Epoch 93/350\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3179 - val_accuracy: 0.9435\n",
      "Epoch 94/350\n",
      "761/782 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00094: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.3228 - val_accuracy: 0.9460\n",
      "Epoch 95/350\n",
      "782/782 [==============================] - ETA: 0s - loss: 8.4511e-04 - accuracy: 0.9998\n",
      "Epoch 00095: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.4511e-04 - accuracy: 0.9998 - val_loss: 0.3243 - val_accuracy: 0.9453\n",
      "Epoch 96/350\n",
      "755/782 [===========================>..] - ETA: 0s - loss: 7.9958e-04 - accuracy: 1.0000\n",
      "Epoch 00096: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 7.9332e-04 - accuracy: 1.0000 - val_loss: 0.3244 - val_accuracy: 0.9442\n",
      "Epoch 97/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 9.0558e-04 - accuracy: 0.9999\n",
      "Epoch 00097: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.9790e-04 - accuracy: 0.9999 - val_loss: 0.3304 - val_accuracy: 0.9456\n",
      "Epoch 98/350\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00098: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.3348 - val_accuracy: 0.9417\n",
      "Epoch 99/350\n",
      "778/782 [============================>.] - ETA: 0s - loss: 8.3844e-04 - accuracy: 1.0000\n",
      "Epoch 00099: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.3534e-04 - accuracy: 1.0000 - val_loss: 0.3339 - val_accuracy: 0.9417\n",
      "Epoch 100/350\n",
      "749/782 [===========================>..] - ETA: 0s - loss: 9.4830e-04 - accuracy: 0.9997\n",
      "Epoch 00100: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.2471e-04 - accuracy: 0.9998 - val_loss: 0.3277 - val_accuracy: 0.9431\n",
      "Epoch 101/350\n",
      "757/782 [============================>.] - ETA: 0s - loss: 8.0448e-04 - accuracy: 0.9999\n",
      "Epoch 00101: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.1871e-04 - accuracy: 0.9999 - val_loss: 0.3240 - val_accuracy: 0.9428\n",
      "Epoch 102/350\n",
      "757/782 [============================>.] - ETA: 0s - loss: 7.4965e-04 - accuracy: 0.9999\n",
      "Epoch 00102: val_accuracy did not improve from 0.94744\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 7.5028e-04 - accuracy: 0.9999 - val_loss: 0.3189 - val_accuracy: 0.9456\n",
      "Epoch 103/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/782 [============================>.] - ETA: 0s - loss: 8.4075e-04 - accuracy: 0.9999\n",
      "Epoch 00103: val_accuracy improved from 0.94744 to 0.94780, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.4054e-04 - accuracy: 0.9999 - val_loss: 0.3140 - val_accuracy: 0.9478\n",
      "Epoch 104/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 8.9225e-04 - accuracy: 0.9998\n",
      "Epoch 00104: val_accuracy did not improve from 0.94780\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.7777e-04 - accuracy: 0.9998 - val_loss: 0.3231 - val_accuracy: 0.9464\n",
      "Epoch 105/350\n",
      "782/782 [==============================] - ETA: 0s - loss: 8.8921e-04 - accuracy: 0.9998\n",
      "Epoch 00105: val_accuracy did not improve from 0.94780\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.8921e-04 - accuracy: 0.9998 - val_loss: 0.3215 - val_accuracy: 0.9474\n",
      "Epoch 106/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 8.0071e-04 - accuracy: 0.9999\n",
      "Epoch 00106: val_accuracy did not improve from 0.94780\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.1913e-04 - accuracy: 0.9998 - val_loss: 0.3313 - val_accuracy: 0.9438\n",
      "Epoch 107/350\n",
      "769/782 [============================>.] - ETA: 0s - loss: 7.2959e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy improved from 0.94780 to 0.94960, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 7.2164e-04 - accuracy: 1.0000 - val_loss: 0.3270 - val_accuracy: 0.9496\n",
      "Epoch 108/350\n",
      "759/782 [============================>.] - ETA: 0s - loss: 6.2285e-04 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.94960\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 6.2799e-04 - accuracy: 1.0000 - val_loss: 0.3272 - val_accuracy: 0.9467\n",
      "Epoch 109/350\n",
      "781/782 [============================>.] - ETA: 0s - loss: 6.8589e-04 - accuracy: 0.9999\n",
      "Epoch 00109: val_accuracy did not improve from 0.94960\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 6.8573e-04 - accuracy: 0.9999 - val_loss: 0.3318 - val_accuracy: 0.9464\n",
      "Epoch 110/350\n",
      "749/782 [===========================>..] - ETA: 0s - loss: 9.5882e-04 - accuracy: 0.9998\n",
      "Epoch 00110: val_accuracy did not improve from 0.94960\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.2968e-04 - accuracy: 0.9998 - val_loss: 0.3320 - val_accuracy: 0.9474\n",
      "Epoch 111/350\n",
      "770/782 [============================>.] - ETA: 0s - loss: 6.7374e-04 - accuracy: 0.9999\n",
      "Epoch 00111: val_accuracy did not improve from 0.94960\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 6.8303e-04 - accuracy: 0.9999 - val_loss: 0.3309 - val_accuracy: 0.9456\n",
      "Epoch 112/350\n",
      "765/782 [============================>.] - ETA: 0s - loss: 9.5219e-04 - accuracy: 0.9998\n",
      "Epoch 00112: val_accuracy did not improve from 0.94960\n",
      "Restoring model weights from the end of the best epoch.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 9.4500e-04 - accuracy: 0.9998 - val_loss: 0.3289 - val_accuracy: 0.9471\n",
      "Epoch 00112: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:28<02:28, 148.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.72      0.80      0.76       560\n",
      "        car_horn       0.81      0.90      0.85       210\n",
      "children_playing       0.81      0.72      0.76       700\n",
      "        dog_bark       0.80      0.79      0.79       700\n",
      "           siren       0.80      0.81      0.81       560\n",
      "\n",
      "        accuracy                           0.78      2730\n",
      "       macro avg       0.79      0.80      0.79      2730\n",
      "    weighted avg       0.79      0.78      0.78      2730\n",
      "\n",
      "Model: \"Model_CNN_1D_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 228, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 228, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 228, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6384)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                319250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 330,291\n",
      "Trainable params: 330,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24998, 234, 1)\n",
      "Epoch 1/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.7778 - accuracy: 0.7519\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83585, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7778 - accuracy: 0.7519 - val_loss: 0.5536 - val_accuracy: 0.8359\n",
      "Epoch 2/150\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.5030 - accuracy: 0.8480\n",
      "Epoch 00002: val_accuracy improved from 0.83585 to 0.86537, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5028 - accuracy: 0.8481 - val_loss: 0.4692 - val_accuracy: 0.8654\n",
      "Epoch 3/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.4177 - accuracy: 0.8761\n",
      "Epoch 00003: val_accuracy improved from 0.86537 to 0.87545, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.4180 - accuracy: 0.8761 - val_loss: 0.4344 - val_accuracy: 0.8754\n",
      "Epoch 4/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8923\n",
      "Epoch 00004: val_accuracy improved from 0.87545 to 0.88373, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3664 - accuracy: 0.8924 - val_loss: 0.4085 - val_accuracy: 0.8837\n",
      "Epoch 5/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.9043\n",
      "Epoch 00005: val_accuracy improved from 0.88373 to 0.88805, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3335 - accuracy: 0.9040 - val_loss: 0.3947 - val_accuracy: 0.8880\n",
      "Epoch 6/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.9176\n",
      "Epoch 00006: val_accuracy improved from 0.88805 to 0.89237, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3029 - accuracy: 0.9176 - val_loss: 0.3861 - val_accuracy: 0.8924\n",
      "Epoch 7/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9208\n",
      "Epoch 00007: val_accuracy improved from 0.89237 to 0.89813, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2860 - accuracy: 0.9208 - val_loss: 0.3730 - val_accuracy: 0.8981\n",
      "Epoch 8/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.2675 - accuracy: 0.9273\n",
      "Epoch 00008: val_accuracy did not improve from 0.89813\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2674 - accuracy: 0.9273 - val_loss: 0.3731 - val_accuracy: 0.8934\n",
      "Epoch 9/150\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9307\n",
      "Epoch 00009: val_accuracy improved from 0.89813 to 0.90029, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2577 - accuracy: 0.9306 - val_loss: 0.3649 - val_accuracy: 0.9003\n",
      "Epoch 10/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9357\n",
      "Epoch 00010: val_accuracy did not improve from 0.90029\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2437 - accuracy: 0.9357 - val_loss: 0.3599 - val_accuracy: 0.8985\n",
      "Epoch 11/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.2297 - accuracy: 0.9404\n",
      "Epoch 00011: val_accuracy improved from 0.90029 to 0.90677, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2304 - accuracy: 0.9401 - val_loss: 0.3438 - val_accuracy: 0.9068\n",
      "Epoch 12/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.9424\n",
      "Epoch 00012: val_accuracy did not improve from 0.90677\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2210 - accuracy: 0.9423 - val_loss: 0.3650 - val_accuracy: 0.9032\n",
      "Epoch 13/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9477\n",
      "Epoch 00013: val_accuracy improved from 0.90677 to 0.90713, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2082 - accuracy: 0.9476 - val_loss: 0.3570 - val_accuracy: 0.9071\n",
      "Epoch 14/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9497\n",
      "Epoch 00014: val_accuracy improved from 0.90713 to 0.90929, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2024 - accuracy: 0.9495 - val_loss: 0.3535 - val_accuracy: 0.9093\n",
      "Epoch 15/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1955 - accuracy: 0.9521\n",
      "Epoch 00015: val_accuracy did not improve from 0.90929\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1961 - accuracy: 0.9516 - val_loss: 0.3732 - val_accuracy: 0.9046\n",
      "Epoch 16/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9542\n",
      "Epoch 00016: val_accuracy did not improve from 0.90929\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1876 - accuracy: 0.9542 - val_loss: 0.3584 - val_accuracy: 0.9068\n",
      "Epoch 17/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9560\n",
      "Epoch 00017: val_accuracy did not improve from 0.90929\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1814 - accuracy: 0.9562 - val_loss: 0.3627 - val_accuracy: 0.9082\n",
      "Epoch 18/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.9587\n",
      "Epoch 00018: val_accuracy improved from 0.90929 to 0.91001, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1758 - accuracy: 0.9587 - val_loss: 0.3582 - val_accuracy: 0.9100\n",
      "Epoch 19/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.9598\n",
      "Epoch 00019: val_accuracy did not improve from 0.91001\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1698 - accuracy: 0.9596 - val_loss: 0.3617 - val_accuracy: 0.9082\n",
      "Epoch 20/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.1687 - accuracy: 0.9599\n",
      "Epoch 00020: val_accuracy did not improve from 0.91001\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1687 - accuracy: 0.9600 - val_loss: 0.3652 - val_accuracy: 0.9071\n",
      "Epoch 21/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9609\n",
      "Epoch 00021: val_accuracy did not improve from 0.91001\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1613 - accuracy: 0.9610 - val_loss: 0.3624 - val_accuracy: 0.9100\n",
      "Epoch 22/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.1628 - accuracy: 0.9624\n",
      "Epoch 00022: val_accuracy improved from 0.91001 to 0.91181, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1634 - accuracy: 0.9620 - val_loss: 0.3578 - val_accuracy: 0.9118\n",
      "Epoch 23/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9633\n",
      "Epoch 00023: val_accuracy did not improve from 0.91181\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1567 - accuracy: 0.9634 - val_loss: 0.3510 - val_accuracy: 0.9078\n",
      "Epoch 24/150\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.1518 - accuracy: 0.9665\n",
      "Epoch 00024: val_accuracy improved from 0.91181 to 0.91505, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1519 - accuracy: 0.9664 - val_loss: 0.3580 - val_accuracy: 0.9150\n",
      "Epoch 25/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.1508 - accuracy: 0.9653\n",
      "Epoch 00025: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1508 - accuracy: 0.9654 - val_loss: 0.3633 - val_accuracy: 0.9089\n",
      "Epoch 26/150\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.1493 - accuracy: 0.9655\n",
      "Epoch 00026: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1503 - accuracy: 0.9653 - val_loss: 0.3539 - val_accuracy: 0.9136\n",
      "Epoch 27/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.9679\n",
      "Epoch 00027: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1428 - accuracy: 0.9677 - val_loss: 0.3642 - val_accuracy: 0.9111\n",
      "Epoch 28/150\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9677\n",
      "Epoch 00028: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1447 - accuracy: 0.9678 - val_loss: 0.3653 - val_accuracy: 0.9107\n",
      "Epoch 29/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9690\n",
      "Epoch 00029: val_accuracy did not improve from 0.91505\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1408 - accuracy: 0.9692 - val_loss: 0.3604 - val_accuracy: 0.9114\n",
      "Epoch 30/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9698\n",
      "Epoch 00030: val_accuracy improved from 0.91505 to 0.91541, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1345 - accuracy: 0.9698 - val_loss: 0.3663 - val_accuracy: 0.9154\n",
      "Epoch 31/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9703\n",
      "Epoch 00031: val_accuracy did not improve from 0.91541\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1338 - accuracy: 0.9703 - val_loss: 0.3620 - val_accuracy: 0.9147\n",
      "Epoch 32/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9725\n",
      "Epoch 00032: val_accuracy did not improve from 0.91541\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1279 - accuracy: 0.9725 - val_loss: 0.3592 - val_accuracy: 0.9147\n",
      "Epoch 33/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9719\n",
      "Epoch 00033: val_accuracy did not improve from 0.91541\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1296 - accuracy: 0.9717 - val_loss: 0.4030 - val_accuracy: 0.9078\n",
      "Epoch 34/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9720\n",
      "Epoch 00034: val_accuracy improved from 0.91541 to 0.91865, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1288 - accuracy: 0.9722 - val_loss: 0.3558 - val_accuracy: 0.9186\n",
      "Epoch 35/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9738\n",
      "Epoch 00035: val_accuracy did not improve from 0.91865\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1227 - accuracy: 0.9736 - val_loss: 0.3713 - val_accuracy: 0.9125\n",
      "Epoch 36/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9746\n",
      "Epoch 00036: val_accuracy did not improve from 0.91865\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1234 - accuracy: 0.9746 - val_loss: 0.3680 - val_accuracy: 0.9172\n",
      "Epoch 37/150\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9745\n",
      "Epoch 00037: val_accuracy did not improve from 0.91865\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1213 - accuracy: 0.9746 - val_loss: 0.3628 - val_accuracy: 0.9136\n",
      "Epoch 38/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9765\n",
      "Epoch 00038: val_accuracy did not improve from 0.91865\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1150 - accuracy: 0.9764 - val_loss: 0.3671 - val_accuracy: 0.9150\n",
      "Epoch 39/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9760\n",
      "Epoch 00039: val_accuracy improved from 0.91865 to 0.92117, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1179 - accuracy: 0.9758 - val_loss: 0.3611 - val_accuracy: 0.9212\n",
      "Epoch 40/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9766\n",
      "Epoch 00040: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1140 - accuracy: 0.9766 - val_loss: 0.3808 - val_accuracy: 0.9172\n",
      "Epoch 41/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9764\n",
      "Epoch 00041: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1139 - accuracy: 0.9762 - val_loss: 0.3648 - val_accuracy: 0.9179\n",
      "Epoch 42/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9753\n",
      "Epoch 00042: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1154 - accuracy: 0.9753 - val_loss: 0.3573 - val_accuracy: 0.9154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9743\n",
      "Epoch 00043: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1171 - accuracy: 0.9742 - val_loss: 0.3618 - val_accuracy: 0.9201\n",
      "Epoch 44/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 0.9767\n",
      "Epoch 00044: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1143 - accuracy: 0.9763 - val_loss: 0.3623 - val_accuracy: 0.9140\n",
      "Epoch 45/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9763\n",
      "Epoch 00045: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1127 - accuracy: 0.9761 - val_loss: 0.3751 - val_accuracy: 0.9176\n",
      "Epoch 46/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.9795\n",
      "Epoch 00046: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1042 - accuracy: 0.9795 - val_loss: 0.3638 - val_accuracy: 0.9212\n",
      "Epoch 47/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9798\n",
      "Epoch 00047: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1047 - accuracy: 0.9796 - val_loss: 0.3778 - val_accuracy: 0.9176\n",
      "Epoch 48/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9780\n",
      "Epoch 00048: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1059 - accuracy: 0.9779 - val_loss: 0.3599 - val_accuracy: 0.9183\n",
      "Epoch 49/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9792\n",
      "Epoch 00049: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1051 - accuracy: 0.9792 - val_loss: 0.3665 - val_accuracy: 0.9165\n",
      "Epoch 50/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.1008 - accuracy: 0.9799\n",
      "Epoch 00050: val_accuracy did not improve from 0.92117\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1009 - accuracy: 0.9799 - val_loss: 0.3597 - val_accuracy: 0.9190\n",
      "Epoch 51/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9797\n",
      "Epoch 00051: val_accuracy improved from 0.92117 to 0.92261, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1024 - accuracy: 0.9799 - val_loss: 0.3730 - val_accuracy: 0.9226\n",
      "Epoch 52/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9799\n",
      "Epoch 00052: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9800 - val_loss: 0.3814 - val_accuracy: 0.9176\n",
      "Epoch 53/150\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9800\n",
      "Epoch 00053: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1010 - accuracy: 0.9800 - val_loss: 0.3837 - val_accuracy: 0.9168\n",
      "Epoch 54/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9827\n",
      "Epoch 00054: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0958 - accuracy: 0.9825 - val_loss: 0.3690 - val_accuracy: 0.9197\n",
      "Epoch 55/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9797\n",
      "Epoch 00055: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0983 - accuracy: 0.9797 - val_loss: 0.3948 - val_accuracy: 0.9129\n",
      "Epoch 56/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9817\n",
      "Epoch 00056: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0955 - accuracy: 0.9817 - val_loss: 0.3912 - val_accuracy: 0.9194\n",
      "Epoch 57/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0973 - accuracy: 0.9815\n",
      "Epoch 00057: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0977 - accuracy: 0.9814 - val_loss: 0.3960 - val_accuracy: 0.9147\n",
      "Epoch 58/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9815\n",
      "Epoch 00058: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0944 - accuracy: 0.9815 - val_loss: 0.3726 - val_accuracy: 0.9204\n",
      "Epoch 59/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.9830\n",
      "Epoch 00059: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0906 - accuracy: 0.9830 - val_loss: 0.3822 - val_accuracy: 0.9219\n",
      "Epoch 60/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9825\n",
      "Epoch 00060: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0920 - accuracy: 0.9823 - val_loss: 0.3891 - val_accuracy: 0.9208\n",
      "Epoch 61/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9822\n",
      "Epoch 00061: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0946 - accuracy: 0.9821 - val_loss: 0.3966 - val_accuracy: 0.9165\n",
      "Epoch 62/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9824\n",
      "Epoch 00062: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0910 - accuracy: 0.9825 - val_loss: 0.3829 - val_accuracy: 0.9186\n",
      "Epoch 63/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9827\n",
      "Epoch 00063: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.9827 - val_loss: 0.3858 - val_accuracy: 0.9176\n",
      "Epoch 64/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9822\n",
      "Epoch 00064: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0916 - accuracy: 0.9821 - val_loss: 0.3882 - val_accuracy: 0.9179\n",
      "Epoch 65/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0910 - accuracy: 0.9816\n",
      "Epoch 00065: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0907 - accuracy: 0.9817 - val_loss: 0.3673 - val_accuracy: 0.9172\n",
      "Epoch 66/150\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9818\n",
      "Epoch 00066: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0911 - accuracy: 0.9818 - val_loss: 0.3875 - val_accuracy: 0.9172\n",
      "Epoch 67/150\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9822\n",
      "Epoch 00067: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9822 - val_loss: 0.3815 - val_accuracy: 0.9190\n",
      "Epoch 68/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9833\n",
      "Epoch 00068: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0880 - accuracy: 0.9833 - val_loss: 0.3954 - val_accuracy: 0.9158\n",
      "Epoch 69/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0874 - accuracy: 0.9837\n",
      "Epoch 00069: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0878 - accuracy: 0.9835 - val_loss: 0.3816 - val_accuracy: 0.9143\n",
      "Epoch 70/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9836\n",
      "Epoch 00070: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0856 - accuracy: 0.9834 - val_loss: 0.3786 - val_accuracy: 0.9186\n",
      "Epoch 71/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9839\n",
      "Epoch 00071: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9838 - val_loss: 0.3845 - val_accuracy: 0.9208\n",
      "Epoch 72/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9857\n",
      "Epoch 00072: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0808 - accuracy: 0.9858 - val_loss: 0.3944 - val_accuracy: 0.9150\n",
      "Epoch 73/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9833\n",
      "Epoch 00073: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9833 - val_loss: 0.3737 - val_accuracy: 0.9197\n",
      "Epoch 74/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0814 - accuracy: 0.9848\n",
      "Epoch 00074: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0814 - accuracy: 0.9848 - val_loss: 0.3961 - val_accuracy: 0.9168\n",
      "Epoch 75/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9852\n",
      "Epoch 00075: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9852 - val_loss: 0.4033 - val_accuracy: 0.9168\n",
      "Epoch 76/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9845\n",
      "Epoch 00076: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0813 - accuracy: 0.9844 - val_loss: 0.3922 - val_accuracy: 0.9194\n",
      "Epoch 77/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9848\n",
      "Epoch 00077: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0824 - accuracy: 0.9847 - val_loss: 0.4006 - val_accuracy: 0.9190\n",
      "Epoch 78/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9848\n",
      "Epoch 00078: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0818 - accuracy: 0.9847 - val_loss: 0.3924 - val_accuracy: 0.9212\n",
      "Epoch 79/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9842\n",
      "Epoch 00079: val_accuracy did not improve from 0.92261\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0827 - accuracy: 0.9842 - val_loss: 0.3989 - val_accuracy: 0.9197\n",
      "Epoch 80/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9865\n",
      "Epoch 00080: val_accuracy improved from 0.92261 to 0.92585, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0761 - accuracy: 0.9864 - val_loss: 0.3798 - val_accuracy: 0.9258\n",
      "Epoch 81/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9863\n",
      "Epoch 00081: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0776 - accuracy: 0.9863 - val_loss: 0.3742 - val_accuracy: 0.9222\n",
      "Epoch 82/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9846\n",
      "Epoch 00082: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0797 - accuracy: 0.9844 - val_loss: 0.4086 - val_accuracy: 0.9204\n",
      "Epoch 83/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9863\n",
      "Epoch 00083: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0773 - accuracy: 0.9865 - val_loss: 0.4015 - val_accuracy: 0.9197\n",
      "Epoch 84/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9859\n",
      "Epoch 00084: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0779 - accuracy: 0.9859 - val_loss: 0.3834 - val_accuracy: 0.9237\n",
      "Epoch 85/150\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.0788 - accuracy: 0.9846\n",
      "Epoch 00085: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0788 - accuracy: 0.9846 - val_loss: 0.3980 - val_accuracy: 0.9222\n",
      "Epoch 86/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9864\n",
      "Epoch 00086: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9864 - val_loss: 0.4013 - val_accuracy: 0.9201\n",
      "Epoch 87/150\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9852\n",
      "Epoch 00087: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0779 - accuracy: 0.9852 - val_loss: 0.4028 - val_accuracy: 0.9208\n",
      "Epoch 88/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9853\n",
      "Epoch 00088: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0777 - accuracy: 0.9854 - val_loss: 0.4014 - val_accuracy: 0.9208\n",
      "Epoch 89/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9852\n",
      "Epoch 00089: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9853 - val_loss: 0.3977 - val_accuracy: 0.9233\n",
      "Epoch 90/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9864\n",
      "Epoch 00090: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0756 - accuracy: 0.9864 - val_loss: 0.3834 - val_accuracy: 0.9212\n",
      "Epoch 91/150\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9850\n",
      "Epoch 00091: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9849 - val_loss: 0.3826 - val_accuracy: 0.9215\n",
      "Epoch 92/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9848\n",
      "Epoch 00092: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0782 - accuracy: 0.9848 - val_loss: 0.4132 - val_accuracy: 0.9212\n",
      "Epoch 93/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9852\n",
      "Epoch 00093: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9851 - val_loss: 0.4130 - val_accuracy: 0.9176\n",
      "Epoch 94/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9878\n",
      "Epoch 00094: val_accuracy did not improve from 0.92585\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9878 - val_loss: 0.3988 - val_accuracy: 0.9226\n",
      "Epoch 95/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9864\n",
      "Epoch 00095: val_accuracy improved from 0.92585 to 0.92801, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0747 - accuracy: 0.9866 - val_loss: 0.3939 - val_accuracy: 0.9280\n",
      "Epoch 96/150\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9878\n",
      "Epoch 00096: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9878 - val_loss: 0.4159 - val_accuracy: 0.9197\n",
      "Epoch 97/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9880\n",
      "Epoch 00097: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0711 - accuracy: 0.9879 - val_loss: 0.3976 - val_accuracy: 0.9233\n",
      "Epoch 98/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9877\n",
      "Epoch 00098: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9878 - val_loss: 0.4077 - val_accuracy: 0.9208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9872\n",
      "Epoch 00099: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0712 - accuracy: 0.9872 - val_loss: 0.3966 - val_accuracy: 0.9215\n",
      "Epoch 100/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9874\n",
      "Epoch 00100: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0722 - accuracy: 0.9874 - val_loss: 0.3938 - val_accuracy: 0.9244\n",
      "Epoch 101/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9872\n",
      "Epoch 00101: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0699 - accuracy: 0.9873 - val_loss: 0.3949 - val_accuracy: 0.9237\n",
      "Epoch 102/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9882\n",
      "Epoch 00102: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0703 - accuracy: 0.9882 - val_loss: 0.3984 - val_accuracy: 0.9233\n",
      "Epoch 103/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9885\n",
      "Epoch 00103: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9884 - val_loss: 0.4079 - val_accuracy: 0.9204\n",
      "Epoch 104/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9871\n",
      "Epoch 00104: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0722 - accuracy: 0.9869 - val_loss: 0.3903 - val_accuracy: 0.9255\n",
      "Epoch 105/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9882\n",
      "Epoch 00105: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9884 - val_loss: 0.3866 - val_accuracy: 0.9226\n",
      "Epoch 106/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9862\n",
      "Epoch 00106: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0705 - accuracy: 0.9862 - val_loss: 0.4204 - val_accuracy: 0.9240\n",
      "Epoch 107/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9880\n",
      "Epoch 00107: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0680 - accuracy: 0.9881 - val_loss: 0.4149 - val_accuracy: 0.9183\n",
      "Epoch 108/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9867\n",
      "Epoch 00108: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0691 - accuracy: 0.9867 - val_loss: 0.4040 - val_accuracy: 0.9222\n",
      "Epoch 109/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9878\n",
      "Epoch 00109: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0688 - accuracy: 0.9878 - val_loss: 0.4018 - val_accuracy: 0.9251\n",
      "Epoch 110/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9900\n",
      "Epoch 00110: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0622 - accuracy: 0.9900 - val_loss: 0.3975 - val_accuracy: 0.9255\n",
      "Epoch 111/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9881\n",
      "Epoch 00111: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9879 - val_loss: 0.4106 - val_accuracy: 0.9212\n",
      "Epoch 112/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9876\n",
      "Epoch 00112: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0705 - accuracy: 0.9876 - val_loss: 0.4118 - val_accuracy: 0.9208\n",
      "Epoch 113/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9901\n",
      "Epoch 00113: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0616 - accuracy: 0.9901 - val_loss: 0.4072 - val_accuracy: 0.9212\n",
      "Epoch 114/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9879\n",
      "Epoch 00114: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0671 - accuracy: 0.9880 - val_loss: 0.4100 - val_accuracy: 0.9226\n",
      "Epoch 115/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9885\n",
      "Epoch 00115: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0653 - accuracy: 0.9885 - val_loss: 0.4085 - val_accuracy: 0.9190\n",
      "Epoch 116/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9891\n",
      "Epoch 00116: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0659 - accuracy: 0.9891 - val_loss: 0.4128 - val_accuracy: 0.9237\n",
      "Epoch 117/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0648 - accuracy: 0.9890\n",
      "Epoch 00117: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9889 - val_loss: 0.4002 - val_accuracy: 0.9215\n",
      "Epoch 118/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9893\n",
      "Epoch 00118: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9892 - val_loss: 0.4057 - val_accuracy: 0.9248\n",
      "Epoch 119/150\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.0601 - accuracy: 0.9908\n",
      "Epoch 00119: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0601 - accuracy: 0.9908 - val_loss: 0.4049 - val_accuracy: 0.9273\n",
      "Epoch 120/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9890\n",
      "Epoch 00120: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0645 - accuracy: 0.9890 - val_loss: 0.4340 - val_accuracy: 0.9215\n",
      "Epoch 121/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0648 - accuracy: 0.9876\n",
      "Epoch 00121: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9877 - val_loss: 0.3876 - val_accuracy: 0.9244\n",
      "Epoch 122/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9875\n",
      "Epoch 00122: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9875 - val_loss: 0.4273 - val_accuracy: 0.9172\n",
      "Epoch 123/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.9886\n",
      "Epoch 00123: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0652 - accuracy: 0.9885 - val_loss: 0.4201 - val_accuracy: 0.9219\n",
      "Epoch 124/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9877\n",
      "Epoch 00124: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0669 - accuracy: 0.9876 - val_loss: 0.3927 - val_accuracy: 0.9237\n",
      "Epoch 125/150\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9893\n",
      "Epoch 00125: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0646 - accuracy: 0.9893 - val_loss: 0.4079 - val_accuracy: 0.9230\n",
      "Epoch 126/150\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9888\n",
      "Epoch 00126: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0645 - accuracy: 0.9888 - val_loss: 0.4064 - val_accuracy: 0.9226\n",
      "Epoch 127/150\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9879\n",
      "Epoch 00127: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9879 - val_loss: 0.4128 - val_accuracy: 0.9230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9898\n",
      "Epoch 00128: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0634 - accuracy: 0.9898 - val_loss: 0.3984 - val_accuracy: 0.9222\n",
      "Epoch 129/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9894\n",
      "Epoch 00129: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0598 - accuracy: 0.9895 - val_loss: 0.4147 - val_accuracy: 0.9219\n",
      "Epoch 130/150\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9881\n",
      "Epoch 00130: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0641 - accuracy: 0.9882 - val_loss: 0.4096 - val_accuracy: 0.9255\n",
      "Epoch 131/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9898\n",
      "Epoch 00131: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9899 - val_loss: 0.4041 - val_accuracy: 0.9197\n",
      "Epoch 132/150\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9888\n",
      "Epoch 00132: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0627 - accuracy: 0.9889 - val_loss: 0.4106 - val_accuracy: 0.9215\n",
      "Epoch 133/150\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9888\n",
      "Epoch 00133: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9888 - val_loss: 0.3990 - val_accuracy: 0.9248\n",
      "Epoch 134/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9887\n",
      "Epoch 00134: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0619 - accuracy: 0.9888 - val_loss: 0.4096 - val_accuracy: 0.9251\n",
      "Epoch 135/150\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9895\n",
      "Epoch 00135: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9894 - val_loss: 0.3852 - val_accuracy: 0.9248\n",
      "Epoch 136/150\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9896\n",
      "Epoch 00136: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9896 - val_loss: 0.4229 - val_accuracy: 0.9226\n",
      "Epoch 137/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0601 - accuracy: 0.9892\n",
      "Epoch 00137: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0600 - accuracy: 0.9893 - val_loss: 0.4139 - val_accuracy: 0.9237\n",
      "Epoch 138/150\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9900\n",
      "Epoch 00138: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0599 - accuracy: 0.9900 - val_loss: 0.4165 - val_accuracy: 0.9172\n",
      "Epoch 139/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0594 - accuracy: 0.9898\n",
      "Epoch 00139: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0595 - accuracy: 0.9899 - val_loss: 0.4046 - val_accuracy: 0.9230\n",
      "Epoch 140/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0632 - accuracy: 0.9897\n",
      "Epoch 00140: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0630 - accuracy: 0.9897 - val_loss: 0.4158 - val_accuracy: 0.9194\n",
      "Epoch 141/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9893\n",
      "Epoch 00141: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0586 - accuracy: 0.9892 - val_loss: 0.4045 - val_accuracy: 0.9215\n",
      "Epoch 142/150\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.0540 - accuracy: 0.9912\n",
      "Epoch 00142: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0541 - accuracy: 0.9912 - val_loss: 0.4144 - val_accuracy: 0.9266\n",
      "Epoch 143/150\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9891\n",
      "Epoch 00143: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0613 - accuracy: 0.9892 - val_loss: 0.4072 - val_accuracy: 0.9201\n",
      "Epoch 144/150\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.0611 - accuracy: 0.9888\n",
      "Epoch 00144: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0610 - accuracy: 0.9889 - val_loss: 0.4094 - val_accuracy: 0.9226\n",
      "Epoch 145/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0562 - accuracy: 0.9907\n",
      "Epoch 00145: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0562 - accuracy: 0.9907 - val_loss: 0.4090 - val_accuracy: 0.9226\n",
      "Epoch 146/150\n",
      "769/782 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9902\n",
      "Epoch 00146: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0576 - accuracy: 0.9903 - val_loss: 0.4000 - val_accuracy: 0.9244\n",
      "Epoch 147/150\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.0566 - accuracy: 0.9908\n",
      "Epoch 00147: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0564 - accuracy: 0.9908 - val_loss: 0.4086 - val_accuracy: 0.9222\n",
      "Epoch 148/150\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9900\n",
      "Epoch 00148: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0577 - accuracy: 0.9899 - val_loss: 0.3998 - val_accuracy: 0.9240\n",
      "Epoch 149/150\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9893\n",
      "Epoch 00149: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0571 - accuracy: 0.9892 - val_loss: 0.4202 - val_accuracy: 0.9208\n",
      "Epoch 150/150\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9894\n",
      "Epoch 00150: val_accuracy did not improve from 0.92801\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0582 - accuracy: 0.9894 - val_loss: 0.4002 - val_accuracy: 0.9183\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:06<00:00, 243.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.69      0.78      0.73       560\n",
      "        car_horn       0.91      0.91      0.91       210\n",
      "children_playing       0.73      0.67      0.70       700\n",
      "        dog_bark       0.77      0.74      0.76       700\n",
      "           siren       0.79      0.80      0.79       560\n",
      "\n",
      "        accuracy                           0.76      2730\n",
      "       macro avg       0.78      0.78      0.78      2730\n",
      "    weighted avg       0.76      0.76      0.76      2730\n",
      "\n",
      "Validation fold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape...:(27734, 375)\n",
      "X_val_norm shape.....:(2772, 375)\n",
      "\n",
      "Sum of elements: 0.980110730570213\n",
      "Number of elements summed: 235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_ANN_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 235)               55460     \n",
      "_________________________________________________________________\n",
      "Hiden_1 (Dense)              (None, 235)               55460     \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 235)               0         \n",
      "_________________________________________________________________\n",
      "Hiden_2 (Dense)              (None, 750)               177000    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 3755      \n",
      "=================================================================\n",
      "Total params: 291,675\n",
      "Trainable params: 291,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "ANN\n",
      "(24960, 235)\n",
      "Epoch 1/350\n",
      "  1/780 [..............................] - ETA: 0s - loss: 1.7406 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.8157 - accuracy: 0.7033\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82120, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.8080 - accuracy: 0.7060 - val_loss: 0.4974 - val_accuracy: 0.8212\n",
      "Epoch 2/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.4643 - accuracy: 0.8351\n",
      "Epoch 00002: val_accuracy improved from 0.82120 to 0.86662, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.4637 - accuracy: 0.8354 - val_loss: 0.3835 - val_accuracy: 0.8666\n",
      "Epoch 3/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8684\n",
      "Epoch 00003: val_accuracy improved from 0.86662 to 0.88536, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.3643 - accuracy: 0.8685 - val_loss: 0.3209 - val_accuracy: 0.8854\n",
      "Epoch 4/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8963\n",
      "Epoch 00004: val_accuracy improved from 0.88536 to 0.90195, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.2924 - accuracy: 0.8963 - val_loss: 0.2891 - val_accuracy: 0.9019\n",
      "Epoch 5/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.2450 - accuracy: 0.9121\n",
      "Epoch 00005: val_accuracy improved from 0.90195 to 0.91060, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.2449 - accuracy: 0.9121 - val_loss: 0.2630 - val_accuracy: 0.9106\n",
      "Epoch 6/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.2068 - accuracy: 0.9283\n",
      "Epoch 00006: val_accuracy improved from 0.91060 to 0.91781, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.2066 - accuracy: 0.9285 - val_loss: 0.2431 - val_accuracy: 0.9178\n",
      "Epoch 7/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9385\n",
      "Epoch 00007: val_accuracy improved from 0.91781 to 0.92430, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.9387 - val_loss: 0.2182 - val_accuracy: 0.9243\n",
      "Epoch 8/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9488\n",
      "Epoch 00008: val_accuracy improved from 0.92430 to 0.92610, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.1485 - accuracy: 0.9490 - val_loss: 0.2081 - val_accuracy: 0.9261\n",
      "Epoch 9/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9574\n",
      "Epoch 00009: val_accuracy improved from 0.92610 to 0.93223, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.1265 - accuracy: 0.9574 - val_loss: 0.2004 - val_accuracy: 0.9322\n",
      "Epoch 10/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9641\n",
      "Epoch 00010: val_accuracy improved from 0.93223 to 0.93439, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.1083 - accuracy: 0.9641 - val_loss: 0.1914 - val_accuracy: 0.9344\n",
      "Epoch 11/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 0.0906 - accuracy: 0.9711\n",
      "Epoch 00011: val_accuracy improved from 0.93439 to 0.93511, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0908 - accuracy: 0.9708 - val_loss: 0.1891 - val_accuracy: 0.9351\n",
      "Epoch 12/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9760\n",
      "Epoch 00012: val_accuracy improved from 0.93511 to 0.94304, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0754 - accuracy: 0.9758 - val_loss: 0.1826 - val_accuracy: 0.9430\n",
      "Epoch 13/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0672 - accuracy: 0.9786\n",
      "Epoch 00013: val_accuracy improved from 0.94304 to 0.94557, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0669 - accuracy: 0.9785 - val_loss: 0.1765 - val_accuracy: 0.9456\n",
      "Epoch 14/350\n",
      "742/780 [===========================>..] - ETA: 0s - loss: 0.0578 - accuracy: 0.9826\n",
      "Epoch 00014: val_accuracy did not improve from 0.94557\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0573 - accuracy: 0.9828 - val_loss: 0.1763 - val_accuracy: 0.9456\n",
      "Epoch 15/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 0.0484 - accuracy: 0.9848\n",
      "Epoch 00015: val_accuracy improved from 0.94557 to 0.94845, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0482 - accuracy: 0.9848 - val_loss: 0.1830 - val_accuracy: 0.9484\n",
      "Epoch 16/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0419 - accuracy: 0.9879\n",
      "Epoch 00016: val_accuracy improved from 0.94845 to 0.95061, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0418 - accuracy: 0.9879 - val_loss: 0.1802 - val_accuracy: 0.9506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/350\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0369 - accuracy: 0.9890\n",
      "Epoch 00017: val_accuracy did not improve from 0.95061\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0368 - accuracy: 0.9890 - val_loss: 0.1827 - val_accuracy: 0.9466\n",
      "Epoch 18/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9904\n",
      "Epoch 00018: val_accuracy improved from 0.95061 to 0.95169, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0316 - accuracy: 0.9905 - val_loss: 0.1748 - val_accuracy: 0.9517\n",
      "Epoch 19/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9926\n",
      "Epoch 00019: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0264 - accuracy: 0.9925 - val_loss: 0.1908 - val_accuracy: 0.9470\n",
      "Epoch 20/350\n",
      "742/780 [===========================>..] - ETA: 0s - loss: 0.0242 - accuracy: 0.9933\n",
      "Epoch 00020: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0243 - accuracy: 0.9933 - val_loss: 0.1868 - val_accuracy: 0.9510\n",
      "Epoch 21/350\n",
      "740/780 [===========================>..] - ETA: 0s - loss: 0.0205 - accuracy: 0.9946\n",
      "Epoch 00021: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0202 - accuracy: 0.9947 - val_loss: 0.1861 - val_accuracy: 0.9495\n",
      "Epoch 22/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9958\n",
      "Epoch 00022: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0179 - accuracy: 0.9958 - val_loss: 0.1948 - val_accuracy: 0.9488\n",
      "Epoch 23/350\n",
      "744/780 [===========================>..] - ETA: 0s - loss: 0.0162 - accuracy: 0.9957\n",
      "Epoch 00023: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0159 - accuracy: 0.9958 - val_loss: 0.1963 - val_accuracy: 0.9513\n",
      "Epoch 24/350\n",
      "747/780 [===========================>..] - ETA: 0s - loss: 0.0165 - accuracy: 0.9959\n",
      "Epoch 00024: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0164 - accuracy: 0.9959 - val_loss: 0.2006 - val_accuracy: 0.9503\n",
      "Epoch 25/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0141 - accuracy: 0.9959\n",
      "Epoch 00025: val_accuracy did not improve from 0.95169\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0142 - accuracy: 0.9958 - val_loss: 0.2049 - val_accuracy: 0.9517\n",
      "Epoch 26/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9968\n",
      "Epoch 00026: val_accuracy improved from 0.95169 to 0.95278, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0130 - accuracy: 0.9968 - val_loss: 0.2026 - val_accuracy: 0.9528\n",
      "Epoch 27/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9977\n",
      "Epoch 00027: val_accuracy did not improve from 0.95278\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.2070 - val_accuracy: 0.9528\n",
      "Epoch 28/350\n",
      "745/780 [===========================>..] - ETA: 0s - loss: 0.0112 - accuracy: 0.9971\n",
      "Epoch 00028: val_accuracy did not improve from 0.95278\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.2125 - val_accuracy: 0.9481\n",
      "Epoch 29/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 0.0099 - accuracy: 0.9980\n",
      "Epoch 00029: val_accuracy did not improve from 0.95278\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0097 - accuracy: 0.9981 - val_loss: 0.2139 - val_accuracy: 0.9524\n",
      "Epoch 30/350\n",
      "751/780 [===========================>..] - ETA: 0s - loss: 0.0080 - accuracy: 0.9980\n",
      "Epoch 00030: val_accuracy did not improve from 0.95278\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0080 - accuracy: 0.9981 - val_loss: 0.2184 - val_accuracy: 0.9499\n",
      "Epoch 31/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9986\n",
      "Epoch 00031: val_accuracy improved from 0.95278 to 0.95494, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0078 - accuracy: 0.9986 - val_loss: 0.2210 - val_accuracy: 0.9549\n",
      "Epoch 32/350\n",
      "743/780 [===========================>..] - ETA: 0s - loss: 0.0079 - accuracy: 0.9983\n",
      "Epoch 00032: val_accuracy did not improve from 0.95494\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0080 - accuracy: 0.9982 - val_loss: 0.2130 - val_accuracy: 0.9503\n",
      "Epoch 33/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9984\n",
      "Epoch 00033: val_accuracy did not improve from 0.95494\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0081 - accuracy: 0.9984 - val_loss: 0.2121 - val_accuracy: 0.9542\n",
      "Epoch 34/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9987\n",
      "Epoch 00034: val_accuracy did not improve from 0.95494\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.2210 - val_accuracy: 0.9521\n",
      "Epoch 35/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9987\n",
      "Epoch 00035: val_accuracy did not improve from 0.95494\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.2156 - val_accuracy: 0.9549\n",
      "Epoch 36/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9985\n",
      "Epoch 00036: val_accuracy improved from 0.95494 to 0.95710, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.2269 - val_accuracy: 0.9571\n",
      "Epoch 37/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9986\n",
      "Epoch 00037: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0061 - accuracy: 0.9986 - val_loss: 0.2205 - val_accuracy: 0.9546\n",
      "Epoch 38/350\n",
      "740/780 [===========================>..] - ETA: 0s - loss: 0.0055 - accuracy: 0.9986\n",
      "Epoch 00038: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0055 - accuracy: 0.9987 - val_loss: 0.2186 - val_accuracy: 0.9560\n",
      "Epoch 39/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9991\n",
      "Epoch 00039: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.2243 - val_accuracy: 0.9564\n",
      "Epoch 40/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9991\n",
      "Epoch 00040: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.2230 - val_accuracy: 0.9564\n",
      "Epoch 41/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 00041: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.2304 - val_accuracy: 0.9531\n",
      "Epoch 42/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9992\n",
      "Epoch 00042: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.2265 - val_accuracy: 0.9531\n",
      "Epoch 43/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9992\n",
      "Epoch 00043: val_accuracy did not improve from 0.95710\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.2240 - val_accuracy: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00044: val_accuracy improved from 0.95710 to 0.95818, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2284 - val_accuracy: 0.9582\n",
      "Epoch 45/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 0.0040 - accuracy: 0.9992\n",
      "Epoch 00045: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.2261 - val_accuracy: 0.9567\n",
      "Epoch 46/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9996\n",
      "Epoch 00046: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.2292 - val_accuracy: 0.9560\n",
      "Epoch 47/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 00047: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2279 - val_accuracy: 0.9542\n",
      "Epoch 48/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 00048: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.2360 - val_accuracy: 0.9571\n",
      "Epoch 49/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 00049: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.2359 - val_accuracy: 0.9557\n",
      "Epoch 50/350\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9997\n",
      "Epoch 00050: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.2354 - val_accuracy: 0.9560\n",
      "Epoch 51/350\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00051: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2382 - val_accuracy: 0.9549\n",
      "Epoch 52/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 00052: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.2380 - val_accuracy: 0.9575\n",
      "Epoch 53/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9996\n",
      "Epoch 00053: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2432 - val_accuracy: 0.9528\n",
      "Epoch 54/350\n",
      "755/780 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 00054: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.2407 - val_accuracy: 0.9575\n",
      "Epoch 55/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00055: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2411 - val_accuracy: 0.9553\n",
      "Epoch 56/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9992\n",
      "Epoch 00056: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.2393 - val_accuracy: 0.9553\n",
      "Epoch 57/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 00057: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.2485 - val_accuracy: 0.9535\n",
      "Epoch 58/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 00058: val_accuracy did not improve from 0.95818\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2447 - val_accuracy: 0.9542\n",
      "Epoch 59/350\n",
      "750/780 [===========================>..] - ETA: 0s - loss: 0.0019 - accuracy: 0.9999\n",
      "Epoch 00059: val_accuracy improved from 0.95818 to 0.95854, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.2395 - val_accuracy: 0.9585\n",
      "Epoch 60/350\n",
      "741/780 [===========================>..] - ETA: 0s - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 00060: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2440 - val_accuracy: 0.9567\n",
      "Epoch 61/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 00061: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2465 - val_accuracy: 0.9567\n",
      "Epoch 62/350\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 00062: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2517 - val_accuracy: 0.9571\n",
      "Epoch 63/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00063: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2522 - val_accuracy: 0.9546\n",
      "Epoch 64/350\n",
      "744/780 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 00064: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2530 - val_accuracy: 0.9531\n",
      "Epoch 65/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 00065: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.2518 - val_accuracy: 0.9539\n",
      "Epoch 66/350\n",
      "758/780 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00066: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2496 - val_accuracy: 0.9557\n",
      "Epoch 67/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 00067: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2496 - val_accuracy: 0.9560\n",
      "Epoch 68/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 00068: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2522 - val_accuracy: 0.9542\n",
      "Epoch 69/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 00069: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.2601 - val_accuracy: 0.9546\n",
      "Epoch 70/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 00070: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.2611 - val_accuracy: 0.9546\n",
      "Epoch 71/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 00071: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.2564 - val_accuracy: 0.9560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/350\n",
      "737/780 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00072: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2576 - val_accuracy: 0.9571\n",
      "Epoch 73/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 00073: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2617 - val_accuracy: 0.9546\n",
      "Epoch 74/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 00074: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.2616 - val_accuracy: 0.9571\n",
      "Epoch 75/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 00075: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2683 - val_accuracy: 0.9557\n",
      "Epoch 76/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 00076: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.2672 - val_accuracy: 0.9542\n",
      "Epoch 77/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00077: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2638 - val_accuracy: 0.9564\n",
      "Epoch 78/350\n",
      "743/780 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 00078: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2636 - val_accuracy: 0.9557\n",
      "Epoch 79/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 00079: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2641 - val_accuracy: 0.9546\n",
      "Epoch 80/350\n",
      "736/780 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 00080: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.2633 - val_accuracy: 0.9571\n",
      "Epoch 81/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00081: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2637 - val_accuracy: 0.9582\n",
      "Epoch 82/350\n",
      "744/780 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00082: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2641 - val_accuracy: 0.9560\n",
      "Epoch 83/350\n",
      "753/780 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 00083: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2599 - val_accuracy: 0.9571\n",
      "Epoch 84/350\n",
      "742/780 [===========================>..] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 00084: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2672 - val_accuracy: 0.9571\n",
      "Epoch 85/350\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00085: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.2713 - val_accuracy: 0.9557\n",
      "Epoch 86/350\n",
      "738/780 [===========================>..] - ETA: 0s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 00086: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2711 - val_accuracy: 0.9567\n",
      "Epoch 87/350\n",
      "759/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00087: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2714 - val_accuracy: 0.9575\n",
      "Epoch 88/350\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00088: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2697 - val_accuracy: 0.9578\n",
      "Epoch 89/350\n",
      "756/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00089: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2701 - val_accuracy: 0.9571\n",
      "Epoch 90/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 9.7086e-04 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.8090e-04 - accuracy: 1.0000 - val_loss: 0.2746 - val_accuracy: 0.9531\n",
      "Epoch 91/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 8.2040e-04 - accuracy: 0.9999\n",
      "Epoch 00091: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.1128e-04 - accuracy: 0.9999 - val_loss: 0.2723 - val_accuracy: 0.9571\n",
      "Epoch 92/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 00092: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.2733 - val_accuracy: 0.9560\n",
      "Epoch 93/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 00093: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.2735 - val_accuracy: 0.9560\n",
      "Epoch 94/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00094: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.2744 - val_accuracy: 0.9564\n",
      "Epoch 95/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 7.5294e-04 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.4811e-04 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 0.9560\n",
      "Epoch 96/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 9.9652e-04 - accuracy: 0.9998\n",
      "Epoch 00096: val_accuracy did not improve from 0.95854\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.9652e-04 - accuracy: 0.9998 - val_loss: 0.2741 - val_accuracy: 0.9560\n",
      "Epoch 97/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 00097: val_accuracy improved from 0.95854 to 0.95926, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.9643e-04 - accuracy: 0.9998 - val_loss: 0.2675 - val_accuracy: 0.9593\n",
      "Epoch 98/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 6.2664e-04 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.2481e-04 - accuracy: 1.0000 - val_loss: 0.2702 - val_accuracy: 0.9585\n",
      "Epoch 99/350\n",
      "738/780 [===========================>..] - ETA: 0s - loss: 9.1450e-04 - accuracy: 0.9998\n",
      "Epoch 00099: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.3431e-04 - accuracy: 0.9998 - val_loss: 0.2690 - val_accuracy: 0.9571\n",
      "Epoch 100/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 8.4710e-04 - accuracy: 0.9999\n",
      "Epoch 00100: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.4408e-04 - accuracy: 0.9999 - val_loss: 0.2695 - val_accuracy: 0.9582\n",
      "Epoch 101/350\n",
      "749/780 [===========================>..] - ETA: 0s - loss: 8.8281e-04 - accuracy: 0.9997\n",
      "Epoch 00101: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.7738e-04 - accuracy: 0.9997 - val_loss: 0.2680 - val_accuracy: 0.9578\n",
      "Epoch 102/350\n",
      "737/780 [===========================>..] - ETA: 0s - loss: 7.1866e-04 - accuracy: 0.9999\n",
      "Epoch 00102: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.2609e-04 - accuracy: 0.9999 - val_loss: 0.2659 - val_accuracy: 0.9582\n",
      "Epoch 103/350\n",
      "738/780 [===========================>..] - ETA: 0s - loss: 8.8963e-04 - accuracy: 0.9999\n",
      "Epoch 00103: val_accuracy did not improve from 0.95926\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.9809e-04 - accuracy: 0.9999 - val_loss: 0.2692 - val_accuracy: 0.9585\n",
      "Epoch 104/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 8.1042e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy improved from 0.95926 to 0.95963, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.1042e-04 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9596\n",
      "Epoch 105/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 7.6933e-04 - accuracy: 0.9999\n",
      "Epoch 00105: val_accuracy did not improve from 0.95963\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.1357e-04 - accuracy: 0.9998 - val_loss: 0.2706 - val_accuracy: 0.9596\n",
      "Epoch 106/350\n",
      "748/780 [===========================>..] - ETA: 0s - loss: 6.8645e-04 - accuracy: 0.9999\n",
      "Epoch 00106: val_accuracy did not improve from 0.95963\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.0037e-04 - accuracy: 0.9998 - val_loss: 0.2755 - val_accuracy: 0.9582\n",
      "Epoch 107/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 6.9545e-04 - accuracy: 0.9999\n",
      "Epoch 00107: val_accuracy improved from 0.95963 to 0.96071, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_ANN_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.9941e-04 - accuracy: 0.9999 - val_loss: 0.2700 - val_accuracy: 0.9607\n",
      "Epoch 108/350\n",
      "754/780 [============================>.] - ETA: 0s - loss: 8.0139e-04 - accuracy: 0.9999\n",
      "Epoch 00108: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.9019e-04 - accuracy: 0.9999 - val_loss: 0.2686 - val_accuracy: 0.9593\n",
      "Epoch 109/350\n",
      "770/780 [============================>.] - ETA: 0s - loss: 8.7919e-04 - accuracy: 0.9998\n",
      "Epoch 00109: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.7049e-04 - accuracy: 0.9998 - val_loss: 0.2666 - val_accuracy: 0.9593\n",
      "Epoch 110/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 9.1229e-04 - accuracy: 0.9998\n",
      "Epoch 00110: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.0902e-04 - accuracy: 0.9998 - val_loss: 0.2695 - val_accuracy: 0.9589\n",
      "Epoch 111/350\n",
      "737/780 [===========================>..] - ETA: 0s - loss: 6.8044e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.8279e-04 - accuracy: 1.0000 - val_loss: 0.2720 - val_accuracy: 0.9560\n",
      "Epoch 112/350\n",
      "745/780 [===========================>..] - ETA: 0s - loss: 7.4403e-04 - accuracy: 0.9999\n",
      "Epoch 00112: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.2691e-04 - accuracy: 0.9999 - val_loss: 0.2715 - val_accuracy: 0.9578\n",
      "Epoch 113/350\n",
      "761/780 [============================>.] - ETA: 0s - loss: 7.7192e-04 - accuracy: 0.9999\n",
      "Epoch 00113: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.6203e-04 - accuracy: 0.9999 - val_loss: 0.2714 - val_accuracy: 0.9560\n",
      "Epoch 114/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 6.6453e-04 - accuracy: 0.9999\n",
      "Epoch 00114: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.7686e-04 - accuracy: 0.9999 - val_loss: 0.2804 - val_accuracy: 0.9560\n",
      "Epoch 115/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 7.8668e-04 - accuracy: 0.9999\n",
      "Epoch 00115: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.8570e-04 - accuracy: 0.9999 - val_loss: 0.2733 - val_accuracy: 0.9585\n",
      "Epoch 116/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 8.1687e-04 - accuracy: 0.9998\n",
      "Epoch 00116: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 8.1441e-04 - accuracy: 0.9998 - val_loss: 0.2756 - val_accuracy: 0.9585\n",
      "Epoch 117/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 7.5736e-04 - accuracy: 0.9998\n",
      "Epoch 00117: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.5672e-04 - accuracy: 0.9998 - val_loss: 0.2738 - val_accuracy: 0.9582\n",
      "Epoch 118/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 7.4303e-04 - accuracy: 0.9999\n",
      "Epoch 00118: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.3474e-04 - accuracy: 0.9999 - val_loss: 0.2705 - val_accuracy: 0.9567\n",
      "Epoch 119/350\n",
      "738/780 [===========================>..] - ETA: 0s - loss: 9.3083e-04 - accuracy: 0.9999\n",
      "Epoch 00119: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 9.0055e-04 - accuracy: 0.9999 - val_loss: 0.2699 - val_accuracy: 0.9582\n",
      "Epoch 120/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 5.9976e-04 - accuracy: 0.9999\n",
      "Epoch 00120: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.8556e-04 - accuracy: 0.9999 - val_loss: 0.2678 - val_accuracy: 0.9600\n",
      "Epoch 121/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 6.6961e-04 - accuracy: 0.9999\n",
      "Epoch 00121: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.6577e-04 - accuracy: 0.9999 - val_loss: 0.2712 - val_accuracy: 0.9593\n",
      "Epoch 122/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 4.6168e-04 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.5767e-04 - accuracy: 1.0000 - val_loss: 0.2707 - val_accuracy: 0.9585\n",
      "Epoch 123/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 3.8904e-04 - accuracy: 1.0000\n",
      "Epoch 00123: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 3.8667e-04 - accuracy: 1.0000 - val_loss: 0.2711 - val_accuracy: 0.9603\n",
      "Epoch 124/350\n",
      "766/780 [============================>.] - ETA: 0s - loss: 6.7056e-04 - accuracy: 0.9998\n",
      "Epoch 00124: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.6430e-04 - accuracy: 0.9998 - val_loss: 0.2761 - val_accuracy: 0.9582\n",
      "Epoch 125/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 4.8554e-04 - accuracy: 0.9999\n",
      "Epoch 00125: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.8554e-04 - accuracy: 0.9999 - val_loss: 0.2745 - val_accuracy: 0.9585\n",
      "Epoch 126/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 4.9617e-04 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.9558e-04 - accuracy: 1.0000 - val_loss: 0.2755 - val_accuracy: 0.9571\n",
      "Epoch 127/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/780 [============================>.] - ETA: 0s - loss: 5.3943e-04 - accuracy: 0.9999\n",
      "Epoch 00127: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.3879e-04 - accuracy: 0.9999 - val_loss: 0.2754 - val_accuracy: 0.9571\n",
      "Epoch 128/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 6.2164e-04 - accuracy: 0.9999\n",
      "Epoch 00128: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.1073e-04 - accuracy: 0.9999 - val_loss: 0.2760 - val_accuracy: 0.9582\n",
      "Epoch 129/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 5.6246e-04 - accuracy: 0.9999\n",
      "Epoch 00129: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.7001e-04 - accuracy: 0.9999 - val_loss: 0.2788 - val_accuracy: 0.9578\n",
      "Epoch 130/350\n",
      "769/780 [============================>.] - ETA: 0s - loss: 4.1367e-04 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.0864e-04 - accuracy: 1.0000 - val_loss: 0.2779 - val_accuracy: 0.9578\n",
      "Epoch 131/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 4.7257e-04 - accuracy: 0.9999\n",
      "Epoch 00131: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.7257e-04 - accuracy: 0.9999 - val_loss: 0.2766 - val_accuracy: 0.9585\n",
      "Epoch 132/350\n",
      "740/780 [===========================>..] - ETA: 0s - loss: 7.0389e-04 - accuracy: 0.9999\n",
      "Epoch 00132: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.8302e-04 - accuracy: 0.9999 - val_loss: 0.2702 - val_accuracy: 0.9596\n",
      "Epoch 133/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 6.2908e-04 - accuracy: 0.9999\n",
      "Epoch 00133: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.2908e-04 - accuracy: 0.9999 - val_loss: 0.2723 - val_accuracy: 0.9585\n",
      "Epoch 134/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 7.1124e-04 - accuracy: 0.9998\n",
      "Epoch 00134: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.0847e-04 - accuracy: 0.9998 - val_loss: 0.2748 - val_accuracy: 0.9582\n",
      "Epoch 135/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 5.7990e-04 - accuracy: 1.0000\n",
      "Epoch 00135: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.7822e-04 - accuracy: 1.0000 - val_loss: 0.2760 - val_accuracy: 0.9585\n",
      "Epoch 136/350\n",
      "776/780 [============================>.] - ETA: 0s - loss: 4.9082e-04 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.8893e-04 - accuracy: 1.0000 - val_loss: 0.2750 - val_accuracy: 0.9593\n",
      "Epoch 137/350\n",
      "773/780 [============================>.] - ETA: 0s - loss: 4.7750e-04 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.7529e-04 - accuracy: 1.0000 - val_loss: 0.2783 - val_accuracy: 0.9567\n",
      "Epoch 138/350\n",
      "739/780 [===========================>..] - ETA: 0s - loss: 7.9248e-04 - accuracy: 0.9999\n",
      "Epoch 00138: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 7.9306e-04 - accuracy: 0.9999 - val_loss: 0.2830 - val_accuracy: 0.9582\n",
      "Epoch 139/350\n",
      "779/780 [============================>.] - ETA: 0s - loss: 6.9190e-04 - accuracy: 0.9999\n",
      "Epoch 00139: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.9101e-04 - accuracy: 0.9999 - val_loss: 0.2831 - val_accuracy: 0.9578\n",
      "Epoch 140/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 6.2824e-04 - accuracy: 0.9999\n",
      "Epoch 00140: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.2610e-04 - accuracy: 0.9999 - val_loss: 0.2810 - val_accuracy: 0.9582\n",
      "Epoch 141/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 5.3081e-04 - accuracy: 0.9999\n",
      "Epoch 00141: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.3735e-04 - accuracy: 0.9999 - val_loss: 0.2800 - val_accuracy: 0.9582\n",
      "Epoch 142/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 3.7069e-04 - accuracy: 1.0000\n",
      "Epoch 00142: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 3.7262e-04 - accuracy: 1.0000 - val_loss: 0.2803 - val_accuracy: 0.9582\n",
      "Epoch 143/350\n",
      "764/780 [============================>.] - ETA: 0s - loss: 6.5873e-04 - accuracy: 0.9999\n",
      "Epoch 00143: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.5523e-04 - accuracy: 0.9999 - val_loss: 0.2802 - val_accuracy: 0.9578\n",
      "Epoch 144/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 4.9294e-04 - accuracy: 0.9999\n",
      "Epoch 00144: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.9341e-04 - accuracy: 0.9999 - val_loss: 0.2870 - val_accuracy: 0.9557\n",
      "Epoch 145/350\n",
      "761/780 [============================>.] - ETA: 0s - loss: 6.0297e-04 - accuracy: 0.9999\n",
      "Epoch 00145: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.0392e-04 - accuracy: 0.9999 - val_loss: 0.2844 - val_accuracy: 0.9571\n",
      "Epoch 146/350\n",
      "757/780 [============================>.] - ETA: 0s - loss: 4.6447e-04 - accuracy: 1.0000\n",
      "Epoch 00146: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.6763e-04 - accuracy: 1.0000 - val_loss: 0.2857 - val_accuracy: 0.9575\n",
      "Epoch 147/350\n",
      "775/780 [============================>.] - ETA: 0s - loss: 7.4619e-04 - accuracy: 0.9998\n",
      "Epoch 00147: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 2ms/step - loss: 7.8075e-04 - accuracy: 0.9998 - val_loss: 0.2866 - val_accuracy: 0.9557\n",
      "Epoch 148/350\n",
      "763/780 [============================>.] - ETA: 0s - loss: 4.4744e-04 - accuracy: 1.0000\n",
      "Epoch 00148: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.1457e-04 - accuracy: 1.0000 - val_loss: 0.2853 - val_accuracy: 0.9564\n",
      "Epoch 149/350\n",
      "752/780 [===========================>..] - ETA: 0s - loss: 5.7549e-04 - accuracy: 0.9998\n",
      "Epoch 00149: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.6505e-04 - accuracy: 0.9998 - val_loss: 0.2835 - val_accuracy: 0.9578\n",
      "Epoch 150/350\n",
      "772/780 [============================>.] - ETA: 0s - loss: 3.9451e-04 - accuracy: 1.0000\n",
      "Epoch 00150: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 3.9107e-04 - accuracy: 1.0000 - val_loss: 0.2838 - val_accuracy: 0.9585\n",
      "Epoch 151/350\n",
      "774/780 [============================>.] - ETA: 0s - loss: 5.1882e-04 - accuracy: 0.9999\n",
      "Epoch 00151: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 5.1515e-04 - accuracy: 0.9999 - val_loss: 0.2904 - val_accuracy: 0.9578\n",
      "Epoch 152/350\n",
      "780/780 [==============================] - ETA: 0s - loss: 3.7115e-04 - accuracy: 1.0000\n",
      "Epoch 00152: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 3.7115e-04 - accuracy: 1.0000 - val_loss: 0.2903 - val_accuracy: 0.9575\n",
      "Epoch 153/350\n",
      "777/780 [============================>.] - ETA: 0s - loss: 4.3738e-04 - accuracy: 0.9999\n",
      "Epoch 00153: val_accuracy did not improve from 0.96071\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 4.3617e-04 - accuracy: 0.9999 - val_loss: 0.2898 - val_accuracy: 0.9607\n",
      "Epoch 154/350\n",
      "768/780 [============================>.] - ETA: 0s - loss: 7.0683e-04 - accuracy: 0.9998\n",
      "Epoch 00154: val_accuracy did not improve from 0.96071\n",
      "Restoring model weights from the end of the best epoch.\n",
      "780/780 [==============================] - 1s 1ms/step - loss: 6.9783e-04 - accuracy: 0.9998 - val_loss: 0.2916 - val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00154: early stopping\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:41<02:41, 161.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.85      0.85      0.85       574\n",
      "        car_horn       0.82      0.91      0.86       224\n",
      "children_playing       0.82      0.77      0.80       700\n",
      "        dog_bark       0.79      0.74      0.76       700\n",
      "           siren       0.87      0.97      0.92       574\n",
      "\n",
      "        accuracy                           0.83      2772\n",
      "       macro avg       0.83      0.85      0.84      2772\n",
      "    weighted avg       0.83      0.83      0.83      2772\n",
      "\n",
      "Model: \"Model_CNN_1D_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1D_1 (Conv1D)            (None, 229, 28)           224       \n",
      "_________________________________________________________________\n",
      "Conv1D_2 (Conv1D)            (None, 229, 34)           4794      \n",
      "_________________________________________________________________\n",
      "Conv1D_3 (Conv1D)            (None, 229, 56)           5768      \n",
      "_________________________________________________________________\n",
      "MaxPool1D_3 (MaxPooling1D)   (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 114, 56)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6384)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 50)                319250    \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 330,291\n",
      "Trainable params: 330,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CNN_1D\n",
      "(24960, 235, 1)\n",
      "Epoch 1/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.7479 - accuracy: 0.7617\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84643, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 3s 3ms/step - loss: 0.7438 - accuracy: 0.7634 - val_loss: 0.5203 - val_accuracy: 0.8464\n",
      "Epoch 2/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.8524\n",
      "Epoch 00002: val_accuracy improved from 0.84643 to 0.86373, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.4890 - accuracy: 0.8524 - val_loss: 0.4505 - val_accuracy: 0.8637\n",
      "Epoch 3/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.4245 - accuracy: 0.8744\n",
      "Epoch 00003: val_accuracy improved from 0.86373 to 0.88104, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.4242 - accuracy: 0.8743 - val_loss: 0.4096 - val_accuracy: 0.8810\n",
      "Epoch 4/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8898\n",
      "Epoch 00004: val_accuracy improved from 0.88104 to 0.89402, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3751 - accuracy: 0.8897 - val_loss: 0.3780 - val_accuracy: 0.8940\n",
      "Epoch 5/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.9048\n",
      "Epoch 00005: val_accuracy improved from 0.89402 to 0.89690, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3377 - accuracy: 0.9046 - val_loss: 0.3662 - val_accuracy: 0.8969\n",
      "Epoch 6/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.9124\n",
      "Epoch 00006: val_accuracy improved from 0.89690 to 0.90375, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.3129 - accuracy: 0.9124 - val_loss: 0.3476 - val_accuracy: 0.9037\n",
      "Epoch 7/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9196\n",
      "Epoch 00007: val_accuracy did not improve from 0.90375\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2918 - accuracy: 0.9192 - val_loss: 0.3445 - val_accuracy: 0.9034\n",
      "Epoch 8/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.2727 - accuracy: 0.9251\n",
      "Epoch 00008: val_accuracy did not improve from 0.90375\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2730 - accuracy: 0.9252 - val_loss: 0.3371 - val_accuracy: 0.9027\n",
      "Epoch 9/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.9297\n",
      "Epoch 00009: val_accuracy improved from 0.90375 to 0.91132, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2621 - accuracy: 0.9297 - val_loss: 0.3250 - val_accuracy: 0.9113\n",
      "Epoch 10/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.2466 - accuracy: 0.9345\n",
      "Epoch 00010: val_accuracy did not improve from 0.91132\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2464 - accuracy: 0.9345 - val_loss: 0.3280 - val_accuracy: 0.9084\n",
      "Epoch 11/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.9402\n",
      "Epoch 00011: val_accuracy did not improve from 0.91132\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2344 - accuracy: 0.9402 - val_loss: 0.3298 - val_accuracy: 0.9041\n",
      "Epoch 12/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.2249 - accuracy: 0.9417\n",
      "Epoch 00012: val_accuracy did not improve from 0.91132\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2247 - accuracy: 0.9417 - val_loss: 0.3564 - val_accuracy: 0.9016\n",
      "Epoch 13/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.2155 - accuracy: 0.9444\n",
      "Epoch 00013: val_accuracy improved from 0.91132 to 0.91456, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2159 - accuracy: 0.9444 - val_loss: 0.3232 - val_accuracy: 0.9146\n",
      "Epoch 14/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9477\n",
      "Epoch 00014: val_accuracy did not improve from 0.91456\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2104 - accuracy: 0.9476 - val_loss: 0.3373 - val_accuracy: 0.9081\n",
      "Epoch 15/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9503\n",
      "Epoch 00015: val_accuracy did not improve from 0.91456\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.2009 - accuracy: 0.9503 - val_loss: 0.3113 - val_accuracy: 0.9066\n",
      "Epoch 16/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9510\n",
      "Epoch 00016: val_accuracy did not improve from 0.91456\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1987 - accuracy: 0.9507 - val_loss: 0.3230 - val_accuracy: 0.9102\n",
      "Epoch 17/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1896 - accuracy: 0.9514\n",
      "Epoch 00017: val_accuracy improved from 0.91456 to 0.91673, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1899 - accuracy: 0.9512 - val_loss: 0.3125 - val_accuracy: 0.9167\n",
      "Epoch 18/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.1841 - accuracy: 0.9543\n",
      "Epoch 00018: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1839 - accuracy: 0.9545 - val_loss: 0.3187 - val_accuracy: 0.9128\n",
      "Epoch 19/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9559\n",
      "Epoch 00019: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1786 - accuracy: 0.9559 - val_loss: 0.3239 - val_accuracy: 0.9092\n",
      "Epoch 20/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1739 - accuracy: 0.9589\n",
      "Epoch 00020: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1742 - accuracy: 0.9588 - val_loss: 0.3255 - val_accuracy: 0.9149\n",
      "Epoch 21/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.9584\n",
      "Epoch 00021: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1709 - accuracy: 0.9582 - val_loss: 0.3322 - val_accuracy: 0.9099\n",
      "Epoch 22/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1672 - accuracy: 0.9593\n",
      "Epoch 00022: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1666 - accuracy: 0.9595 - val_loss: 0.3284 - val_accuracy: 0.9124\n",
      "Epoch 23/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1626 - accuracy: 0.9621\n",
      "Epoch 00023: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1625 - accuracy: 0.9622 - val_loss: 0.3381 - val_accuracy: 0.9095\n",
      "Epoch 24/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9627\n",
      "Epoch 00024: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1597 - accuracy: 0.9626 - val_loss: 0.3141 - val_accuracy: 0.9156\n",
      "Epoch 25/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9625\n",
      "Epoch 00025: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1566 - accuracy: 0.9625 - val_loss: 0.3339 - val_accuracy: 0.9110\n",
      "Epoch 26/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.9641\n",
      "Epoch 00026: val_accuracy did not improve from 0.91673\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1530 - accuracy: 0.9641 - val_loss: 0.3307 - val_accuracy: 0.9167\n",
      "Epoch 27/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9659\n",
      "Epoch 00027: val_accuracy improved from 0.91673 to 0.91889, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1482 - accuracy: 0.9661 - val_loss: 0.3305 - val_accuracy: 0.9189\n",
      "Epoch 28/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1493 - accuracy: 0.9658\n",
      "Epoch 00028: val_accuracy did not improve from 0.91889\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1492 - accuracy: 0.9659 - val_loss: 0.3307 - val_accuracy: 0.9124\n",
      "Epoch 29/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.1484 - accuracy: 0.9650\n",
      "Epoch 00029: val_accuracy did not improve from 0.91889\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1491 - accuracy: 0.9649 - val_loss: 0.3383 - val_accuracy: 0.9142\n",
      "Epoch 30/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.9679\n",
      "Epoch 00030: val_accuracy improved from 0.91889 to 0.92033, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1398 - accuracy: 0.9682 - val_loss: 0.3324 - val_accuracy: 0.9203\n",
      "Epoch 31/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1366 - accuracy: 0.9680\n",
      "Epoch 00031: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1363 - accuracy: 0.9681 - val_loss: 0.3310 - val_accuracy: 0.9164\n",
      "Epoch 32/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.9685\n",
      "Epoch 00032: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1386 - accuracy: 0.9686 - val_loss: 0.3479 - val_accuracy: 0.9135\n",
      "Epoch 33/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.9703\n",
      "Epoch 00033: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1336 - accuracy: 0.9703 - val_loss: 0.3433 - val_accuracy: 0.9203\n",
      "Epoch 34/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9712\n",
      "Epoch 00034: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1311 - accuracy: 0.9713 - val_loss: 0.3382 - val_accuracy: 0.9117\n",
      "Epoch 35/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9705\n",
      "Epoch 00035: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1306 - accuracy: 0.9705 - val_loss: 0.3325 - val_accuracy: 0.9156\n",
      "Epoch 36/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9710\n",
      "Epoch 00036: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1291 - accuracy: 0.9712 - val_loss: 0.3317 - val_accuracy: 0.9178\n",
      "Epoch 37/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9713\n",
      "Epoch 00037: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1263 - accuracy: 0.9711 - val_loss: 0.3289 - val_accuracy: 0.9193\n",
      "Epoch 38/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9736\n",
      "Epoch 00038: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1245 - accuracy: 0.9736 - val_loss: 0.3413 - val_accuracy: 0.9138\n",
      "Epoch 39/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9721\n",
      "Epoch 00039: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1241 - accuracy: 0.9720 - val_loss: 0.3373 - val_accuracy: 0.9110\n",
      "Epoch 40/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9760\n",
      "Epoch 00040: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1167 - accuracy: 0.9761 - val_loss: 0.3326 - val_accuracy: 0.9167\n",
      "Epoch 41/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9756\n",
      "Epoch 00041: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1173 - accuracy: 0.9756 - val_loss: 0.3501 - val_accuracy: 0.9149\n",
      "Epoch 42/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9748\n",
      "Epoch 00042: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1206 - accuracy: 0.9746 - val_loss: 0.3352 - val_accuracy: 0.9160\n",
      "Epoch 43/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9746\n",
      "Epoch 00043: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1179 - accuracy: 0.9746 - val_loss: 0.3340 - val_accuracy: 0.9135\n",
      "Epoch 44/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9734\n",
      "Epoch 00044: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1230 - accuracy: 0.9735 - val_loss: 0.3333 - val_accuracy: 0.9117\n",
      "Epoch 45/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1142 - accuracy: 0.9754\n",
      "Epoch 00045: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1144 - accuracy: 0.9752 - val_loss: 0.3350 - val_accuracy: 0.9174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9756\n",
      "Epoch 00046: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1141 - accuracy: 0.9755 - val_loss: 0.3279 - val_accuracy: 0.9167\n",
      "Epoch 47/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9774\n",
      "Epoch 00047: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1117 - accuracy: 0.9775 - val_loss: 0.3415 - val_accuracy: 0.9146\n",
      "Epoch 48/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9774\n",
      "Epoch 00048: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1096 - accuracy: 0.9773 - val_loss: 0.3411 - val_accuracy: 0.9135\n",
      "Epoch 49/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.1078 - accuracy: 0.9769\n",
      "Epoch 00049: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1080 - accuracy: 0.9769 - val_loss: 0.3386 - val_accuracy: 0.9153\n",
      "Epoch 50/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9773\n",
      "Epoch 00050: val_accuracy did not improve from 0.92033\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1107 - accuracy: 0.9771 - val_loss: 0.3388 - val_accuracy: 0.9142\n",
      "Epoch 51/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.1083 - accuracy: 0.9777\n",
      "Epoch 00051: val_accuracy improved from 0.92033 to 0.92394, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1083 - accuracy: 0.9776 - val_loss: 0.3187 - val_accuracy: 0.9239\n",
      "Epoch 52/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9775\n",
      "Epoch 00052: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1067 - accuracy: 0.9774 - val_loss: 0.3260 - val_accuracy: 0.9203\n",
      "Epoch 53/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9768\n",
      "Epoch 00053: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9767 - val_loss: 0.3314 - val_accuracy: 0.9182\n",
      "Epoch 54/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.1028 - accuracy: 0.9797\n",
      "Epoch 00054: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1027 - accuracy: 0.9798 - val_loss: 0.3380 - val_accuracy: 0.9189\n",
      "Epoch 55/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9786\n",
      "Epoch 00055: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1042 - accuracy: 0.9786 - val_loss: 0.3351 - val_accuracy: 0.9239\n",
      "Epoch 56/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.1045 - accuracy: 0.9773\n",
      "Epoch 00056: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1048 - accuracy: 0.9774 - val_loss: 0.3502 - val_accuracy: 0.9182\n",
      "Epoch 57/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9795\n",
      "Epoch 00057: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1030 - accuracy: 0.9795 - val_loss: 0.3471 - val_accuracy: 0.9156\n",
      "Epoch 58/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9802\n",
      "Epoch 00058: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9803 - val_loss: 0.3360 - val_accuracy: 0.9203\n",
      "Epoch 59/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9802\n",
      "Epoch 00059: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0979 - accuracy: 0.9802 - val_loss: 0.3309 - val_accuracy: 0.9221\n",
      "Epoch 60/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9803\n",
      "Epoch 00060: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0996 - accuracy: 0.9803 - val_loss: 0.3431 - val_accuracy: 0.9189\n",
      "Epoch 61/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.1002 - accuracy: 0.9786\n",
      "Epoch 00061: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.1002 - accuracy: 0.9786 - val_loss: 0.3531 - val_accuracy: 0.9171\n",
      "Epoch 62/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9797\n",
      "Epoch 00062: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0999 - accuracy: 0.9797 - val_loss: 0.3432 - val_accuracy: 0.9189\n",
      "Epoch 63/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0992 - accuracy: 0.9785\n",
      "Epoch 00063: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0987 - accuracy: 0.9787 - val_loss: 0.3456 - val_accuracy: 0.9167\n",
      "Epoch 64/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9809\n",
      "Epoch 00064: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0958 - accuracy: 0.9808 - val_loss: 0.3622 - val_accuracy: 0.9171\n",
      "Epoch 65/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9810\n",
      "Epoch 00065: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0938 - accuracy: 0.9810 - val_loss: 0.3464 - val_accuracy: 0.9207\n",
      "Epoch 66/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9813\n",
      "Epoch 00066: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0955 - accuracy: 0.9810 - val_loss: 0.3394 - val_accuracy: 0.9167\n",
      "Epoch 67/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9813\n",
      "Epoch 00067: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0943 - accuracy: 0.9812 - val_loss: 0.3432 - val_accuracy: 0.9189\n",
      "Epoch 68/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0936 - accuracy: 0.9819\n",
      "Epoch 00068: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0935 - accuracy: 0.9819 - val_loss: 0.3531 - val_accuracy: 0.9225\n",
      "Epoch 69/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9817\n",
      "Epoch 00069: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0938 - accuracy: 0.9816 - val_loss: 0.3528 - val_accuracy: 0.9211\n",
      "Epoch 70/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9822\n",
      "Epoch 00070: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0918 - accuracy: 0.9822 - val_loss: 0.3639 - val_accuracy: 0.9185\n",
      "Epoch 71/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9810\n",
      "Epoch 00071: val_accuracy did not improve from 0.92394\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0962 - accuracy: 0.9809 - val_loss: 0.3670 - val_accuracy: 0.9153\n",
      "Epoch 72/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9814\n",
      "Epoch 00072: val_accuracy improved from 0.92394 to 0.92574, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0915 - accuracy: 0.9812 - val_loss: 0.3361 - val_accuracy: 0.9257\n",
      "Epoch 73/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9803\n",
      "Epoch 00073: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0941 - accuracy: 0.9803 - val_loss: 0.3422 - val_accuracy: 0.9221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9825\n",
      "Epoch 00074: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0874 - accuracy: 0.9826 - val_loss: 0.3417 - val_accuracy: 0.9200\n",
      "Epoch 75/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0872 - accuracy: 0.9830\n",
      "Epoch 00075: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0869 - accuracy: 0.9832 - val_loss: 0.3430 - val_accuracy: 0.9214\n",
      "Epoch 76/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9846\n",
      "Epoch 00076: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0840 - accuracy: 0.9845 - val_loss: 0.3440 - val_accuracy: 0.9203\n",
      "Epoch 77/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9833\n",
      "Epoch 00077: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0869 - accuracy: 0.9832 - val_loss: 0.3359 - val_accuracy: 0.9247\n",
      "Epoch 78/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0885 - accuracy: 0.9828\n",
      "Epoch 00078: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0885 - accuracy: 0.9828 - val_loss: 0.3391 - val_accuracy: 0.9200\n",
      "Epoch 79/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9837\n",
      "Epoch 00079: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0853 - accuracy: 0.9836 - val_loss: 0.3372 - val_accuracy: 0.9211\n",
      "Epoch 80/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9836\n",
      "Epoch 00080: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0840 - accuracy: 0.9836 - val_loss: 0.3440 - val_accuracy: 0.9211\n",
      "Epoch 81/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9846\n",
      "Epoch 00081: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0833 - accuracy: 0.9845 - val_loss: 0.3513 - val_accuracy: 0.9193\n",
      "Epoch 82/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9840\n",
      "Epoch 00082: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0846 - accuracy: 0.9841 - val_loss: 0.3526 - val_accuracy: 0.9232\n",
      "Epoch 83/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9840\n",
      "Epoch 00083: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0863 - accuracy: 0.9841 - val_loss: 0.3428 - val_accuracy: 0.9254\n",
      "Epoch 84/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0837 - accuracy: 0.9837\n",
      "Epoch 00084: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0837 - accuracy: 0.9837 - val_loss: 0.3389 - val_accuracy: 0.9257\n",
      "Epoch 85/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0835 - accuracy: 0.9849\n",
      "Epoch 00085: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0833 - accuracy: 0.9850 - val_loss: 0.3463 - val_accuracy: 0.9232\n",
      "Epoch 86/150\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9839\n",
      "Epoch 00086: val_accuracy did not improve from 0.92574\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0834 - accuracy: 0.9837 - val_loss: 0.3394 - val_accuracy: 0.9232\n",
      "Epoch 87/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9869\n",
      "Epoch 00087: val_accuracy improved from 0.92574 to 0.92826, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0770 - accuracy: 0.9869 - val_loss: 0.3397 - val_accuracy: 0.9283\n",
      "Epoch 88/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9835\n",
      "Epoch 00088: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0813 - accuracy: 0.9835 - val_loss: 0.3455 - val_accuracy: 0.9236\n",
      "Epoch 89/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9836\n",
      "Epoch 00089: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0830 - accuracy: 0.9835 - val_loss: 0.3399 - val_accuracy: 0.9218\n",
      "Epoch 90/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9840\n",
      "Epoch 00090: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0821 - accuracy: 0.9839 - val_loss: 0.3365 - val_accuracy: 0.9229\n",
      "Epoch 91/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0825 - accuracy: 0.9836\n",
      "Epoch 00091: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9836 - val_loss: 0.3495 - val_accuracy: 0.9229\n",
      "Epoch 92/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9848\n",
      "Epoch 00092: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0784 - accuracy: 0.9849 - val_loss: 0.3383 - val_accuracy: 0.9247\n",
      "Epoch 93/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9845\n",
      "Epoch 00093: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0789 - accuracy: 0.9845 - val_loss: 0.3287 - val_accuracy: 0.9265\n",
      "Epoch 94/150\n",
      "775/780 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9836\n",
      "Epoch 00094: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9837 - val_loss: 0.3364 - val_accuracy: 0.9272\n",
      "Epoch 95/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9853\n",
      "Epoch 00095: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0779 - accuracy: 0.9853 - val_loss: 0.3397 - val_accuracy: 0.9221\n",
      "Epoch 96/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9865\n",
      "Epoch 00096: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0739 - accuracy: 0.9867 - val_loss: 0.3454 - val_accuracy: 0.9200\n",
      "Epoch 97/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9843\n",
      "Epoch 00097: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9842 - val_loss: 0.3452 - val_accuracy: 0.9239\n",
      "Epoch 98/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9855\n",
      "Epoch 00098: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0773 - accuracy: 0.9855 - val_loss: 0.3358 - val_accuracy: 0.9221\n",
      "Epoch 99/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9849\n",
      "Epoch 00099: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9851 - val_loss: 0.3351 - val_accuracy: 0.9243\n",
      "Epoch 100/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9857\n",
      "Epoch 00100: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9857 - val_loss: 0.3414 - val_accuracy: 0.9196\n",
      "Epoch 101/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9859\n",
      "Epoch 00101: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9861 - val_loss: 0.3355 - val_accuracy: 0.9257\n",
      "Epoch 102/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9868\n",
      "Epoch 00102: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0737 - accuracy: 0.9867 - val_loss: 0.3312 - val_accuracy: 0.9239\n",
      "Epoch 103/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9860\n",
      "Epoch 00103: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9859 - val_loss: 0.3374 - val_accuracy: 0.9268\n",
      "Epoch 104/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9849\n",
      "Epoch 00104: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0756 - accuracy: 0.9848 - val_loss: 0.3408 - val_accuracy: 0.9200\n",
      "Epoch 105/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9851\n",
      "Epoch 00105: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9851 - val_loss: 0.3493 - val_accuracy: 0.9247\n",
      "Epoch 106/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9869\n",
      "Epoch 00106: val_accuracy did not improve from 0.92826\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0727 - accuracy: 0.9870 - val_loss: 0.3365 - val_accuracy: 0.9232\n",
      "Epoch 107/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9867\n",
      "Epoch 00107: val_accuracy improved from 0.92826 to 0.92898, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9865 - val_loss: 0.3439 - val_accuracy: 0.9290\n",
      "Epoch 108/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9858\n",
      "Epoch 00108: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0743 - accuracy: 0.9857 - val_loss: 0.3363 - val_accuracy: 0.9250\n",
      "Epoch 109/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9855\n",
      "Epoch 00109: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0750 - accuracy: 0.9855 - val_loss: 0.3366 - val_accuracy: 0.9247\n",
      "Epoch 110/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9868\n",
      "Epoch 00110: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9868 - val_loss: 0.3565 - val_accuracy: 0.9196\n",
      "Epoch 111/150\n",
      "763/780 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9871\n",
      "Epoch 00111: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9868 - val_loss: 0.3566 - val_accuracy: 0.9247\n",
      "Epoch 112/150\n",
      "764/780 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9877\n",
      "Epoch 00112: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0701 - accuracy: 0.9876 - val_loss: 0.3534 - val_accuracy: 0.9193\n",
      "Epoch 113/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9865\n",
      "Epoch 00113: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9865 - val_loss: 0.3373 - val_accuracy: 0.9247\n",
      "Epoch 114/150\n",
      "776/780 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9858\n",
      "Epoch 00114: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0720 - accuracy: 0.9858 - val_loss: 0.3538 - val_accuracy: 0.9243\n",
      "Epoch 115/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9857\n",
      "Epoch 00115: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0743 - accuracy: 0.9857 - val_loss: 0.3462 - val_accuracy: 0.9250\n",
      "Epoch 116/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9858\n",
      "Epoch 00116: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0723 - accuracy: 0.9858 - val_loss: 0.3492 - val_accuracy: 0.9214\n",
      "Epoch 117/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9869\n",
      "Epoch 00117: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0708 - accuracy: 0.9869 - val_loss: 0.3460 - val_accuracy: 0.9247\n",
      "Epoch 118/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9861\n",
      "Epoch 00118: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0731 - accuracy: 0.9861 - val_loss: 0.3304 - val_accuracy: 0.9275\n",
      "Epoch 119/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0686 - accuracy: 0.9875\n",
      "Epoch 00119: val_accuracy did not improve from 0.92898\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0686 - accuracy: 0.9875 - val_loss: 0.3520 - val_accuracy: 0.9268\n",
      "Epoch 120/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9857\n",
      "Epoch 00120: val_accuracy improved from 0.92898 to 0.93079, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0716 - accuracy: 0.9857 - val_loss: 0.3343 - val_accuracy: 0.9308\n",
      "Epoch 121/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 0.9876\n",
      "Epoch 00121: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0694 - accuracy: 0.9876 - val_loss: 0.3536 - val_accuracy: 0.9211\n",
      "Epoch 122/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9879\n",
      "Epoch 00122: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0669 - accuracy: 0.9879 - val_loss: 0.3439 - val_accuracy: 0.9290\n",
      "Epoch 123/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9867\n",
      "Epoch 00123: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0706 - accuracy: 0.9867 - val_loss: 0.3648 - val_accuracy: 0.9236\n",
      "Epoch 124/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9885\n",
      "Epoch 00124: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0662 - accuracy: 0.9884 - val_loss: 0.3433 - val_accuracy: 0.9257\n",
      "Epoch 125/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9878\n",
      "Epoch 00125: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9876 - val_loss: 0.3518 - val_accuracy: 0.9275\n",
      "Epoch 126/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9879\n",
      "Epoch 00126: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9879 - val_loss: 0.3355 - val_accuracy: 0.9293\n",
      "Epoch 127/150\n",
      "761/780 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9887\n",
      "Epoch 00127: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9885 - val_loss: 0.3558 - val_accuracy: 0.9297\n",
      "Epoch 128/150\n",
      "760/780 [============================>.] - ETA: 0s - loss: 0.0696 - accuracy: 0.9871\n",
      "Epoch 00128: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0698 - accuracy: 0.9871 - val_loss: 0.3357 - val_accuracy: 0.9297\n",
      "Epoch 129/150\n",
      "762/780 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9872\n",
      "Epoch 00129: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9872 - val_loss: 0.3611 - val_accuracy: 0.9261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/150\n",
      "768/780 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9891\n",
      "Epoch 00130: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0643 - accuracy: 0.9890 - val_loss: 0.3548 - val_accuracy: 0.9257\n",
      "Epoch 131/150\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9877\n",
      "Epoch 00131: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9877 - val_loss: 0.3478 - val_accuracy: 0.9297\n",
      "Epoch 132/150\n",
      "767/780 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9876\n",
      "Epoch 00132: val_accuracy did not improve from 0.93079\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0660 - accuracy: 0.9877 - val_loss: 0.3436 - val_accuracy: 0.9265\n",
      "Epoch 133/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9879\n",
      "Epoch 00133: val_accuracy improved from 0.93079 to 0.93187, saving model to C:\\Andre_Florentino\\03_particular\\04_mestrado-FEI\\97_master\\US8K_AV_saved_models\\Model_CNN_1D_weights_0_best_std_windowed.hdf5\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9877 - val_loss: 0.3394 - val_accuracy: 0.9319\n",
      "Epoch 134/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.9883\n",
      "Epoch 00134: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0653 - accuracy: 0.9882 - val_loss: 0.3462 - val_accuracy: 0.9243\n",
      "Epoch 135/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.9871\n",
      "Epoch 00135: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0690 - accuracy: 0.9871 - val_loss: 0.3407 - val_accuracy: 0.9290\n",
      "Epoch 136/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9880\n",
      "Epoch 00136: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0665 - accuracy: 0.9880 - val_loss: 0.3567 - val_accuracy: 0.9283\n",
      "Epoch 137/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9880\n",
      "Epoch 00137: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0647 - accuracy: 0.9880 - val_loss: 0.3468 - val_accuracy: 0.9243\n",
      "Epoch 138/150\n",
      "769/780 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9888\n",
      "Epoch 00138: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0637 - accuracy: 0.9888 - val_loss: 0.3423 - val_accuracy: 0.9272\n",
      "Epoch 139/150\n",
      "779/780 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9890\n",
      "Epoch 00139: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9889 - val_loss: 0.3566 - val_accuracy: 0.9286\n",
      "Epoch 140/150\n",
      "778/780 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9885\n",
      "Epoch 00140: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0649 - accuracy: 0.9885 - val_loss: 0.3432 - val_accuracy: 0.9261\n",
      "Epoch 141/150\n",
      "770/780 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9872\n",
      "Epoch 00141: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0668 - accuracy: 0.9869 - val_loss: 0.3366 - val_accuracy: 0.9279\n",
      "Epoch 142/150\n",
      "765/780 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9868\n",
      "Epoch 00142: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0673 - accuracy: 0.9867 - val_loss: 0.3558 - val_accuracy: 0.9286\n",
      "Epoch 143/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9897\n",
      "Epoch 00143: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0597 - accuracy: 0.9897 - val_loss: 0.3498 - val_accuracy: 0.9239\n",
      "Epoch 144/150\n",
      "772/780 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9888\n",
      "Epoch 00144: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0629 - accuracy: 0.9887 - val_loss: 0.3367 - val_accuracy: 0.9290\n",
      "Epoch 145/150\n",
      "774/780 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9889\n",
      "Epoch 00145: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9889 - val_loss: 0.3468 - val_accuracy: 0.9283\n",
      "Epoch 146/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9884\n",
      "Epoch 00146: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0640 - accuracy: 0.9883 - val_loss: 0.3428 - val_accuracy: 0.9265\n",
      "Epoch 147/150\n",
      "777/780 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9891\n",
      "Epoch 00147: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0607 - accuracy: 0.9891 - val_loss: 0.3478 - val_accuracy: 0.9290\n",
      "Epoch 148/150\n",
      "773/780 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9893\n",
      "Epoch 00148: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0599 - accuracy: 0.9893 - val_loss: 0.3575 - val_accuracy: 0.9272\n",
      "Epoch 149/150\n",
      "766/780 [============================>.] - ETA: 0s - loss: 0.0603 - accuracy: 0.9892\n",
      "Epoch 00149: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0602 - accuracy: 0.9893 - val_loss: 0.3510 - val_accuracy: 0.9283\n",
      "Epoch 150/150\n",
      "771/780 [============================>.] - ETA: 0s - loss: 0.0612 - accuracy: 0.9890\n",
      "Epoch 00150: val_accuracy did not improve from 0.93187\n",
      "780/780 [==============================] - 2s 3ms/step - loss: 0.0611 - accuracy: 0.9890 - val_loss: 0.3570 - val_accuracy: 0.9254\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [07:59<00:00, 239.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      background       0.77      0.82      0.79       574\n",
      "        car_horn       0.83      0.89      0.86       224\n",
      "children_playing       0.78      0.75      0.76       700\n",
      "        dog_bark       0.76      0.67      0.71       700\n",
      "           siren       0.87      0.96      0.91       574\n",
      "\n",
      "        accuracy                           0.80      2772\n",
      "       macro avg       0.80      0.82      0.81      2772\n",
      "    weighted avg       0.80      0.80      0.80      2772\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Option for scalerOpt is either \"normalization\" or \"standardization\"\n",
    "\n",
    "metrics_set, models_set, batch_name = model_classifiers(classifiers, \n",
    "                                                        DB_from_pkl, \n",
    "                                                        scalerOpt = 'standardization',\n",
    "                                                        use_PCA = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a578bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>Accuracy(Val)</th>\n",
       "      <th>...</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Conf_M</th>\n",
       "      <th>Process_time</th>\n",
       "      <th>Class_report(Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>[[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>[[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]</td>\n",
       "      <td>265.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANN</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>[[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>[[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>[[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]</td>\n",
       "      <td>109.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>[[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]</td>\n",
       "      <td>703.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>[[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>[[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]</td>\n",
       "      <td>359.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>[[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>[[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]</td>\n",
       "      <td>812.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>[[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]</td>\n",
       "      <td>140.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>[[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ANN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>[[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>[[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>[[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>[[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]</td>\n",
       "      <td>437.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ANN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>[[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>[[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]</td>\n",
       "      <td>328.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>[[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>[[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]</td>\n",
       "      <td>546.875</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model Fold  Accuracy(Train)  Accuracy(Val)  ...  Recall(Val)                                                                                                               Conf_M  Process_time  \\\n",
       "0      ANN    1         1.000000       0.802990  ...     0.802990       [[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]        62.500   \n",
       "1   CNN_1D    1         0.999758       0.798007  ...     0.798007       [[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]       265.625   \n",
       "2      ANN   10         1.000000       0.763382  ...     0.763382    [[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]        62.500   \n",
       "3   CNN_1D   10         0.999275       0.747358  ...     0.747358     [[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]       343.750   \n",
       "4      ANN    2         1.000000       0.741592  ...     0.741592   [[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]       109.375   \n",
       "5   CNN_1D    2         0.999677       0.731269  ...     0.731269    [[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]       703.125   \n",
       "6      ANN    3         1.000000       0.823355  ...     0.823355         [[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]        78.125   \n",
       "7   CNN_1D    3         0.999713       0.808239  ...     0.808239        [[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]       359.375   \n",
       "8      ANN    4         1.000000       0.761993  ...     0.761993  [[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]        78.125   \n",
       "9   CNN_1D    4         0.999875       0.755102  ...     0.755102  [[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]       812.500   \n",
       "10     ANN    5         1.000000       0.774420  ...     0.774420     [[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]       140.625   \n",
       "11  CNN_1D    5         0.999878       0.748779  ...     0.748779    [[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]       343.750   \n",
       "12     ANN    6         1.000000       0.772500  ...     0.772500       [[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]        62.500   \n",
       "13  CNN_1D    6         0.999799       0.747500  ...     0.747500     [[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]        93.750   \n",
       "14     ANN    7         1.000000       0.738977  ...     0.738977     [[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]        62.500   \n",
       "15  CNN_1D    7         0.999880       0.714638  ...     0.714638   [[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]       437.500   \n",
       "16     ANN    8         1.000000       0.784982  ...     0.784982       [[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]        62.500   \n",
       "17  CNN_1D    8         0.999600       0.756410  ...     0.756410         [[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]       328.125   \n",
       "18     ANN    9         1.000000       0.831890  ...     0.831890         [[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]        93.750   \n",
       "19  CNN_1D    9         0.999679       0.798341  ...     0.798341       [[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]       546.875   \n",
       "\n",
       "                                                                                                          Class_report(Val)  \n",
       "0                     precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...  \n",
       "1                     precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...  \n",
       "2                     precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...  \n",
       "3                     precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...  \n",
       "4                     precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...  \n",
       "5                     precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...  \n",
       "6                     precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...  \n",
       "7                     precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...  \n",
       "8                     precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...  \n",
       "9                     precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...  \n",
       "10                    precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...  \n",
       "11                    precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...  \n",
       "12                    precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...  \n",
       "13                    precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...  \n",
       "14                    precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...  \n",
       "15                    precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...  \n",
       "16                    precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...  \n",
       "17                    precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...  \n",
       "18                    precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...  \n",
       "19                    precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...  \n",
       "\n",
       "[20 rows x 13 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e29d1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Conf_M</th>\n",
       "      <th>Process_time</th>\n",
       "      <th>Class_report(Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>ANN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>[[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>ANN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>[[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]</td>\n",
       "      <td>109.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>ANN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>[[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ANN</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>[[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>ANN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>[[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>ANN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>[[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]</td>\n",
       "      <td>140.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>ANN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>[[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>ANN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>[[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>ANN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>[[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>ANN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>[[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>[[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]</td>\n",
       "      <td>437.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>[[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]</td>\n",
       "      <td>703.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>[[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>[[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>[[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>[[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]</td>\n",
       "      <td>812.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>[[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]</td>\n",
       "      <td>328.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>[[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]</td>\n",
       "      <td>265.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>[[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]</td>\n",
       "      <td>546.875</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>[[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]</td>\n",
       "      <td>359.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   Model Fold  Accuracy(Train)  ...  Recall(Val)                                                                                                               Conf_M  Process_time  \\\n",
       "0      14     ANN    7         1.000000  ...     0.738977     [[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]        62.500   \n",
       "1       4     ANN    2         1.000000  ...     0.741592   [[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]       109.375   \n",
       "2       8     ANN    4         1.000000  ...     0.761993  [[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]        78.125   \n",
       "3       2     ANN   10         1.000000  ...     0.763382    [[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]        62.500   \n",
       "4      12     ANN    6         1.000000  ...     0.772500       [[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]        62.500   \n",
       "5      10     ANN    5         1.000000  ...     0.774420     [[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]       140.625   \n",
       "6      16     ANN    8         1.000000  ...     0.784982       [[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]        62.500   \n",
       "7       0     ANN    1         1.000000  ...     0.802990       [[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]        62.500   \n",
       "8       6     ANN    3         1.000000  ...     0.823355         [[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]        78.125   \n",
       "9      18     ANN    9         1.000000  ...     0.831890         [[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]        93.750   \n",
       "10     15  CNN_1D    7         0.999880  ...     0.714638   [[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]       437.500   \n",
       "11      5  CNN_1D    2         0.999677  ...     0.731269    [[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]       703.125   \n",
       "12      3  CNN_1D   10         0.999275  ...     0.747358     [[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]       343.750   \n",
       "13     13  CNN_1D    6         0.999799  ...     0.747500     [[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]        93.750   \n",
       "14     11  CNN_1D    5         0.999878  ...     0.748779    [[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]       343.750   \n",
       "15      9  CNN_1D    4         0.999875  ...     0.755102  [[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]       812.500   \n",
       "16     17  CNN_1D    8         0.999600  ...     0.756410         [[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]       328.125   \n",
       "17      1  CNN_1D    1         0.999758  ...     0.798007       [[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]       265.625   \n",
       "18     19  CNN_1D    9         0.999679  ...     0.798341       [[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]       546.875   \n",
       "19      7  CNN_1D    3         0.999713  ...     0.808239        [[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]       359.375   \n",
       "\n",
       "                                                                                                          Class_report(Val)  \n",
       "0                     precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...  \n",
       "1                     precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...  \n",
       "2                     precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...  \n",
       "3                     precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...  \n",
       "4                     precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...  \n",
       "5                     precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...  \n",
       "6                     precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...  \n",
       "7                     precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...  \n",
       "8                     precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...  \n",
       "9                     precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...  \n",
       "10                    precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...  \n",
       "11                    precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...  \n",
       "12                    precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...  \n",
       "13                    precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...  \n",
       "14                    precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...  \n",
       "15                    precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...  \n",
       "16                    precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...  \n",
       "17                    precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...  \n",
       "18                    precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...  \n",
       "19                    precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...  \n",
       "\n",
       "[20 rows x 14 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by Model and Accuracy test. Reset the index.\n",
    "\n",
    "metrics_set = metrics_set.sort_values(['Model', 'Accuracy(Val)'], ascending = [True, True]).reset_index()\n",
    "metrics_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c5af9afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3b3b9_row0_col1 {\n",
       "  background-color: #cee0f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row1_col1 {\n",
       "  background-color: #cadef0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row2_col1 {\n",
       "  background-color: #92c4de;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row3_col1 {\n",
       "  background-color: #8dc1dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row4_col1 {\n",
       "  background-color: #6dafd7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row5_col1 {\n",
       "  background-color: #68acd5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row6_col1 {\n",
       "  background-color: #4a98c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row7_col1 {\n",
       "  background-color: #2070b4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row8_col1 {\n",
       "  background-color: #084387;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row9_col1 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row10_col1 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row11_col1 {\n",
       "  background-color: #dbe9f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row12_col1, #T_3b3b9_row13_col1 {\n",
       "  background-color: #bdd7ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row14_col1 {\n",
       "  background-color: #b9d6ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row15_col1 {\n",
       "  background-color: #a8cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row16_col1 {\n",
       "  background-color: #a4cce3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3b3b9_row17_col1, #T_3b3b9_row18_col1 {\n",
       "  background-color: #2b7bba;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b3b9_row19_col1 {\n",
       "  background-color: #1764ab;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3b3b9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3b3b9_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_3b3b9_level0_col1\" class=\"col_heading level0 col1\" >Accuracy(Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3b3b9_row0_col0\" class=\"data row0 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row0_col1\" class=\"data row0 col1\" >0.738977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3b3b9_row1_col0\" class=\"data row1 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row1_col1\" class=\"data row1 col1\" >0.741592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3b3b9_row2_col0\" class=\"data row2 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row2_col1\" class=\"data row2 col1\" >0.761993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3b3b9_row3_col0\" class=\"data row3 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row3_col1\" class=\"data row3 col1\" >0.763382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3b3b9_row4_col0\" class=\"data row4 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row4_col1\" class=\"data row4 col1\" >0.772500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3b3b9_row5_col0\" class=\"data row5 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row5_col1\" class=\"data row5 col1\" >0.774420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3b3b9_row6_col0\" class=\"data row6 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row6_col1\" class=\"data row6 col1\" >0.784982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3b3b9_row7_col0\" class=\"data row7 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row7_col1\" class=\"data row7 col1\" >0.802990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_3b3b9_row8_col0\" class=\"data row8 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row8_col1\" class=\"data row8 col1\" >0.823355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_3b3b9_row9_col0\" class=\"data row9 col0\" >ANN</td>\n",
       "      <td id=\"T_3b3b9_row9_col1\" class=\"data row9 col1\" >0.831890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_3b3b9_row10_col0\" class=\"data row10 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row10_col1\" class=\"data row10 col1\" >0.714638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_3b3b9_row11_col0\" class=\"data row11 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row11_col1\" class=\"data row11 col1\" >0.731269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_3b3b9_row12_col0\" class=\"data row12 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row12_col1\" class=\"data row12 col1\" >0.747358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_3b3b9_row13_col0\" class=\"data row13 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row13_col1\" class=\"data row13 col1\" >0.747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_3b3b9_row14_col0\" class=\"data row14 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row14_col1\" class=\"data row14 col1\" >0.748779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_3b3b9_row15_col0\" class=\"data row15 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row15_col1\" class=\"data row15 col1\" >0.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_3b3b9_row16_col0\" class=\"data row16 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row16_col1\" class=\"data row16 col1\" >0.756410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_3b3b9_row17_col0\" class=\"data row17 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row17_col1\" class=\"data row17 col1\" >0.798007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_3b3b9_row18_col0\" class=\"data row18 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row18_col1\" class=\"data row18 col1\" >0.798341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b3b9_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_3b3b9_row19_col0\" class=\"data row19 col0\" >CNN_1D</td>\n",
       "      <td id=\"T_3b3b9_row19_col1\" class=\"data row19 col1\" >0.808239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26783abd250>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_set[['Model', 'Accuracy(Val)']].style.background_gradient(cmap = cmap_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "74b6fa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "ANN       0.831890\n",
       "CNN_1D    0.808239\n",
       "Name: Accuracy(Val), dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_accuracy = metrics_set.groupby('Model')['Accuracy(Val)'].max()\n",
    "highest_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a40226fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary of each classifier and its data explanation\n",
    "\n",
    "unique_models = []\n",
    "results       = {}\n",
    "\n",
    "for c in classifiers:\n",
    "    unique_models.append(c)\n",
    "\n",
    "for model in unique_models:\n",
    "    result = metrics_set[metrics_set['Model'] == model].describe().round(4)\n",
    "    results[model] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1b6a5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model...: ANN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>Accuracy(Val)</th>\n",
       "      <th>F1(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Precision(Val)</th>\n",
       "      <th>Recall(Train)</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Process_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7827</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>81.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.0553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>26.3523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>62.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7702</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7623</td>\n",
       "      <td>62.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7735</td>\n",
       "      <td>70.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>89.8438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8319</td>\n",
       "      <td>140.6250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Accuracy(Train)  Accuracy(Val)  F1(Train)  ...  Precision(Val)  Recall(Train)  Recall(Val)  Process_time\n",
       "count  10.0000             10.0        10.0000       10.0  ...         10.0000           10.0      10.0000       10.0000\n",
       "mean    9.0000              1.0         0.7796        1.0  ...          0.7827            1.0       0.7796       81.2500\n",
       "std     6.0553              0.0         0.0316        0.0  ...          0.0313            0.0       0.0316       26.3523\n",
       "min     0.0000              1.0         0.7390        1.0  ...          0.7372            1.0       0.7390       62.5000\n",
       "25%     4.5000              1.0         0.7623        1.0  ...          0.7702            1.0       0.7623       62.5000\n",
       "50%     9.0000              1.0         0.7735        1.0  ...          0.7811            1.0       0.7735       70.3125\n",
       "75%    13.5000              1.0         0.7985        1.0  ...          0.8008            1.0       0.7985       89.8438\n",
       "max    18.0000              1.0         0.8319        1.0  ...          0.8301            1.0       0.8319      140.6250\n",
       "\n",
       "[8 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model...: CNN_1D\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>Accuracy(Val)</th>\n",
       "      <th>F1(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Precision(Val)</th>\n",
       "      <th>Recall(Train)</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Process_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7606</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7668</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7606</td>\n",
       "      <td>423.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.0553</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>212.0054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.7146</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7161</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.7146</td>\n",
       "      <td>93.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.5000</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7526</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>332.0312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7519</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7651</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.7519</td>\n",
       "      <td>351.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.5000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.7876</td>\n",
       "      <td>519.5312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.0000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>812.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Accuracy(Train)  Accuracy(Val)  F1(Train)  ...  Precision(Val)  Recall(Train)  Recall(Val)  Process_time\n",
       "count  10.0000          10.0000        10.0000    10.0000  ...         10.0000        10.0000      10.0000       10.0000\n",
       "mean   10.0000           0.9997         0.7606     0.9997  ...          0.7668         0.9997       0.7606      423.4375\n",
       "std     6.0553           0.0002         0.0309     0.0002  ...          0.0312         0.0002       0.0309      212.0054\n",
       "min     1.0000           0.9993         0.7146     0.9993  ...          0.7161         0.9993       0.7146       93.7500\n",
       "25%     5.5000           0.9997         0.7474     0.9997  ...          0.7526         0.9997       0.7474      332.0312\n",
       "50%    10.0000           0.9997         0.7519     0.9997  ...          0.7651         0.9997       0.7519      351.5625\n",
       "75%    14.5000           0.9999         0.7876     0.9999  ...          0.7892         0.9999       0.7876      519.5312\n",
       "max    19.0000           0.9999         0.8082     0.9999  ...          0.8126         0.9999       0.8082      812.5000\n",
       "\n",
       "[8 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in results.keys():\n",
    "    print(f'Model...: {model}')\n",
    "    display(results[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "47b38499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Precision(Val)</th>\n",
       "      <th>Recall(Train)</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Process_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>ANN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>ANN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>109.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>ANN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771040</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>78.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ANN</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769877</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>ANN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>ANN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787695</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>140.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>ANN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>ANN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>ANN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>78.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>ANN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>93.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716088</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>437.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729723</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>703.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761811</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>343.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750839</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>93.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768301</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>343.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770036</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>812.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757799</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>328.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805613</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>265.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795552</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>546.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812591</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>359.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   Model Fold  Accuracy(Train)  ...  Precision(Val)  Recall(Train)  Recall(Val)  Process_time\n",
       "0      14     ANN    7         1.000000  ...        0.739424       1.000000     0.738977        62.500\n",
       "1       4     ANN    2         1.000000  ...        0.737229       1.000000     0.741592       109.375\n",
       "2       8     ANN    4         1.000000  ...        0.771040       1.000000     0.761993        78.125\n",
       "3       2     ANN   10         1.000000  ...        0.769877       1.000000     0.763382        62.500\n",
       "4      12     ANN    6         1.000000  ...        0.775436       1.000000     0.772500        62.500\n",
       "5      10     ANN    5         1.000000  ...        0.787695       1.000000     0.774420       140.625\n",
       "6      16     ANN    8         1.000000  ...        0.786730       1.000000     0.784982        62.500\n",
       "7       0     ANN    1         1.000000  ...        0.805109       1.000000     0.802990        62.500\n",
       "8       6     ANN    3         1.000000  ...        0.824088       1.000000     0.823355        78.125\n",
       "9      18     ANN    9         1.000000  ...        0.830138       1.000000     0.831890        93.750\n",
       "10     15  CNN_1D    7         0.999880  ...        0.716088       0.999880     0.714638       437.500\n",
       "11      5  CNN_1D    2         0.999677  ...        0.729723       0.999677     0.731269       703.125\n",
       "12      3  CNN_1D   10         0.999275  ...        0.761811       0.999275     0.747358       343.750\n",
       "13     13  CNN_1D    6         0.999799  ...        0.750839       0.999799     0.747500        93.750\n",
       "14     11  CNN_1D    5         0.999878  ...        0.768301       0.999878     0.748779       343.750\n",
       "15      9  CNN_1D    4         0.999875  ...        0.770036       0.999875     0.755102       812.500\n",
       "16     17  CNN_1D    8         0.999600  ...        0.757799       0.999600     0.756410       328.125\n",
       "17      1  CNN_1D    1         0.999758  ...        0.805613       0.999758     0.798007       265.625\n",
       "18     19  CNN_1D    9         0.999679  ...        0.795552       0.999679     0.798341       546.875\n",
       "19      7  CNN_1D    3         0.999713  ...        0.812591       0.999713     0.808239       359.375\n",
       "\n",
       "[20 rows x 12 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_set_no_cm = metrics_set.drop(['Conf_M', 'Class_report(Val)'], axis=1)\n",
    "metrics_set_no_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2ea27fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US8K_AV_metrics_set_NN_std_PCA_windowed.pkl\n",
      "US8K_AV_metrics_set_NN_std_PCA_windowed_no_cm.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_set_name       = nom_dataset + '_metrics_set_NN' + batch_name +  model_surname + '.pkl'\n",
    "metrics_set_name_no_cm = nom_dataset + '_metrics_set_NN' + batch_name +  model_surname + '_no_cm.csv'\n",
    "\n",
    "print(metrics_set_name)\n",
    "print(metrics_set_name_no_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3d17b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes de results to a PKL and CSV file\n",
    "\n",
    "with open(os.path.join(path_models, metrics_set_name), 'wb') as file:\n",
    "    pickle.dump(metrics_set, file)\n",
    "    \n",
    "metrics_set_no_cm.to_csv(os.path.join(path_models, metrics_set_name_no_cm), sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "edb90a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Accuracy(Train)</th>\n",
       "      <th>...</th>\n",
       "      <th>Recall(Val)</th>\n",
       "      <th>Conf_M</th>\n",
       "      <th>Process_time</th>\n",
       "      <th>Class_report(Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>ANN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>[[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>ANN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741592</td>\n",
       "      <td>[[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]</td>\n",
       "      <td>109.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>ANN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>[[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ANN</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763382</td>\n",
       "      <td>[[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>ANN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>[[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>ANN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>[[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]</td>\n",
       "      <td>140.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>ANN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784982</td>\n",
       "      <td>[[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>ANN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802990</td>\n",
       "      <td>[[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]</td>\n",
       "      <td>62.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>ANN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>[[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]</td>\n",
       "      <td>78.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>ANN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>[[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>[[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]</td>\n",
       "      <td>437.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>[[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]</td>\n",
       "      <td>703.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>[[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>[[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]</td>\n",
       "      <td>93.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748779</td>\n",
       "      <td>[[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]</td>\n",
       "      <td>343.750</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>[[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]</td>\n",
       "      <td>812.500</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>[[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]</td>\n",
       "      <td>328.125</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798007</td>\n",
       "      <td>[[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]</td>\n",
       "      <td>265.625</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>[[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]</td>\n",
       "      <td>546.875</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>CNN_1D</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>[[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]</td>\n",
       "      <td>359.375</td>\n",
       "      <td>precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index   Model Fold  Accuracy(Train)  ...  Recall(Val)                                                                                                               Conf_M  Process_time  \\\n",
       "0      14     ANN    7         1.000000  ...     0.738977     [[514, 14, 73, 43, 56], [21, 135, 23, 11, 6], [82, 5, 503, 78, 32], [15, 0, 49, 605, 31], [94, 11, 29, 67, 338]]        62.500   \n",
       "1       4     ANN    2         1.000000  ...     0.741592   [[413, 26, 68, 25, 140], [44, 166, 15, 45, 24], [36, 1, 621, 29, 13], [36, 18, 46, 596, 4], [60, 32, 54, 60, 431]]       109.375   \n",
       "2       8     ANN    4         1.000000  ...     0.761993  [[631, 30, 50, 45, 42], [119, 243, 14, 28, 9], [83, 12, 447, 134, 24], [41, 7, 41, 590, 21], [12, 5, 127, 54, 964]]        78.125   \n",
       "3       2     ANN   10         1.000000  ...     0.763382    [[631, 20, 33, 14, 23], [26, 184, 16, 3, 2], [22, 4, 557, 95, 22], [43, 13, 82, 540, 22], [33, 12, 126, 83, 327]]        62.500   \n",
       "4      12     ANN    6         1.000000  ...     0.772500       [[513, 36, 65, 38, 34], [4, 189, 0, 3, 0], [32, 12, 579, 53, 24], [52, 3, 87, 532, 26], [99, 16, 28, 25, 350]]        62.500   \n",
       "5      10     ANN    5         1.000000  ...     0.774420     [[586, 13, 35, 49, 10], [159, 458, 9, 40, 20], [68, 7, 520, 71, 34], [31, 3, 120, 537, 9], [13, 14, 3, 31, 436]]       140.625   \n",
       "6      16     ANN    8         1.000000  ...     0.784982       [[448, 12, 32, 42, 26], [7, 188, 14, 1, 0], [66, 25, 504, 68, 37], [45, 3, 54, 551, 47], [54, 4, 22, 28, 452]]        62.500   \n",
       "7       0     ANN    1         1.000000  ...     0.802990       [[606, 12, 27, 73, 38], [17, 231, 0, 2, 2], [44, 4, 546, 82, 24], [19, 28, 52, 588, 13], [46, 4, 88, 18, 446]]        62.500   \n",
       "8       6     ANN    3         1.000000  ...     0.823355         [[680, 40, 48, 28, 44], [2, 294, 0, 5, 0], [54, 3, 547, 69, 27], [24, 5, 86, 560, 25], [107, 1, 22, 6, 697]]        78.125   \n",
       "9      18     ANN    9         1.000000  ...     0.831890         [[486, 25, 29, 14, 20], [10, 204, 1, 7, 2], [12, 7, 542, 107, 32], [58, 13, 87, 515, 27], [5, 0, 4, 6, 559]]        93.750   \n",
       "10     15  CNN_1D    7         0.999880  ...     0.714638   [[493, 12, 106, 54, 35], [35, 123, 22, 14, 2], [79, 4, 492, 65, 60], [24, 1, 49, 599, 27], [117, 10, 43, 50, 319]]       437.500   \n",
       "11      5  CNN_1D    2         0.999677  ...     0.731269    [[445, 33, 82, 30, 82], [108, 146, 1, 19, 20], [31, 2, 597, 40, 30], [34, 14, 74, 571, 7], [65, 31, 55, 49, 437]]       703.125   \n",
       "12      3  CNN_1D   10         0.999275  ...     0.747358     [[648, 5, 41, 13, 14], [39, 175, 15, 1, 1], [30, 1, 574, 63, 32], [46, 3, 116, 519, 16], [37, 17, 173, 78, 276]]       343.750   \n",
       "13     13  CNN_1D    6         0.999799  ...     0.747500     [[502, 13, 76, 34, 61], [21, 172, 0, 1, 2], [36, 6, 581, 59, 18], [69, 4, 105, 502, 20], [114, 21, 19, 28, 336]]        93.750   \n",
       "14     11  CNN_1D    5         0.999878  ...     0.748779    [[588, 9, 62, 22, 12], [184, 460, 12, 3, 27], [88, 6, 495, 75, 36], [39, 2, 128, 519, 12], [21, 17, 25, 43, 391]]       343.750   \n",
       "15      9  CNN_1D    4         0.999875  ...     0.755102  [[600, 19, 110, 38, 31], [160, 205, 19, 20, 9], [87, 3, 451, 148, 11], [22, 6, 56, 590, 26], [13, 1, 68, 77, 1003]]       812.500   \n",
       "16     17  CNN_1D    8         0.999600  ...     0.756410         [[435, 8, 41, 48, 28], [6, 192, 12, 0, 0], [89, 3, 469, 83, 56], [45, 3, 94, 521, 37], [60, 6, 24, 22, 448]]       328.125   \n",
       "17      1  CNN_1D    1         0.999758  ...     0.798007       [[577, 24, 57, 70, 28], [13, 228, 1, 8, 2], [39, 1, 557, 89, 14], [11, 28, 62, 582, 17], [21, 1, 88, 34, 458]]       265.625   \n",
       "18     19  CNN_1D    9         0.999679  ...     0.798341       [[469, 25, 35, 19, 26], [12, 199, 1, 12, 0], [28, 4, 523, 110, 35], [90, 11, 109, 470, 20], [8, 1, 5, 8, 552]]       546.875   \n",
       "19      7  CNN_1D    3         0.999713  ...     0.808239        [[698, 41, 58, 20, 23], [5, 289, 0, 7, 0], [46, 1, 551, 81, 21], [45, 1, 92, 536, 26], [125, 1, 36, 18, 653]]       359.375   \n",
       "\n",
       "                                                                                                          Class_report(Val)  \n",
       "0                     precision    recall  f1-score   support\\n\\n      background       0.71      0.73      0.72       7...  \n",
       "1                     precision    recall  f1-score   support\\n\\n      background       0.70      0.61      0.66       6...  \n",
       "2                     precision    recall  f1-score   support\\n\\n      background       0.71      0.79      0.75       7...  \n",
       "3                     precision    recall  f1-score   support\\n\\n      background       0.84      0.88      0.86       7...  \n",
       "4                     precision    recall  f1-score   support\\n\\n      background       0.73      0.75      0.74       6...  \n",
       "5                     precision    recall  f1-score   support\\n\\n      background       0.68      0.85      0.76       6...  \n",
       "6                     precision    recall  f1-score   support\\n\\n      background       0.72      0.80      0.76       5...  \n",
       "7                     precision    recall  f1-score   support\\n\\n      background       0.83      0.80      0.81       7...  \n",
       "8                     precision    recall  f1-score   support\\n\\n      background       0.78      0.81      0.80       8...  \n",
       "9                     precision    recall  f1-score   support\\n\\n      background       0.85      0.85      0.85       5...  \n",
       "10                    precision    recall  f1-score   support\\n\\n      background       0.66      0.70      0.68       7...  \n",
       "11                    precision    recall  f1-score   support\\n\\n      background       0.65      0.66      0.66       6...  \n",
       "12                    precision    recall  f1-score   support\\n\\n      background       0.81      0.90      0.85       7...  \n",
       "13                    precision    recall  f1-score   support\\n\\n      background       0.68      0.73      0.70       6...  \n",
       "14                    precision    recall  f1-score   support\\n\\n      background       0.64      0.85      0.73       6...  \n",
       "15                    precision    recall  f1-score   support\\n\\n      background       0.68      0.75      0.71       7...  \n",
       "16                    precision    recall  f1-score   support\\n\\n      background       0.69      0.78      0.73       5...  \n",
       "17                    precision    recall  f1-score   support\\n\\n      background       0.87      0.76      0.81       7...  \n",
       "18                    precision    recall  f1-score   support\\n\\n      background       0.77      0.82      0.79       5...  \n",
       "19                    precision    recall  f1-score   support\\n\\n      background       0.76      0.83      0.79       8...  \n",
       "\n",
       "[20 rows x 14 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_set_from_pkl = pd.read_pickle(os.path.join(path_models, metrics_set_name))\n",
    "metrics_set_from_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "faea2f5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANN': {'Accuracy(Val)': 0.8318903318903319,\n",
       "  'Conf_M': array([[486,  25,  29,  14,  20],\n",
       "         [ 10, 204,   1,   7,   2],\n",
       "         [ 12,   7, 542, 107,  32],\n",
       "         [ 58,  13,  87, 515,  27],\n",
       "         [  5,   0,   4,   6, 559]], dtype=int64)},\n",
       " 'CNN_1D': {'Accuracy(Val)': 0.8082394783639597,\n",
       "  'Conf_M': array([[698,  41,  58,  20,  23],\n",
       "         [  5, 289,   0,   7,   0],\n",
       "         [ 46,   1, 551,  81,  21],\n",
       "         [ 45,   1,  92, 536,  26],\n",
       "         [125,   1,  36,  18, 653]], dtype=int64)}}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = metrics_set.groupby('Model')['Accuracy(Val)'].idxmax()\n",
    "conf_matrices = metrics_set.loc[idx, ['Model','Accuracy(Val)','Conf_M']]\n",
    "conf_matrices.set_index('Model', inplace=True)\n",
    "conf_matrices_dict = conf_matrices.to_dict('index')\n",
    "conf_matrices_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1603ff77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[486,  25,  29,  14,  20],\n",
       "       [ 10, 204,   1,   7,   2],\n",
       "       [ 12,   7, 542, 107,  32],\n",
       "       [ 58,  13,  87, 515,  27],\n",
       "       [  5,   0,   4,   6, 559]], dtype=int64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrices_dict['ANN']['Conf_M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4c32d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ANN\n",
      "0.8318903318903319\n",
      "[[486  25  29  14  20]\n",
      " [ 10 204   1   7   2]\n",
      " [ 12   7 542 107  32]\n",
      " [ 58  13  87 515  27]\n",
      " [  5   0   4   6 559]]\n",
      "2\n",
      "CNN_1D\n",
      "0.8082394783639597\n",
      "[[698  41  58  20  23]\n",
      " [  5 289   0   7   0]\n",
      " [ 46   1 551  81  21]\n",
      " [ 45   1  92 536  26]\n",
      " [125   1  36  18 653]]\n"
     ]
    }
   ],
   "source": [
    "for i, idx in zip(conf_matrices_dict.keys(), range(1, len(conf_matrices_dict) + 1)):\n",
    "    print(idx)\n",
    "    print(i)\n",
    "    print(conf_matrices_dict[i]['Accuracy(Val)'])\n",
    "    print(conf_matrices_dict[i]['Conf_M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e5e53550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix for the highest accuracy test classifiers\n",
    "\n",
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.suptitle(nom_dataset + model_surname + batch_name + ' - Confusion matrices of the best results for each classifier', fontsize = 16,  y=0.99)\n",
    "for i, idx in zip(conf_matrices_dict.keys(), range(1, len(conf_matrices_dict) + 1)):\n",
    "    title = 'Classifier '+ i + ' (Highest accuracy validation of the best models: ' + str(\"{:0.4f}\".format(conf_matrices_dict[i]['Accuracy(Val)'])) +')'\n",
    "    plt.subplot(1,2,idx)\n",
    "    plot_confusion_matrix(conf_matrices_dict[i]['Conf_M'],  \n",
    "                          nom_classes, \n",
    "                          title,\n",
    "                          cmap = None,                          \n",
    "                          normalize = False)\n",
    "\n",
    "plt.savefig(os.path.join(path_pic, picture_name))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "75f0df34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.suptitle(f'{nom_dataset} - Box plot each classifier (batch type: {model_surname + batch_name})', fontsize = 16,  y=0.97)\n",
    "box_plot = sns.boxplot(data=metrics_set, x=\"Model\", y=\"Accuracy(Val)\", showfliers = True)\n",
    "\n",
    "medians = list(metrics_set.groupby(['Model'])['Accuracy(Val)'].median())\n",
    "medians = [round(element, 2) for element in medians]\n",
    "\n",
    "vertical_offset = metrics_set['Accuracy(Val)'].median()*0.001  # offset from median for display\n",
    "\n",
    "for xtick in box_plot.get_xticks():\n",
    "    box_plot.text(xtick, medians[xtick] + vertical_offset, medians[xtick], \n",
    "            horizontalalignment='center',size='medium',color='w',weight='semibold')\n",
    "plt.savefig(os.path.join(path_pic, picture_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41968385",
   "metadata": {},
   "source": [
    "## Results ESC-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cece92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e275303",
   "metadata": {},
   "source": [
    "## Results BDLib2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69cc24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b29ab96b",
   "metadata": {},
   "source": [
    "## Results US8K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568845ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85e46c9c",
   "metadata": {},
   "source": [
    "## Results US8K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294ed2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb45a0d9",
   "metadata": {},
   "source": [
    "# End of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93509039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d9f7d3",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57de80",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abcdd9",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
