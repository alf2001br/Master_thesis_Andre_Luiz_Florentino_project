{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3faa6a4b",
   "metadata": {},
   "source": [
    "### Faculdade de Engenharia Industrial - FEI\n",
    "\n",
    "### Centro Universitário da Fundação Educacional Inaciana \"Padre Sabóia de Medeiros\" (FEI)\n",
    "\n",
    "\n",
    "*FEI's Stricto Sensu Graduate Program in Electrical Engineering*\n",
    "\n",
    "Concentration area: ARTIFICIAL INTELLIGENCE APPLIED TO AUTOMATION AND ROBOTICS\n",
    "\n",
    "Master's thesis student Andre Luiz Florentino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd16632",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02245d16",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffabcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "pd = tf.config.experimental.list_physical_devices()\n",
    "for i in pd:\n",
    "    print(i)\n",
    "print('------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "\n",
    "print(tf.test.is_built_with_cuda)\n",
    "# <function is_built_with_cuda at 0x000001AA24AFEC10>\n",
    "\n",
    "print(tf.test.gpu_device_name())\n",
    "# /device:GPU:0\n",
    "\n",
    "#gvd = tf.config.get_visible_devices()\n",
    "for j in tf.config.get_visible_devices():\n",
    "    print(j)\n",
    "# PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
    "# PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de63c0",
   "metadata": {},
   "source": [
    "# Chapter 5: Machine learning k-fold cross-validation\n",
    "\n",
    "ASSUMPTION: run for datasets ESC-10, BDLib2 and US8k:\n",
    "\n",
    "* *01_Feature_extraction_exploration.ipynb*\n",
    "* *02_PreProcessing_and_ML_modeling.ipynb* \n",
    "\n",
    "And run for the new dataset US8K_AV:\n",
    "* *03_New_dataset_US8K_AV.ipynb*\n",
    "\n",
    "After that, run for all datasets?\n",
    "* *04_ML_modeling.ipynb*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4504379",
   "metadata": {},
   "source": [
    "## Importe modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import mimetypes\n",
    "import itertools\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "import numpy   as np\n",
    "from IPython.display import display\n",
    "\n",
    "from matplotlib  import pyplot as plt\n",
    "\n",
    "from tqdm                      import tqdm\n",
    "\n",
    "from sklearn                   import metrics\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, chi2\n",
    "from sklearn.linear_model      import LogisticRegression\n",
    "from sklearn.naive_bayes       import GaussianNB\n",
    "from sklearn.svm               import SVC\n",
    "from sklearn.neighbors         import KNeighborsClassifier\n",
    "from sklearn.ensemble          import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.decomposition     import PCA\n",
    "from sklearn.metrics           import classification_report\n",
    "\n",
    "mimetypes.init()\n",
    "mimetypes.add_type('audio/ogg','.ogg')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 300)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "cmap_cm   = plt.cm.Blues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6556a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# For the picture names\n",
    "pic_first_name = '05_Machine_learning_cross_validation_'\n",
    "\n",
    "# For Librosa\n",
    "FRAME_SIZE  = 1024\n",
    "HOP_LENGTH  = 512\n",
    "SEED        = 1000\n",
    "SR          = 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53326eea",
   "metadata": {},
   "source": [
    "# Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64563cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset\n",
    "\n",
    "opc = 0\n",
    "while str(opc) not in '1234':\n",
    "    print()\n",
    "    print(\"1-) ESC-10\")\n",
    "    print(\"2-) BDLib2\")\n",
    "    print(\"3-) US8K\")\n",
    "    print(\"4-) US8K_AV\")\n",
    "\n",
    "\n",
    "    opc = input(\"\\nSelect the dataset: \")\n",
    "    if opc.isdigit():\n",
    "        opc = int(opc)\n",
    "    else:\n",
    "        opc = 0\n",
    "\n",
    "if opc == 1:\n",
    "\n",
    "    path        = os.path.join(current_path, \"_dataset\", \"ESC-10\")\n",
    "    path_pic    = os.path.join(current_path, \"ESC-10_results\")\n",
    "    path_models = os.path.join(current_path, \"ESC-10_saved_models\")\n",
    "    \n",
    "   \n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'ESC-10' \n",
    "    csv_file    = 'ESC-10.csv'\n",
    "    fold        = '1'\n",
    "\n",
    "    pkl_features           = 'ESC-10_features_original.pkl'\n",
    "    pkl_aug_features      = 'ESC-10_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'ESC-10_features_augmented.pkl'\n",
    "\n",
    "    \n",
    "if opc == 2:\n",
    "    \n",
    "    path        = os.path.join(current_path, \"_dataset\", \"BDLib2\")\n",
    "    path_pic    = os.path.join(current_path, \"BDLib2_results\")\n",
    "    path_models = os.path.join(current_path, \"BDLib2_saved_models\")\n",
    "\n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'BDLib2' \n",
    "    csv_file    = 'BDLib2.csv'\n",
    "    fold        = 'fold-1'\n",
    "\n",
    "    pkl_features          = 'BDLib2_features_original.pkl'\n",
    "    pkl_aug_features      = 'BDLib2_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'BDLib2_features_augmented.pkl'\n",
    "\n",
    "    \n",
    "if opc == 3:\n",
    "    \n",
    "    path        = os.path.join(current_path, \"_dataset\", \"US8K\")\n",
    "    path_pic    = os.path.join(current_path, \"US8K_results\")\n",
    "    path_models = os.path.join(current_path, \"US8K_saved_models\")\n",
    "    \n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'US8K' \n",
    "    csv_file    = 'US8K.csv'\n",
    "    fold        = '1'\n",
    "    \n",
    "    pkl_features          = 'US8K_features_original.pkl'\n",
    "    pkl_aug_features      = 'US8K_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'US8K_features_windowed.pkl' # augmented and windowed makes no sense. Dataset is already quite large\n",
    "    \n",
    "\n",
    "if opc == 4:\n",
    "\n",
    "    path        = os.path.join(current_path, \"_dataset\", \"US8K_AV\")\n",
    "    path_pic    = os.path.join(current_path, \"US8K_AV_results\")\n",
    "    path_models = os.path.join(current_path, \"US8K_AV_saved_models\")\n",
    "\n",
    "    subfolders  = next(os.walk(path))[1]\n",
    "    nom_dataset = 'US8K_AV' \n",
    "    csv_file    = 'US8K_AV.csv'\n",
    "    fold        = '1'\n",
    "\n",
    "    pkl_features          = 'US8K_AV_features_original.pkl'\n",
    "    pkl_aug_features      = 'US8K_AV_features_augmented_no_windowing.pkl'\n",
    "    pkl_aug_wind_features = 'US8K_AV_features_windowed.pkl' # augmented and windowed makes no sense. Dataset is already quite large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a054a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_file_number(folder: str):\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) and f.startswith(pic_first_name)]\n",
    "    if not files:\n",
    "        return 1\n",
    "    else:\n",
    "        numbers = [int(f.split('.')[0].split('_')[-1]) for f in files]\n",
    "        return max(numbers) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MT_loadDataset import loadDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ecfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadDataset = loadDataset(path)\n",
    "DB          = loadDataset.db_B\n",
    "\n",
    "print(\"\\nClasses:\\n--------------------\")\n",
    "print(DB[\"Class_categorical\"].value_counts())\n",
    "print(\"\\nTotal number of unique files..........: \", len(np.unique(DB[\"File_name\"])))\n",
    "print(\"Total number of AUDIO files...........: \", len(DB))\n",
    "DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa52adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the class balancing\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "gTitle = f'{nom_dataset} - Number of classes = ' + str(len(pd.Series(DB['Class_categorical']).unique()))\n",
    "g = sns.displot(DB,x='Class_categorical', hue='Class_categorical',height = 5, aspect = 2).set(title=gTitle)\n",
    "g.set_xticklabels(rotation=90)\n",
    "g.set_titles('Number of classes')\n",
    "\n",
    "# Retrieve the axes object from the plot\n",
    "axes = g.ax\n",
    "\n",
    "# Iterate over each bar in the plot\n",
    "for p in axes.patches:\n",
    "    # Get the coordinates of the bar\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    cord_x, cord_y = p.get_xy()\n",
    "    if height > 0:\n",
    "        axes.annotate(f'{height}', (cord_x + width/2, cord_y + height), ha='center')\n",
    "        \n",
    "g._legend.remove()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82624f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pkl file with the augmented features extracted\n",
    "\n",
    "opc = 0\n",
    "while str(opc) not in '123':\n",
    "    print()\n",
    "    print(\"1-) Features original\")\n",
    "    print(\"2-) Features augmented\")\n",
    "    print(\"3-) Features augmented and windowed (US8K only windowed)\")\n",
    "\n",
    "    opc = input(\"\\nSelect the dataset: \")\n",
    "    if opc.isdigit():\n",
    "        opc = int(opc)\n",
    "    else:\n",
    "        opc = 0\n",
    "\n",
    "if opc == 1:\n",
    "    DB_from_pkl   = pd.read_pickle(os.path.join(path_models, pkl_features))\n",
    "    model_surname = '_original'\n",
    "\n",
    "if opc == 2:\n",
    "    DB_from_pkl   = pd.read_pickle(os.path.join(path_models, pkl_aug_features))\n",
    "    model_surname = '_augmented'\n",
    "\n",
    "if opc == 3:\n",
    "    DB_from_pkl = pd.read_pickle(os.path.join(path_models, pkl_aug_wind_features))\n",
    "    model_surname = '_windowed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_from_pkl.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for audio in DB_from_pkl['Audio']:\n",
    "    total_duration = total_duration + librosa.get_duration(y=audio)\n",
    "print('Total duration of the dataset: ' , \"{:0.4f} h\".format(total_duration / 3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdf122",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_from_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the class and get one random sample of each class\n",
    "k = DB_from_pkl.groupby('Class_categorical')['Class_OHEV'].apply(lambda s: s.sample(1))\n",
    "print(k)\n",
    "\n",
    "# Convert the pandas series into a dataframe\n",
    "temp_k_df = k.reset_index()\n",
    "\n",
    "# Delete the index from the grouppby result\n",
    "del temp_k_df['level_1']\n",
    "\n",
    "# Set the \"Class\" as the dataframe index\n",
    "temp_k_df.set_index(\"Class_categorical\", inplace=True)\n",
    "\n",
    "# Convert the dataframe to a dictionary (Class: Class_encoder)\n",
    "encoder_dict = temp_k_df[\"Class_OHEV\"].to_dict()\n",
    "encoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d05be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_classes = list(encoder_dict.keys())\n",
    "nom_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0504cb8",
   "metadata": {},
   "source": [
    "## ML Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae0b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutual_info (X: list, X_val: list, X_norm: list, y: list):\n",
    "    \n",
    "    mutual_info = mutual_info_classif(X_norm, y)\n",
    "    mutual_info = pd.Series(mutual_info)\n",
    "    \n",
    "    threshold = np.mean(mutual_info.sort_values(ascending=False))\n",
    "    print(f' Threshold: {threshold}')\n",
    "    \n",
    "    count_mutual = sum(1 for element in mutual_info if element > threshold)\n",
    "    print(f' Count mutual: {count_mutual}\\n')\n",
    "    \n",
    "    chi2_selector    = SelectKBest(chi2, k=count_mutual).fit(X_norm, y)\n",
    "    mutual_idx_train = chi2_selector.get_support(1)\n",
    "    \n",
    "    X_mutual     = X[X.columns[mutual_idx_train]] # final features\n",
    "    X_val_mutual = X_val[X_val.columns[mutual_idx_train]] # final features\n",
    "\n",
    "    print(f'X_train_mutual...: {X_mutual.shape}')\n",
    "    print(f'X_val_mutual.....: {X_val_mutual.shape}\\n')\n",
    "\n",
    "    return X_mutual, X_val_mutual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, labels, title, cmap, normalize):\n",
    "\n",
    "    if labels is not None:\n",
    "        tick_marks = np.arange(len(labels))\n",
    "        plt.xticks(tick_marks, labels, fontsize=10, rotation=45)\n",
    "        plt.yticks(tick_marks, labels, fontsize=10)\n",
    "   \n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 8)\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 8)\n",
    "\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title, fontsize=13)\n",
    "    plt.colorbar(shrink=1)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a186ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to run the classifiers and their metrics using k-fold stratification according to the dataset specification\n",
    "\n",
    "def model_classifiers(db: pd.DataFrame, \n",
    "                      scalerOpt: str, \n",
    "                      use_PCA = False,\n",
    "                      use_mutual = False):\n",
    "    \n",
    "    if use_PCA:\n",
    "        p = 2\n",
    "    else:\n",
    "        p = 1\n",
    "    \n",
    "    classifiers = ['Naïve Bayes', 'SVC', 'LogisticR', 'KNN', 'Forest', 'Voting']\n",
    "    \n",
    "    \n",
    "    NB_c          = GaussianNB()\n",
    "    SVC_c         = SVC(kernel = 'linear', C = 0.5, probability = True, random_state = SEED)\n",
    "    LogisticR_c   = LogisticRegression(solver = 'saga', max_iter = 500, C = 0.5, n_jobs = -1)\n",
    "    KNN_c         = KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = p, leaf_size = 20, n_jobs = -1)\n",
    "    forest_c      = RandomForestClassifier(n_estimators = 500, criterion = 'gini',bootstrap = True, n_jobs = -1)\n",
    "    voting_c      = VotingClassifier(estimators = [(classifiers[0], NB_c),\n",
    "                                                   (classifiers[1], SVC_c),\n",
    "                                                   (classifiers[2], LogisticR_c),\n",
    "                                                   (classifiers[3], KNN_c)],\n",
    "                                     voting = 'soft')\n",
    "    \n",
    "    dic_classifiers = {'GaussianNB': NB_c, \n",
    "                       'SVC': SVC_c, \n",
    "                       'LogisticR' : LogisticR_c, \n",
    "                       'KNN': KNN_c, \n",
    "                       'Forest': forest_c, \n",
    "                       'Voting': voting_c}\n",
    "\n",
    "    count       = 1\n",
    "    ratio       = []\n",
    "    models      = []\n",
    "    acc_set     = pd.DataFrame(index=None, columns=['Model',\n",
    "                                                    'Fold',\n",
    "                                                    'Accuracy(Train)',\n",
    "                                                    'Accuracy(Validation)',\n",
    "                                                    'F1(Train)',\n",
    "                                                    'F1(Validation)', \n",
    "                                                    'Precision(Train)',\n",
    "                                                    'Precision(Validation)', \n",
    "                                                    'Recall(Train)',\n",
    "                                                    'Recall(Validation)', \n",
    "                                                    'Conf_M',\n",
    "                                                    'Process_time'])\n",
    "    \n",
    "    for fold in np.unique(db['Fold']):\n",
    "        print(f\"Validation fold: {fold}\")\n",
    "    \n",
    "        DB_VAL = db[db['Fold'] == fold]\n",
    "        DB_TRN = db[db['Fold'] != fold]\n",
    "        \n",
    "        X      = DB_TRN.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "        y      = np.array(DB_TRN.Class_categorical.to_list())\n",
    "        y_OHEV = np.array(DB_TRN.Class_OHEV.to_list())\n",
    "\n",
    "        X_val      = DB_VAL.drop(columns=['Audio','Class_categorical','Class_OHEV', 'Fold'])\n",
    "        y_val      = np.array(DB_VAL.Class_categorical.to_list())\n",
    "        y_OHEV_val = np.array(DB_VAL.Class_OHEV.to_list())\n",
    "\n",
    "        X_statistics = pd.DataFrame({'mean': X.mean(), 'std': X.std(), 'min': X.min(), 'max': X.max()})\n",
    "\n",
    "        X_mean   = X_statistics.values[:, 0]\n",
    "        X_std    = X_statistics.values[:, 1]\n",
    "        X_min    = X_statistics.values[:, 2]\n",
    "        X_max    = X_statistics.values[:, 3]\n",
    "        \n",
    "        if scalerOpt == \"normalization\":\n",
    "            X_train_norm = (X.values - X_min) / (X_max - X_min)\n",
    "            X_val_norm   = (X_val.values - X_min) / (X_max - X_min)\n",
    "            batch_type    = '_norm'\n",
    "            print(f'X_train_norm shape...:{X_train_norm.shape}')\n",
    "            print(f'X_val_norm shape.....:{X_val_norm.shape}\\n')\n",
    "            \n",
    "        elif scalerOpt == \"standardization\":\n",
    "            X_train_norm = (X.values - X_mean) / X_std\n",
    "            X_val_norm   = (X_val.values - X_mean) / X_std\n",
    "            batch_type    = '_std'\n",
    "            print(f'X_train_norm shape...:{X_train_norm.shape}')\n",
    "            print(f'X_val_norm shape.....:{X_val_norm.shape}\\n')\n",
    "            \n",
    "        else:\n",
    "            sys.exit()\n",
    "            \n",
    "        if use_mutual:\n",
    "            if scalerOpt == \"standardization\":\n",
    "                print(f'Mutual information requires normalization!')\n",
    "                sys.exit()\n",
    "            else:\n",
    "                X_train_norm, X_val_norm = get_mutual_info(X, X_val, X_train_norm, y)\n",
    "                classifiers = ['Forest']\n",
    "                dic_classifiers = {'Forest': forest_c}\n",
    "                batch_type    = batch_type + '_mutual'\n",
    "   \n",
    "        if use_PCA:\n",
    "            pcaT = PCA()\n",
    "            pcaT.fit(X_train_norm)\n",
    "            ratio = pcaT.explained_variance_ratio_\n",
    "            \n",
    "            batch_type = batch_type + '_PCA'\n",
    "            \n",
    "            T           = 0.98\n",
    "            current_sum = 0\n",
    "            countComp   = 0\n",
    "\n",
    "            for element in ratio:\n",
    "                current_sum += element\n",
    "                countComp   += 1\n",
    "\n",
    "                if current_sum >= T:\n",
    "                    break\n",
    "\n",
    "            # Print the result\n",
    "            print(f'Sum of elements............: {current_sum}')\n",
    "            print(f'Number of elements summed..: {countComp}\\n')           \n",
    "            \n",
    "            pca          = PCA(n_components = countComp)\n",
    "            X_train_norm = pca.fit_transform(X_train_norm)\n",
    "            X_val_norm   = pca.transform(X_val_norm)\n",
    "            \n",
    "        for c in tqdm(dic_classifiers.items()):\n",
    "            \n",
    "            name         = c[0]\n",
    "            model_name   = ('Model_' + c[0] + '_' + str(count))\n",
    "            count        = count + 1\n",
    "            model        = c[1]\n",
    "            print(name)\n",
    "            print(model)\n",
    "            print(np.shape(X_train_norm))\n",
    "            \n",
    "            # Fitting the model to the classifier\n",
    "            model.fit(X_train_norm, y)\n",
    "\n",
    "            # Get the model predictions\n",
    "            y_train_predicted = model.predict(X_train_norm)\n",
    "            \n",
    "            t_srt             = time.process_time_ns()\n",
    "            y_val_predicited  = model.predict(X_val_norm)\n",
    "            t_end             = time.process_time_ns()\n",
    "            proc_time         = ((t_end - t_srt) / 1000000)\n",
    "\n",
    "            # Compute the classifier metrics\n",
    "            accuracy_train = metrics.accuracy_score(y, y_train_predicted)\n",
    "            accuracy_val   = metrics.accuracy_score(y_val,  y_val_predicited)\n",
    "\n",
    "            f1_Score_train = metrics.f1_score(y, y_train_predicted, average = 'weighted')\n",
    "            f1_Score_val   = metrics.f1_score(y_val,  y_val_predicited, average = 'weighted')\n",
    "\n",
    "            precision_score_train = metrics.precision_score(y, y_train_predicted, average = 'weighted')\n",
    "            precision_score_val   = metrics.precision_score(y_val,  y_val_predicited, average = 'weighted')\n",
    "\n",
    "            recall_score_train = metrics.recall_score(y, y_train_predicted, average = 'weighted')\n",
    "            recall_score_val   = metrics.recall_score(y_val,  y_val_predicited, average = 'weighted')\n",
    "\n",
    "            class_report_val = classification_report(y_val, y_val_predicited)\n",
    "            print(class_report_val)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            CM = metrics.confusion_matrix(y_val, y_val_predicited)\n",
    "\n",
    "            # Store the name, validation accuracy results and model\n",
    "            models.append((name, accuracy_val, model))\n",
    "            \n",
    "            del model\n",
    "                    \n",
    "            acc_set = pd.concat([acc_set, pd.DataFrame({'Model': [name],\n",
    "                                                        'Fold': [fold],\n",
    "                                                        'Accuracy(Train)': [accuracy_train],\n",
    "                                                        'Accuracy(Validation)': [accuracy_val],\n",
    "                                                        'F1(Train)': [f1_Score_train],\n",
    "                                                        'F1(Validation)': [f1_Score_val],\n",
    "                                                        'Precision(Train)': [precision_score_train],\n",
    "                                                        'Precision(Validation)': [precision_score_val],\n",
    "                                                        'Recall(Train)': [recall_score_train],\n",
    "                                                        'Recall(Validation)': [recall_score_val],\n",
    "                                                        'Conf_M': [CM],\n",
    "                                                        'Process_time': [proc_time]})], ignore_index = True)\n",
    "                   \n",
    "    return acc_set, models, ratio, batch_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358bab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option for scalerOpt is either \"normalization\" or \"standardization\"\n",
    "\n",
    "# PCA with mutual is ilogical, results will be useless\n",
    "# Mutual does not work with negative numbers, therefore \"standardization\" returns an error\n",
    "# Mutual will trigger only Random Forest\n",
    "\n",
    "metrics_set, models_set,  ratio, batch_name = model_classifiers(DB_from_pkl,\n",
    "                                                                scalerOpt = 'standardization',\n",
    "                                                                use_PCA = False,\n",
    "                                                                use_mutual = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73169e4d",
   "metadata": {},
   "source": [
    "## Metrics for the classifiers\n",
    "\n",
    "\n",
    "1. Accuracy: Accuracy is a measure of how many correct predictions a model makes overall, i.e., the ratio of correct predictions to the total number of predictions. It's a commonly used metric for evaluating models, but it may not be suitable in certain situations.\n",
    "\n",
    "2. Precision: Precision measures the ratio of true positives (correctly predicted positive instances) to all instances predicted as positive. It focuses on the accuracy of positive predictions.\n",
    "\n",
    "3. Recall: Recall, also known as sensitivity or true positive rate, measures the ratio of true positives to all actual positive instances. It focuses on how well a model captures all the positive instances.\n",
    "\n",
    "4. F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure that takes into account both false positives and false negatives. The F1 score is especially useful when you want to strike a balance between precision and recall.\n",
    "\n",
    "\n",
    "The F1 score is a metric that combines precision and recall, and it is particularly useful in situations where class imbalance or unequal misclassification costs are present. In such contexts, the F1 score can be more informative and meaningful than accuracy.\n",
    "\n",
    "A context where considering the F1 score makes more sense than accuracy:\n",
    "\n",
    "**Medical Diagnosis:**\n",
    "\n",
    "Imagine you're developing a model to diagnose a rare disease, and only 5% of the population has this disease. In this case, you have a significant class imbalance, where the majority of cases are negative (non-disease) and only a small fraction are positive (disease). If you were to use accuracy as the evaluation metric, the model could achieve a high accuracy by simply predicting \"negative\" for every case, because it would be correct 95% of the time due to the class imbalance. However, this would be entirely useless for detecting the actual disease.\n",
    "\n",
    "In this scenario, you'd be more interested in the F1 score. The F1 score considers both precision and recall, helping you find a balance between correctly identifying the disease (high recall) and not making too many false positive predictions (high precision). A high F1 score in this context indicates that your model is effective at correctly identifying the disease while minimizing false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Model and Accuracy test. Reset the index.\n",
    "\n",
    "metrics_set = metrics_set.sort_values(['Model', 'Accuracy(Validation)'], ascending = [True, True]).reset_index()\n",
    "metrics_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af9afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_set[['Model', 'Accuracy(Validation)']].style.background_gradient(cmap = cmap_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy = metrics_set.groupby('Model')['Accuracy(Validation)'].max()\n",
    "highest_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f740a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classifiers = metrics_set['Model'].unique().tolist()\n",
    "list_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40226fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary of each classifier and its data explanation\n",
    "\n",
    "unique_models = []\n",
    "results       = {}\n",
    "\n",
    "for c in list_classifiers:\n",
    "    unique_models.append(c)\n",
    "\n",
    "for model in unique_models:\n",
    "    result = metrics_set[metrics_set['Model'] == model].describe().round(4)\n",
    "    results[model] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in results.keys():\n",
    "    print(f'Model...: {model}')\n",
    "    display(results[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_set_no_cm = metrics_set.drop('Conf_M', axis=1)\n",
    "metrics_set_no_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be91243",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_set_name       = nom_dataset + '_metrics_set' + batch_name +  model_surname + '.csv'\n",
    "metrics_set_name_no_cm = nom_dataset + '_metrics_set' + batch_name +  model_surname + '_no_cm.csv'\n",
    "\n",
    "print(metrics_set_name)\n",
    "print(metrics_set_name_no_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes de results to a CSV file\n",
    "\n",
    "metrics_set.to_csv(os.path.join(path_models, metrics_set_name), sep='\\t', encoding='utf-8')\n",
    "metrics_set_no_cm.to_csv(os.path.join(path_models, metrics_set_name_no_cm), sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = metrics_set.groupby('Model')['Accuracy(Validation)'].idxmax()\n",
    "conf_matrices = metrics_set.loc[idx, ['Model','Accuracy(Validation)','Conf_M']]\n",
    "conf_matrices.set_index('Model', inplace=True)\n",
    "conf_matrices_dict = conf_matrices.to_dict('index')\n",
    "conf_matrices_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrices_dict['LogisticR']['Conf_M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idx in zip(conf_matrices_dict.keys(), range(1, len(conf_matrices_dict) + 1)):\n",
    "    print(idx)\n",
    "    print(i)\n",
    "    print(conf_matrices_dict[i]['Accuracy(Validation)'])\n",
    "    print(conf_matrices_dict[i]['Conf_M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e53550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix for the highest accuracy test classifiers\n",
    "\n",
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.suptitle('Confusion matrices of the best results for each classifier', fontsize = 16,  y=0.97)\n",
    "for i, idx in zip(conf_matrices_dict.keys(), range(1, len(conf_matrices_dict) + 1)):\n",
    "    title = nom_dataset + model_surname + batch_name + ' - Classifier '+ i + ' (Highest accuracy (Validation): ' + str(\"{:0.4f}\".format(conf_matrices_dict[i]['Accuracy(Validation)'])) +')'\n",
    "    plt.subplot(3,2,idx)\n",
    "    plot_confusion_matrix(conf_matrices_dict[i]['Conf_M'],  \n",
    "                          nom_classes, \n",
    "                          title,\n",
    "                          cmap = None,                          \n",
    "                          normalize = False)\n",
    "plt.tight_layout(pad=4.0)\n",
    "plt.savefig(os.path.join(path_pic, picture_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742e05f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "picture_name = f'{pic_first_name}{get_next_file_number(path_pic):02d}.png'\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.suptitle(f'{nom_dataset} - Box plot each classifier (batch type: {model_surname + batch_name})', fontsize = 16,  y=0.97)\n",
    "box_plot = sns.boxplot(data=metrics_set, x=\"Model\", y=\"Accuracy(Validation)\", showfliers = True)\n",
    "\n",
    "medians = list(metrics_set.groupby(['Model'])['Accuracy(Validation)'].median())\n",
    "medians = [round(element, 2) for element in medians]\n",
    "\n",
    "vertical_offset = metrics_set['Accuracy(Validation)'].median()*0.001  # offset from median for display\n",
    "\n",
    "for xtick in box_plot.get_xticks():\n",
    "    box_plot.text(xtick, medians[xtick] + vertical_offset, medians[xtick], \n",
    "            horizontalalignment='center',size='medium',color='w',weight='semibold')\n",
    "plt.savefig(os.path.join(path_pic, picture_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a789e38",
   "metadata": {},
   "source": [
    "## Results ESC-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2692c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6b58377",
   "metadata": {},
   "source": [
    "## Results BDLib2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf61ea6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a889f3",
   "metadata": {},
   "source": [
    "## Results US8K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1aa32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "688d9c4e",
   "metadata": {},
   "source": [
    "## Results US8K_AV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c625c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dff244f0",
   "metadata": {},
   "source": [
    "# End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23c64a-cc72-4172-a4a7-aa5c28d7e5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
